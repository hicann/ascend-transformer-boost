---
language:
- zh
- en
tags:
- codegeex
- glm
- chatglm
- thudm
---

![](resources/codegeex_logo.png)

<p align="center">
    ğŸ  <a href="https://codegeex.cn" target="_blank">Homepage</a>ï½œğŸ’» <a href="https://github.com/THUDM/CodeGeeX2" target="_blank">GitHub</a>ï½œğŸ›  Tools <a href="https://marketplace.visualstudio.com/items?itemName=aminer.codegeex" target="_blank">VS Code</a>, <a href="https://plugins.jetbrains.com/plugin/20587-codegeex" target="_blank">Jetbrains</a>ï½œğŸ¤— <a href="https://huggingface.co/THUDM/codegeex2-6b" target="_blank">HF Repo</a>ï½œğŸ“„ <a href="https://arxiv.org/abs/2303.17568" target="_blank">Paper</a>
</p>

<p align="center">
    ğŸ‘‹ Join our <a href="https://discord.gg/8gjHdkmAN6" target="_blank">Discord</a>, <a href="https://join.slack.com/t/codegeexworkspace/shared_invite/zt-1s118ffrp-mpKKhQD0tKBmzNZVCyEZLw" target="_blank">Slack</a>, <a href="https://t.me/+IipIayJ32B1jOTg1" target="_blank">Telegram</a>, <a href="https://github.com/THUDM/CodeGeeX2/blob/main/resources/wechat.md"target="_blank">WeChat</a>
</p>

INT4é‡åŒ–ç‰ˆæœ¬ï½œINT4 quantized version [codegeex2-6b-int4](https://huggingface.co/THUDM/codegeex2-6b-int4)

# CodeGeeX2: æ›´å¼ºå¤§çš„å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹
# A More Powerful Multilingual Code Generation Model

CodeGeeX2 æ˜¯å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹ [CodeGeeX](https://github.com/THUDM/CodeGeeX) ([KDDâ€™23](https://arxiv.org/abs/2303.17568)) çš„ç¬¬äºŒä»£æ¨¡å‹ã€‚CodeGeeX2 åŸºäº [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B) æ¶æ„åŠ å…¥ä»£ç é¢„è®­ç»ƒå®ç°ï¼Œå¾—ç›Šäº ChatGLM2 çš„æ›´ä¼˜æ€§èƒ½ï¼ŒCodeGeeX2 åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå–å¾—æ€§èƒ½æå‡ï¼ˆ+107% > CodeGeeXï¼›ä»…60äº¿å‚æ•°å³è¶…è¿‡150äº¿å‚æ•°çš„ StarCoder-15B è¿‘10%ï¼‰ï¼Œæ›´å¤šç‰¹æ€§åŒ…æ‹¬ï¼š

* **æ›´å¼ºå¤§çš„ä»£ç èƒ½åŠ›**ï¼šåŸºäº ChatGLM2-6B åŸºåº§è¯­è¨€æ¨¡å‹ï¼ŒCodeGeeX2-6B è¿›ä¸€æ­¥ç»è¿‡äº† 600B ä»£ç æ•°æ®é¢„è®­ç»ƒï¼Œç›¸æ¯”ä¸€ä»£æ¨¡å‹ï¼Œåœ¨ä»£ç èƒ½åŠ›ä¸Šå…¨é¢æå‡ï¼Œ[HumanEval-X](https://huggingface.co/datasets/THUDM/humaneval-x) è¯„æµ‹é›†çš„å…­ç§ç¼–ç¨‹è¯­è¨€å‡å¤§å¹…æå‡ (Python +57%, C++ +71%, Java +54%, JavaScript +83%, Go +56%, Rust +321\%)ï¼Œåœ¨Pythonä¸Šè¾¾åˆ° 35.9\% çš„ Pass@1 ä¸€æ¬¡é€šè¿‡ç‡ï¼Œè¶…è¶Šè§„æ¨¡æ›´å¤§çš„ StarCoder-15Bã€‚
* **æ›´ä¼˜ç§€çš„æ¨¡å‹ç‰¹æ€§**ï¼šç»§æ‰¿ ChatGLM2-6B æ¨¡å‹ç‰¹æ€§ï¼ŒCodeGeeX2-6B æ›´å¥½æ”¯æŒä¸­è‹±æ–‡è¾“å…¥ï¼Œæ”¯æŒæœ€å¤§ 8192 åºåˆ—é•¿åº¦ï¼Œæ¨ç†é€Ÿåº¦è¾ƒä¸€ä»£ CodeGeeX-13B å¤§å¹…æå‡ï¼Œé‡åŒ–åä»…éœ€6GBæ˜¾å­˜å³å¯è¿è¡Œï¼Œæ”¯æŒè½»é‡çº§æœ¬åœ°åŒ–éƒ¨ç½²ã€‚
* **æ›´å…¨é¢çš„AIç¼–ç¨‹åŠ©æ‰‹**ï¼šCodeGeeXæ’ä»¶ï¼ˆ[VS Code](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex), [Jetbrains](https://plugins.jetbrains.com/plugin/20587-codegeex)ï¼‰åç«¯å‡çº§ï¼Œæ”¯æŒè¶…è¿‡100ç§ç¼–ç¨‹è¯­è¨€ï¼Œæ–°å¢ä¸Šä¸‹æ–‡è¡¥å…¨ã€è·¨æ–‡ä»¶è¡¥å…¨ç­‰å®ç”¨åŠŸèƒ½ã€‚ç»“åˆ Ask CodeGeeX äº¤äº’å¼AIç¼–ç¨‹åŠ©æ‰‹ï¼Œæ”¯æŒä¸­è‹±æ–‡å¯¹è¯è§£å†³å„ç§ç¼–ç¨‹é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸”ä¸é™äºä»£ç è§£é‡Šã€ä»£ç ç¿»è¯‘ã€ä»£ç çº é”™ã€æ–‡æ¡£ç”Ÿæˆç­‰ï¼Œå¸®åŠ©ç¨‹åºå‘˜æ›´é«˜æ•ˆå¼€å‘ã€‚
* **æ›´å¼€æ”¾çš„åè®®**ï¼šCodeGeeX2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¡«å†™[ç™»è®°è¡¨](https://open.bigmodel.cn/mla/form?mcode=CodeGeeX2-6B)ç”³è¯·å•†ä¸šä½¿ç”¨ã€‚


CodeGeeX2 is the second-generation model of the multilingual code generation model [CodeGeeX](https://github.com/THUDM/CodeGeeX) ([KDDâ€™23](https://arxiv.org/abs/2303.17568)), which is implemented based on the [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B) architecture trained on more code data. Due to the advantage of ChatGLM2, CodeGeeX2 has been comprehensively improved in coding capability (+107% > CodeGeeX; with only 6B parameters, surpassing larger StarCoder-15B for some tasks). It has the following features:

* **More Powerful Coding Capabilities**: Based on the ChatGLM2-6B model, CodeGeeX2-6B has been further pre-trained on 600B code tokens, which has been comprehensively improved in coding capability compared to the first-generation. On the [HumanEval-X](https://huggingface.co/datasets/THUDM/humaneval-x) benchmark, all six languages have been significantly improved (Python +57%, C++ +71%, Java +54%, JavaScript +83%, Go +56%, Rust +321\%), and in Python it reached 35.9% of Pass@1 one-time pass rate, surpassing the larger StarCoder-15B.
* **More Useful Features**: Inheriting the ChatGLM2-6B model features, CodeGeeX2-6B better supports both Chinese and English prompts, maximum 8192 sequence length, and the inference speed is significantly improved compared to the first-generation. After quantization, it only needs 6GB of GPU memory for inference, thus supports lightweight local deployment.
* **Comprehensive AI Coding Assistant**: The backend of CodeGeeX plugin ([VS Code](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex), [Jetbrains](https://plugins.jetbrains.com/plugin/20587-codegeex)) is upgraded, supporting 100+ programming languages, and adding practical functions such as infilling and cross-file completion. Combined with the "Ask CodeGeeX" interactive AI coding assistant, it can be used to solve various programming problems via Chinese or English dialogue, including but not limited to code summarization, code translation, debugging, and comment generation, which helps increasing the efficiency of developpers.
* **Open Liscense**: CodeGeeX2-6B weights are fully open to academic research, and please apply for commercial use by filling in the [registration form](https://open.bigmodel.cn/mla/form?mcode=CodeGeeX2-6B).


## è½¯ä»¶ä¾èµ– ï½œ Dependency

```shell
pip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate
```

## å¿«é€Ÿå¼€å§‹ ï½œ Get Started

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/codegeex2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/codegeex2-6b", trust_remote_code=True, device='cuda')
model = model.eval()

# remember adding a language tag for better performance
prompt = "# language: Python\n# write a bubble sort function\n"
inputs = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(inputs, max_length=256, top_k=1)
response = tokenizer.decode(outputs[0])

>>> print(response)
# language: Python
# write a bubble sort function


def bubble_sort(list):
    for i in range(len(list) - 1):
        for j in range(len(list) - 1):
            if list[j] > list[j + 1]:
                list[j], list[j + 1] = list[j + 1], list[j]
    return list


print(bubble_sort([5, 2, 1, 8, 4]))
```

å…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒ CodeGeeX2 çš„ [Github Repo](https://github.com/THUDM/CodeGeeX2)ã€‚

For more information, please refer to CodeGeeX2's [Github Repo](https://github.com/THUDM/CodeGeeX2).

## åè®® ï½œ License

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) åè®®å¼€æºï¼Œæ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚

The code in this repository is open source under the [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) license. The model weights are licensed under the [Model License](MODEL_LICENSE).

## å¼•ç”¨ ï½œ Citation

å¦‚æœè§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š

If you find our work helpful, please feel free to cite the following paper:

```
@inproceedings{zheng2023codegeex,
      title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X}, 
      author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},
      booktitle={KDD},
      year={2023}
}
```
