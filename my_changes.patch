diff --git a/CMakeLists.txt b/CMakeLists.txt
index f770624c..8c066102 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -50,7 +50,7 @@ set(CMAKE_VISIBILITY_INLINES_HIDDEN 1)
 set(CMAKE_SKIP_RPATH TRUE)
 set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -fexceptions")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -pipe -fstack-protector-all")
-set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wall -Wextra -Werror")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wall -Wextra -Werror -Wno-dev")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-deprecated-copy")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fPIC -Wl,--build-id=none")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fexceptions")
@@ -119,12 +119,11 @@ endif()
 set(CMAKE_INSTALL_PREFIX "${CMAKE_SOURCE_DIR}/output/atb/cxx_abi_${cxx_abi}")
 
 message(STATUS "CMAKE_INSTALL_PREFIX:${CMAKE_INSTALL_PREFIX}")
+
 install(FILES ${PROJECT_SOURCE_DIR}/scripts/set_env.sh DESTINATION ./..)
 install(DIRECTORY ${PROJECT_SOURCE_DIR}/ops_configs DESTINATION ./configs)
 install(FILES ${PROJECT_SOURCE_DIR}/3rdparty/mki/lib/libmki.so DESTINATION lib)
 install(FILES ${PROJECT_SOURCE_DIR}/3rdparty/asdops/lib/liblcal.so DESTINATION lib)
 install(FILES ${PROJECT_SOURCE_DIR}/3rdparty/asdops/lib/liblcal_static.a DESTINATION lib)
 install(FILES ${PROJECT_SOURCE_DIR}/3rdparty/asdops/lib/libasdops_aicpu_kernels.so DESTINATION lib OPTIONAL)
-install(FILES ${PROJECT_SOURCE_DIR}/3rdparty/asdops/lib/libtbe_adapter.so DESTINATION lib OPTIONAL)
-install(FILES ${PROJECT_SOURCE_DIR}/3rdparty/asdops/lib/libcann_ops_adapter.so DESTINATION lib OPTIONAL)
 install(DIRECTORY ${PROJECT_SOURCE_DIR}/include/atb DESTINATION include)
diff --git a/configs/ops/tbe_tactic_info.ini b/configs/ops/tbe_tactic_info.ini
index de54195d..d14e3fe2 100644
--- a/configs/ops/tbe_tactic_info.ini
+++ b/configs/ops/tbe_tactic_info.ini
@@ -913,188 +913,6 @@ formatOut=FRACTAL_NZ
 attrs=True,True
 mode=high_performance
 
-[PpMatMulF16NDF16NDF16NDKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=float16,float16,float32,float32
-dtypeOut=float16
-formatIn=ND,ND,ND,ND
-formatOut=ND
-attrs=False,False,False,0
-mode=high_performance
-socSupport=ascend910b
- 
-[PpMatMulBF16NDBF16NDBF16NDKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=bfloat16,bfloat16,float32,float32
-dtypeOut=bfloat16
-formatIn=ND,ND,ND,ND
-formatOut=ND
-attrs=False,False,False,0
-mode=high_performance
-socSupport=ascend910b
- 
-[PpMatMulF16NDF16NDF32NDKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=float16,float16,float32,float32
-dtypeOut=float32
-formatIn=ND,ND,ND,ND
-formatOut=ND
-attrs=False,False,False,0
-mode=high_performance
-socSupport=ascend910b
- 
-[PpMatMulBF16NDBF16NDF32NDKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=bfloat16,bfloat16,float32,float32
-dtypeOut=float32
-formatIn=ND,ND,ND,ND
-formatOut=ND
-attrs=False,False,False,0
-mode=high_performance
-socSupport=ascend910b
- 
-[PpMatMulF16NDF16NZF16NDKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=float16,float16,float32,float32
-dtypeOut=float16
-formatIn=ND,FRACTAL_NZ,ND,ND
-formatOut=ND
-attrs=False,False,False,3
-mode=high_performance
-socSupport=ascend910b
- 
-[PpMatMulBF16NDBF16NZBF16NDKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=bfloat16,bfloat16,float32,float32
-dtypeOut=bfloat16
-formatIn=ND,FRACTAL_NZ,ND,ND
-formatOut=ND
-attrs=False,False,False,3
-mode=high_performance
-socSupport=ascend910b
- 
-[PpMatMulF16NZF16NZF16NZKernel]
-ops=pp_mat_mul
-operationName=MatMulOperation
-inputCount=4
-outputCount=1
-dtypeIn=float16,float16,float32,float32
-dtypeOut=float16
-formatIn=FRACTAL_NZ,FRACTAL_NZ,ND,ND
-formatOut=FRACTAL_NZ
-attrs=False,False,False,0
-mode=high_performance
-socSupport=ascend310p
-
-[PpMatmulW8A8Kernel]
-ops=pp_matmul_w8a8
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,uint64,float32
-dtypeOut=float16
-formatIn=ND,ND,ND,ND,ND
-formatOut=ND
-attrs=False,False
-mode=high_performance
-socSupport=ascend910b
-
-[PpMatmulW8A8WeightNzKernel]
-ops=pp_matmul_w8a8
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,uint64,float32
-dtypeOut=float16
-formatIn=ND,FRACTAL_NZ,ND,ND,ND
-formatOut=ND
-attrs=False,False
-mode=high_performance
-socSupport=ascend910b
-
-[PpMatmulW8A8Bf16NDNDKernel]
-ops=pp_matmul_w8a8
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,float32,float32
-dtypeOut=bfloat16
-formatIn=ND,ND,ND,ND,ND
-formatOut=ND
-attrs=False,False
-mode=high_performance
-socSupport=ascend910b
-
-[PpMatmulW8A8Bf16NDNZKernel]
-ops=pp_matmul_w8a8
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,float32,float32
-dtypeOut=bfloat16
-formatIn=ND,FRACTAL_NZ,ND,ND,ND
-formatOut=ND
-attrs=False,False
-mode=high_performance
-socSupport=ascend910b
-
-[PpMatmulW8A8PertokenFP16Kernel]
-ops=pp_matmul_w8a8
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,float32,float32
-dtypeOut=float16
-formatIn=ND,ND,ND,ND,ND
-formatOut=ND
-attrs=False,False
-mode=high_performance
-socSupport=ascend910b
-
-[PpMatmulW8A8NzKernel]
-ops=pp_matmul_w8a8
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,uint64,float32
-dtypeOut=float16
-formatIn=FRACTAL_NZ,FRACTAL_NZ,ND,ND,ND
-formatOut=FRACTAL_NZ
-attrs=False,True
-mode=high_performance
-socSupport=ascend310p
-
-[PpMatmulW8A8NzCompressKernel]
-ops=pp_matmul_w8a8_compress
-operationName=MatMulOperation
-inputCount=5
-outputCount=1
-dtypeIn=int8,int8,int32,uint64,int8
-dtypeOut=float16
-formatIn=FRACTAL_NZ,FRACTAL_NZ,ND,ND,ND
-formatOut=FRACTAL_NZ
-attrs=False,True,0,0
-mode=high_performance
-socSupport=ascend310p
-
 [LayernormF16Kernel]
 ops=layer_norm_v3
 operationName=NormOperation
@@ -1421,26 +1239,6 @@ dtypeOut=bfloat16
 attrs=-1.0,1.0,0.0
 socSupport=ascend910b
 
-[QuantPerChannelKernel]
-ops=quant_per_channel
-operationName=ElewiseOperation
-inputCount=3
-outputCount=1
-dtypeIn=float16,float16,int8
-dtypeOut=int8
-mode=high_performance
-socSupport=ascend910b,ascend310p
-
-[DequantPerChannelKernel]
-ops=dequant_per_channel
-operationName=ElewiseOperation
-inputCount=3
-outputCount=1
-dtypeIn=int8,float16,int8
-dtypeOut=float16
-mode=high_performance
-socSupport=ascend910b,ascend310p
-
 [ReduceMaxInt32Kernel]
 ops=reduce_max
 operationName=ReduceOperation
diff --git a/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.json b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.json
new file mode 100644
index 00000000..9c344dcf
--- /dev/null
+++ b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.json
@@ -0,0 +1,148 @@
+{
+    "binFileName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance",
+    "binFileSuffix": ".o",
+    "blockDim": -1,
+    "compileInfo": {
+        "global_variable_link": true,
+        "vars": {
+            "cache_line": 512,
+            "core_num": 48,
+            "dtype": "int8",
+            "mte_pad": 1,
+            "ub_size": 6144,
+            "vg_cond": 0
+        }
+    },
+    "coreType": "VectorCore",
+    "deterministic": "ignore",
+    "intercoreSync": 0,
+    "kernelList": [
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_0"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_1"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_2"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_3"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_4"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_5"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_6"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_7"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_8"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_9"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_10"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_11"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_12"
+        },
+        {
+            "deterministic": "ignore",
+            "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_13"
+        }
+    ],
+    "kernelName": "Transpose_6e512c21e258210f21d4172b6f11379e_high_performance",
+    "magic": "RT_DEV_BINARY_MAGIC_ELF_AIVEC",
+    "memoryStamping": [
+        -1
+    ],
+    "opParaSize": 16392,
+    "parameters": [
+        null,
+        null,
+        null,
+        null,
+        null
+    ],
+    "sha256": "453bfea1d2456e8657a26543d7e66f4cb2ddaa4974a1fdee7b1fa3bf029ed7ef",
+    "workspace": {
+        "num": 1,
+        "size": [
+            1024
+        ],
+        "type": [
+            0
+        ]
+    },
+    "supportInfo": {
+        "int64Mode": false,
+        "simplifiedKeyMode": 0,
+        "simplifiedKey": [
+            "Transpose/d=0,p=0/2,2/9,2/2,2",
+            "Transpose/d=1,p=0/2,2/9,2/2,2"
+        ],
+        "staticKey": "5c4ff6f77201e4ce0f4ff10f614dfdda73f1964b8855fe59dc5f3215e4aed7b7",
+        "inputs": [
+            {
+                "name": "x",
+                "index": 0,
+                "dtype": "int8",
+                "format": "ND",
+                "paramType": "required",
+                "shape": [
+                    -2
+                ],
+                "dtype_match_mode": "DtypeByte"
+            },
+            {
+                "name": "perm",
+                "index": 1,
+                "dtype": "int64",
+                "format": "ND",
+                "paramType": "required",
+                "shape": [
+                    -2
+                ]
+            }
+        ],
+        "outputs": [
+            {
+                "name": "y",
+                "index": 0,
+                "dtype": "int8",
+                "format": "ND",
+                "paramType": "required",
+                "shape": [
+                    -2
+                ],
+                "dtype_match_mode": "DtypeByte"
+            }
+        ],
+        "opMode": "dynamic",
+        "deterministic": "ignore"
+    },
+    "filePath": "ascend910b/transpose/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.json"
+}
\ No newline at end of file
diff --git a/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.o b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.o
new file mode 100644
index 00000000..c9c031aa
Binary files /dev/null and b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance.o differ
diff --git a/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_11_host.o b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_11_host.o
new file mode 100644
index 00000000..c9c031aa
Binary files /dev/null and b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_11_host.o differ
diff --git a/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_5_host.o b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_5_host.o
new file mode 100644
index 00000000..c9c031aa
Binary files /dev/null and b/extra-info/data-dump/0/Transpose_6e512c21e258210f21d4172b6f11379e_high_performance_5_host.o differ
diff --git a/extra-info/data-dump/0/exception_info.39.3.20250901095150161 b/extra-info/data-dump/0/exception_info.39.3.20250901095150161
new file mode 100644
index 00000000..906143d5
Binary files /dev/null and b/extra-info/data-dump/0/exception_info.39.3.20250901095150161 differ
diff --git a/extra-info/data-dump/0/exception_info.5.0.20250901093823555 b/extra-info/data-dump/0/exception_info.5.0.20250901093823555
new file mode 100644
index 00000000..02c95936
Binary files /dev/null and b/extra-info/data-dump/0/exception_info.5.0.20250901093823555 differ
diff --git a/src/kernels/configs/build_config.json b/src/kernels/configs/build_config.json
deleted file mode 100644
index 8f8b66dc..00000000
--- a/src/kernels/configs/build_config.json
+++ /dev/null
@@ -1,8 +0,0 @@
-{
-    "targets": {
-        "ascend310b": true,
-        "ascend310p": true,
-        "ascend910b": true,
-        "ascend910": true
-    }
-}
diff --git a/src/kernels/kernels/elewise/CMakeLists.txt b/src/kernels/kernels/elewise/CMakeLists.txt
index fc0c47c2..eac4b361 100644
--- a/src/kernels/kernels/elewise/CMakeLists.txt
+++ b/src/kernels/kernels/elewise/CMakeLists.txt
@@ -35,6 +35,18 @@ add_kernel(dynamic_quant ascend310p vector
     dynamic_quant/op_kernel/dynamic_quant.cpp
     DynamicQuantKernel)
 
+add_kernel(quant_per_channel ascend910b vector
+    simple_broadcast/kernel/quant_per_channel.cpp
+    QuantPerChannelKernel)
+
+add_kernel(dequant_per_channel ascend910b vector
+    simple_broadcast/kernel/dequant_per_channel.cpp
+    DequantPerChannelKernel)
+
+add_kernel(quant_per_channel ascend310p vector
+    simple_broadcast/kernel/quant_per_channel.cpp
+    QuantPerChannelKernel)
+
 add_kernel(quant_per_channel ascend910 vector
     simple_broadcast/kernel/quant_per_channel.cpp
     QuantPerChannelKernel)
diff --git a/src/kernels/kernels/elewise/elewise_operation.cpp b/src/kernels/kernels/elewise/elewise_operation.cpp
index 0ce35e20..eafdd988 100644
--- a/src/kernels/kernels/elewise/elewise_operation.cpp
+++ b/src/kernels/kernels/elewise/elewise_operation.cpp
@@ -19,7 +19,6 @@
 #include <mki_loader/op_register.h>
 
 #include "asdops/params/elewise.h"
-#include "sink_common.h"
 
 namespace AsdOps {
 using namespace Mki;
@@ -447,32 +446,14 @@ protected:
                 return status;
             }
             case OpParam::Elewise::ELEWISE_QUANT_PER_CHANNEL: {
-                MKI_LOG(INFO) << "ELEWISE_QUANT_PER_CHANNEL enter";
-
-                Mki::PlatformType platform = Mki::PlatformInfo::Instance().GetPlatformType();
-                if (platform == Mki::PlatformType::ASCEND_910A || platform == Mki::PlatformType::ASCEND_310B) {
-                    Status status = SimplyBroadcastInferShape(launchParam, outTensors);
-                    outTensors[0].desc.dtype = TENSOR_DTYPE_INT8;
-                    return status;
-                }
-
-                for (auto &t: outTensors) {
-                    Mki::TensorDesc desc;
-                    desc.format = Mki::TENSOR_FORMAT_ND;
-                    t.desc = desc;
-                }
-                return opInferShape::CallGeInferShape("QuantPerChannel", launchParam, outTensors,
-                                                      AsdOps::GetMkiSpecificAttr<OpParam::Elewise>);
+                Status status = SimplyBroadcastInferShape(launchParam, outTensors);
+                outTensors[0].desc.dtype = TENSOR_DTYPE_INT8;
+                return status;
             }
             case OpParam::Elewise::ELEWISE_DEQUANT_PER_CHANNEL: {
-                MKI_LOG(INFO) << "ELEWISE_DEQUANT_PER_CHANNEL enter";
-                for (auto &t: outTensors) {
-                    Mki::TensorDesc desc;
-                    desc.format = Mki::TENSOR_FORMAT_ND;
-                    t.desc = desc;
-                }
-                return opInferShape::CallGeInferShape("DequantPerChannel", launchParam, outTensors,
-                                                      AsdOps::GetMkiSpecificAttr<OpParam::Elewise>);
+                Status status = SimplyBroadcastInferShape(launchParam, outTensors);
+                outTensors[0].desc.dtype = TENSOR_DTYPE_FLOAT16;
+                return status;
             }
             default:
                 return Status::FailStatus(ERROR_INVALID_VALUE, "no matched elewiseType");
diff --git a/src/kernels/kernels/elewise/simple_broadcast/simple_broadcast_kernel.cpp b/src/kernels/kernels/elewise/simple_broadcast/simple_broadcast_kernel.cpp
index 1aefe0a7..5b63faa7 100644
--- a/src/kernels/kernels/elewise/simple_broadcast/simple_broadcast_kernel.cpp
+++ b/src/kernels/kernels/elewise/simple_broadcast/simple_broadcast_kernel.cpp
@@ -14,8 +14,6 @@
 #include <mki/utils/log/log.h>
 #include <mki/utils/platform/platform_info.h>
 #include "kernels/elewise/simple_broadcast/tiling/simple_broadcast_tiling.h"
-#include "asdops/params/params.h"
-#include "sink_common.h"
 
 namespace AsdOps {
 using namespace Mki;
@@ -77,14 +75,9 @@ public:
 
     Status InitImpl(const LaunchParam &launchParam) override
     {
-        Mki::PlatformType platform = Mki::PlatformInfo::Instance().GetPlatformType();
-        if (platform == Mki::PlatformType::ASCEND_910A || platform == Mki::PlatformType::ASCEND_310B) {
-            BroadcastInfo broadcastInfo;
-            FillBroadCastInfoImpl(launchParam, broadcastInfo);
-            return QuantPerChannelTiling(broadcastInfo, launchParam, kernelInfo_);
-        }
-        return optiling::CallGeTiling("QuantPerChannel", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::Elewise>, kernelInfo_);
+        BroadcastInfo broadcastInfo;
+        FillBroadCastInfoImpl(launchParam, broadcastInfo);
+        return QuantPerChannelTiling(broadcastInfo, launchParam, kernelInfo_);
     }
 
 protected:
@@ -137,8 +130,9 @@ public:
 
     Status InitImpl(const LaunchParam &launchParam) override
     {
-        return optiling::CallGeTiling("DequantPerChannel", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::Elewise>, kernelInfo_);
+        BroadcastInfo broadcastInfo;
+        FillBroadCastInfoImpl(launchParam, broadcastInfo);
+        return DequantPerChannelTiling(broadcastInfo, launchParam, kernelInfo_);
     }
 
 protected:
diff --git a/src/kernels/kernels/matmul/CMakeLists.txt b/src/kernels/kernels/matmul/CMakeLists.txt
index 5ba5ad65..85449828 100644
--- a/src/kernels/kernels/matmul/CMakeLists.txt
+++ b/src/kernels/kernels/matmul/CMakeLists.txt
@@ -4,12 +4,19 @@ set(matmul_srcs
     ${CMAKE_CURRENT_LIST_DIR}/batch_matmul_kernel/batch_matmul_nz_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/matmul_kernel/matmul_nd_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/matmul_kernel/matmul_nz_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_bf16_kernel/pp_matmul_bf16_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_bf16_nd_nz_nd_kernel/pp_matmul_bf16_nd_nz_nd_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_f16_mix_kernel/pp_matmul_f16_mix_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_f16_kernel/pp_matmul_f16_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_f16_opt_kernel/pp_matmul_f16_opt_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_i8_kernel/pp_matmul_i8_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_f16_m300_kernel/pp_matmul_f16_m300_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_i8_nz_kernel/pp_matmul_i8_nz_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_i8_nz_compress_kernel/pp_matmul_i8_nz_compress_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_kernel/pp_matmul_nz_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_accum_kernel/pp_matmul_accum_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_with_bias_kernel/pp_matmul_with_bias_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/pp_matmul_ein_sum_kernel/pp_matmul_ein_sum_kernel.cpp
     ${CMAKE_CURRENT_LIST_DIR}/tiling/matmul_nd_tiling.cpp
     ${CMAKE_CURRENT_LIST_DIR}/tiling/matmul_nz_tiling.cpp
     ${CMAKE_CURRENT_LIST_DIR}/tiling/pp_matmul_mix_tiling.cpp
@@ -17,13 +24,6 @@ set(matmul_srcs
     ${CMAKE_CURRENT_LIST_DIR}/tiling/pp_matmul_i8_nz_tiling.cpp
     ${CMAKE_CURRENT_LIST_DIR}/tiling/pp_matmul_nz_tiling.cpp
     ${CMAKE_CURRENT_LIST_DIR}/tiling/pp_matmul_nd_tiling.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_f16nd_f16nd_f16nd_kernel/pp_mat_mul_f16nd_f16nd_f16nd_kernel.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_f16nd_f16nd_f32nd_kernel/pp_mat_mul_f16nd_f16nd_f32nd_kernel.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_f16nd_f16nz_f16nd_kernel/pp_mat_mul_f16nd_f16nz_f16nd_kernel.cpp
-    ${CMAKE_CURRENT_LIST_DIR}/pp_mat_mul_f16nz_f16nz_f16nz_kernel/pp_mat_mul_f16nz_f16nz_f16nz_kernel.cpp
 )
 
 add_operation(MatMulOperation "${matmul_srcs}")
diff --git a/src/kernels/kernels/matmul/matmul_operation.cpp b/src/kernels/kernels/matmul/matmul_operation.cpp
index 30da7d02..b1096b02 100644
--- a/src/kernels/kernels/matmul/matmul_operation.cpp
+++ b/src/kernels/kernels/matmul/matmul_operation.cpp
@@ -30,21 +30,19 @@ constexpr uint32_t PP_MATMUL_I8_BF16_WEIGHT_NZ_KERNEL_KEY = 0b1'00'00'10'0'1'0'1
 constexpr uint32_t PP_MATMUL_I8_FP16_WEIGHT_NZ_KERNEL_KEY = 0b1'00'00'01'0'1'0'11;
 constexpr uint32_t PP_MATMUL_I8_KERNEL_KEY = 0b1'00'00'01'0'0'0'11;
 constexpr uint32_t PP_MATMUL_I8_WEIGHT_NZ_KERNEL_KEY = 0b1'00'00'01'0'1'0'11;
+constexpr uint32_t PP_MATMUL_F16_KERNEL_KEY = 0b1'01'01'01'0'0'0'00;
 constexpr uint32_t PP_MATMUL_F16_MIX_KERNEL_KEY = 0b1'01'01'01'0'0'0'01;
+constexpr uint32_t PP_MATMUL_BF16_KERNEL_KEY = 0b1'10'10'10'0'0'0'00;
+constexpr uint32_t PP_MATMUL_F16_OPT_KERNEL_KEY = 0b1'01'01'01'0'1'0'00;
+constexpr uint32_t PP_MATMUL_BF16_ND_NZ_ND_KERNEL_KEY = 0b1'10'10'10'0'1'0'00;
 constexpr uint32_t PP_MATMUL_NZ_F16_KERNEL_KEY = 0b0'01'01'01'1'1'1'00;
 constexpr uint32_t PP_MATMUL_I8_NZ_KERNEL_KEY = 0b0'00'00'01'1'1'1'11;
 constexpr uint32_t PP_MATMUL_I8_NZ_COMPRESS_KERNEL_KEY = 0b0'00'00'01'1'1'1'11;
+constexpr uint32_t PP_MATMUL_ACCUM_KERNEL_KEY = 0b1'10'10'11'0'0'0'00;
 constexpr uint32_t PP_MATMUL_FP_ND_ND_KERNEL_KEY = 0b0'01'01'01'0'0'0'00;
 constexpr uint32_t PP_MATMUL_I8_NZ_M300_KERNEL_KEY = 0b0'00'00'01'0'1'0'11;
 constexpr uint32_t PP_MATMUL_I8_ND_M300_KERNEL_KEY = 0b0'00'00'01'0'0'0'11;
 constexpr uint32_t PP_MATMUL_F16_NZ_M300_KERNEL_KEY = 0b0'01'01'01'0'1'0'00;
-constexpr uint32_t PP_MatMul_F16ND_F16ND_F16ND_KERNEL_KEY = 0b1'01'01'01'0'0'0'10;
-constexpr uint32_t PP_MatMul_BF16ND_BF16ND_BF16ND_KERNEL_KEY = 0b1'10'10'10'0'0'0'10;
-constexpr uint32_t PP_MatMul_F16ND_F16ND_F32ND_KERNEL_KEY = 0b1'01'01'11'0'0'0'10;
-constexpr uint32_t PP_MatMul_BF16ND_BF16ND_F32ND_KERNEL_KEY = 0b1'10'10'11'0'0'0'10;
-constexpr uint32_t PP_MatMul_F16ND_F16NZ_F16ND_KERNEL_KEY = 0b1'01'01'01'0'1'0'10;
-constexpr uint32_t PP_MatMul_BF16ND_BF16NZ_BF16ND_KERNEL_KEY = 0b1'10'10'10'0'1'0'10;
-constexpr uint32_t PP_MatMul_F16NZ_F16NZ_F16NZ_KERNEL_KEY = 0b0'01'01'01'1'1'1'10;
 
 } // namespace
 
@@ -74,6 +72,7 @@ public:
         const TensorDType dtypeC = outTensorDescC.dtype;
         bool isSparseDequant = (opParam.enDequant && opParam.tilingK > 0 && opParam.tilingN > 0);
         if (!isSparseDequant) {
+            // Validate input tensor format, excluding PpMatMulI8NzCompressKernel.
             MKI_CHECK((formatA == TENSOR_FORMAT_ND) || ValidNzFormat(launchParam.GetInTensor(0)),
                       "Invalid format of matrix A.", return nullptr);
             MKI_CHECK((formatB == TENSOR_FORMAT_ND) || ValidNzFormat(launchParam.GetInTensor(1)),
@@ -109,106 +108,16 @@ public:
         kernelKey = (kernelKey << INPUT_BIT_COUNT) + (inTensorCount - DIM_2);
         MKI_LOG(INFO) << "kernelKey: " << kernelKey;
         MKI_LOG(INFO) << ">>> PpMatmulType:" << static_cast<uint32_t>(opParam.matmulType);
-        // 先判断w8a8compress
-        if (isSparseDequant) {
-            switch (kernelKey) {
-                case PP_MATMUL_I8_NZ_COMPRESS_KERNEL_KEY: return GetKernelByName("PpMatmulW8A8NzCompressKernel");
-                default: MKI_LOG(ERROR) << "No matched kernel for matmul operation."; return nullptr;
-            }
-        }
-
-        if (opParam.quantMode == OpParam::MatMul::QuantMode::PER_TOKEN_SYMM) {
-            switch (kernelKey) {
-                case PP_MATMUL_I8_FP16_KERNEL_KEY:  return GetKernelByName("PpMatmulW8A8PertokenFP16Kernel");
-                case PP_MATMUL_I8_BF16_KERNEL_KEY: return GetKernelByName("PpMatmulW8A8Bf16NDNDKernel");
-                default: MKI_LOG(ERROR) << "No matched kernel for matmul operation."; return nullptr;
-            }
+        if (opParam.matmulType == MmType::MATMUL_ACCUM_ATOMIC) {
+            return GetKernelByName("PpMatmulAccumAtomicKernel");
         }
-
-        // 判断w8a8
-        switch (kernelKey) {
-            case PP_MATMUL_I8_BF16_KERNEL_KEY: return GetKernelByName("PpMatmulW8A8Bf16NDNDKernel");
-            case PP_MATMUL_I8_BF16_WEIGHT_NZ_KERNEL_KEY: return GetKernelByName("PpMatmulW8A8Bf16NDNZKernel");
-            case PP_MATMUL_I8_KERNEL_KEY: return GetKernelByName("PpMatmulW8A8Kernel");
-            case PP_MATMUL_I8_WEIGHT_NZ_KERNEL_KEY: return GetKernelByName("PpMatmulW8A8WeightNzKernel");
-            case PP_MATMUL_F16_MIX_KERNEL_KEY: return GetKernelByName("PpMatMulF16MixKernel");
-            case PP_MATMUL_I8_NZ_KERNEL_KEY:
-                if (platform == PlatformType::ASCEND_910A) {
-                    return GetKernelByName("PpMatMulI8NzKernel");
-                } else {
-                    return GetKernelByName("PpMatmulW8A8NzKernel");
-                }
-            case PP_MATMUL_FP_ND_ND_KERNEL_KEY: return GetKernelByName("PpMatmulF16NdM300Kernel");
-            case PP_MATMUL_I8_ND_M300_KERNEL_KEY: return GetKernelByName("PpMatMulI8Kernel");
-            case PP_MATMUL_I8_NZ_M300_KERNEL_KEY: return GetKernelByName("PpMatMulI8NdNzKernel");
-            case PP_MATMUL_F16_NZ_M300_KERNEL_KEY: return GetKernelByName("PpMatmulF16NzM300Kernel");
-            case PP_MatMul_F16ND_F16ND_F16ND_KERNEL_KEY: return GetKernelByName("PpMatMulF16NDF16NDF16NDKernel");
-            case PP_MatMul_BF16ND_BF16ND_BF16ND_KERNEL_KEY: return GetKernelByName("PpMatMulBF16NDBF16NDBF16NDKernel");
-            case PP_MatMul_F16ND_F16ND_F32ND_KERNEL_KEY: return GetKernelByName("PpMatMulF16NDF16NDF32NDKernel");
-            case PP_MatMul_BF16ND_BF16ND_F32ND_KERNEL_KEY: return GetKernelByName("PpMatMulBF16NDBF16NDF32NDKernel");
-            case PP_MatMul_F16ND_F16NZ_F16ND_KERNEL_KEY: return GetKernelByName("PpMatMulF16NDF16NZF16NDKernel");
-            case PP_MatMul_BF16ND_BF16NZ_BF16ND_KERNEL_KEY: return GetKernelByName("PpMatMulBF16NDBF16NZBF16NDKernel");
-            case PP_MATMUL_NZ_F16_KERNEL_KEY: return GetKernelByName("PpMatMulNzF16Kernel");
-            case PP_MatMul_F16NZ_F16NZ_F16NZ_KERNEL_KEY: return GetKernelByName("PpMatMulF16NZF16NZF16NZKernel");
-            default: MKI_LOG(ERROR) << "No matched kernel for matmul operation."; return nullptr;
+        if (opParam.matmulType == MmType::MATMUL_WITH_BIAS) {
+            return GetKernelByName("PpMatmulWithBiasKernel");
         }
-        return nullptr;
-    }
-
-    Kernel *GetBestKernelFallback(const LaunchParam &launchParam) const
-    {
-        MKI_CHECK(IsConsistent(launchParam), "Failed to check IsConsistent", return nullptr);
-        auto inTensorDescA = launchParam.GetInTensor(0).desc;
-        auto inTensorDescB = launchParam.GetInTensor(1).desc;
-        auto outTensorDescC = launchParam.GetOutTensor(0).desc;
-        size_t inTensorCount = launchParam.GetInTensorCount();
-        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::MatMul), "Invalid specificParam type.",
-                  return nullptr);
-        auto opParam = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        const TensorFormat formatA = inTensorDescA.format;
-        const TensorFormat formatB = inTensorDescB.format;
-        const TensorFormat formatC = outTensorDescC.format;
-        const TensorDType dtypeA = inTensorDescA.dtype;
-        const TensorDType dtypeB = inTensorDescB.dtype;
-        const TensorDType dtypeC = outTensorDescC.dtype;
-        bool isSparseDequant = (opParam.enDequant && opParam.tilingK > 0 && opParam.tilingN > 0);
-        if (!isSparseDequant) {
-            // Validate input tensor format, excluding PpMatMulI8NzCompressKernel.
-            MKI_CHECK((formatA == TENSOR_FORMAT_ND) || ValidNzFormat(launchParam.GetInTensor(0)),
-                      "Invalid format of matrix A.", return nullptr);
-            MKI_CHECK((formatB == TENSOR_FORMAT_ND) || ValidNzFormat(launchParam.GetInTensor(1)),
-                      "Invalid format of matrix B.", return nullptr);
+        if (opParam.matmulType == MmType::MATMUL_EIN_SUM) {
+            MKI_CHECK(!opParam.transposeA, "Unsupported transposed A_matrix", return nullptr);
+            return GetKernelByName("PpMatmulEinSumKernel");
         }
-        PlatformType platform = PlatformInfo::Instance().GetPlatformType();
-        std::unordered_map<PlatformType, uint32_t> archTypeMap = {{PlatformType::ASCEND_310P, 0},
-                                                                  {PlatformType::ASCEND_910A, 0},
-                                                                  {PlatformType::ASCEND_310B, 0},
-                                                                  {PlatformType::ASCEND_910B, 1}};
-        std::unordered_map<TensorFormat, uint32_t> tensorFormatMap = {{TENSOR_FORMAT_ND, 0},
-                                                                      {TENSOR_FORMAT_FRACTAL_NZ, 1}};
-        std::unordered_map<TensorDType, uint32_t> tensorDtypeMap = {
-            {TENSOR_DTYPE_INT8, 0}, {TENSOR_DTYPE_FLOAT16, 1}, {TENSOR_DTYPE_BF16, 2}, {TENSOR_DTYPE_FLOAT, 3}};
-
-        MKI_CHECK(archTypeMap.find(platform) != archTypeMap.end(), "Unsupported platform.", return nullptr);
-        MKI_CHECK(tensorFormatMap.find(formatA) != tensorFormatMap.end(), "Unsupported format of matrix A.",
-                  return nullptr);
-        MKI_CHECK(tensorFormatMap.find(formatB) != tensorFormatMap.end(), "Unsupported format of matrix B.",
-                  return nullptr);
-        MKI_CHECK(tensorDtypeMap.find(dtypeA) != tensorDtypeMap.end(), "Unsupported dtype of matrix A.",
-                  return nullptr);
-        MKI_CHECK(tensorDtypeMap.find(dtypeB) != tensorDtypeMap.end(), "Unsupported dtype of matrix B.",
-                  return nullptr);
-
-        uint32_t kernelKey = archTypeMap[platform];                             // ArchType
-        kernelKey = (kernelKey << DTYPE_BIT_COUNT) + tensorDtypeMap[dtypeA];    // DtypeA
-        kernelKey = (kernelKey << DTYPE_BIT_COUNT) + tensorDtypeMap[dtypeB];    // DtypeB
-        kernelKey = (kernelKey << DTYPE_BIT_COUNT) + tensorDtypeMap[dtypeC];    // DtypeC
-        kernelKey = (kernelKey << FORMAT_BIT_COUNT) + tensorFormatMap[formatA]; // FormatA
-        kernelKey = (kernelKey << FORMAT_BIT_COUNT) + tensorFormatMap[formatB]; // FormatB
-        kernelKey = (kernelKey << FORMAT_BIT_COUNT) + tensorFormatMap[formatC]; // FormatC
-        kernelKey = (kernelKey << INPUT_BIT_COUNT) + (inTensorCount - DIM_2);
-        MKI_LOG(INFO) << "kernelKey: " << kernelKey;
-        MKI_LOG(INFO) << ">>> PpMatmulType:" << static_cast<uint32_t>(opParam.matmulType);
         // 先判断w8a8compress
         if (isSparseDequant) {
             switch (kernelKey) {
@@ -235,7 +144,12 @@ public:
                 return GetKernelByName("PpMatMulI8Bf16Kernel");
             case PP_MATMUL_I8_KERNEL_KEY: return GetKernelByName("PpMatMulI8Kernel");
             case PP_MATMUL_I8_WEIGHT_NZ_KERNEL_KEY: return GetKernelByName("PpMatMulI8WeightNzKernel");
+            case PP_MATMUL_F16_KERNEL_KEY: return GetKernelByName("PpMatMulF16Kernel");
             case PP_MATMUL_F16_MIX_KERNEL_KEY: return GetKernelByName("PpMatMulF16MixKernel");
+            case PP_MATMUL_BF16_KERNEL_KEY: return GetKernelByName("PpMatMulBf16Kernel");
+            case PP_MATMUL_F16_OPT_KERNEL_KEY: return GetKernelByName("PpMatMulF16OptKernel");
+            case PP_MATMUL_BF16_ND_NZ_ND_KERNEL_KEY: return GetKernelByName("PpMatMulBf16NdNzNdKernel");
+            case PP_MATMUL_NZ_F16_KERNEL_KEY: return GetKernelByName("PpMatMulNzF16Kernel");
             case PP_MATMUL_I8_NZ_KERNEL_KEY: return GetKernelByName("PpMatMulI8NzKernel");
             case PP_MATMUL_FP_ND_ND_KERNEL_KEY: return GetKernelByName("PpMatmulF16NdM300Kernel");
             case PP_MATMUL_I8_ND_M300_KERNEL_KEY: return GetKernelByName("PpMatMulI8Kernel");
@@ -256,11 +170,12 @@ public:
         if (param.enDequant) {
             return 5; // There're 5 inputs if enable post dequant.
         }
-        PlatformType platform = PlatformInfo::Instance().GetPlatformType();
-        if (platform == PlatformType::ASCEND_910A) {
-            return 2; // matmul has 2 inputs
+        bool fuseAdd = (param.withBias || param.matmulType == MmType::MATMUL_ACCUM_ATOMIC ||
+                        param.matmulType == MmType::MATMUL_WITH_BIAS);
+        if (fuseAdd) {
+            return 3; // 3 withBias matmul
         }
-        return 4; // matmul has 4 inputs
+        return 2; // matmul has 2 inputs
     }
 
 protected:
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel.cpp
deleted file mode 100644
index a51af2b7..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nd_bf16nd_kernel.cpp
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulBF16NDBF16NDBF16NDKernel : public KernelBase {
-public:
-    explicit PpMatMulBF16NDBF16NDBF16NDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_910B, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descA.dims.size() == descB.dims.size(), "tensor dims invalid", return false);
-        auto opParam = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        if (opParam.matmulType == OpParam::MatMul::MatMulType::MATMUL_EIN_SUM) {
-            MKI_CHECK(descA.dims.size() == 3, "Matrix A must be a 3D-tensor in MATMUL_EIN_SUM scenario.", return false);
-            MKI_CHECK(descA.dims[1] == descB.dims[0], "tensor dims invalid: batchA != batchB", return false);
-        } else {
-            MKI_CHECK((descA.dims.size() == 2) || (descA.dims[0] == descB.dims[0]), "tensor dims invalid: batchA != batchB", return false);
-        }
-
-        return true;
-    }
-    
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulBF16NDBF16NDBF16NDKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel.cpp
deleted file mode 100644
index b7d2b18c..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel/pp_mat_mul_bf16nd_bf16nd_f32nd_kernel.cpp
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulBF16NDBF16NDF32NDKernel : public KernelBase {
-public:
-    explicit PpMatMulBF16NDBF16NDF32NDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_910B, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-
-        MKI_CHECK((descA.dims.size() == 2) || (descA.dims[0] == descB.dims[0]), "tensor dims invalid: batchA != batchB", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulBF16NDBF16NDF32NDKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel.cpp
deleted file mode 100644
index 3deb5ad2..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel/pp_mat_mul_bf16nd_bf16nz_bf16nd_kernel.cpp
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulBF16NDBF16NZBF16NDKernel : public KernelBase {
-public:
-    explicit PpMatMulBF16NDBF16NZBF16NDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_910B, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_FRACTAL_NZ, "tensor format invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-
-        auto opParam = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        if (opParam.matmulType == OpParam::MatMul::MatMulType::MATMUL_EIN_SUM) {
-            MKI_CHECK(descA.dims.size() == 3, "Matrix A must be a 3D-tensor in MATMUL_EIN_SUM scenario.", return false);
-            MKI_CHECK(descA.dims[1] == descB.dims[0], "tensor dims invalid: batchA != batchB", return false);
-        } else {
-            if (descA.dims.size() == 2) { // 2: The first input is a two-dimensional tensor while batch size is one.
-                MKI_CHECK(descB.dims[0] == 1, "tensor dims invalid", return false);
-            } else {
-                MKI_CHECK(descA.dims.size() == 3, "tensor dims invalid", return false);
-                MKI_CHECK(descA.dims[0] == descB.dims[0], "tensor dims invalid", return false);
-            }
-        }
-
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulBF16NDBF16NZBF16NDKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nd_f16nd_kernel/pp_mat_mul_f16nd_f16nd_f16nd_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nd_f16nd_kernel/pp_mat_mul_f16nd_f16nd_f16nd_kernel.cpp
deleted file mode 100644
index 11abda49..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nd_f16nd_kernel/pp_mat_mul_f16nd_f16nd_f16nd_kernel.cpp
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulF16NDF16NDF16NDKernel : public KernelBase {
-public:
-    explicit PpMatMulF16NDF16NDF16NDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_910B, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        auto opParam = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        if (opParam.matmulType == OpParam::MatMul::MatMulType::MATMUL_EIN_SUM) {
-            MKI_CHECK(descA.dims.size() == 3, "Matrix A must be a 3D-tensor in MATMUL_EIN_SUM scenario.", return false);
-            MKI_CHECK(descA.dims[1] == descB.dims[0], "tensor dims invalid: batchA != batchB", return false);
-        } else {
-            MKI_CHECK((descA.dims.size() == 2) || (descA.dims[0] == descB.dims[0]), "tensor dims invalid: batchA != batchB", return false);
-        }
-
-        return true;
-    }
-    
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulF16NDF16NDF16NDKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nd_f32nd_kernel/pp_mat_mul_f16nd_f16nd_f32nd_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nd_f32nd_kernel/pp_mat_mul_f16nd_f16nd_f32nd_kernel.cpp
deleted file mode 100644
index 59c0aedf..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nd_f32nd_kernel/pp_mat_mul_f16nd_f16nd_f32nd_kernel.cpp
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulF16NDF16NDF32NDKernel : public KernelBase {
-public:
-    explicit PpMatMulF16NDF16NDF32NDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_910B, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-        MKI_CHECK((descA.dims.size() == 2) || (descA.dims[0] == descB.dims[0]), "tensor dims invalid: batchA != batchB", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulF16NDF16NDF32NDKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nz_f16nd_kernel/pp_mat_mul_f16nd_f16nz_f16nd_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nz_f16nd_kernel/pp_mat_mul_f16nd_f16nz_f16nd_kernel.cpp
deleted file mode 100644
index 374a2b2b..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_f16nd_f16nz_f16nd_kernel/pp_mat_mul_f16nd_f16nz_f16nd_kernel.cpp
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulF16NDF16NZF16NDKernel : public KernelBase {
-public:
-    explicit PpMatMulF16NDF16NZF16NDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_910B, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_FRACTAL_NZ, "tensor format invalid", return false);
-        MKI_CHECK(descB.dims.size() == 4, "tensor dims invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        auto opParam = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        if (opParam.matmulType == OpParam::MatMul::MatMulType::MATMUL_EIN_SUM) {
-            MKI_CHECK(descA.dims.size() == 3, "Matrix A must be a 3D-tensor in MATMUL_EIN_SUM scenario.", return false);
-            MKI_CHECK(descA.dims[1] == descB.dims[0], "tensor dims invalid: batchA != batchB", return false);
-        } else {
-            if (descA.dims.size() == 2) { // 2: The first input is a two-dimensional tensor while batch size is one.
-                MKI_CHECK(descB.dims[0] == 1, "tensor dims invalid", return false);
-            } else {
-                MKI_CHECK(descA.dims.size() == 3, "tensor dims invalid", return false);
-                MKI_CHECK(descA.dims[0] == descB.dims[0], "tensor dims invalid", return false);
-            }
-        }
-
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulF16NDF16NZF16NDKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_mat_mul_f16nz_f16nz_f16nz_kernel/pp_mat_mul_f16nz_f16nz_f16nz_kernel.cpp b/src/kernels/kernels/matmul/pp_mat_mul_f16nz_f16nz_f16nz_kernel/pp_mat_mul_f16nz_f16nz_f16nz_kernel.cpp
deleted file mode 100644
index 6430a2d4..00000000
--- a/src/kernels/kernels/matmul/pp_mat_mul_f16nz_f16nz_f16nz_kernel/pp_mat_mul_f16nz_f16nz_f16nz_kernel.cpp
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Copyright (c) 2025 Huawei Technologies Co., Ltd.
- * This file is a part of the CANN Open Software.
- * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
- * Please refer to the License for details. You may not use this file except in compliance with the License.
- * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
- * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
- * See LICENSE in the root of the software repository for the full text of the License.
- */
-#include <mki/base/kernel_base.h>
-#include <mki_loader/op_register.h>
-#include <mki/utils/log/log.h>
-#include "asdops/params/params.h"
-#include "kernels/matmul/tiling/pp_matmul_tiling.h"
-#include "kernels/matmul/tiling/tiling_data.h"
-#include "kernels/matmul/common/common.h"
-#include "kernels/matmul/common/common_tiling.h"
-#include "sink_common.h"
-
-namespace AsdOps {
-class PpMatMulF16NZF16NZF16NZKernel : public KernelBase {
-public:
-    explicit PpMatMulF16NZF16NZF16NZKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(PlatformInfo::Instance().GetPlatformType() == PlatformType::ASCEND_310P, "platform not support",
-                     return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 4, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        const auto &descA = launchParam.GetInTensor(0).desc;
-        const auto &descB = launchParam.GetInTensor(1).desc;
-
-        MKI_CHECK(descA.format == TENSOR_FORMAT_FRACTAL_NZ, "tensor format invalid", return false);
-        MKI_CHECK(descA.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        MKI_CHECK(descB.format == TENSOR_FORMAT_FRACTAL_NZ, "tensor format invalid", return false);
-        MKI_CHECK(descB.dims.size() == 4, "tensor dims invalid", return false);
-        MKI_CHECK(descB.dtype == TENSOR_DTYPE_FLOAT16, "tensor dtype invalid", return false);
-
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatMul", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatMulF16NZF16NZF16NZKernel);
-} // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_matmul_ein_sum_kernel/pp_matmul_ein_sum_kernel.cpp b/src/kernels/kernels/matmul/pp_matmul_ein_sum_kernel/pp_matmul_ein_sum_kernel.cpp
new file mode 100644
index 00000000..ef588d8d
--- /dev/null
+++ b/src/kernels/kernels/matmul/pp_matmul_ein_sum_kernel/pp_matmul_ein_sum_kernel.cpp
@@ -0,0 +1,51 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#include <mki/base/kernel_base.h>
+#include <mki/utils/log/log.h>
+#include <mki_loader/op_register.h>
+#include "asdops/params/params.h"
+#include "kernels/matmul/common/common.h"
+#include "kernels/matmul/tiling/pp_matmul_tiling.h"
+#include "kernels/matmul/tiling/tiling_data.h"
+
+namespace AsdOps {
+using namespace Mki;
+class PpMatmulEinSumKernel : public KernelBase {
+public:
+    explicit PpMatmulEinSumKernel(const std::string &kernelName, const BinHandle *handle) noexcept
+        : KernelBase(kernelName, handle)
+    {
+    }
+
+    bool CanSupport(const LaunchParam &launchParam) const override
+    {
+        auto attr = launchParam.GetParam();
+        MKI_CHECK(attr.Type() == typeid(OpParam::MatMul), "Invalid launch param type.", return false);
+        auto mmType = AnyCast<OpParam::MatMul>(attr).matmulType;
+        MKI_CHECK(mmType == OpParam::MatMul::MatMulType::MATMUL_EIN_SUM, "Invalid matmul type.", return false);
+        const auto &inTensor1 = launchParam.GetInTensor(1);
+        if (inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ) {
+            return CheckAsdOpsWeightNZ(launchParam, 2); // 输入参数数量为2
+        } else {
+            return CheckAsdOpsND(launchParam, 2); // 输入参数数量为2
+        }
+    }
+
+    uint64_t GetTilingSize(const LaunchParam &launchParam) const override
+    {
+        (void)launchParam;
+        constexpr uint32_t CONST_256 = 256;
+        return (sizeof(PpMatmulTilingData) + CONST_256 - 1) / CONST_256 * CONST_256;
+    }
+
+    Status InitImpl(const LaunchParam &launchParam) override { return PpMatmulTiling(launchParam, kernelInfo_); }
+};
+REG_KERNEL_BASE(PpMatmulEinSumKernel);
+} // namespace AsdOps
\ No newline at end of file
diff --git a/src/kernels/kernels/matmul/pp_matmul_f16_kernel/pp_matmul_f16_kernel.cpp b/src/kernels/kernels/matmul/pp_matmul_f16_kernel/pp_matmul_f16_kernel.cpp
new file mode 100644
index 00000000..b0d9818e
--- /dev/null
+++ b/src/kernels/kernels/matmul/pp_matmul_f16_kernel/pp_matmul_f16_kernel.cpp
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#include <mki/base/kernel_base.h>
+#include <mki_loader/op_register.h>
+#include <mki/utils/log/log.h>
+#include "asdops/params/params.h"
+#include "kernels/matmul/tiling/pp_matmul_tiling.h"
+#include "kernels/matmul/tiling/tiling_data.h"
+#include "kernels/matmul/common/common.h"
+#include "kernels/matmul/common/common_tiling.h"
+
+namespace AsdOps {
+class PpMatMulF16Kernel : public KernelBase {
+public:
+    explicit PpMatMulF16Kernel(const std::string &kernelName, const BinHandle *handle) noexcept
+        : KernelBase(kernelName, handle)
+    {
+    }
+
+    bool CanSupport(const LaunchParam &launchParam) const override
+    {
+        return CheckAsdOpsND(launchParam, 2); // 输入参数数量为2
+    }
+
+    uint64_t GetTilingSize(const LaunchParam &launchParam) const override
+    {
+        (void)launchParam;
+        constexpr uint32_t CONST_256 = 256;
+        return Round<CONST_256>(sizeof(PpMatmulTilingData));
+    }
+
+    Status InitImpl(const LaunchParam &launchParam) override
+    {
+        return PpMatmulTiling(launchParam, kernelInfo_);
+    }
+};
+REG_KERNEL_BASE(PpMatMulF16Kernel);
+} // namespace AsdOps
\ No newline at end of file
diff --git a/src/kernels/kernels/matmul/pp_matmul_i8_kernel/pp_matmul_i8_kernel.cpp b/src/kernels/kernels/matmul/pp_matmul_i8_kernel/pp_matmul_i8_kernel.cpp
index 5041bd31..e4c4f5cc 100644
--- a/src/kernels/kernels/matmul/pp_matmul_i8_kernel/pp_matmul_i8_kernel.cpp
+++ b/src/kernels/kernels/matmul/pp_matmul_i8_kernel/pp_matmul_i8_kernel.cpp
@@ -14,7 +14,6 @@
 #include "kernels/matmul/common/common.h"
 #include "kernels/matmul/tiling/pp_matmul_tiling.h"
 #include "kernels/matmul/tiling/tiling_data.h"
-#include "sink_common.h"
 
 namespace AsdOps {
 using namespace Mki;
@@ -184,252 +183,4 @@ public:
 };
 REG_KERNEL_BASE(PpMatMulI8WeightNzKernel);
 
-class PpMatmulW8A8CommKernel : public KernelBase {
-public:
-    explicit PpMatmulW8A8CommKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(CheckAsdOpsND(launchParam, 5), "CheckAsdOpsND failed", return false);
-        const auto &inTensor2 = launchParam.GetInTensor(2);
-        const auto &attrs = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        if (attrs.withBias) {
-            MKI_CHECK(inTensor2.desc.format == TENSOR_FORMAT_ND, "inTensor2 format invalid", return false);
-            MKI_CHECK(inTensor2.desc.dtype == TENSOR_DTYPE_INT32, "inTensor2 dtype invalid", return false);
-        }
-
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatmulW8A8", *GetBinHandle(), launchParam, AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-
-class PpMatmulW8A8Kernel : public PpMatmulW8A8CommKernel {
-public:
-    explicit PpMatmulW8A8Kernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : PpMatmulW8A8CommKernel(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK_NO_LOG(PpMatmulW8A8CommKernel::CanSupport(launchParam), return false);
-        const auto &inTensor3 = launchParam.GetInTensor(3);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_INT64 || inTensor3.desc.dtype == TENSOR_DTYPE_UINT64,
-                  "inTensor3 dtype invalid", return false);
-        return true;
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8Kernel);
-
-
-class PpMatmulW8A8WeightNzKernel : public PpMatmulW8A8CommKernel {
-    static constexpr uint32_t NUM_INPUT = 5;
-
-public:
-    explicit PpMatmulW8A8WeightNzKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : PpMatmulW8A8CommKernel(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(launchParam.GetInTensorCount() == NUM_INPUT, "Invalid number of input tensor.", return false);
-
-        const auto &inTensor0 = launchParam.GetInTensor(0);
-        MKI_CHECK(inTensor0.desc.format == TENSOR_FORMAT_ND, "Invalid format of input tensor[0].", return false);
-        MKI_CHECK(inTensor0.desc.dtype == TENSOR_DTYPE_INT8, "Invalid dtype of input tensor[0].", return false);
-
-        const auto &inTensor1 = launchParam.GetInTensor(1);
-        MKI_CHECK(inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "Invalid format of input tensor[1].",
-                  return false);
-        MKI_CHECK(inTensor1.desc.dtype == TENSOR_DTYPE_INT8, "Invalid dtype of input tensor[1].", return false);
-
-        const auto &inTensor2 = launchParam.GetInTensor(2);
-        const auto &attrs = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-        if (attrs.withBias) {
-            MKI_CHECK(inTensor2.desc.format == TENSOR_FORMAT_ND, "inTensor2 format invalid", return false);
-            MKI_CHECK(inTensor2.desc.dtype == TENSOR_DTYPE_INT32, "inTensor2 dtype invalid", return false);
-        }
-
-        const auto &inTensor3 = launchParam.GetInTensor(3);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_INT64, "inTensor3 dtype invalid", return false);
-        return true;
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8WeightNzKernel);
-
-class PpMatmulW8A8Bf16NDNDKernel : public PpMatmulW8A8CommKernel {
-public:
-    explicit PpMatmulW8A8Bf16NDNDKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : PpMatmulW8A8CommKernel(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(launchParam.GetInTensorCount() == size_t(5), "inTensor count invalid", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "outTensor count invalid", return false);
-        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::MatMul), "check param type failed!",
-                    return false);
-
-        const auto &inTensor0 = launchParam.GetInTensor(0);
-        const auto &inTensor1 = launchParam.GetInTensor(1);
-        const auto &outTensor = launchParam.GetOutTensor(0);
-        const auto &attrs = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-
-        MKI_CHECK(inTensor0.desc.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(inTensor0.desc.dtype == TENSOR_DTYPE_INT8,
-                "tensor dtype invalid", return false);
-        MKI_CHECK(inTensor0.desc.dims.size() == 2 || inTensor0.desc.dims.size() == 3, "tensor dims invalid",
-                return false);
-
-        MKI_CHECK((inTensor1.desc.format == TENSOR_FORMAT_ND) || (inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ),
-                    "tensor format invalid", return false);
-        MKI_CHECK(inTensor1.desc.dtype == TENSOR_DTYPE_INT8,
-                "tensor dtype invalid", return false);
-        MKI_CHECK(inTensor1.desc.dims.size() == 2 || inTensor1.desc.dims.size() == 3 ||
-                    inTensor1.desc.dims.size() == 4, "tensor dims invalid", return false);
-
-        MKI_CHECK(outTensor.desc.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        if (attrs.quantMode == OpParam::MatMul::QuantMode::PER_TOKEN_SYMM) {
-            MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_BF16 || outTensor.desc.dtype == TENSOR_DTYPE_FLOAT16,
-                      "tensor dtype invalid", return false);
-        } else {
-            MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-        }
-        MKI_CHECK(outTensor.desc.dims.size() == 2 || outTensor.desc.dims.size() == 3, "tensor dims invalid",
-                return false);
-
-        const auto &inTensor3 = launchParam.GetInTensor(3);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_FLOAT, "inTensor3 dtype invalid", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatmulW8A8", *GetBinHandle(), launchParam, AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8Bf16NDNDKernel);
-
-class PpMatmulW8A8Bf16NDNZKernel : public PpMatmulW8A8CommKernel {
-public:
-    explicit PpMatmulW8A8Bf16NDNZKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : PpMatmulW8A8CommKernel(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(launchParam.GetInTensorCount() == size_t(5), "inTensor count invalid", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "outTensor count invalid", return false);
-        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::MatMul), "check param type failed!",
-                    return false);
-
-        const auto &inTensor0 = launchParam.GetInTensor(0);
-        const auto &inTensor1 = launchParam.GetInTensor(1);
-        const auto &outTensor = launchParam.GetOutTensor(0);
-        const auto &attrs = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-
-        MKI_CHECK(inTensor0.desc.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(inTensor0.desc.dtype == TENSOR_DTYPE_INT8,
-                "tensor dtype invalid", return false);
-        MKI_CHECK(inTensor0.desc.dims.size() == 2 || inTensor0.desc.dims.size() == 3, "tensor dims invalid",
-                return false);
-
-        MKI_CHECK((inTensor1.desc.format == TENSOR_FORMAT_ND) || (inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ),
-                    "tensor format invalid", return false);
-        MKI_CHECK(inTensor1.desc.dtype == TENSOR_DTYPE_INT8,
-                "tensor dtype invalid", return false);
-        MKI_CHECK(inTensor1.desc.dims.size() == 2 || inTensor1.desc.dims.size() == 3 ||
-                    inTensor1.desc.dims.size() == 4, "tensor dims invalid", return false);
-
-        MKI_CHECK(outTensor.desc.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        if (attrs.quantMode == OpParam::MatMul::QuantMode::PER_TOKEN_SYMM) {
-            MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_BF16 || outTensor.desc.dtype == TENSOR_DTYPE_FLOAT16,
-                      "tensor dtype invalid", return false);
-        } else {
-            MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-        }
-        MKI_CHECK(outTensor.desc.dims.size() == 2 || outTensor.desc.dims.size() == 3, "tensor dims invalid",
-                return false);
-
-        const auto &inTensor3 = launchParam.GetInTensor(3);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_FLOAT, "inTensor3 dtype invalid", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatmulW8A8", *GetBinHandle(), launchParam, AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8Bf16NDNZKernel);
-
-class PpMatmulW8A8PertokenFP16Kernel : public PpMatmulW8A8CommKernel {
-public:
-    explicit PpMatmulW8A8PertokenFP16Kernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : PpMatmulW8A8CommKernel(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(launchParam.GetInTensorCount() == size_t(5), "inTensor count invalid", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "outTensor count invalid", return false);
-        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::MatMul), "check param type failed!",
-                    return false);
-
-        const auto &inTensor0 = launchParam.GetInTensor(0);
-        const auto &inTensor1 = launchParam.GetInTensor(1);
-        const auto &outTensor = launchParam.GetOutTensor(0);
-        const auto &attrs = AnyCast<OpParam::MatMul>(launchParam.GetParam());
-
-        MKI_CHECK(inTensor0.desc.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        MKI_CHECK(inTensor0.desc.dtype == TENSOR_DTYPE_INT8,
-                "tensor dtype invalid", return false);
-        MKI_CHECK(inTensor0.desc.dims.size() == 2 || inTensor0.desc.dims.size() == 3, "tensor dims invalid",
-                return false);
-
-        MKI_CHECK((inTensor1.desc.format == TENSOR_FORMAT_ND) || (inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ),
-                    "tensor format invalid", return false);
-        MKI_CHECK(inTensor1.desc.dtype == TENSOR_DTYPE_INT8,
-                "tensor dtype invalid", return false);
-        MKI_CHECK(inTensor1.desc.dims.size() == 2 || inTensor1.desc.dims.size() == 3 ||
-                    inTensor1.desc.dims.size() == 4, "tensor dims invalid", return false);
-
-        MKI_CHECK(outTensor.desc.format == TENSOR_FORMAT_ND, "tensor format invalid", return false);
-        if (attrs.quantMode == OpParam::MatMul::QuantMode::PER_TOKEN_SYMM) {
-            MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_BF16 || outTensor.desc.dtype == TENSOR_DTYPE_FLOAT16,
-                      "tensor dtype invalid", return false);
-        } else {
-            MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_BF16, "tensor dtype invalid", return false);
-        }
-        MKI_CHECK(outTensor.desc.dims.size() == 2 || outTensor.desc.dims.size() == 3, "tensor dims invalid",
-                return false);
-
-        const auto &inTensor3 = launchParam.GetInTensor(3);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_FLOAT, "inTensor3 dtype invalid", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatmulW8A8", *GetBinHandle(), launchParam, AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8PertokenFP16Kernel);
-
 } // namespace AsdOps
diff --git a/src/kernels/kernels/matmul/pp_matmul_i8_nz_compress_kernel/pp_matmul_i8_nz_compress_kernel.cpp b/src/kernels/kernels/matmul/pp_matmul_i8_nz_compress_kernel/pp_matmul_i8_nz_compress_kernel.cpp
index 5bde3b87..09247b46 100644
--- a/src/kernels/kernels/matmul/pp_matmul_i8_nz_compress_kernel/pp_matmul_i8_nz_compress_kernel.cpp
+++ b/src/kernels/kernels/matmul/pp_matmul_i8_nz_compress_kernel/pp_matmul_i8_nz_compress_kernel.cpp
@@ -14,7 +14,6 @@
 #include <mki/utils/log/log.h>
 #include "asdops/params/params.h"
 #include "kernels/matmul/tiling/pp_matmul_i8_nz_tiling.h"
-#include "sink_common.h"
 constexpr uint32_t ALIGN32 = 32;
 constexpr uint32_t ALIGN16 = 16;
 
@@ -74,53 +73,4 @@ public:
     }
 };
 REG_KERNEL_BASE(PpMatMulI8NzCompressKernel);
-
-class PpMatmulW8A8NzCompressKernel : public KernelBase {
-public:
-    explicit PpMatmulW8A8NzCompressKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::MatMul),
-                     "check param type failed!", return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == DIM_5, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == DIM_1, "check outTensor count failed", return false);
-
-        auto inTensor0 = launchParam.GetInTensor(DIM_0);
-        auto inTensor1 = launchParam.GetInTensor(DIM_1);
-        auto inTensor2 = launchParam.GetInTensor(DIM_2);
-        auto inTensor3 = launchParam.GetInTensor(DIM_3);
-        auto inTensor4 = launchParam.GetInTensor(DIM_4);
-        auto outTensor = launchParam.GetOutTensor(DIM_0);
-        MKI_CHECK(inTensor0.desc.dtype == TENSOR_DTYPE_INT8, "inTensor0 dtype invalid", return false);
-        MKI_CHECK(inTensor0.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "inTensor0 format invalid", return false);
-        MKI_CHECK(inTensor0.desc.dims.size() == DIM_4, "inTensor0 dims invalid", return false);
-        MKI_CHECK((inTensor0.desc.dims.at(DIM_3) == ALIGN32) && ((inTensor0.desc.dims.at(DIM_2) % ALIGN16) == 0),
-                     "inTensor0 dims value invalid", return false);
-        MKI_CHECK(inTensor1.desc.dtype == TENSOR_DTYPE_INT8, "inTensor1 dtype invalid", return false);
-        MKI_CHECK(inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "inTensor1 format invalid", return false);
-        MKI_CHECK(inTensor1.desc.dims.size() == DIM_4 || inTensor1.desc.dims.size() == DIM_2,
-                     "inTensor1 dims invalid", return false);
-        MKI_CHECK(inTensor2.desc.dtype == TENSOR_DTYPE_INT32, "inTensor2 dtype invalid", return false);
-        MKI_CHECK(inTensor2.desc.format == TENSOR_FORMAT_ND, "inTensor2 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_UINT64 || inTensor3.desc.dtype == TENSOR_DTYPE_INT64,
-                     "inTensor3 dtype invalid", return false);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-        MKI_CHECK(inTensor4.desc.dtype == TENSOR_DTYPE_INT8, "inTensor4 dtype invalid", return false);
-        MKI_CHECK(inTensor4.desc.format == TENSOR_FORMAT_ND, "inTensor4 format invalid", return false);
-        MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_FLOAT16, "outTensor dtype invalid", return false);
-        MKI_CHECK(outTensor.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "outTensor format invalid", return false);
-        MKI_CHECK(outTensor.desc.dims.size() == 4, "outTensor dims invalid", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatmulW8A8", *GetBinHandle(), launchParam, AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8NzCompressKernel);
 } // namespace AsdOps
\ No newline at end of file
diff --git a/src/kernels/kernels/matmul/pp_matmul_i8_nz_kernel/pp_matmul_i8_nz_kernel.cpp b/src/kernels/kernels/matmul/pp_matmul_i8_nz_kernel/pp_matmul_i8_nz_kernel.cpp
index 4d0585c2..2bf480a4 100644
--- a/src/kernels/kernels/matmul/pp_matmul_i8_nz_kernel/pp_matmul_i8_nz_kernel.cpp
+++ b/src/kernels/kernels/matmul/pp_matmul_i8_nz_kernel/pp_matmul_i8_nz_kernel.cpp
@@ -13,7 +13,6 @@
 #include <mki_loader/op_register.h>
 #include "asdops/params/params.h"
 #include "kernels/matmul/tiling/pp_matmul_i8_nz_tiling.h"
-#include "sink_common.h"
 
 namespace AsdOps {
 class PpMatMulI8NzKernel : public KernelBase {
@@ -63,49 +62,4 @@ public:
     Status InitImpl(const LaunchParam &launchParam) override { return PpTiling310P(launchParam, kernelInfo_); }
 };
 REG_KERNEL_BASE(PpMatMulI8NzKernel);
-
-class PpMatmulW8A8NzKernel : public KernelBase {
-public:
-    explicit PpMatmulW8A8NzKernel(const std::string &kernelName, const BinHandle *handle) noexcept
-        : KernelBase(kernelName, handle)
-    {
-    }
-
-    bool CanSupport(const LaunchParam &launchParam) const override
-    {
-        MKI_CHECK(launchParam.GetParam().Type() == typeid(OpParam::MatMul), "check param type failed!", return false);
-        MKI_CHECK(launchParam.GetInTensorCount() == 5, "check inTensor count failed", return false);
-        MKI_CHECK(launchParam.GetOutTensorCount() == 1, "check outTensor count failed", return false);
-
-        auto inTensor0 = launchParam.GetInTensor(DIM_0);
-        auto inTensor1 = launchParam.GetInTensor(DIM_1);
-        auto inTensor2 = launchParam.GetInTensor(DIM_2);
-        auto inTensor3 = launchParam.GetInTensor(DIM_3);
-        auto outTensor = launchParam.GetOutTensor(DIM_0);
-
-        MKI_CHECK(inTensor0.desc.dtype == TENSOR_DTYPE_INT8, "inTensor0 dtype invalid", return false);
-        MKI_CHECK(inTensor0.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "inTensor0 format invalid", return false);
-        MKI_CHECK(inTensor0.desc.dims.size() == DIM_4, "inTensor0 dims invalid", return false);
-        MKI_CHECK(inTensor1.desc.dtype == TENSOR_DTYPE_INT8, "inTensor1 dtype invalid", return false);
-        MKI_CHECK(inTensor1.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "inTensor1 format invalid", return false);
-        MKI_CHECK(inTensor1.desc.dims.size() == DIM_4, "inTensor1 dims invalid", return false);
-        MKI_CHECK(inTensor2.desc.dtype == TENSOR_DTYPE_INT32, "inTensor2 dtype invalid", return false);
-        MKI_CHECK(inTensor2.desc.format == TENSOR_FORMAT_ND, "inTensor2 format invalid", return false);
-        MKI_CHECK(inTensor3.desc.dtype == TENSOR_DTYPE_UINT64 || inTensor3.desc.dtype == TENSOR_DTYPE_INT64 ||
-                      inTensor3.desc.dtype == TENSOR_DTYPE_FLOAT,
-                  "inTensor3 dtype invalid", return false);
-        MKI_CHECK(inTensor3.desc.format == TENSOR_FORMAT_ND, "inTensor3 format invalid", return false);
-
-        MKI_CHECK(outTensor.desc.dtype == TENSOR_DTYPE_FLOAT16, "outTensor dtype invalid", return false);
-        MKI_CHECK(outTensor.desc.format == TENSOR_FORMAT_FRACTAL_NZ, "outTensor format invalid", return false);
-        MKI_CHECK(outTensor.desc.dims.size() == DIM_4, "outTensor dims invalid", return false);
-        return true;
-    }
-
-    Status InitImpl(const LaunchParam &launchParam) override
-    {
-        return optiling::CallGeTiling("PpMatmulW8A8", *GetBinHandle(), launchParam, AsdOps::GetMkiSpecificAttr<OpParam::MatMul>, kernelInfo_);
-    }
-};
-REG_KERNEL_BASE(PpMatmulW8A8NzKernel);
 } // namespace AsdOps
diff --git a/src/kernels/mixkernels/CMakeLists.txt b/src/kernels/mixkernels/CMakeLists.txt
index c9c947b2..59f09267 100644
--- a/src/kernels/mixkernels/CMakeLists.txt
+++ b/src/kernels/mixkernels/CMakeLists.txt
@@ -73,11 +73,11 @@ list(APPEND BINARY_SRC_LIST ${CMAKE_CURRENT_LIST_DIR}/param_to_json.cpp
 )
 add_library(atb_mixops SHARED ${BINARY_SRC_LIST})
 add_dependencies(atb_mixops MIX_BINARY_SRC_TARGET)
-target_link_libraries(atb_mixops PRIVATE ${ops_objects} mki tbe_adapter tiling_api register cann_ops_adapter)
+target_link_libraries(atb_mixops PRIVATE ${ops_objects} mki tbe_adapter tiling_api register)
 
 add_library(atb_mixops_static STATIC $<TARGET_OBJECTS:atb_mixops>)
 add_dependencies(atb_mixops_static MIX_BINARY_SRC_TARGET)
-target_link_libraries(atb_mixops_static PRIVATE ${ops_objects} mki tbe_adapter tiling_api register cann_ops_adapter)
+target_link_libraries(atb_mixops_static PRIVATE ${ops_objects} mki tbe_adapter tiling_api register)
 
 add_executable(atbops_sym_check sym_check.cpp)
 target_link_libraries(atbops_sym_check PRIVATE atb_mixops mki)
diff --git a/src/kernels/mixkernels/rope/CMakeLists.txt b/src/kernels/mixkernels/rope/CMakeLists.txt
index c4493e15..703422c6 100644
--- a/src/kernels/mixkernels/rope/CMakeLists.txt
+++ b/src/kernels/mixkernels/rope/CMakeLists.txt
@@ -11,6 +11,8 @@ set(rope_srcs
     ${CMAKE_CURRENT_LIST_DIR}/tiling/rope_tiling.cpp
 )
 
+
+
 add_operation(RopeOperation "${rope_srcs}")
 
 add_kernel(rotary_pos_emb ascend910b vector
diff --git a/src/kernels/mixkernels/rope/rope_kernel.cpp b/src/kernels/mixkernels/rope/rope_kernel.cpp
index fd6567dc..25445aab 100644
--- a/src/kernels/mixkernels/rope/rope_kernel.cpp
+++ b/src/kernels/mixkernels/rope/rope_kernel.cpp
@@ -88,10 +88,15 @@ public:
         return RopeDtypeCheck(launchParam, TENSOR_DTYPE_FLOAT16) || RopeDtypeCheck(launchParam, TENSOR_DTYPE_BF16);
     }
 
+    uint64_t GetTilingSize(const LaunchParam &launchParam) const override
+    {
+        (void)launchParam;
+        return sizeof(RopeTilingData);
+    }
+
     Status InitImpl(const LaunchParam &launchParam) override
     {
-        return optiling::CallGeTiling("RotaryPosEmbInfer", *GetBinHandle(), launchParam,
-                                      AsdOps::GetMkiSpecificAttr<OpParam::Rope>, kernelInfo_);
+        return RopeTiling(launchParam, kernelInfo_);
     }
 };
 
diff --git a/src/kernels/mixkernels/toppsample/toppsample_kernel.cpp b/src/kernels/mixkernels/toppsample/toppsample_kernel.cpp
index f4181450..3c3afa17 100644
--- a/src/kernels/mixkernels/toppsample/toppsample_kernel.cpp
+++ b/src/kernels/mixkernels/toppsample/toppsample_kernel.cpp
@@ -15,7 +15,6 @@
 #include "mixkernels/utils/common.h"
 #include "tiling/toppsample_tiling.h"
 #include "tiling/tiling_data.h"
-#include "sink_common.h"
 
 static constexpr uint32_t TENSOR_INPUT_NUM = 2;
 static constexpr uint32_t TENSOR_OUTPUT_NUM = 2;
@@ -53,8 +52,6 @@ public:
 
     Status InitImpl(const LaunchParam &launchParam) override
     {
-        // auto geTiling = optiling::CallGeTiling("TopPSample", *GetBinHandle(), launchParam,
-        //                                        AsdOps::GetMkiSpecificAttr<OpParam::Toppsample>, kernelInfo_);
         auto geTiling = ToppsampleTiling(launchParam, kernelInfo_);
         kernelInfo_.SetMemsetInfo(TENSOR_INPUT_NUM + TENSOR_OUTPUT_NUM, kernelInfo_.GetScratchSizes()[0]);
         return geTiling;
diff --git a/src/kernels/mixkernels/toppsample/toppsample_operation.cpp b/src/kernels/mixkernels/toppsample/toppsample_operation.cpp
index bcf8cf30..84450688 100644
--- a/src/kernels/mixkernels/toppsample/toppsample_operation.cpp
+++ b/src/kernels/mixkernels/toppsample/toppsample_operation.cpp
@@ -15,7 +15,6 @@
 #include <mki/utils/checktensor/check_tensor.h>
 #include <mki/utils/log/log.h>
 #include "atbops/params/params.h"
-#include "sink_common.h"
 
 namespace AtbOps {
 using namespace Mki;
diff --git a/src/ops_infer/linear/linear_ops_runner.cpp b/src/ops_infer/linear/linear_ops_runner.cpp
index dac57202..fbc0435e 100644
--- a/src/ops_infer/linear/linear_ops_runner.cpp
+++ b/src/ops_infer/linear/linear_ops_runner.cpp
@@ -148,7 +148,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmul910B()
     KernelGraphNode &matmulNode = kernelGraph_.nodes.at(0);
 
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    matmulNode.inTensors = {&xTensor, &weightTensor, &nullTensor_, &nullTensor_ };
+    matmulNode.inTensors = {&xTensor, &weightTensor};
     matmulNode.outTensors = {&outTensor};
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (xNeedMergeAxis_) {
@@ -202,11 +202,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulWeightNdNot910B()
     }
 
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    if (GetSingleton<Config>().Is910A()) {
-        matmulNode.inTensors = {&transdataAOutTensor, &transdataBOutTensor};
-    } else {
-        matmulNode.inTensors = {&transdataAOutTensor, &transdataBOutTensor, &nullTensor_, &nullTensor_ };
-    }
+    matmulNode.inTensors = {&transdataAOutTensor, &transdataBOutTensor};
     matmulNode.outTensors = {&matmulOutTensor};
 
     transdataOutNode.opDesc = {0, "TransdataOperation", transdataNzToNdParam_};
@@ -244,11 +240,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulWeightNzNot910B()
     }
 
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    if (GetSingleton<Config>().Is910A()) {
-        matmulNode.inTensors = {&transdataAOutTensor, &weightTensor};
-    } else {
-        matmulNode.inTensors = {&transdataAOutTensor, &weightTensor, &nullTensor_, &nullTensor_ };
-    }
+    matmulNode.inTensors = {&transdataAOutTensor, &weightTensor};
     matmulNode.outTensors = {&matmulOutTensor};
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (isWeightNz_) {
@@ -281,7 +273,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulElewiseAdd910B()
     KernelGraphNode &addNode = kernelGraph_.nodes.at(1);
 
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    matmulNode.inTensors = {&xTensor, &weightTensor, &nullTensor_, &nullTensor_};
+    matmulNode.inTensors = {&xTensor, &weightTensor};
     matmulNode.outTensors = {&matmulOutTensor};
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (xNeedMergeAxis_) {
@@ -345,7 +337,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulElewiseAddWeightNdNot910B()
     }
 
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    matmulNode.inTensors = {&transdataAOutTensor, &transdataBOutTensor, &nullTensor_, &nullTensor_};
+    matmulNode.inTensors = {&transdataAOutTensor, &transdataBOutTensor};
     matmulNode.outTensors = {&matmulOutTensor};
 
     transdataOutNode.opDesc = {0, "TransdataOperation", transdataNzToNdParam_};
@@ -394,7 +386,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulElewiseAddWeightNzNot910B()
     }
 
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    matmulNode.inTensors = {&transdataAOutTensor, &weightTensor, &nullTensor_, &nullTensor_ };
+    matmulNode.inTensors = {&transdataAOutTensor, &weightTensor};
     matmulNode.outTensors = {&matmulOutTensor};
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (isWeightNz_) {
@@ -432,7 +424,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulWithBias()
     matmulParam_.withBias = true;
     matmulParam_.matmulType = AsdOps::OpParam::MatMul::MatMulType::MATMUL_WITH_BIAS;
     matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-    matmulNode.inTensors = {&xTensor, &weightTensor, &biasTensor, &nullTensor_};
+    matmulNode.inTensors = {&xTensor, &weightTensor, &biasTensor};
     matmulNode.outTensors = {&outTensor};
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (xNeedMergeAxis_) {
@@ -481,7 +473,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulAccum()
 
         matmulParam_.matmulType = AsdOps::OpParam::MatMul::MatMulType::MATMUL_ACCUM_ATOMIC;
         matmulNode.opDesc = {1, "MatMulOperation", matmulParam_};
-        matmulNode.inTensors = {&transposedXtensor, &weightTensor, &accumTensor, &nullTensor_};
+        matmulNode.inTensors = {&transposedXtensor, &weightTensor, &accumTensor};
         matmulNode.outTensors = {&outTensor};
         matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
         if (xNeedMergeAxis_) {
@@ -496,7 +488,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulAccum()
 
         matmulParam_.matmulType = AsdOps::OpParam::MatMul::MatMulType::MATMUL_ACCUM_ATOMIC;
         matmulNode.opDesc = {0, "MatMulOperation", matmulParam_};
-        matmulNode.inTensors = {&xTensor, &weightTensor, &accumTensor, &nullTensor_};
+        matmulNode.inTensors = {&xTensor, &weightTensor, &accumTensor};
         matmulNode.outTensors = {&outTensor};
         matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
         if (xNeedMergeAxis_) {
@@ -522,7 +514,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulEin()
  
     matmulParam_.matmulType = AsdOps::OpParam::MatMul::MatMulType::MATMUL_EIN_SUM;
     matmulNode.opDesc = { 0, "MatMulOperation", matmulParam_ };
-    matmulNode.inTensors = { &xTensor, &weightTensor, &nullTensor_, &nullTensor_ };
+    matmulNode.inTensors = { &xTensor, &weightTensor};
     matmulNode.outTensors = { &outTensor };
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (isWeightNz_) {
@@ -552,7 +544,7 @@ Status LinearOpsRunner::SetupKernelGraphMatmulEinElewiseAdd()
  
     matmulParam_.matmulType = AsdOps::OpParam::MatMul::MatMulType::MATMUL_EIN_SUM;
     matmulNode.opDesc = { 0, "MatMulOperation", matmulParam_ };
-    matmulNode.inTensors = { &xTensor, &weightTensor, &nullTensor_, &nullTensor_ };
+    matmulNode.inTensors = { &xTensor, &weightTensor};
     matmulNode.outTensors = { &matmuloutTensor };
     matmulNode.inTensorViewFuncs.resize(matmulNode.inTensors.size());
     if (isWeightNz_) {
diff --git a/src/ops_infer/topk_topp_sampling/topk_topp_sampling_ops_runner.cpp b/src/ops_infer/topk_topp_sampling/topk_topp_sampling_ops_runner.cpp
index bbf7f695..bfd4987f 100644
--- a/src/ops_infer/topk_topp_sampling/topk_topp_sampling_ops_runner.cpp
+++ b/src/ops_infer/topk_topp_sampling/topk_topp_sampling_ops_runner.cpp
@@ -14,15 +14,14 @@
 #include "atb/utils/log.h"
 
 namespace atb {
-static const uint64_t INTERNAL_TENSOR_COUNT = 4;
+static const uint64_t INTERNAL_TENSOR_COUNT = 5;
 static const uint64_t NODE_COUNT = 5;
 static const uint64_t EXPONENTIAL_INTERNAL_TENSOR_COUNT = 11;
 static const uint64_t EXPONENTIAL_NODE_COUNT = 11;
 static const uint64_t LOGPROBS_EXPONENTIAL_INTERNAL_TENSOR_COUNT = 16;
 static const uint64_t LOGPROBS_EXPONENTIAL_OUT_TENSOR_COUNT = 3;
 static const uint64_t LOGPROBS_EXPONENTIAL_NODE_COUNT = 17;
-static const uint64_t MULTINOMIAL_INTERNAL_TENSOR_COUNT = 7;
-static const uint64_t LOGPROBS_MULTINOMIAL_INTERNAL_TENSOR_COUNT = 8;
+static const uint64_t MULTINOMIAL_INTERNAL_TENSOR_COUNT = 8;
 static const uint64_t MULTINOMIAL_NODE_COUNT = 8;
 static const uint64_t LOGPROBS_MULTINOMIAL_OUT_TENSOR_COUNT = 3;
 static const uint64_t LOGPROBS_MULTINOMIAL_NODE_COUNT = 9;
@@ -186,6 +185,7 @@ Status TopkToppSamplingOpsRunner::SetupBatchTopKMultinomialSampling()
     auto &topKProbsFilledSortedTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
     auto &cumsumedProbsTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
     auto &indicesTopPSampledTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
+    auto &selectRangeTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
 
     kernelGraph_.nodes.resize(MULTINOMIAL_NODE_COUNT); // exponential sampling has 8 nodes
     int64_t nodeNum = 0;
@@ -234,12 +234,10 @@ Status TopkToppSamplingOpsRunner::SetupBatchTopKMultinomialSampling()
     };
 
     AtbOps::OpParam::Toppsample toppParam;
-    toppParam.randSeed.resize(param_.randSeeds.size());
-    std::transform(param_.randSeeds.begin(), param_.randSeeds.end(), toppParam.randSeed.begin(),
-                   [](uint32_t value) { return static_cast<uint64_t>(value); });
+    toppParam.randSeed = param_.randSeeds;
     mixTopPNode.opDesc = {0, "ToppsampleOperation", toppParam};
     mixTopPNode.inTensors = {&cumsumedProbsTensor, &topPTensor};
-    mixTopPNode.outTensors = {&indicesTopPSampledTensor};
+    mixTopPNode.outTensors = {&indicesTopPSampledTensor, &selectRangeTensor};
 
     indexTopPGatherNode.opDesc = {0, "GatherOperation", AsdOps::OpParam::Gather()};
     indexTopPGatherNode.inTensors = {&indicesSortedTensor, &indicesTopPSampledTensor};
@@ -438,7 +436,7 @@ Status TopkToppSamplingOpsRunner::SetupLogProbsBatchTopKMultinomialSampling()
     auto &probsSampledTensor = kernelGraph_.outTensors.at(outTensorNum++);      // [batch, 1]
     auto &logProbsTensor = kernelGraph_.outTensors.at(outTensorNum++); // [batch, logprobsSize]
 
-    kernelGraph_.internalTensors.resize(LOGPROBS_MULTINOMIAL_INTERNAL_TENSOR_COUNT);
+    kernelGraph_.internalTensors.resize(MULTINOMIAL_INTERNAL_TENSOR_COUNT);
     int64_t internalTensorNum = 0;
     auto &probsSortedTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
     auto &indicesSortedTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
@@ -535,12 +533,13 @@ Status TopkToppSamplingOpsRunner::SetupSingleTopKSampling()
     auto &indicesSampledI32Tensor = kernelGraph_.outTensors.at(outTensorNum++); // [batch, 1]
     auto &probsSampledTensor = kernelGraph_.outTensors.at(outTensorNum++);      // [batch, 1]
 
-    kernelGraph_.internalTensors.resize(INTERNAL_TENSOR_COUNT); // topp has 4 internel tensors
+    kernelGraph_.internalTensors.resize(MULTINOMIAL_INTERNAL_TENSOR_COUNT); // topp has 4 internel tensors
     int64_t internalTensorNum = 0;
     auto &probsSortedTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
     auto &indicesSortedTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
     auto &probsSumedTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
     auto &indicesSortedSampledTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
+    auto &selectRangeTensor = kernelGraph_.internalTensors.at(internalTensorNum++);
 
     kernelGraph_.nodes.resize(NODE_COUNT); // topp has 5 nodes
     int64_t nodeNum = 0;
@@ -573,12 +572,10 @@ Status TopkToppSamplingOpsRunner::SetupSingleTopKSampling()
 
     AtbOps::OpParam::Toppsample toppParam;
     std::vector<uint32_t> randSeeds = {param_.randSeed};
-    toppParam.randSeed.resize(randSeeds.size());
-    std::transform(randSeeds.begin(), randSeeds.end(), toppParam.randSeed.begin(),
-                   [](uint32_t value) { return static_cast<uint64_t>(value); });
+    toppParam.randSeed = randSeeds;
     toppSamplingNode.opDesc = {0, "ToppsampleOperation", toppParam};
     toppSamplingNode.inTensors = {&probsSumedTensor, &pTensor};
-    toppSamplingNode.outTensors = {&indicesSortedSampledTensor};
+    toppSamplingNode.outTensors = {&indicesSortedSampledTensor, &selectRangeTensor};
 
     gatherIndicesNode.opDesc = {0, "GatherOperation", AsdOps::OpParam::Gather()};
     gatherIndicesNode.inTensors = {&indicesSortedTensor, &indicesSortedSampledTensor};
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_accum.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_accum.py
index e30e049a..ad6c50c8 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_accum.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_accum.py
@@ -96,11 +96,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float()],
             [2],
         )
 
@@ -117,11 +117,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float()],
             [2],
         )
 
@@ -138,11 +138,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float()],
             [2],
         )
 
@@ -159,11 +159,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_C.float()],
             [2],
         )
 
@@ -180,11 +180,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.float16)
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float()],
             [2],
         )
 
@@ -201,11 +201,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.float16)
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float()],
             [2],
         )
 
@@ -222,11 +222,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.float16)
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float()],
             [2],
         )
 
@@ -243,11 +243,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.float16)
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float()],
             [2],
         )
 
@@ -264,11 +264,11 @@ class TestPpMatmulAccum(op_test.OpTest):
                 "matmulType": MATMUL_ACCUM_ATOMIC,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), ta, tb, torch.float16)
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_C.float()],
             [2],
         )
 
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_bf16.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_bf16.py
index 8758fb5a..dc19f7e1 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_bf16.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_bf16.py
@@ -106,11 +106,11 @@ class TestPpMatmulBf16(op_test.OpTest):
                     "MatMulOperation",
                     {"transposeA": trans_A, "transposeB": trans_B, "oriShape": [msize, ksize, nsize]},
                 )
-                self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+                self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
                 self.set_output_formats([self.format_nd])
                 self.__gen_test_data((bsize, msize, ksize, nsize), trans_A, trans_B)
                 self.execute(
-                    [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+                    [self.bat_A, self.bat_B],
                     [torch.zeros(self.bat_C.shape).bfloat16()],
                 )
 
@@ -123,11 +123,11 @@ class TestPpMatmulBf16(op_test.OpTest):
                     "MatMulOperation",
                     {"transposeA": trans_A, "transposeB": trans_B, "oriShape": [msize, ksize, nsize]},
                 )
-                self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+                self.set_input_formats([self.format_nd, self.format_nd])
                 self.set_output_formats([self.format_nd])
                 self.__gen_test_data((bsize, msize, ksize, nsize), trans_A, trans_B)
                 self.execute(
-                    [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+                    [self.bat_A, self.bat_B],
                     [torch.zeros(self.bat_C.shape).bfloat16()],
                 )
 
@@ -140,11 +140,11 @@ class TestPpMatmulBf16(op_test.OpTest):
                     "MatMulOperation",
                     {"transposeA": trans_A, "transposeB": trans_B, "oriShape": [msize, ksize, nsize]},
                 )
-                self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+                self.set_input_formats([self.format_nd, self.format_nd])
                 self.set_output_formats([self.format_nd])
                 self.__gen_test_data((bsize, msize, ksize, nsize), trans_A, trans_B)
                 self.execute(
-                    [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+                    [self.bat_A, self.bat_B],
                     [torch.zeros(self.bat_C.shape).bfloat16()],
                 )
 
@@ -156,11 +156,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -172,11 +172,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -188,11 +188,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -204,11 +204,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -220,11 +220,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -236,11 +236,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -252,11 +252,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -268,11 +268,11 @@ class TestPpMatmulBf16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), self.trans_A, self.trans_B)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -285,11 +285,11 @@ class TestPpMatmulBf16(op_test.OpTest):
                     "MatMulOperation",
                     {"transposeA": trans_A, "transposeB": trans_B, "oriShape": [msize, ksize, nsize]},
                 )
-                self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+                self.set_input_formats([self.format_nd, self.format_nz])
                 self.set_output_formats([self.format_nd])
                 self.__gen_test_data((bsize, msize, ksize, nsize), trans_A, trans_B)
                 self.execute(
-                    [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+                    [self.bat_A, self.bat_B],
                     [torch.zeros(self.bat_C.shape).bfloat16()],
                 )
 
@@ -302,11 +302,11 @@ class TestPpMatmulBf16(op_test.OpTest):
                     "MatMulOperation",
                     {"transposeA": trans_A, "transposeB": trans_B, "oriShape": [msize, ksize, nsize]},
                 )
-                self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+                self.set_input_formats([self.format_nd, self.format_nz])
                 self.set_output_formats([self.format_nd])
                 self.__gen_test_data((bsize, msize, ksize, nsize), trans_A, trans_B)
                 self.execute(
-                    [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+                    [self.bat_A, self.bat_B],
                     [torch.zeros(self.bat_C.shape).bfloat16()],
                 )
 
@@ -319,11 +319,11 @@ class TestPpMatmulBf16(op_test.OpTest):
                     "MatMulOperation",
                     {"transposeA": trans_A, "transposeB": trans_B, "oriShape": [msize, ksize, nsize]},
                 )
-                self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+                self.set_input_formats([self.format_nd, self.format_nz])
                 self.set_output_formats([self.format_nd])
                 self.__gen_test_data((bsize, msize, ksize, nsize), trans_A, trans_B)
                 self.execute(
-                    [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+                    [self.bat_A, self.bat_B],
                     [torch.zeros(self.bat_C.shape).bfloat16()],
                 )
 
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_ein_sum_f16.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_ein_sum_f16.py
index d3654f80..46c66a30 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_ein_sum_f16.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_ein_sum_f16.py
@@ -104,11 +104,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "matmulType": 4,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -126,11 +126,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "matmulType": 4,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -148,11 +148,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "matmulType": 4,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -170,11 +170,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "matmulType": 4,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -192,11 +192,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "matmulType": 4,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -214,11 +214,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "matmulType": 4,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -237,11 +237,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "enShuffleK": True,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -260,11 +260,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "enShuffleK": True,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -283,11 +283,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "enShuffleK": True,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
@@ -306,11 +306,11 @@ class TestPpMatmulEinSumFp16(op_test.OpTest):
                 "enShuffleK": True,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype)
         self.execute(
-            [self.bat_A, self.bat_B, torch.Tensor(), torch.Tensor()],
+            [self.bat_A, self.bat_B],
             [torch.zeros(self.bat_C.shape).to(dtype)],
         )
 
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_f16.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_f16.py
index 500c9d07..a72ac5f8 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_f16.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_f16.py
@@ -83,11 +83,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -99,11 +99,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -115,11 +115,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -131,11 +131,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
     @op_test.only_910b
@@ -146,11 +146,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -162,11 +162,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -178,11 +178,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -194,11 +194,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -210,11 +210,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -226,11 +226,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -242,11 +242,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -258,11 +258,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -274,11 +274,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -290,11 +290,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -306,11 +306,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -322,11 +322,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -338,11 +338,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -354,11 +354,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -370,11 +370,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nz])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -386,11 +386,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -402,11 +402,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -418,11 +418,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -434,11 +434,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -450,11 +450,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -466,11 +466,11 @@ class TestPpMatmulF16(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_i8.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_i8.py
index 4c8b7d7d..4340e356 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_i8.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_i8.py
@@ -106,7 +106,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -129,7 +129,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -152,7 +152,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -175,7 +175,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -198,9 +198,9 @@ class TestPpMatmulI8(op_test.OpTest):
                 [
                     torch.tensor(self.bat_A, dtype=torch.int8),
                     torch.tensor(self.bat_B, dtype=torch.int8),
-                    torch.Tensor(),
+                    torch.tensor(self.bat_bias, dtype=torch.int32),
                     torch.tensor(self.bat_scale, dtype=torch.int64),
-                    torch.Tensor()
+                    torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
                 ],
                 [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
             )
@@ -223,9 +223,9 @@ class TestPpMatmulI8(op_test.OpTest):
                 [
                     torch.tensor(self.bat_A, dtype=torch.int8),
                     torch.tensor(self.bat_B, dtype=torch.int8),
-                    torch.Tensor(),
+                    torch.tensor(self.bat_bias, dtype=torch.int32),
                     torch.tensor(self.bat_scale, dtype=torch.int64),
-                    torch.Tensor()
+                    torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
                 ],
                 [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
             )
@@ -250,7 +250,7 @@ class TestPpMatmulI8(op_test.OpTest):
                     torch.tensor(self.bat_B, dtype=torch.int8),
                     torch.tensor(self.bat_bias, dtype=torch.int32),
                     torch.tensor(self.bat_scale, dtype=torch.int64),
-                    torch.Tensor()
+                    torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
                 ],
                 [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
             )
@@ -274,7 +274,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -298,7 +298,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -328,7 +328,7 @@ class TestPpMatmulI8(op_test.OpTest):
                     torch.tensor(self.bat_B, dtype=torch.int8),
                     torch.tensor(self.bat_bias, dtype=torch.int32),
                     torch.tensor(self.bat_scale, dtype=torch.int64),
-                    torch.Tensor()
+                    torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
                 ],
                 [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
             )
@@ -358,7 +358,7 @@ class TestPpMatmulI8(op_test.OpTest):
                     torch.tensor(self.bat_B, dtype=torch.int8),
                     torch.tensor(self.bat_bias, dtype=torch.int32),
                     torch.tensor(self.bat_scale, dtype=torch.int64),
-                    torch.Tensor()
+                    torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
                 ],
                 [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
             )
@@ -387,7 +387,7 @@ class TestPpMatmulI8(op_test.OpTest):
                 torch.tensor(self.bat_B, dtype=torch.int8),
                 torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
@@ -416,9 +416,9 @@ class TestPpMatmulI8(op_test.OpTest):
             [
                 torch.tensor(self.bat_A, dtype=torch.int8),
                 torch.tensor(self.bat_B, dtype=torch.int8),
-                torch.Tensor(),
+                torch.tensor(self.bat_bias, dtype=torch.int32),
                 torch.tensor(self.bat_scale, dtype=torch.int64),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape, dtype=torch.float16)],
         )
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_i8_nz.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_i8_nz.py
index 83d08b88..968fdee1 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_i8_nz.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_i8_nz.py
@@ -117,7 +117,7 @@ class TestPpMatmulI8(op_test.OpTest):
                           torch.tensor(self.bat_B, dtype=torch.int8),
                           torch.tensor(self.bat_bias, dtype=torch.int32),
                           torch.tensor(self.bat_scale, dtype=torch.int64),
-                          torch.Tensor()],
+                          torch.tensor(self.bat_pertoken_descale, dtype=torch.float)],
                          [torch.zeros(self.bat_C.shape, dtype=torch.float16)])
 
 
diff --git a/tests/apitest/kernelstest/matmul/test_pp_matmul_with_bias.py b/tests/apitest/kernelstest/matmul/test_pp_matmul_with_bias.py
index d87b4d60..cfc36974 100644
--- a/tests/apitest/kernelstest/matmul/test_pp_matmul_with_bias.py
+++ b/tests/apitest/kernelstest/matmul/test_pp_matmul_with_bias.py
@@ -87,11 +87,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -108,11 +108,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -129,11 +129,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -150,11 +150,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.half(), self.bat_B.half(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
@@ -171,11 +171,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype=torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -192,11 +192,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype=torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -213,11 +213,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype=torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -234,11 +234,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype=torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
@@ -255,11 +255,11 @@ class TestPpMatmulF16(op_test.OpTest):
                 "matmulType": MATMUL_WITH_BIAS,
             },
         )
-        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nd, self.format_nd, self.format_nd])
         self.set_output_formats([self.format_nd])
         self.__gen_test_data((bsize, msize, ksize, nsize), dtype=torch.bfloat16)
         self.execute(
-            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float(), torch.Tensor()],
+            [self.bat_A.bfloat16(), self.bat_B.bfloat16(), self.bat_bias.float()],
             [torch.zeros(self.bat_C.shape).bfloat16()],
         )
 
diff --git a/tests/apitest/kernelstest/matmul/test_ppmatmul_310p.py b/tests/apitest/kernelstest/matmul/test_ppmatmul_310p.py
index 78f0c42e..faf7cbfc 100644
--- a/tests/apitest/kernelstest/matmul/test_ppmatmul_310p.py
+++ b/tests/apitest/kernelstest/matmul/test_ppmatmul_310p.py
@@ -76,11 +76,11 @@ class TestPpMatmul310p(op_test.OpTest):
             "MatMulOperation",
             {"transposeA": self.trans_A, "transposeB": self.trans_B, "oriShape": [msize, ksize, nsize]},
         )
-        self.set_input_formats([self.format_nz, self.format_nz, self.format_nd, self.format_nd])
+        self.set_input_formats([self.format_nz, self.format_nz])
         self.set_output_formats([self.format_nz])
         self.__gen_test_data((bsize, msize, ksize, nsize))
         self.execute(
-            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half(), torch.Tensor(), torch.Tensor()],
+            [torch.tensor(self.bat_A).half(), torch.tensor(self.bat_B).half()],
             [torch.zeros(self.bat_C.shape).half()],
         )
 
diff --git a/tests/apitest/kernelstest/matmul/test_ppmatmul_310p_dequant.py b/tests/apitest/kernelstest/matmul/test_ppmatmul_310p_dequant.py
index 0aaaa110..72828075 100644
--- a/tests/apitest/kernelstest/matmul/test_ppmatmul_310p_dequant.py
+++ b/tests/apitest/kernelstest/matmul/test_ppmatmul_310p_dequant.py
@@ -107,7 +107,7 @@ class TestPpMatmul310pDequant(op_test.OpTest):
         logging.debug(self.bat_A.shape)
         self.execute([torch.tensor(self.bat_A).to(torch.int8), torch.tensor(self.bat_B).to(torch.int8),
                       torch.tensor(self.bias).to(torch.int32), torch.tensor(self.scale).to(torch.int64),
-                      torch.Tensor()],
+                      torch.tensor(self.bat_pertoken_descale, dtype=torch.float)],
                      [torch.zeros(self.bat_C.shape).half()])
 
     @op_test.only_310p
@@ -126,7 +126,7 @@ class TestPpMatmul310pDequant(op_test.OpTest):
         logging.debug(self.bat_A.shape)
         self.execute([torch.tensor(self.bat_A).to(torch.int8), torch.tensor(self.bat_B).to(torch.int8),
                       torch.tensor(self.bias).to(torch.int32), torch.tensor(self.scale).to(torch.int64),
-                      torch.Tensor()],
+                      torch.tensor(self.bat_pertoken_descale, dtype=torch.float)],
                      [torch.zeros(self.bat_C.shape).half()])
 
     @op_test.only_310p
@@ -145,7 +145,7 @@ class TestPpMatmul310pDequant(op_test.OpTest):
         logging.debug(self.bat_A.shape)
         self.execute([torch.tensor(self.bat_A).to(torch.int8), torch.tensor(self.bat_B).to(torch.int8),
                       torch.tensor(self.bias).to(torch.int32), torch.tensor(self.scale).to(torch.int64),
-                      torch.Tensor()],
+                      torch.tensor(self.bat_pertoken_descale, dtype=torch.float)],
                      [torch.zeros(self.bat_C.shape).half()])
     @op_test.only_310p
 
@@ -165,7 +165,7 @@ class TestPpMatmul310pDequant(op_test.OpTest):
         logging.debug(self.bat_A.shape)
         self.execute([torch.tensor(self.bat_A).to(torch.int8), torch.tensor(self.bat_B).to(torch.int8),
                       torch.tensor(self.bias).to(torch.int32), torch.tensor(self.scale).to(torch.int64),
-                      torch.Tensor()],
+                      torch.tensor(self.bat_pertoken_descale, dtype=torch.float)],
                      [torch.zeros(self.bat_C.shape).half()])
 
     @op_test.only_310p
@@ -185,7 +185,7 @@ class TestPpMatmul310pDequant(op_test.OpTest):
         logging.debug(self.bat_A.shape)
         self.execute([torch.tensor(self.bat_A).to(torch.int8), torch.tensor(self.bat_B).to(torch.int8),
                       torch.tensor(self.bias).to(torch.int32), torch.tensor(self.scale).to(torch.int64),
-                      torch.Tensor()],
+                      torch.tensor(self.bat_pertoken_descale, dtype=torch.float)],
                      [torch.zeros(self.bat_C.shape).half()])
 
 
diff --git a/tests/apitest/kernelstest/matmul/test_ppmatmul_910a_dequant.py b/tests/apitest/kernelstest/matmul/test_ppmatmul_910a_dequant.py
index 0715365b..d8ca039c 100644
--- a/tests/apitest/kernelstest/matmul/test_ppmatmul_910a_dequant.py
+++ b/tests/apitest/kernelstest/matmul/test_ppmatmul_910a_dequant.py
@@ -124,7 +124,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                     torch.tensor(self.bat_B).to(torch.int8),
                     torch.tensor(self.bias).to(torch.int32),
                     torch.tensor(self.scale).to(torch.float32),
-                    torch.Tensor()
+                    torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
                 ],
                 [torch.zeros(self.bat_C.shape).to(torch.half)],
             )
@@ -153,7 +153,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                 torch.tensor(self.bat_B).to(torch.int8),
                 torch.tensor(self.bias).to(torch.int32),
                 torch.tensor(self.scale).float(),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape).half()],
         )
@@ -182,7 +182,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                 torch.tensor(self.bat_B).to(torch.int8),
                 torch.tensor(self.bias).to(torch.int32),
                 torch.tensor(self.scale).float(),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape).half()],
         )
@@ -211,7 +211,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                 torch.tensor(self.bat_B).to(torch.int8),
                 torch.tensor(self.bias).to(torch.int32),
                 torch.tensor(self.scale).float(),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape).half()],
         )
@@ -240,7 +240,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                 torch.tensor(self.bat_B).to(torch.int8),
                 torch.tensor(self.bias).to(torch.int32),
                 torch.tensor(self.scale).float(),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape).half()],
         )
@@ -269,7 +269,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                 torch.tensor(self.bat_B).to(torch.int8),
                 torch.tensor(self.bias).to(torch.int32),
                 torch.tensor(self.scale).float(),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape).half()],
         )
@@ -298,7 +298,7 @@ class TestPpMatmul910aDequant(op_test.OpTest):
                 torch.tensor(self.bat_B).to(torch.int8),
                 torch.tensor(self.bias).to(torch.int32),
                 torch.tensor(self.scale).float(),
-                torch.Tensor()
+                torch.tensor(self.bat_pertoken_descale, dtype=torch.float)
             ],
             [torch.zeros(self.bat_C.shape).half()],
         )
diff --git a/tests/apitest/kernelstest/matmul/test_zzpp_matmul_i8_bf16.py b/tests/apitest/kernelstest/matmul/test_zzpp_matmul_i8_bf16.py
index 0d7e9fd0..293db441 100644
--- a/tests/apitest/kernelstest/matmul/test_zzpp_matmul_i8_bf16.py
+++ b/tests/apitest/kernelstest/matmul/test_zzpp_matmul_i8_bf16.py
@@ -14,6 +14,7 @@ import logging
 import op_test
 
 
+
 def process_deq_scale(deq_scale) -> np.ndarray:
     new_deq_scale = np.frombuffer(deq_scale.tobytes(), dtype=np.uint32)
     return new_deq_scale.astype(np.int64)
@@ -72,7 +73,7 @@ class TestPpMatmulI8(op_test.OpTest):
         self.bat_bias = torch.stack(bat_bias)
         pertoken_descale = torch.empty(msize)
         bat_pertoken_descale.append(pertoken_descale)
-        self.bat_pertoken_descale = torch.Tensor()
+        self.bat_pertoken_descale = torch.stack(bat_pertoken_descale)
         return
 
     def golden_calc(self, in_tensors):
@@ -198,7 +199,7 @@ class TestPpMatmulI8(op_test.OpTest):
             self.set_output_formats([self.format_nd])
             self.__gen_test_data((bsize, msize, ksize, nsize))
             self.execute(
-                [self.bat_A, self.bat_B, torch.Tensor(), self.bat_scale, self.bat_pertoken_descale],
+                [self.bat_A, self.bat_B, self.bat_bias, self.bat_scale, self.bat_pertoken_descale],
                 [torch.zeros(self.bat_C.shape, dtype=torch.bfloat16)],
             )
 
@@ -223,7 +224,7 @@ class TestPpMatmulI8(op_test.OpTest):
             self.set_output_formats([self.format_nd])
             self.__gen_test_data((bsize, msize, ksize, nsize))
             self.execute(
-                [self.bat_A, self.bat_B, torch.Tensor(), self.bat_scale, self.bat_pertoken_descale],
+                [self.bat_A, self.bat_B, self.bat_bias, self.bat_scale, self.bat_pertoken_descale],
                 [torch.zeros(self.bat_C.shape, dtype=torch.bfloat16)],
             )
 
diff --git a/tests/apitest/kernelstest/mix/test_toppsample.py b/tests/apitest/kernelstest/mix/test_toppsample.py
index d895d0c8..ff7368c8 100644
--- a/tests/apitest/kernelstest/mix/test_toppsample.py
+++ b/tests/apitest/kernelstest/mix/test_toppsample.py
@@ -95,7 +95,7 @@ class TestToppSample(op_test.OpTest):
         res_index = np.sum(bool_judge_one, axis=-1, keepdims=True) # 要判断是否为0，为0直接返回0
         res_index[res_index < 0] = 0
 
-        return [torch.tensor(res_index).int()]
+        return [torch.tensor(res_index).int(), torch.tensor(res_select_range).int()]
 
     def golden_compare(self, out_tensors, golden_out_tensors):
         if len(out_tensors) != len(golden_out_tensors):
@@ -122,7 +122,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a        
@@ -136,7 +136,7 @@ class TestToppSample(op_test.OpTest):
         topp = torch.empty(shape1, dtype=torch.half).uniform_(0, 1)
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a        
@@ -152,7 +152,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -168,7 +168,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -184,7 +184,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -200,7 +200,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a                     
@@ -216,7 +216,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int()], torch.zeros(shape1).int())
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -231,7 +231,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b  
     def test_topp_bf16_case65535(self):
@@ -245,7 +245,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b      
     def test_topp_bf16_case254208(self):
@@ -258,7 +258,7 @@ class TestToppSample(op_test.OpTest):
         topp = torch.empty(shape1, dtype=torch.bfloat16).uniform_(0, 1)
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b      
     def test_topp_bf16_case65024(self):
@@ -271,7 +271,7 @@ class TestToppSample(op_test.OpTest):
         topp = torch.empty(shape1, dtype=torch.bfloat16).uniform_(0, 1)
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case128000(self):
@@ -285,7 +285,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int(),])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case32000(self):
@@ -299,7 +299,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case158464(self):
@@ -313,7 +313,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b                   
     def test_topp_bf16_case100352(self):
@@ -327,7 +327,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_case_bf16_bug_fix_182744(self):
@@ -341,7 +341,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
 
 if __name__ == '__main__':
diff --git a/tests/apitest/kernelstest/mix/test_toppsample_one_seed.py b/tests/apitest/kernelstest/mix/test_toppsample_one_seed.py
index 060a10fa..08fc8551 100644
--- a/tests/apitest/kernelstest/mix/test_toppsample_one_seed.py
+++ b/tests/apitest/kernelstest/mix/test_toppsample_one_seed.py
@@ -43,7 +43,7 @@ class TestToppSample(op_test.OpTest):
         res_index = np.sum(bool_judge_one, axis=-1, keepdims=True) # 要判断是否为0，为0直接返回0
         res_index[res_index < 0] = 0
 
-        return [torch.tensor(res_index).int()]
+        return [torch.tensor(res_index).int(), torch.tensor(res_select_range).int()]
 
     def golden_compare(self, out_tensors, golden_out_tensors):
         if len(out_tensors) != len(golden_out_tensors):
@@ -70,7 +70,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a        
@@ -84,7 +84,7 @@ class TestToppSample(op_test.OpTest):
         topp = torch.empty(shape1, dtype=torch.half).uniform_(0, 1)
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a        
@@ -100,7 +100,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -116,7 +116,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -132,7 +132,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -148,7 +148,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a                     
@@ -164,7 +164,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.skip_310b
     @op_test.skip_910a
@@ -179,7 +179,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed), topp], 
-                    [torch.zeros(shape1).int()])
+                    [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b 
     def test_topp_bf16_case65535(self):
@@ -193,7 +193,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                    [torch.zeros(shape1).int()])
+                    [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b       
     def test_topp_bf16_case254208(self):
@@ -206,7 +206,7 @@ class TestToppSample(op_test.OpTest):
         topp = torch.empty(shape1, dtype=torch.bfloat16).uniform_(0, 1)
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b      
     def test_topp_bf16_case65024(self):
@@ -220,7 +220,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case128000(self):
@@ -234,7 +234,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case32000(self):
@@ -248,7 +248,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case158464(self):
@@ -262,7 +262,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp],  
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b                    
     def test_topp_bf16_case100352(self):
@@ -276,7 +276,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
     @op_test.only_910b
     def test_topp_bf16_case_bug_fix_182744(self):
@@ -290,7 +290,7 @@ class TestToppSample(op_test.OpTest):
 
         self.set_param(OP_NAME, OP_PARAM0)
         self.execute([torch.from_numpy(cumsumed).bfloat16(), topp], 
-                     [torch.zeros(shape1).int()])
+                     [torch.zeros(shape1).int(), torch.zeros(shape1).int()])
 
 if __name__ == '__main__':
     unittest.main()
