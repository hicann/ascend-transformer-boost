0. 需要上传到服务器的【所需文件】
a) 镜像文件：llama_wmj_docker.tar
c) 模型权重文件：vicuna-7b.tar

1. 解压步骤
a) 把模型代码文件解压，生成 FastChat 文件夹，放到 /data/wmj/
b) 把模型权重文件解压，生成 vicuna-7b 文件夹，放到 /data/wmj/

2. 加载镜像
docker load -i llama_wmj_docker.tar
【镜像名：llama_wmj:latest】

3. 首次配置，运行容器
docker run --name llama_wmj_dev7 -it -d --net=host --device=/dev/davinci7 \
-w /home \
--device=/dev/davinci_manager \
--device=/dev/hisi_hdc \
--device=/dev/devmm_svm \
--entrypoint=/bin/bash \
-v /usr/local/dcmi:/usr/local/dcmi \
-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
-v /home/wmj/vicuna-7b:/home/ma-user/work/FastChat/vicuna-7b \
-v /home/wmj/FastChat:/home/ma-user/work/FastChat \
llama_wmj:latest

4. 用 root 用户权限进入容器
docker exec -it --user root llama_wmj_dev7 /bin/bash

5. 将模型文件目录vicuna-7b复制到ascend-transformer-acceleration/model/FastChat目录下
6. 看模型是否可以运行(推理)成功
cd ascend-transformer-acceleration/model/FastChat
执行：bash infer_7B.sh

原理：FastChat是个底座，它调用 vicuna 的权重和 /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/transformers 从而可以推理 vicuna。