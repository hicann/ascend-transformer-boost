test

    for testdir in $(ls -d *_test); do
        echo "Running tests in $testdir"
        for testfile in $(find "$testdir" -name "*_test.py"); do
            echo "Running $testfile"
            python3 -m unittest "$testfile"
        done
    done

SVector<bool> GraphOperation::GetEmptyOutTensorPermissions() const
{
    return emptyOutTensorPerms_;
}
 
void GraphOperation::InitEmptyOutTensorPerms()
{
    emptyOutTensorPerms_.reserve(opGraph_.outTensorNum);
    emptyOutTensorPerms_.resize(opGraph_.outTensorNum);
    for (size_t i = 0; i < emptyOutTensorPerms_.size(); ++i) {
        emptyOutTensorPerms_.at(i) = false;
    }
 
    for (size_t nodeId = 0; nodeId < opGraph_.nodes.size(); ++nodeId) {
        auto &node = opGraph_.nodes.at(nodeId);
        if (!node.operation) {
            ATB_LOG(WARN) << GetLogPrefix() << "node[" << nodeId << "] operation is null";
            continue;
        }
        OperationBase *opBase = dynamic_cast<OperationBase *>(node.operation);
        if (!opBase) {
            ATB_LOG(INFO) << GetLogPrefix() << "node[" << nodeId << "] operation is not OperationBase";
            continue;
        }
 
        SVector<bool> childOpEmptyOutTensorPerms = opBase->GetEmptyOutTensorPermissions();
        if (childOpEmptyOutTensorPerms.size() != node.outTensorIds.size()) {
            ATB_LOG(WARN) << GetLogPrefix() << "node[" << nodeId
                          << "] childOpEmptyOutTensorPerms.size:" << childOpEmptyOutTensorPerms.size()
                          << " != outTensorIds.size:" << node.outTensorIds.size();
            continue;
        }
 
        for (size_t i = 0; i < childOpEmptyOutTensorPerms.size(); ++i) {
            uint32_t outTensorId = node.outTensorIds.at(i);
            // graph out tensor id in range [inTensorNum, inTensorNum + outTensorNum)
            if (childOpEmptyOutTensorPerms.at(i) && outTensorId >= opGraph_.inTensorNum &&
                outTensorId < opGraph_.inTensorNum + opGraph_.outTensorNum) {
                emptyOutTensorPerms_.at(outTensorId - opGraph_.inTensorNum) = true;
                ATB_LOG(INFO) << GetLogPrefix() << "node[" << nodeId << "] " << node.operation->GetName()
                              << " outTensor[" << i << "] is allow empty";
            }
        }
    }
    ATB_LOG(INFO) << GetLogPrefix() << "emptyOutTensorPerms:" << emptyOutTensorPerms_;
}

void OperationBase::InitEmptyOutTensorPerms() const
{
    if (!operationIr_) {
        ATB_LOG(DEBUG) << GetLogPrefix() << "operationIr_ is null.";
        return;
    }
    if (!operationIr_->IsValid()) {
        ATB_LOG(ERROR) << GetLogPrefix() << "operationIr_ is invalid";
        return;
    }
    ATB_LOG(DEBUG) << GetLogPrefix() << "operationIr_ : " << operationIr_->ToString();
    const Mki::SVector<Mki::TensorInfoIr> &outTensorInfoIrs = operationIr_->GetOutTensorInfoIrs();
    if (GetOutputNum() != 0 && outTensorInfoIrs.size() != GetOutputNum()) {
        ATB_LOG(ERROR) << GetLogPrefix() << "GetOutTensorInfoIrs size: " << outTensorInfoIrs.size()
                       << " is not equal with GetOutputNum  : " << GetOutputNum();
        return;
    }
    emptyOutTensorPerms_.reserve(outTensorInfoIrs.size());
    emptyOutTensorPerms_.resize(outTensorInfoIrs.size());
    for (size_t outTensorId = 0; outTensorId < outTensorInfoIrs.size(); outTensorId++) {
        if (outTensorInfoIrs[outTensorId].isOptional) {
            emptyOutTensorPerms_.at(outTensorId) = true;
            ATB_LOG(INFO) << GetLogPrefix() << "emptyOutTensorPerms init outTensor[" << outTensorId
                          << "] is isOptional";
        } else {
            emptyOutTensorPerms_.at(outTensorId) = false;
        }
    }
    ATB_LOG(INFO) << GetLogPrefix() << "InitEmptyOutTensorPerms finished:" << emptyOutTensorPerms_;

}
 
SVector<bool> OperationBase::GetEmptyOutTensorPermissions() const
{
    if (emptyOutTensorPerms_.size() == 0) {
        InitEmptyOutTensorPerms();
    }
    return emptyOutTensorPerms_;
}

template <typename TensorType> Status OperationBase::CheckOutTensor(const SVector<TensorType> &outTensors) const
{
    Status st = NO_ERROR;
    SVector<bool> emptyTensorPerms = GetEmptyOutTensorPermissions();
    for (size_t outTensorId = 0; outTensorId < outTensors.size(); outTensorId++) {
        const auto &outTensor = outTensors.at(outTensorId);
        if (outTensorId < emptyTensorPerms.size() && emptyTensorPerms.at(outTensorId) &&
            TensorCheck::IsEmptyTensor(outTensor)) {
            ATB_LOG(INFO) << GetLogPrefix() << "outTensor [" << outTensorId << "] is allowed empty";
            continue;
        }
        st = TensorCheck::CheckTensorShape(outTensor);
        if (st != NO_ERROR) {
            ATB_LOG(ERROR) << GetLogPrefix() << "outTensor [" << outTensorId
                           << "] CheckTensorShape failed. ErrorType: " << st;
            return st;
        }
    }
    return NO_ERROR;
}

    st = CheckOutTensor(variantPack.outTensors);
    if (st != NO_ERROR) {
        return st;

template <typename TensorType>
Status OperationBase::ExecuteVariantPackInTensorCheck(const SVector<TensorType> &inTensors) const
{
    std::string Prefix = GetLogPrefix();
    if (inTensors.size() != runnerVariantPack_.inTensors.size()) {
        ATB_LOG(ERROR) << GetLogPrefix() << "execute inTensors.size:" << inTensors.size()
                       << " != setup inTensors.size:" << runnerVariantPack_.inTensors.size();
        return ERROR_INVALID_PARAM;
    }
    SVector<bool> emptyInTensorPerms = GetEmptyInTensorPermissions();
    for (size_t i = 0; i < inTensors.size(); i++) {
        const Tensor &variantPackInTensor = inTensors.at(i);
        if (Prefix.find("WithStride") == std::string::npos && // "WithStride" indicates non continuous tensors
            variantPackInTensor.dataSize != Utils::GetTensorSize(runnerVariantPack_.inTensors.at(i).desc)) {
            ATB_LOG(ERROR) << GetLogPrefix() << "execute variantPack.inTensors(" << i
                           << ").dataSize is Not equal to the setup dataSize";
            return ERROR_INVALID_PARAM;
        }
        if (i < emptyInTensorPerms.size() && emptyInTensorPerms.at(i) &&
            TensorCheck::IsEmptyTensor(variantPackInTensor)) {
            continue;
        }
        if (!variantPackInTensor.deviceData && !variantPackInTensor.hostData) {
            return ERROR_INVALID_PARAM;
        }
    }
    return NO_ERROR;
}

template <typename TensorType>
Status OperationBase::ExecuteVariantPackOutTensorCheck(const SVector<TensorType> &outTensors) const
{
    std::string Prefix = GetLogPrefix();
    if (outTensors.size() != runnerVariantPack_.outTensors.size()) {
        ATB_LOG(ERROR) << GetLogPrefix() << "execute outTensors.size:" << outTensors.size()
                       << " != setup outTensors.size:" << runnerVariantPack_.outTensors.size();
        return ERROR_INVALID_PARAM;
    }
    SVector<bool> emptyOutTensorPerms = GetEmptyOutTensorPermissions();
    for (size_t i = 0; i < outTensors.size(); i++) {
        const Tensor &variantPackOutTensor = outTensors.at(i);
        if (variantPackOutTensor.dataSize != Utils::GetTensorSize(runnerVariantPack_.outTensors.at(i).desc)) {
            ATB_LOG(ERROR) << GetLogPrefix() << "execute variantPack.outTensors(" << i
                           << ").dataSize is Not equal to the setup dataSize";
            return ERROR_INVALID_PARAM;
        }
        if (i < emptyOutTensorPerms.size() && emptyOutTensorPerms.at(i) &&
            TensorCheck::IsEmptyTensor(variantPackOutTensor)) {
            continue;
        }
        if (!variantPackOutTensor.deviceData && !variantPackOutTensor.hostData) {
            ATB_LOG(ERROR) << GetLogPrefix() << "execute variantPack.outTensors(" << i
                           << ") deviceData&hostData is null";
            return ERROR_INVALID_PARAM;
        }
    }
    return NO_ERROR;
}

Status OperationBase::ExecuteVariantPackCheck(const VariantPack &variantPack)
{
    Status st = NO_ERROR;
    st = ExecuteVariantPackInTensorCheck(variantPack.inTensors);
    if (st != NO_ERROR) {
            ATB_LOG(ERROR) << GetLogPrefix() << "ExecuteVariantPackCheck for inTensor failed, error code: " << st;
            return st;
        }
    st = ExecuteVariantPackOutTensorCheck(variantPack.outTensors);
    if (st != NO_ERROR) {
            ATB_LOG(ERROR) << GetLogPrefix() << "ExecuteVariantPackCheck for outTensor failed, error code: " << st;
            return st;
        }
    return st;
}

    inline static bool IsEmptyTensor(const Tensor &tensor)
    {
        bool emptyShape = (tensor.desc.shape.dimNum == 0);
        for (size_t i = 0; !emptyShape && i < tensor.desc.shape.dimNum; i++) {
            if (tensor.desc.shape.dims[i] == 0) {
                emptyShape = true;
            }
        }
        return emptyShape && tensor.dataSize == 0 && !tensor.deviceData && !tensor.hostData;
    }
    // The tensor has no dimensions or if any dimension is zero, both indicate an empty tensor.
    inline static bool IsEmptyTensor(const TensorDesc &tensorDesc)
    {
        if (tensorDesc.shape.dimNum == 0) {
            return true;
        }
        for (size_t i = 0; i < tensorDesc.shape.dimNum; i++) {
            if (tensorDesc.shape.dims[i] == 0) {
                return true;
            }
        }
        return false;
    }

Status TopkToppSamplingOperation::TopkToppLogProbsOutTensorCheck(const SVector<TensorDesc> &outTensorDescs) const
{
    if (outTensorDescs.at(LOG_PROBS_OUT_TENSOR_INDEX).shape.dimNum != LOG_PROBS_OUT_TENSOR_DIM &&
        (param_.topkToppSamplingType != atb::infer::TopkToppSamplingParam::BATCH_TOPK_MULTINOMIAL_LOGPROBS_SAMPLING &&
         param_.topkToppSamplingType != atb::infer::TopkToppSamplingParam::BATCH_TOPK_EXPONENTIAL_LOGPROBS_SAMPLING)) {
        ATB_LOG(ERROR) << "logProbsTensor shape dimNum: " << outTensorDescs.at(LOG_PROBS_OUT_TENSOR_INDEX).shape.dimNum
                       << ", which should be same as " << LOG_PROBS_OUT_TENSOR_DIM;
        return ERROR_INVALID_TENSOR_DIM_NUM;
    }
    if (outTensorDescs.at(LOG_PROBS_OUT_TENSOR_INDEX).shape.dims[LAST_DIM] != param_.logProbsSize &&
        (param_.topkToppSamplingType != atb::infer::TopkToppSamplingParam::BATCH_TOPK_MULTINOMIAL_LOGPROBS_SAMPLING &&
         param_.topkToppSamplingType != atb::infer::TopkToppSamplingParam::BATCH_TOPK_EXPONENTIAL_LOGPROBS_SAMPLING)) {
        ATB_LOG(ERROR) << "logProbsTensor dim:" << LAST_DIM << ", which should be same as " << param_.logProbsSize;
        return ERROR_INVALID_TENSOR_DIM;
    }
    return NO_ERROR;
}

    py::class_<TopkToppSamplingParam> topkToppSampling(m, "TopkToppSamplingParam");
 
    py::enum_<TopkToppSamplingParam::TopkToppSamplingType>(topkToppSampling, "TopkToppSamplingType")
        .value("SAMPLING_UNDEFINED", TopkToppSamplingParam::TopkToppSamplingType::SAMPLING_UNDEFINED)
        .value("SINGLE_TOPK_SAMPLING", TopkToppSamplingParam::TopkToppSamplingType::SINGLE_TOPK_SAMPLING)
        .value("BATCH_TOPK_MULTINOMIAL_SAMPLING",
               TopkToppSamplingParam::TopkToppSamplingType::BATCH_TOPK_MULTINOMIAL_SAMPLING)
        .value("BATCH_TOPK_EXPONENTIAL_SAMPLING",
               TopkToppSamplingParam::TopkToppSamplingType::BATCH_TOPK_EXPONENTIAL_SAMPLING)
        .value("BATCH_TOPK_MULTINOMIAL_LOGPROBS_SAMPLING",
               TopkToppSamplingParam::TopkToppSamplingType::BATCH_TOPK_MULTINOMIAL_LOGPROBS_SAMPLING)
        .value("BATCH_TOPK_EXPONENTIAL_LOGPROBS_SAMPLING",
               TopkToppSamplingParam::TopkToppSamplingType::BATCH_TOPK_EXPONENTIAL_LOGPROBS_SAMPLING)
        .value("SAMPLING_MAX", TopkToppSamplingParam::TopkToppSamplingType::SAMPLING_MAX);
 
    topkToppSampling
        .def(py::init([](TopkToppSamplingParam::TopkToppSamplingType topkToppSamplingType,
                         const std::vector<uint32_t> &randSeeds, uint32_t randSeed, uint32_t topk,
                         int32_t logProbsSize) {
                TopkToppSamplingParam param;
                param.topkToppSamplingType = topkToppSamplingType;
                AddElements(randSeeds, param.randSeeds);
                param.randSeed = randSeed;
                param.topk = topk;
                param.logProbsSize = logProbsSize;
                return param;
                }),
             py::arg("topk_topp_sampling_type") = TopkToppSamplingParam::TopkToppSamplingType::SINGLE_TOPK_SAMPLING,
             py::arg("rand_seeds") = py::list(),
             py::arg("rand_seed") = 0,
             py::arg("topk") = 100,
             py::arg("log_probs_size") = 0)
        .def_readwrite("topk_topp_sampling_type", &TopkToppSamplingParam::topkToppSamplingType)
        .def_readwrite("rand_seeds", &TopkToppSamplingParam::randSeeds)
        .def_readwrite("rand_seed", &TopkToppSamplingParam::randSeed)
        .def_readwrite("topk", &TopkToppSamplingParam::topk)
        .def_readwrite("log_probs_size", &TopkToppSamplingParam::logProbsSize)
        .def("__repr__", [](const TopkToppSamplingParam &param) {
            return "TopkToppSamplingParam: " + OpParamToJson(param).dump();
        });

    def test_topk_topp_sampling(self):
        test_topk_topp_sampling_param = torch_atb.TopkToppSamplingParam()
 
        expected_topk_topp_sampling_type = 0
        expected_rand_seeds = []
        expected_rand_seed = 0
        expected_topk = 100
        expected_log_probs_size = 0
 
        self.assertEqual(test_topk_topp_sampling_param.topk_topp_sampling_type, expected_topk_topp_sampling_type)
        self.assertEqual(test_topk_topp_sampling_param.rand_seeds, expected_rand_seeds)
        self.assertEqual(test_topk_topp_sampling_param.rand_seed, expected_rand_seed)
        self.assertEqual(test_topk_topp_sampling_param.topk, expected_topk)
        self.assertEqual(test_topk_topp_sampling_param.log_probs_size, expected_log_probs_size)