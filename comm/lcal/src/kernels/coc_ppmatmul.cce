/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
#ifndef __PP_MATMUL__
#define __PP_MATMUL__
#include "coc_internal.cce"
#include "lcoc_workspace.h"
template <typename T_INPUT>
struct GetAccumType {
    using T = float;
};

template <>
struct GetAccumType<int8_t> {
    using T = int32_t;
};

#ifdef __DAV_C220_CUBE__

constexpr int32_t L0AB_PINGPONG_BUFFER_SIZE = 32768;
constexpr int32_t CUBE_MATRIX_SIZE_B16 = 256;
constexpr int32_t CUBE_MATRIX_SIZE_B8 = 16 * 32;
constexpr int64_t ND2NZ_STRIDE_LIMIT = 65536;
constexpr int32_t SCALE_L1_SIZE = 256 * 8;

template<typename T>
inline __aicore__ void CopyCubfToBt(uint64_t dst, __cbuf__ T *src, uint16_t convControl, uint16_t nBurst, uint16_t lenBurst, uint16_t sourceGap, uint16_t dstGap)
{
    DataCopyParams intriParams(nBurst, lenBurst, sourceGap, dstGap);
    uint32_t src_buffer_offset = reinterpret_cast<uint64_t>(src);
    uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst);
    uint8_t src_logicpos = static_cast<uint8_t>(TPosition::C1);
    uint8_t dst_logicpos = static_cast<uint8_t>(TPosition::C2);
    LocalTensor<T> srcTensor;
    LocalTensor<T> dstTensor;
    srcTensor = CreateLocalTensor<T>(src_buffer_offset, src_logicpos);
    dstTensor = CreateLocalTensor<T>(dst_buffer_offset, dst_logicpos);
    DataCopy(dstTensor, srcTensor, intriParams);
}

template<typename T>
inline __aicore__ void CopyGmToCbuf(__cbuf__ T *dst, __gm__ T *src, uint8_t sid, uint16_t nBurst, uint16_t lenBurst, uint16_t srcStride, uint16_t dstStride, pad_t padMode)
{
    DataCopyParams intriParams(nBurst, lenBurst, srcStride, dstStride);
    GlobalTensor<T> srcTensor;
    srcTensor.SetGloalBuffer(src);
    uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst);
    uint8_t logicpos = static_cast<uint8_t>(TPosition::C1);
    LocalTensor<T> dstTensor;
    dstTensor = CreateLocalTensor<T>(dst_buffer_offset, logicpos);
    DataCopy(dstTensor, srcTensor, intriParams);
}

template<typename T>
inline __aicore__ void SetFpc(__fbuf__ T *src)
{
    LocalTensor<T> tensor;
    uint32_t src_buffer_offset = reinterpret_cast<uint64_t>(src);
    tensor = CreateLocalTensor<T>(src_buffer_offset);
    SetFixPipeConfig(tensor);
}

template<typename T>
inline __aicore__ void LoadCbufToCaTranspose(__ca__ T *dst, __cbuf__ T *src, uint16_t indexID, uint8_t repeat, uint16_t srcStride, uint16_t dstStride, bool addrmode, uint16_t dstFracStride)
{
    LoadData2dTransposeParams params(
        indexID,
        repeat,
        srcStride,
        dstStride,
        dstFracStride,
        addrmode
    );
    uint32_t src_buffer_offset = reinterpret_cast<uint64_t>(src);
    uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst>);
    uint8_t src_logicpos = static_cast<uint8_t>(TPosition::C1);
    uint8_t dst_logicpos = static_cast<uint8_t>(TPosition::A2);
    LocalTensor<T> srcTensor;
    LocalTensor<T> dstTensor;
    srcTensor = CreateLocalTensor<T>(src_buffer_offset, src_logicpos);
    dstTensor = CreateLocalTensor<T>(dst_buffer_offset, dst_logicpos);
    LoadDataWithTranspose(dstTensor, srcTensor, params);
}

template<typename T>
inline __aicore__ void LoadCbufToCbTranspose(__cb__ T *dst, __cbuf__ T *src, uint16_t indexID, uint8_t repeat, uint16_t srcStride, uint16_t dstStride, bool addrmode, uint16_t dstFracStride)
{
    LoadData2dTransposeParams params(
        indexID,
        repeat,
        srcStride,
        dstStride,
        dstFracStride,
        addrmode
    );
    uint32_t src_buffer_offset = reinterpret_cast<uint64_t>(src);
    uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst>);
    uint8_t src_logicpos = static_cast<uint8_t>(TPosition::C1);
    uint8_t dst_logicpos = static_cast<uint8_t>(TPosition::B2);
    LocalTensor<T> srcTensor;
    LocalTensor<T> dstTensor;
    srcTensor = CreateLocalTensor<T>(src_buffer_offset, src_logicpos);
    dstTensor = CreateLocalTensor<T>(dst_buffer_offset, dst_logicpos);
    LoadDataWithTranspose(dstTensor, srcTensor, params);
}

template<typename T>
inline __aicore__ void LoadCbufToCa(__ca__ T *dst, __cbuf__ T *src, uint16_t baseIdx, uint8_t repeat, uint16_t srcStride, uint16_t dstStride, uint8_t sid, bool transpose, uint8_t addr_cal_mode)
{
    LoadData2dParams params(
        baseIdx,
        repeat,
        srcStride,
        sid,
        dstStride,
        transpose,
        addr_cal_mode
    );
    uint32_t src_buffer_offset = reinterpret_cast<uint64_t>(src);
    uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst>);
    uint8_t src_logicpos = static_cast<uint8_t>(TPosition::C1);
    uint8_t dst_logicpos = static_cast<uint8_t>(TPosition::A2);
    LocalTensor<T> srcTensor;
    LocalTensor<T> dstTensor;
    srcTensor = CreateLocalTensor<T>(src_buffer_offset, src_logicpos);
    dstTensor = CreateLocalTensor<T>(dst_buffer_offset, dst_logicpos);
    LoadData(dstTensor, srcTensor, params);
}

template<typename T>
inline __aicore__ void LoadCbufToCb(__cb__ T *dst, __cbuf__ T *src, uint16_t baseIdx, uint8_t repeat, uint16_t srcStride, uint16_t dstStride, uint8_t sid, bool transpose, uint8_t addr_cal_mode)
{
    LoadData2dParams params(
        baseIdx,
        repeat,
        srcStride,
        sid,
        dstStride,
        transpose,
        addr_cal_mode
    );
    uint32_t src_buffer_offset = reinterpret_cast<uint64_t>(src);
    uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst>);
    uint8_t src_logicpos = static_cast<uint8_t>(TPosition::C1);
    uint8_t dst_logicpos = static_cast<uint8_t>(TPosition::B2);
    LocalTensor<T> srcTensor;
    LocalTensor<T> dstTensor;
    srcTensor = CreateLocalTensor<T>(src_buffer_offset, src_logicpos);
    dstTensor = CreateLocalTensor<T>(dst_buffer_offset, dst_logicpos);
    LoadData(dstTensor, srcTensor, params);
}

template<typename T>
struct IntrinsicCopyGmToL1Nd2Nz {
    static inline __aicore__ void move(
        __cbuf__ T *dst, __gm__ T *src
        uint8_t sid, uint16_t ndNum, uint16_t nValue, uint16_t dValue,
        uint16_t srcNdMatrixStride, uint16_t srcDValue, uint16_t dstNzC0Stride,
        uint16_t dstNzNStride, uint16_t dstNzMatrixStride){
        Nd2NzParams nd2nzParams(
            ndNum, nValue, dValue,
            srcNdMatrixStride, srcDValue, dstNzC0Stride,
            dstNzNStride, dstNzMatrixStride
        );
        uint32_t dst_buffer_offset = reinterpret_cast<uint64_t>(dst);
        uint8_t dst_logicpos = static_cast<uint8_t>(TPosition::C1);
        LocalTensor<T> dstTensor;
        dstTensor = CreateLocalTensor<T>(dst_buffer_offset, dst_logicpos);
        GlobalTensor<T> srcTensor;
        srcTensor.SetGloalBuffer(src);
        DataCopy(dstTensor, srcTensor, nd2nzParams);
    }
};

template <typename T>
struct CopyGmToL1Nd2zN {
    static inline __aicore__ void move(
        __cbuf__ T *dst, __gm__ T *src,
        uint16_t nValue, uint16_t dValue, uint16_t srcDValue, uint16_t dstNzC0Stride) {
        constexpr int BLOCK_LEN = 32 / sizeof(T);
        if (srcDValue < ND2NZ_STRIDE_LIMIT) {
            IntrinsicCopyGmToL1Nd2Nz<T>::move(
                dst,
                src,
                0,
                1,
                nValue,
                dValue,
                0,
                srcDValue,
                dstNzC0Stride,
                1,
                0
            );
        } else {
            for (int i = 0; i < nValue; i++) {
                IntrinsicCopyGmToL1Nd2Nz<T>::move(
                    dst + i * BLOCK_LEN,
                    src + i * srcDValue,
                    0,
                    1,
                    1,
                    dValue,
                    0,
                    0,
                    dstNzC0Stride,
                    0,
                    0
                );
            }
        }
    }
};

template<uint32_t RUN_TYPE, typename MmadDtype, typename OutDtype, bool TA, bool TB>
class PpMatmul {
    using T_ACCUM = typename GetAccumType<MmadDtype>::T;
    static constexpr bool IS_INT8 = std::is_same<MmadDtype, int8_t>::value;
public:
    __aicore__ explicit PpMatmul() {};

    inline __aicore__ void SetArgs(PP_MATMUL_AIC_ARGS_FUN(MmadDtype, OutDtype))
    {
        this->gm_c = reinterpret_cast<__gm__ OutDtype *>(gm_c);
        this->gm_peer_mem = reinterpret_cast<__gm__ OutDtype *>(gm_peer_mem);
        this->gm_dequant_scale = reinterpret_cast<__gm__ int64_t *>(gm_dequant_scale);
        has_offset = gm_dequant_offset != nullptr;

        this->batch_size = batch_size;
        this->m = m;
        this->k = k;
        this->n = n;
        this->weight_nz = weight_nz;

        cube_matrix_size = IS_INT8 ? CUBE_MATRIX_SIZE_B8 : CUBE_MATRIX_SIZE_B16;

        m_align = Block512B<MmadDtype>::AlignUp(m);
        k_align = Block512B<MmadDtype>::AlignUp(k);
        n_align = Block512B<MmadDtype>::AlignUp(n);

        this->m0 = m0;
        this->k0 = k0;
        this->n0 = n0;

        this->dequant_granularity = dequant_granularity;

        AlignJudge(TA, TB, m, k, n, m_align, k_align, n_align, aligned_a, aligned_b);
        bool has_a_align = IsQuant(quant_granularity) || aligned_a;
        bool has_b_align = IsQuant(dequant_granularity) && !IS_INT8 || aligned_b;
        if (weight_nz) {
            k_align16 = (k + 16 - 1) / 16 * 16;
            n_align16 = Block32B<MmadDtype>::AlignUp(n);
            aligned_b = 0;
            has_b_align = false; 
        }
        bool has_accum = IsQuant(dequant_granularity) && IS_INT8 && std::is_same<OutDtype, bfloat16_t>::value;
        bool has_format_dequant_offset = (dequant_granularity == QuantGranularity::PER_TENSOR) && IS_INT8 && has_offset;
        int32_t accum_rank_size = 1;
        
        bool has_dequant_param = (dequant_granularity == QuantGranularity::PER_TOKEN || dequant_granularity == QuantGranularity::PER_TENSOR);
        bool hasFormatDequantScale = (dequant_granularity == QuantGranularity::PER_CHANNEL);

        workspace_info = GetLcalWorkspaceInfo(gm_workspace, batch_size, m, k, n, m_align, k_align, n_align,
                TA, TB, sizeof(MmadDtype), has_a_align, has_b_align, accum_rank_size, has_accum, 0, has_dequant_param,
                hasFormatDequantScale, is_deterministic, false, false, 0, 0, 0);
            
        gm_a_src = reinterpret_cast<__gm__ MmadDtype *>(has_a_align ? workspace_info.gm_a_align : gm_a);
        gm_b_src = reinterpret_cast<__gm__ MmadDtype *>(has_b_align ? workspace_info.gm_b_align : gm_b);
        gm_accum = reinterpret_cast<__gm__ int32_t *>(workspace_info.gm_accum);
        gm_format_dequant_offset = reinterpret_cast<__gm__ int32_t *>(has_format_dequant_offset ?
                workspace_info.gm_dequant_param : gm_dequant_offset);

        block_size = 32 / sizeof(MmadDtype);

        L1_PINGPONG_BUFFER_LEN = ((m0 * k0 + cube_matrix_size - 1) / cube_matrix_size * cube_matrix_size +
                        (n0 * k0 + cube_matrix_size - 1) / cube_matrix_size * cube_matrix_size * (IS_INT8 ? 2 : 1));
        L0AB_PINGPONG_BUFFER_LEN = L0AB_PINGPONG_BUFFER_LEN / sizeof(MmadDtype);

        int32_t a_l1_size = m0 * k0 * sizeof(MmadDtype);
        int32_t a_l1_size_round = DivCeil(a_l1_size, 512) * 512;
        int32_t b_l1_size = n0 * k0 * sizeof(MmadDtype);
        int32_t b_l1_size_round = DivCeil(b_l1_size, 512) * 512;
        l1_base_a = reinterpret_cast<__cbuf__ MmadDtype *>((uintptr_t)(IS_INT8 ? SCALE_L1_SIZE : 0));
        l1_base_b = reinterpret_cast<__cbuf__ MmadDtype *>(a_l1_size_round * (IS_INT8 ? 2 : 1) + (uintptr_t)l1_base_a);

        core_num = get_block_num();
        core_idx = get_block_idx();

        this->m_loop = m_loop;
        this->k_loop = k_loop;
        this->n_loop = n_loop;
        this->core_loop = core_loop;
        this->swizzl_count = swizzl_count;
        this->swizzl_direct = swizzl_direct;
        this->is_91093 = is_91093;
        ping_flag = 1;
        this->rank = rank;
        this->rank_size = rank_size;
        this->p_value = p_value;
        this->withSerialMode = withSerialMode;
        loop_num_per_comm = p_value * core_num;
        this->buffer_size = buffer_size;

        this->ag_dim = ag_dim;
        this->rs_dim = rs_dim;
        this->inner_dim_is_Ag = inner_dim_is_Ag;
        if (inner_dim_is_Ag) {
            this->ag_rank_idx = rank % ag_dim;
            this->rs_rank_idx = rank / ag_dim;
        } else {
            this->ag_rank_idx = rank / rs_dim;
            this->rs_rank_idx = rank % rs_dim;
        }   
    }

    inline __aicore__ void CalLoop(int64_t batch_idx, int64_t m_idx, int64_t n_idx, int32_t m_actual, int32_t n_actual,
                                __gm__ MmadDtype *gm_a_src_tmp) {
        int64_t offset_a, offset_b, offset_a_next, offset_b_next;
        int32_t m_round, n_round;
        if (IS_INT8) {
            if (TA) {
                m_round = DivCeil(m_actual, BLOCK_SIZE_32) * BLOCK_SIZE_32;
            } else {
                m_round = DivCeil(m_actual, BLOCK_SIZE_16) * BLOCK_SIZE_16;
            }
            if (TB) {
                n_round = DivCeil(n_actual, BLOCK_SIZE_32) * BLOCK_SIZE_32;
            } else {
                n_round = DivCeil(n_actual, BLOCK_SIZE_16) * BLOCK_SIZE_16;
            }
        } else {
            m_round = DivCeil(m_actual, BLOCK_SIZE_16) * BLOCK_SIZE_16;
            n_round = DivCeil(n_actual, BLOCK_SIZE_16) * BLOCK_SIZE_16;
        }

        int32_t mn_max = m_round > n_round ? m_round : n_round;
        int32_t k_part_len = L0AB_PINGPONG_BUFFER_LEN / mn_max / block_size * block_size;
        if (TA) {
            if (aligned_a == 1) {
                offset_a = batch_idx * k * m_align + m_idx * m0;
            } else {
                offset_a = batch_idx * k * m + m_idx * m0;
            }
        } else {
            if (aligned_a == 1) {
                offset_a = batch_idx * m * k_align + m_idx * m0 * k_align;
            } else {
                offset_a = batch_idx * m * k + m_idx * m0 * k;
            }
        }
        if (TB) {
            if (aligned_b == 1) {
                offset_b = n_idx * n0 * k_align;
            } else {
                if (weight_nz) {
                    offset_b = n_idx * n0 * block_size;
                } else {
                    offset_b = n_idx * n0 * k;
                }               
            }
        } else {
            if (aligned_b == 1) {
                offset_b = n_idx * n0;
            } else {
                if (weight_nz) {
                    offset_b = n_idx * n0 * k_align16;
                } else {
                    offset_b = n_idx * n0;
                }  
            }
        }
        int64_t dequant_param_offset = n_idx * n0;

        int32_t k_actual = (k_loop == 1) ? k : k0;
        int32_t k_round = DivCeil(k_actual, block_size) * block_size;

        auto l1_buf_a = ping_flag ? l1_base_a : l1_base_a + L1_PINGPONG_BUFFER_LEN;
        auto l1_buf_b = ping_flag ? l1_base_b : l1_base_b + L1_PINGPONG_BUFFER_LEN;
        auto l0a_buf = ping_flag ? l0a_base : l0a_base + L0AB_PINGPONG_BUFFER_LEN;
        auto l0b_buf = ping_flag ? l0b_base : l0b_base + L0AB_PINGPONG_BUFFER_LEN;
        auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

        if (IS_INT8 && has_offset) {
            PipeBarrier<PIPE_MTE2>();
            IntrinsicCopyGmToL1Nd2Nz<int32_t>::move(
                ((__cbuf__ int32_t *) bias_l1),
                ((__gm__ int32_t *)gm_format_dequant_offset) + dequant_param_offset,
                0,
                1,
                1,
                n_actual,
                0,
                n,
                1,
                1,
                0
            );
            SetFlag<HardEvent::MTE2_MTE1>(EVENT_ID0);
            WaitFlag<HardEvent::MTE2_MTE1>(EVENT_ID0);
            WaitFlag<HardEvent::FIX_MTE1>(EVENT_ID1);
            CopyCubfToBt(((uint64_t)bias_bt), ((__cbuf__ int32_t *)bias_l1), 
                        (uint16_t)0ULL, 1, (n_actual * 4 + 63) / 64, 0, 0);
            SetFlag<HardEvent::FIX_MTE2>(EVENT_ID1);
            SetFlag<HardEvent::FIX_M>(EVENT_ID1);
            WaitFlag<HardEvent::FIX_MTE2>(EVENT_ID1);
        }

        auto gm_src_a = gm_a_src_tmp + offset_a;
        auto gm_src_b = gm_b_src + offset_b;

        WaitFlag<HardEvent::MTE1_MTE2>(event_id);
        if (m == 1 || m_actual == 1 && !TA) {
            CopyGmToCbuf(
                l1_buf_a,
                gm_src_a,
                0,
                1,
                k_round,
                0,
                0,
                PAD_NONE
            );
        } else {
            if (TA) {
                auto src_len = m;
                if (aligned_a == 1) {
                    src_len = m_align;
                }
                CopyGmToL1Nd2zN<MmadDtype>::move(l1_buf_a, gm_src_a, k_actual, m_actual, src_len, k_round);
            } else {
                auto src_len = k;
                if (aligned_a == 1) {
                    src_len = k_align;
                }
                CopyGmToL1Nd2zN<MmadDtype>::move(l1_buf_a, gm_src_a, m_actual, k_actual, src_len, m_round);
            }
        }
        SetFlag<HardEvent::MTE2_MTE1>(event_id);
        WaitFlag<HardEvent::MTE1_MTE2>(event_id + 2);
        if (TB) {
            auto src_len = k;
            if (aligned_b == 1) {
                src_len = k_align;
            }
            if (weight_nz) {
                int32_t num_col = DivCeil(k_actual, block_size);
                CopyGmToCbuf(l1_buf_b, gm_src_b, 0, num_col, n_actual, n_align16 - n_actual, n_round - n_actual, PAD_NONE);                
            } else {
                CopyGmToL1Nd2zN<MmadDtype>::move(l1_buf_b, gm_src_b, n_actual, k_actual, src_len, n_round);
            }
        } else {
            auto src_len = n;
            if (aligned_b == 1) {
                src_len = n_align;
            }
            if (weight_nz) {
                int32_t num_col = DivCeil(n_actual, block_size);
                CopyGmToCbuf(l1_buf_b, gm_src_b, 0, num_col, k_actual, k_align16 - k_actual, k_round - k_actual, PAD_NONE);                
            } else {
                CopyGmToL1Nd2zN<MmadDtype>::move(l1_buf_b, gm_src_b, k_actual, n_actual, src_len, k_round);
            }
        }
        SetFlag<HardEvent::MTE2_MTE1>(event_id + 2);

        int mte1_mad_ping_flag = 1;

        for (int64_t k_idx = 0; k_idx < k_loop; k_idx++) {
            int32_t k_actual = (k_idx == (k_loop - 1)) ? (k - k_idx * k0) : k0;
            int32_t k_round = DivCeil(k_actual, block_size) * block_size;
            int32_t k_part_loop = DivCeil(k_actual, k_part_len);

            __cbuf__ MmadDtype *l1_buf_a = ping_flag ? l1_base_a : l1_base_a + L1_PINGPONG_BUFFER_LEN;
            __cbuf__ MmadDtype *l1_buf_b = ping_flag ? l1_base_b : l1_base_b + L1_PINGPONG_BUFFER_LEN;
            auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

            if (k_idx < k_loop - 1) {
                if (TA) {
                    if (aligned_a == 1){
                        offset_a_next = batch_idx * k * m_align + (k_idx + 1) * k0 * m_align + m_idx * m0;
                    } else {
                        offset_a_next = batch_idx * k * m + (k_idx + 1) * k0 * m + m_idx * m0;
                    }
                } else {
                    if (aligned_a == 1){
                        offset_a_next = batch_idx * m * k_align + m_idx * m0 * k_align + (k_idx + 1) * k0;
                    } else {
                        offset_a_next = batch_idx * m * k +  m_idx * m0 * k + (k_idx + 1) * k0;
                    }
                }
                if (TB) {
                    if (aligned_b == 1) {
                        offset_b_next = batch_idx * n * k_align + n_idx * n0 * k_align + (k_idx + 1) * k0;
                    } else {
                        if (weight_nz) {
                            offset_b_next = batch_idx * n * k + (k_idx + 1) * k0 * n_align16 + n_idx * n0 * block_size;
                        } else {
                            offset_b_next = batch_idx * n * k + (k_idx + 1) * k0 + n_idx * n0;
                        }
                    }
                } else {
                    if (aligned_b == 1) {
                        offset_b_next = batch_idx * k * n_align + n_idx * n0 + (k_idx + 1) * k0 * n_align;
                    } else {
                        if (weight_nz) {
                            offset_b_next = batch_idx * k * n + (k_idx + 1) * k0 * block_size + n_idx * n0 * k_align16;
                        } else {
                            offset_b_next = batch_idx * k * n + (k_idx + 1) * k0 * n + n_idx * n0;
                        }
                    }
                }

                int32_t k_actual_next = ((k_idx + 1) == (k_loop -1)) ? (k - (k_idx + 1) * k0) : k0;
                int32_t k_round_next = DivCeil(k_actual_next, block_size) * block_size;

                __cbuf__ MmadDtype *l1_buf_a_next = (1 - ping_flag) ? l1_base_a : l1_base_a + L1_PINGPONG_BUFFER_LEN;
                __cbuf__ MmadDtype *l1_buf_b_next = (1 - ping_flag) ? l1_base_b : l1_base_b + L1_PINGPONG_BUFFER_LEN;
                auto event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                auto gm_src_a = gm_a_src_tmp + offset_a_next;
                auto gm_src_b = gm_b_src + offset_b_next;

                WaitFlag<HardEvent:MTE1_MTE2>(event_id_next);
                if (m == 1 || m_actual == 1 && !TA) {
                    CopyGmToCbuf(
                        l1_buf_a_next,
                        gm_src_a,
                        0,
                        1,
                        k_round_next,
                        0,
                        0,
                        PAD_NONE
                    );
                } else {
                    if (TA) {
                        auto src_len = m;
                        if (aligned_a == 1) {
                            src_len = m_align;
                        }
                        CopyGmToL1Nd2zN<MmadDtype>::move(
                            l1_buf_a_next, gm_src_a, k_actual_next, m_actual, src_len, k_round_next);
                    } else {
                        auto src_len = k;
                        if (aligned_a == 1) {
                            src_len = k_align;
                        }
                        CopyGmToL1Nd2zN<MmadDtype>::move(
                            l1_buf_a_next, gm_src_a, m_actual, k_actual_next, src_len, m_round);
                    }
                }
                SetFlag<HardEvent::MTE2_MTE1>(event_id_next);

                WaitFlag<HardEvent::MTE1_MTE2>(event_id_next + 2);
                if (TB) {
                    auto src_len = k;
                    if (aligned_b == 1) {
                        src_len = k_align;
                    }
                    if (weight_nz) {
                        int32_t num_col = DivCeil(k_actual_next, block_size);
                        CopyGmToCbuf(l1_buf_b_next, gm_src_b, 0, num_col, n_actual, n_align16 - n_actual, n_round - n_actual, PAD_NONE);                
                    } else {
                        CopyGmToL1Nd2zN<MmadDtype>::move(l1_buf_b_next, gm_src_b, n_actual, k_actual_next, src_len, n_round);
                    }
                } else {
                    auto src_len = n;
                    if (aligned_b == 1) {
                        src_len = n_align;
                    }
                    if (weight_nz) {
                        int32_t num_col = DivCeil(n_actual, block_size);
                        CopyGmToCbuf(l1_buf_b_next, gm_src_b, 0, num_col, k_actual_next, k_align16 - k_actual_next, k_round_next - k_actual_next, PAD_NONE);                
                    } else {
                        CopyGmToL1Nd2zN<MmadDtype>::move(l1_buf_b_next, gm_src_b, k_actual_next, n_actual, src_len, k_round_next);
                    }
                }
                SetFlag<HardEvent::MTE2_MTE1>(event_id_next + 2);
            }

            for (int k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
                int32_t k0_round = (k_part_idx < k_part_loop - 1) ?
                    k_part_len : k_round - k_part_idx * k_part_len;
                int32_t k0_actual = (k_part_idx < k_part_loop - 1) ?
                    k_part_len : k_actual - k_part_idx * k_part_len;
                
                auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
                auto l0a_buf = l0a_base + (1 - mte1_mad_ping_flag) * L0AB_PINGPONG_BUFFER_LEN;
                auto l0b_buf = l0b_base + (1 - mte1_mad_ping_flag) * L0AB_PINGPONG_BUFFER_LEN;

                if (k_part_idx == 0) {
                    WaitFlag<HardEvent::MTE2_MTE1>(event_id);
                }
                WaitFlag<HardEvent::M_MTE1>(mte1_mad_event_id);
                if (m == 1 || m_actual == 1 && !TA) {
                    LoadCbufToCa(
                        l0a_buf,
                        l1_buf_a + k_part_idx * k_part_len,
                        0,
                        DivCeil(k0_round, cube_matrix_size),
                        1,
                        0,
                        0,
                        false,
                        inc
                    );
                } else {
                    if (TA) {
                        if (IS_INT8) {
                            for (int i = 0; i < m_round / BLOCK_SIZE_32; i++) {
                                LoadCbufToCaTranspose(
                                    l0a_buf + i * k0_round * BLOCK_SIZE_32,
                                    l1_buf_a + k_part_idx * k_part_len * BLOCK_SIZE_32 +
                                        i * k_round * BLOCK_SIZE_32,
                                    0,
                                    k0_round,
                                    1,
                                    0,
                                    0,
                                    k0_round
                                );
                            }
                        } else {
                            for (int i = 0; i < m_round / BLOCK_SIZE_16; i++) {
                                LoadCbufToCa(
                                    l0a_buf + i * k0_round * BLOCK_SIZE_16,
                                    l1_buf_a + k_part_idx * k_part_len * BLOCK_SIZE_16 +
                                        i * k_round * BLOCK_SIZE_16,
                                    0,
                                    k0_round,
                                    1,
                                    0,
                                    0,
                                    true,
                                    inc
                                );
                            }
                        }                        
                    } else {
                        for (int i = 0; i < m_round / BLOCK_SIZE_16; i++) {                            
                            LoadCbufToCa(
                                l0a_buf + i * cube_matrix_size,
                                l1_buf_a + k_part_idx * k_part_len * m_round +
                                    i * m_round * block_size,
                                0,
                                m_round / BLOCK_SIZE_16,
                                1,
                                k0_round / block_size - 1,
                                0,
                                false,
                                inc
                            );
                        }
                    }
                }
                if (k_part_idx == k_part_loop -1) {
                    SetFlag<HardEvent::MTE1_MTE2>(event_id);
                }

                if (k_part_idx == 0) {
                    WaitFlag<HardEvent::MTE2_MTE1>(event_id + 2);
                }
                if (TB) {
                    LoadCbufToCb(
                        l0b_buf,
                        l1_buf_b + k_part_idx * k_part_len * n_round,
                        0,
                        k0_round * n_round / cube_matrix_size,
                        1,
                        0,
                        0,
                        false,
                        inc
                    );
                } else {
                    if (IS_INT8) {
                        for (int32_t i = 0; i < k0_round / BLOCK_SIZE_32; i++) {
                            LoadCbufToCbTranspose(
                                l0b_buf + i * ((n_actual + 15) / 16 * 16) * BLOCK_SIZE_32,
                                l1_buf_b + (k_part_idx * k_part_len + i * BLOCK_SIZE_32) * BLOCK_SIZE_32,
                                0,
                                n_round,
                                k_round,
                                1,
                                0,
                                0,
                            );
                        }
                    } else {
                        for (int32_t i = 0; i < k0_round / BLOCK_SIZE_16; i++) {
                            LoadCbufToCb(
                                l0b_buf + i * n_round * BLOCK_SIZE_16,
                                l1_buf_b + (k_part_idx * k_part_len + i * BLOCK_SIZE_16) * BLOCK_SIZE_16,
                                0,
                                n_round,
                                k_round,
                                0,
                                0,
                                true,
                                inc
                            );
                        }
                    }
                }
                if (k_part_idx == k_part_loop - 1) {
                    SetFlag<HardEvent::MTE1_MTE2>(event_id + 2);
                }

                SetFlag<HardEvent::MTE1_M>(mte1_mad_event_id);
                WaitFlag<HardEvent::MTE1_M>(mte1_mad_event_id);

                bool init_c = (k_idx == 0 && k_part_idx == 0);
                if (init_c) {
                    WaitFlag<HardEvent::FIX_M>(EVENT_ID0);
                }

                if (IS_INT8 && has_offset) {
                    if (init_c) {
                        WaitFlag<HardEvent::FIX_M>(EVENT_ID1);
                    }
                    PipeBarrier<PIPE_M>();
                    if (m != 1 && m_actual == 1 && TA) {
                        mad((__cc__ int32_t *)l0c_buf,
                            (__ca__ int8_t *)l0a_buf,
                            (__cb__ int8_t *)l0b_buf,
                            ((uint64_t)bias_bt),
                            16,
                            k0_actual,
                            n_actual,
                            0,
                            0,
                            init_c,
                            0
                        );
                    } else {
                        mad((__cc__ int32_t *)l0c_buf,
                            (__ca__ int8_t *)l0a_buf,
                            (__cb__ int8_t *)l0b_buf,
                            ((uint64_t)bias_bt),
                            m_actual,
                            k0_actual,
                            n_actual,
                            0,
                            0,
                            init_c,
                            0
                        );
                    }
                } else {
                    PipeBarrier<PIPE_M>();
                    if (m != 1 && m_actual == 1 && TA) {
                        mad(l0c_buf,
                            l0a_buf,
                            l0b_buf,
                            16,
                            k0_actual,
                            n_actual,
                            0,
                            0,
                            0,
                            init_c
                        );
                    } else {
                        mad(l0c_buf,
                            l0a_buf,
                            l0b_buf,
                            m_actual,
                            k0_actual,
                            n_actual,
                            0,
                            0,
                            0,
                            init_c
                        );
                    }
                }
                PipeBarrier<PIPE_M>();
                SetFlag<HardEvent::M_MTE1>(mte1_mad_event_id);

                mte1_mad_ping_flag = 1 - mte1_mad_ping_flag;
            }
            ping_flag = 1 - ping_flag;
        }

        if (IS_INT8 && std::is_same<OutDtype, half>::value && (dequant_granularity == QuantGranularity::PER_CHANNEL ||
            dequant_granularity == QuantGranularity::PER_TOKEN)) {
            WaitFlag<HardEvent::FIX_MTE2>(EVENT_ID0);
            PipeBarrier<PIPE_MTE2>();
            CopyGmToCbuf(
                scale_l1,
                gm_dequant_scale + dequant_param_offset,
                0,
                1,
                (n_actual * sizeof(int64_t) + 31) / 32,
                0,
                0,
                PAD_NONE
            );
            SetFlag<HardEvent::MTE2_FIX>(EVENT_ID0);

            WaitFlag<HardEvent::MTE2_FIX>(EVENT_ID0);

            copy_cbuf_to_fbuf(
                scale_FB,
                scale_l1,
                1,
                (n_actual * sizeof(int64_t) + 127) / 128,
                0,
                0
            );
            PipeBarrier<PIPE_FIX>();
       }
    }

    inline __aicore__ void MoveL0CToGM(__gm__ OutDtype *gm_dst, int64_t offset_c, int32_t m_actual, int32_t n_actual, int32_t src_stride, int32_t dst_stride) {
        #if (__CCE__AICORE__ == 220)
        FixpipeParamsV220 FixpipeParams(
            n_actual,
            m_actual,
            src_stride,
            dst_stride,
            false
        );
        #elif (defined(__DAV_C310__))
        FixpipeParamsV310 FixpipeParams(
            n_actual,
            m_actual,
            src_stride,
            dst_stride,
        );
        #endif
        uint64_t src_addr = reinterpret_cast<uint64_t>(l0c_buf);
        LocalTensor<T_ACCUM> srcTensor = CreateLocalTensor<T_ACCUM>
            (reinterpret_cast<uint64_t>(l0c_buf), static_cast<uint8_t>(TPosition::CO1));
        GlobalTensor<OutDtype> dstTensor = CreateGlobalTensor<OutDtype>(gm_dst + offset_c);

        if (IS_INT8) {
            if constexpr (std::is_same<OutDtype, half>::value) {
                if (dequant_granularity == QuantGranularity::PER_CHANNEL || dequant_granularity == QuantGranularity::PER_TOKEN) {
                    SetFpc(scale_FB);
                    FixpipeParams.quantPre = VDEQF16;
                    Fixpipe<OutDtype, T_ACCUM, CFG_ROW_MAJOR>(dstTensor, srcTensor, FixpipeParams);
                    SetFlag<HardEvent::FIX_MTE2>(EVENT_ID0);
                } else if (QuantGranularity::PER_TENSOR) {
                    FixpipeParams.quantPre = DEQF16;
                    FixpipeParams.deqScalar = gm_dequant_scale[0];
                    Fixpipe<OutDtype, T_ACCUM, CFG_ROW_MAJOR>(dstTensor, srcTensor, FixpipeParams);
                }
            } else if constexpr (std::is_same<OutDtype, bfloat16_t>::value) {
                GlobalTensor<int32_t> dstAccum = CreateGlobalTensor<int32_t>(gm_accum + offset_c);
                Fixpipe<int32_t, T_ACCUM, CFG_ROW_MAJOR>(dstAccum, srcTensor, FixpipeParams);
            }
        } else {
            if constexpr (std::is_same<OutDtype, __bf16>::value) {
                FixpipeParams.quantPre = F322BF16;
                Fixpipe<OutDtype, T_ACCUM, CFG_ROW_MAJOR>(dstTensor, srcTensor, FixpipeParams);
            } else {
                FixpipeParams.quantPre = F322F16;
                Fixpipe<OutDtype, T_ACCUM, CFG_ROW_MAJOR>(dstTensor, srcTensor, FixpipeParams);
            }
        }
        SetFlag<HardEvent::FIX_M>(EVENT_ID0);
        if (IS_INT8 && has_offset) {
            SetFlag<HardEvent::FIX_MTE1>(EVENT_ID1);
        }
    }

    inline __aicore__ void InitFlags() {
        WaitEvent(AIC_WAIT_AIV_FINISH_ALIGN_FLAG_ID);
        SetFlag<HardEvent::MTE1_MTE2>(EVENT_ID0);
        SetFlag<HardEvent::MTE1_MTE2>(EVENT_ID1);
        SetFlag<HardEvent::MTE1_MTE2>(EVENT_ID2);
        SetFlag<HardEvent::MTE1_MTE2>(EVENT_ID3);
        SetFlag<HardEvent::FIX_M>(EVENT_ID0);
        SetFlag<HardEvent::FIX_MTE2>(EVENT_ID0);
        SetFlag<HardEvent::M_MTE1>(EVENT_ID0);
        SetFlag<HardEvent::M_MTE1>(EVENT_ID1);
        SetFlag<HardEvent::FIX_MTE1>(EVENT_ID1);
    }

    inline __aicore__ void Endflags() {
        WaitFlag<HardEvent::FIX_MTE1>(EVENT_ID1);
        WaitFlag<HardEvent::M_MTE1>(EVENT_ID0);
        WaitFlag<HardEvent::M_MTE1>(EVENT_ID1);
        WaitFlag<HardEvent::FIX_M>(EVENT_ID0);
        WaitFlag<HardEvent::FIX_MTE2>(EVENT_ID0);
        WaitFlag<HardEvent::MTE1_MTE2>(EVENT_ID0);
        WaitFlag<HardEvent::MTE1_MTE2>(EVENT_ID1);
        WaitFlag<HardEvent::MTE1_MTE2>(EVENT_ID2);
        WaitFlag<HardEvent::MTE1_MTE2>(EVENT_ID3);
    }

    inline __aicore__ void RunPureMatmul() {

        InitFlags();
        for (int32_t loop_idx = 0; loop_idx < core_loop; loop_idx++) {
            if (loop_idx % core_num != core_idx) {
                continue;
            }

            int64_t batch_idx = loop_idx / (m_loop * n_loop);
            int64_t m_idx, n_idx;
            GetBlockIdx(loop_idx, m_loop, n_loop, swizzl_direct, swizzl_count, m_idx, n_idx);
            int32_t m_actual = (m_idx == (m_loop -1)) ? (m - m_idx * m0) : m0;
            int32_t n_actual = (n_idx == (n_loop -1)) ? (n - n_idx * n0) : n0;
            CalLoop(batch_idx, m_idx, n_idx, m_actual, n_actual, gm_a_src);

            SetFlag<HardEvent::M_FIX>(EVENT_ID0);
            WaitFlag<HardEvent::M_FIX>(EVENT_ID0);

            int64_t offset_c = batch_idx * m * n + m_idx * m0 * n + n_idx * n0;
            MoveL0CToGM(gm_c, offset_c, m_actual, n_actual, (m_actual + 15) / 16 * 16, n);
        }
        Endflags();
        PipeBarrier<PIPE_ALL>();

        FFTSCrossCoreSync<PIPE_FIX>(0, AIC_FINISH_MATMUL_FLAG_ID);
        WaitEvent(AIC_FINISH_MATMUL_FLAG_ID);

        FFTSCrossCoreSync<PIPE_FIX>(2, AIV_WAIT_AIC_FINISH_MATMUL_FLAG_ID);
        PipeBarrier<PIPE_ALL>();
    }

    inline __aicore__ void RunMatmulAllReduce() {
        InitFlags();
        int32_t comm_count = DivCeil(core_loop, loop_num_per_comm);
        int32_t pipe_depth = is_91093 ? BLOCK_COUNT_4 : MAX_BLOCK_COUNT;
        for (int32_t cal_idx = 0; cal_idx < comm_count; cal_idx++) {
            int32_t loop_idx = cal_idx * core_num + core_idx;
            int32_t flag_idx = cal_idx % pipe_depth;
            if (cal_idx >= pipe_depth) {
                WaitEvent(flag_idx);
            }
            int32_t actual_loop_num = loop_num_per_comm;
            if (cal_idx == comm_count - 1){
                actual_loop_num = core_loop - cal_idx * loop_num_per_comm;
            }
            for (int32_t p = 0; p < p_value; p++) {
                int loop_idx = cal_idx * p_value * core_num + p * core_num + core_idx;
                if (loop_idx >= core_loop)
                    break;
                int64_t batch_idx = loop_idx / (m_loop * n_loop);
                int64_t m_idx , n_idx;
                GetBlockIdx(loop_idx, m_loop, n_loop, swizzl_direct, swizzl_count, m_idx, n_idx);
                int32_t m_actual = (m_idx == (m_loop -1)) ? (m - m_idx * m0) : m0;
                int32_t n_actual = (n_idx == (n_loop -1)) ? (n - n_idx * n0) : n0;
                CalLoop(batch_idx, m_idx, n_idx, m_actual, n_actual, gm_a_src);

                SetFlag<HardEvent::M_FIX>(EVENT_ID0);
                WaitFlag<HardEvent::M_FIX>(EVENT_ID0);

                int64_t offset_c;
                int32_t n_stride;
                offset_c = flag_idx * m0 * loop_num_per_comm * n0 +
                        (loop_idx % loop_num_per_comm) * m0 * n0;
                n_stride = n0;
                MoveL0CToGM(gm_peer_mem, offset_c, m_actual, n_actual, (m_actual + 15) / 16 * 16, n_stride);
            }
            FFTSCrossCoreSync<PIPE_FIX>(2, flag_idx);
        }
        Endflags();
        PipeBarrier<PIPE_ALL>();
    }

    inline __aicore__ void RunAllGatherMatmulReduceScatter() {

        InitFlags();
        int32_t twod_big_dim = ag_dim > rs_dim ? ag_dim : rs_dim;
        int64_t gm_a_pingpong_size = m0 * k_align * p_value * twod_big_dim;
        int64_t gm_c_pingpong_size = p_value * twod_big_dim * n_loop * m0 * n0;
        int32_t m_loop_per_bigdim = DivCeil(m_loop * ag_dim, twod_big_dim);
        int64_t m_per_bigdim = m * ag_dim / twod_big_dim;
        int32_t comm_count = DivCeil(batch_size, * m_loop_per_bigdim, p_value);
        int32_t loop_num_per_cal = p_value * n_loop * twod_big_dim;
        int32_t ag_part_dim = twod_big_dim / ag_dim;
        int32_t rs_part_dim = twod_big_dim / rs_dim;
        for (int32_t comm_idx = 0; comm_idx < comm_count; comm_idx++) {
            uint64_t flag_id = comm_idx % MAX_BLOCK_COUNT;
            int32_t actual_p_value = p_value;
            if (comm_idx == comm_count - 1) {
                actual_p_value = m_loop_per_bigdim - comm_idx * p_value;
            }
            WaitEvent(flag_id);

            int32_t actual_loop_num = actual_p_value * twod_big_dim * n_loop;
            int32_t core_loop_num = DivCeil(actual_p_value * twod_big_dim * n_loop, core_num);
            for (int32_t core_loop_idx = 0; core_loop_idx < core_loop_num; core_loop_idx++) {
                int32_t loop_offset = core_loop_idx * core_num + core_idx;
                if (loop_offset >= actual_loop_num) {
                    continue;
                }
                int32_t loop_idx = comm_idx * loop_num_per_cal + loop_offset;
                int64_t batch_idx = loop_idx / (m_loop * n_loop * twod_big_dim);

                int64_t m_idx, n_idx;
                GetBlockIdx(loop_offset, actual_p_value * twod_big_dim, n_loop, swizzl_direct, swizzl_count, m_idx, n_idx);

                int32_t m_idx_in_rank = m_idx % actual_p_value;
                int64_t m_idx_in_c = comm_idx * p_value + m_idx_in_rank;
                int32_t m_actual = (m_idx_in_c == (m_loop_per_bigdim - 1)) ? (m_per_bigdim - m_idx_in_c * m0) : m0;
                int32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
                int64_t bigdim_dix = m_idx / actual_p_value;

                int32_t ag_src_idx = bigdim_dix / ag_part_dim;
                int32_t ag_part_idx = bigdim_dix % ag_part_dim;
                int32_t rs_dst_idx = bigdim_dix / rs_part_dim;
                int32_t rs_part_idx = bigdim_dix % rs_part_dim;

                __gm__ MmadDtype *gm_mem_st;
                if (ag_src_idx != ag_rank_idx) {
                    gm_mem_st = reinterpret_cast<__gm__ MmadDtype *>(gm_peer_mem)
                        + (comm_idx % MAX_BLOCK_COUNT) * gm_a_pingpong_size
                        + bigdim_dix * p_value * m0 * k_align;
                } else {
                    gm_mem_st = gm_a_src + (comm_idx * p_value) * m0 * k_align + ag_part_idx * m_per_bigdim * k_align;
                }

                CalLoop(batch_idx, m_idx_in_rank, n_idx, m_actual, n_actual, gm_mem_st);
                SetFlag<HardEvent::M_FIX>(EVENT_ID0);
                WaitFlag<HardEvent::M_FIX>(EVENT_ID0);

                int64_t offset_c;
                int32_t n_stride;
                __gm__ OutDtype *gm_dst = nullptr;

                if (rs_dst_idx != rs_rank_idx) {
                    offset_c = gm_c_pingpong_size * (comm_idx % MAX_BLOCK_COUNT)
                    + (m_idx * n_loop + n_idx) * m0 * n0
                    + LCAL_2DTP_C_OFFSET;
                    gm_dst = gm_peer_mem;
                    dst_stride = n0;
                } else {
                    offset_c = rs_part_idx * m_per_bigdim * n
                    + m_idx_in_c * m0 * n
                    + n_idx * n0;
                    gm_dst = gm_c;
                    dst_stride = n;
                }
                MoveL0CToGM(gm_dst, offset_c, m_actual, n_actual, (m_actual + 15) / 16 * 16, dst_stride);
            }
            FFTSCrossCoreSync<PIPE_FIX>(2, flag_id);
        }

        Endflags();
        PipeBarrier<PIPE_ALL>();
    }

    inline __aicore__ void Run() {
        if (RUN_TYPE == PPMATMUL_RUN_MATMUL_ALLREDUCE) {
            if (withSerialMode) {
                gm_c = gm_peer_mem;
                RunPureMatmul();
            } else {
                RunMatmulAllReduce();
            }
        } else if (RUN_TYPE == PPMATMUL_RUN_ALL_GATHER_MATMUL_REDUCE_SCATTER) {
            RunAllGatherMatmulReduceScatter();
        }
    }

protected:
    __gm__ MmadDtype *gm_a_src{nullptr};
    __gm__ MmadDtype *gm_b_src{nullptr};

    __gm__ OutDtype *gm_c{nullptr};
    __gm__ OutDtype *gm_peer_mem(nullptr);
    __gm__ int64_t *gm_dequant_scale{nullptr};
    __gm__ int32_t *gm_format_dequant_offset{nullptr};
    __gm__ int32_t *gm_accum{nullptr};

    __cbuf__ MmadDtype *l1_base_a = reinterpret_cast<__cbuf__ MmadDtype *>((uintptr_t) SCALE_L1_SIZE);
    __cbuf__ MmadDtype *l1_base_b = reinterpret_cast<__cbuf__ MmadDtype *>((uintptr_t) (128 * 1024));

    __ca__ MmadDtype *l0a_base = reinterpret_cast<__ca__ MmadDtype *>((uintptr_t) 0);
    __cb__ MmadDtype *l0b_base = reinterpret_cast<__cb__ MmadDtype *>((uintptr_t) 0);

    __cc__ T_ACCUM *l0c_buf = reinterpret_cast<__cc__ T_ACCUM *>((uintptr_t) 0);

    __cbuf__ int64_t *scale_l1 = reinterpret_cast<__cbuf__ int64_t *>((uintptr_t) 0);
    __fbuf__ int64_t *scale_FB = (__fbuf__ int64_t *)(0);

    __cbuf__ int32_t * bias_l1 = reinterpret_cast<__cbuf__ int32_t *>((uintptr_t)0);
    uint16_t bias_bt = 0;
    bool has_offset{false};
    LcalWorkspaceInfo workspace_info;

    int32_t core_num;

    int32_t batch_size;
    int32_t m;
    int32_t k;
    int32_t n;
    int32_t m_align;
    int32_t k_align;
    int32_t n_align;
    int32_t k_align16;
    int32_t n_align16;
    int32_t m0;
    int32_t k0;
    int32_t n0;

    int32_t m_loop;
    int32_t n_loop;
    int32_t k_loop;
    int32_t core_loop;
    int32_t core_idx;
    int32_t ping_flag;
    int32_t block_size;
    int32_t cube_matrix_size;

    int32_t aligned_a;
    int32_t aligned_b;

    int32_t swizzl_count;
    int32_t swizzl_direct;

    int32_t L1_PINGPONG_BUFFER_LEN;
    int32_t L0AB_PINGPONG_BUFFER_LEN;
    int32_t rank;
    int32_t rank_size;
    int32_t p_value;
    int32_t loop_num_per_comm;

    int32_t withSerialMode;
    int32_t buffer_size;

    int32_t ag_dim;
    int32_t rs_dim;
    bool inner_dim_is_Ag{false};
    int32_t ag_rank_idx;
    int32_t rs_rank_idx;
    bool weight_nz{false};

    bool is_91093{false};
    QuantGranularity dequant_granularity;
};

#endif