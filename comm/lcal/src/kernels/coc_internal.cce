/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
#ifndef LCAL_COC_INTERNAL_H
#define LCAL_COC_INTERNAL_H

#include <type_traits>
#include "kernel_operator.h"
#include "coc_const_args.cce"

using namespace AscendC;

template <typename T>

FORCE_INLINE_AICORE LocalTensor<T> CreateLocalTensor(__ubuf__ T *addr)
{
    LocalTensor<T> tensor;
    TBuffAddr taddr;
    taddr.bufferAddr = reinterpret_cast<uint64_t>(addr);
    tensor.SetAddr(taddr);
    return tensor;
}

template <typename T>
FORCE_INLINE_AICORE LocalTensor<T> CreateLocalTensor(uint32_t buffer_offset)
{
    LocalTensor<T> tensor;
    tensor.address_.bufferAddr = buffer_offset;
    return tensor;
}

template <typename T>
FORCE_INLINE_AICORE LocalTensor<T> CreateLocalTensor(uint32_t buffer_offset, uint8_t logic_pos)
{
    LocalTensor<T> tensor;
    tensor.address_.logicPos = logic_pos;
    tensor.address_.bufferAddr = buffer_offset;
    return tensor;
}

template <typename T>
FORCE_INLINE_AICORE GlobalTensor<T> CreateGlobalTensor(__gm__ T *addr)
{
    GlobalTensor<T> tensor;
    tensor.SetGlobalBuffer(addr);
    return tensor;
}

template <pipe_t pipe>
inline __aicore__ void FFTSCrossCoreSync(uint64_t mode, uint64_t flag_id)
{
    uint64_t config = 1 | (mode << 4) | (flag_id << 8);
    ffts_cross_core_sync(pipe, config);
}

template <typename T>
inline __aicore__ void CopyUB2UB(__ubuf__ T *dst, __ubuf__ T *src, uint8_t sid, uint16_t nBurst, uint16_t lenBurst,
                                 uint16_t srcStride, uint16_t dstStride)
{
    LocalTensor<T> srcTensor = CreateLocalTensor<T>(src);
    LocalTensor<T> dstTensor = CreateLocalTensor<T>(dst);
    DataCopyParams repeatParams(nBurst, lenBurst, srcStride, dstStride);
    DataCopy(dstTensor, srcTensor, repeatParams);
}

template <typename Tdst, typename Tsrc>
inline __aicore__ void Vconv(__ubuf__ Tdst *dst, __ubuf__ Tsrc *src, uint8_t repeat, uint16_t dstBlockStride,
                             uint16_t srcBlockStride, uint8_t dstRepeatStride, uint8_t srcRepeatStride,
                             const RoundMode &roundMode = RoundMode::CAST_NONE)
{
    LocalTensor<Tsrc> srcTensor = CreateLocalTensor<Tsrc>(src);
    LocalTensor<Tdst> dstTensor = CreateLocalTensor<Tdst>(dst);
    UnaryRepeatParams repeatParams(dstBlockStride, srcBlockStride, dstRepeatStride, srcRepeatStride);
    Cast<Tdst, Tsrc, false>(dstTensor, srcTensor, roundMode, -1, repeat, repeatParams);
}

template <typename T>
inline __aicore__ void Vadd(__ubuf__ T *dst, __ubuf__ T *src0, __ubuf__ T *src1, uint8_t repeat, uint8_t dstBlockStride,
                            uint8_t src0BlockStride, uint8_t src1BlockStride, uint8_t dstRepeatStride,
                            uint8_t src0RepeatStride, uint8_t src1RepeatStride)
{
    LocalTensor<T> srcTensor0 = CreateLocalTensor<T>(src0);
    LocalTensor<T> srcTensor1 = CreateLocalTensor<T>(src1);
    LocalTensor<T> dstTensor = CreateLocalTensor<T>(dst);
    BinaryRepeatParams repeatParams(dstBlockStride, src0BlockStride, src1BlockStride, dstRepeatStride, src0RepeatStride,
                                    src1RepeatStride);
    Add<T, false>(dstTensor, srcTensor0, srcTensor1, -1, repeat, repeatParams);
}

template <typename T>
inline __aicore__ void Vadds(__ubuf__ T *dst, __ubuf__ T *src, const T &scalarValue, uint8_t repeat,
                            uint16_t dstBlockStride, uint16_t srcBlockStride, uint8_t dstRepeatStride,
                            uint8_t srcRepeatStride)
{
    LocalTensor<T> srcTensor = CreateLocalTensor<T>(src);
    LocalTensor<T> dstTensor = CreateLocalTensor<T>(dst);
    UnaryRepeatParams repeatParams(dstBlockStride, srcBlockStride, dstRepeatStride, srcRepeatStride);
    Adds<T, false>(dstTensor, srcTensor, scalarValue, -1, repeat, repeatParams);
}

template <typename T>
inline __aicore__ void Vmul(__ubuf__ T *dst, __ubuf__ T *src0, __ubuf__ T *src1, uint8_t repeat, uint8_t dstBlockStride,
                            uint8_t src0BlockStride, uint8_t src1BlockStride, uint8_t dstRepeatStride,
                            uint8_t src0RepeatStride, uint8_t src1RepeatStride)
{
    LocalTensor<T> srcTensor0 = CreateLocalTensor<T>(src0);
    LocalTensor<T> srcTensor1 = CreateLocalTensor<T>(src1);
    LocalTensor<T> dstTensor = CreateLocalTensor<T>(dst);
    BinaryRepeatParams repeatParams(dstBlockStride, src0BlockStride, src1BlockStride, dstRepeatStride, src0RepeatStride,
                                    src1RepeatStride);
    Mul<T, false>(dstTensor, srcTensor0, srcTensor1, -1, repeat, repeatParams);
}

template <typename T>
inline __aicore__ void Vmuls(__ubuf__ T *dst, __ubuf__ T *src, const T &scalarValue, uint8_t repeat,
                            uint16_t dstBlockStride, uint16_t srcBlockStride, uint8_t dstRepeatStride,
                            uint8_t srcRepeatStride)
{
    LocalTensor<T> srcTensor = CreateLocalTensor<T>(src);
    LocalTensor<T> dstTensor = CreateLocalTensor<T>(dst);
    UnaryRepeatParams repeatParams(dstBlockStride, srcBlockStride, dstRepeatStride, srcRepeatStride);
    Muls<T, false>(dstTensor, srcTensor, scalarValue, -1, repeat, repeatParams);
}

inline __aicore__ bool IsQuant(const QuantGranularity &granularity)
{
    return (granularity > QuantGranularity::QUANT_GRANULARITY_UNDEFINED) &&
           (granularity < QuantGranularity::QUANT_GRANULARITY_MAX);
}

#define COC_ARGS_FUN_IIO(T_INPUT1, T_INPUT2, T_OUTPUT) \
    __gm__ T_INPUT1 *gm_a, __gm__ T_INPUT2 *gm_b, __gm__ T_OUTPUT *gm_bias, __gm__ T_OUTPUT *gm_gamma, \
        __gm__ T_OUTPUT *gm_out, __gm__ T_OUTPUT *gm_allgather_out, GM_ADDR gm_workspace, \
        GM_ADDR gm_dequant_scale, GM_ADDR gm_dequant_offset, GM_ADDR gm_quant_scale, \
        GM_ADDR gm_quant_offset, GM_ADDR coc_comm_args, GM_ADDR ffts_addr, \
        __gm__ int32_t* num_local_tokens_per_expert, __gm__ int32_t *num_global_tokens_per_local_expert, \
        __gm__ int32_t *global_tokens_per_expert_matrix, GM_ADDR para_gm

#define COC_ARGS_FUN_IO(T_INPUT, T_OUTPUT) COC_ARGS_FUN_IIO(T_INPUT, T_INPUT, T_OUTPUT)

#define COC_ARGS_FUN(T) COC_ARGS_FUN_IO(T, T)

#define COC_ARGS_CALL() \
    gm_a, gm_b, gm_bias, gm_gamma, gm_out, gm_allgather_out, gm_workspace, gm_dequant_scale, gm_dequant_offset, \
    gm_quant_scale, gm_quant_offset, coc_comm_args, ffts_addr, \
    num_local_tokens_per_expert, num_global_tokens_per_local_expert, \
    global_tokens_per_expert_matrix, para_gm

#define COC_ARGS_CALL_INT8() \
    reinterpret_cast<GM_ADDR>(gm_a), reinterpret_cast<GM_ADDR>(gm_b), reinterpret_cast<GM_ADDR>(gm_bias), \
    reinterpret_cast<GM_ADDR>(gm_gamma), reinterpret_cast<GM_ADDR>(gm_out), \
    reinterpret_cast<GM_ADDR>(gm_allgather_out), gm_workspace, gm_dequant_scale, gm_dequant_offset, \
    gm_quant_scale, gm_quant_offset, coc_comm_args, ffts_addr, \
    num_local_tokens_per_expert, num_global_tokens_per_local_expert, \
    global_tokens_per_expert_matrix, para_gm

#define PP_MATMUL_AIC_ARGS_FUN(T_INPUT, T_OUTPUT) \
    GM_ADDR gm_a, GM_ADDR gm_b, __gm__ T_OUTPUT *gm_bias, __gm__ T_OUTPUT *gm_c, \
        __gm__ T_OUTPUT *gm_peer_mem, GM_ADDR gm_workspace, GM_ADDR gm_dequant_scale, \
        GM_ADDR gm_dequant_offset, int32_t batch_size, int32_t m, int32_t k, int32_t n, int32_t m0, \
        int32_t k0, int32_t n0, int32_t m_loop, int32_t k_loop, int32_t n_loop, int32_t core_loop, \
        int32_t swizzl_count, int32_t swizzl_direct, int32_t rank, int32_t rank_size, int32_t p_value, \
        int32_t withSerialMode, QuantGranularity quant_granularity, QuantGranularity dequant_granularity, \
        int32_t ag_dim, int32_t rs_dim, bool inner_dim_is_Ag, bool weight_nz, bool is_91093, \
        __gm__ int32_t *num_local_tokens_per_expert, __gm__ int32_t * num_global_tokens_per_local_expert, \
        __gm__ int32_t *global_tokens_per_expert_matrix, int32_t local_expert_nums, int32_t EP, int32_t TP, \
        int32_t maxOutputSize, int32_t is_moe, bool is_deterministic, int32_t buffer_size \

#define PP_MATMUL_AIC_ARGS_CALL() \
    reinterpret_cast<GM_ADDR>(gm_a), reinterpret_cast<GM_ADDR>(gm_b), gm_bias, gm_c, gm_peer_mem, \
        reinterpret_cast<GM_ADDR>(gm_workspace), reinterpret_cast<GM_ADDR>(gm_dequant_scale), \
        reinterpret_cast<GM_ADDR>(gm_dequant_offset), batch_size, m, k, n, m0, k0, n0, m_loop, k_loop, \
        n_loop, core_loop, swizzl_count, swizzl_direct, rank, rank_size, p_value, withSerialMode, quant_granularity, \
        dequant_granularity, ag_dim, rs_dim, inner_dim_is_Ag, weight_nz, is_91093, \
        num_local_tokens_per_expert, num_global_tokens_per_local_expert, \
        global_tokens_per_expert_matrix, local_expert_nums, EP, TP, maxOutputSize, is_moe, is_deterministic, buffer_size \

#define PP_MATMUL_AIV_PADDING_ARGS_FUN() \
    GM_ADDR gm_a, GM_ADDR gm_b, GM_ADDR gm_workspace, GM_ADDR gm_dequant_scale, \
        GM_ADDR gm_dequant_offset, GM_ADDR gm_quant_scale, GM_ADDR gm_quant_offset, \
        int32_t batch_size, int32_t m, int32_t k, int32_t n, bool trans_a, bool trans_b, bool is_int8, \
        QuantGranularity dequant_granularity, int32_t dequant_group_size, QuantGranularity quant_granularity, \
        int32_t quant_group_size, int32_t weight_nz, int32_t is_moe, int32_t is_moe_averaged, int32_t is_alltoallvc, \
        int32_t EP, int32_t TP, int32_t local_expert_nums, bool is_deterministic

#define PP_MATMUL_AIV_PADDING_ARGS_CALL() \
    reinterpret_cast<GM_ADDR>(gm_a), reinterpret_cast<GM_ADDR>(gm_b), \
        reinterpret_cast<GM_ADDR>(gm_workspace), reinterpret_cast<GM_ADDR>(gm_dequant_scale), \
        reinterpret_cast<GM_ADDR>(gm_dequant_offset), reinterpret_cast<GM_ADDR>(gm_quant_scale), \
        reinterpret_cast<GM_ADDR>(gm_quant_offset), batch_size, m, k, n, trans_a, trans_b, is_int8, \
        dequant_granularity, dequant_group_size, quant_granularity, quant_group_size, weight_nz, is_moe, \
        is_moe_averaged, is_alltoallvc, EP, TP, local_expert_nums, is_deterministic

#define PP_MATMUL_AIV_ADD_BIAS_ARGS_FUN() \
    GM_ADDR gm_bias, GM_ADDR gm_out, int32_t batch_size, int32_t m, int32_t n, int32_t rank_size

#define PP_MATMUL_AIV_ADD_BIAS_ARGS_CALL() \
    reinterpret_cast<GM_ADDR>(gm_bias), reinterpret_cast<GM_ADDR>(gm_out), batch_size, m, n, rank_size

#define PP_MATMUL_AIV_POST_ARGS_CALL() \
    reinterpret_cast<GM_ADDR>(gm_out), reinterpret_cast<GM_ADDR>(gm_bias), \
        reinterpret_cast<GM_ADDR>(gm_gamma), reinterpret_cast<GM_ADDR>(para_gm)

#define PP_MATMUL_AIV_POST_ARGS_FUN() \
    GM_ADDR gm_out, GM_ADDR gm_bias, GM_ADDR gm_gamma, GM_ADDR para_gm

#define TEMPLATE_ARGS_FUN() bool ALIGN = true, bool IS_INT8 = false, bool HAVE_BIAS = false, typename T = half

#define TEMPLATE_ARGS_CALL() ALIGN, IS_INT8, HAVE_BIAS, T

inline __aicore__ void AlignJudge(bool trans_a, bool trans_b, int32_t m, int32_t k, int32_t n, int32_t m_align,
                                    int32_t k_align, int32_t n_align, int32_t &aligned_a, int32_t &aligned_b)
{
    if (!trans_a) {
        aligned_a = k != k_align;
    } else {
        aligned_a = (m != m_align && m != 1);
    }
    if (!trans_b) {
        aligned_b = (n != n_align);
    } else {
        aligned_b = (k != k_align);
    }
}

inline __aicore__ void GetBlockIdx(int32_t loop_idx, int32_t m_loop, int32_t n_loop, int32_t swizzl_direction,
                                    int32_t swizzl_count, int64_t &m_idx, int64_t &n_idx)
{
    uint32_t in_batch_idx = loop_idx % (m_loop * n_loop);
    if (swizzl_direction == 0) { // Zn
        uint32_t tile_block_loop = (m_loop + swizzl_count - 1) / swizzl_count;
        uint32_t tile_block_idx = in_batch_idx / (swizzl_count * n_loop);
        uint32_t in_tile_block_idx = in_batch_idx % (swizzl_count * n_loop);
        uint32_t n_row = swizzl_count;
        if (tile_block_idx == tile_block_loop - 1) {
            n_row = m_loop - swizzl_count * tile_block_idx;
        }
        m_idx = tile_block_idx * swizzl_count + in_tile_block_idx % n_row;
        n_idx = in_tile_block_idx / n_row;
        if (tile_block_idx % 2 != 0) {
            n_idx = n_loop - n_idx - 1;
        }
    } else if (swizzl_direction == 1) {  // Nz
        uint32_t tile_block_loop = (n_loop + swizzl_count - 1) / swizzl_count;
        uint32_t tile_block_idx = in_batch_idx / (swizzl_count * m_loop);
        uint32_t in_tile_block_idx = in_batch_idx % (swizzl_count * m_loop);
        uint32_t n_col = swizzl_count;
        if (tile_block_idx == tile_block_loop - 1) {
            n_col = n_loop - swizzl_count * tile_block_idx;
        }
        m_idx = in_tile_block_idx / n_col;
        n_idx = tile_block_idx * swizzl_count + in_tile_block_idx % n_col;
        if (tile_block_idx % 2 != 0) {
            m_idx = m_loop - m_idx - 1;
        }
    }
}

template <typename T>
FORCE_INLINE_AICORE void CopyGmToUbufAlign(__ubuf__ T *dst, __gm__ T *src, uint16_t nBurst, uint32_t lenBurst,
                                            uint32_t gmGap, uint32_t ubufGap = 0)
{
    if constexpr (sizeof(T) == 8) {
        CopyGmToUbufAlign(reinterpret_cast<__ubuf__ int32_t *>(dst), reinterpret_cast<__gm__ int32_t *>(src),
                          nBurst * 2, lenBurst * 2, gmGap, ubufGap);
        return;
    }
    DataCopyParams dataCopyParams(nBurst, // blockCount
                                    (Block32B<T>::Count(lenBurst)), // blockLen
                                    (Block32B<T>::Count(gmGap)), // srcStride
                                    (ubufGap) // dstStride
    );
    DataCopyExtParams dataCopyAlignParams(nBurst, lenBurst * sizeof(T), gmGap * sizeof(T), ubufGap, 0);
    LocalTensor<T> ubTensor;
    TBuffAddr ubAddr;
    ubAddr.logicPos = static_cast<uint8_t>(TPosition::VECIN);
    ubAddr.bufferAddr = reinterpret_cast<uint64_t>(dst);
    ubTensor.SetAddr(ubAddr);
    GlobalTensor<T> gmTensor;
    gmTensor.SetGlobalBuffer(src);
    if (Block32B<T>::IsAligned(lenBurst) && Block32B<T>::IsAligned(gmGap)) {
        DataCopy(ubTensor, gmTensor, dataCopyParams);
    } else {
        DataCopyPadExtParams<T> padParams;
        DataCopyPad(ubTensor, gmTensor, dataCopyAlignParams, padParams);
    }
}

template <typename T>
FORCE_INLINE_AICORE void CopyUbufToGmAlign(__gm__ T *dst, __ubuf__ T *src, uint16_t nBurst, uint32_t lenBurst,
                                            uint32_t gmGap, uint32_t ubufGap = 0)
{
    DataCopyParams dataCopyParams(nBurst, // blockCount
                                    static_cast<uint16_t>(Block32B<T>::Count(lenBurst)), // blockLen
                                    static_cast<uint16_t>(ubufGap), // srcStride
                                    static_cast<uint16_t>(Block32B<T>::Count(gmGap)) // dstStride
    );
    DataCopyExtParams dataCopyAlignParams(nBurst, lenBurst * sizeof(T), ubufGap, gmGap * sizeof(T), 0);
    LocalTensor<T> ubTensor;
    TBuffAddr ubAddr;
    ubAddr.logicPos = static_cast<uint8_t>(TPosition::VECIN);
    ubAddr.bufferAddr = reinterpret_cast<uint64_t>(src);
    ubTensor.SetAddr(ubAddr);
    GlobalTensor<T> gmTensor;
    gmTensor.SetGlobalBuffer(dst);
    if (Block32B<T>::IsAligned(lenBurst) && Block32B<T>::IsAligned(gmGap)) {
        DataCopy(gmTensor, ubTensor, dataCopyParams);
    } else {
        DataCopyPadParams padParams;
        DataCopyPad(gmTensor, ubTensor, dataCopyAlignParams);
    }
}

template <typename T>
FORCE_INLINE_AICORE void CopyGmToUbufAlignB16(__ubuf__ T *dst, __gm__ T *src, uint16_t nBurst, uint32_t lenBurst,
                                                uint16_t srcStride, uint16_t dstStride)
{
    DataCopyExtParams dataCopyParams(nBurst, // blockCount
                                    lenBurst, // blockLen
                                    srcStride, // srcStride
                                    dstStride, // dstStride
                                    0);
    LocalTensor<uint8_t> ubTensor;
    TBuffAddr ubAddr;
    ubAddr.logicPos = static_cast<uint8_t>(TPosition::VECIN);
    ubAddr.bufferAddr = reinterpret_cast<uint64_t>(dst);
    ubTensor.SetAddr(ubAddr);
    GlobalTensor<uint8_t> gmTensor;
    gmTensor.SetGlobalBuffer(reinterpret_cast<GM_ADDR>(src));
    DataCopyPadExtParams<uint8_t> padParams;
    DataCopyPad(ubTensor, gmTensor, dataCopyParams, padParams);
}

template <typename T>
FORCE_INLINE_AICORE void CopyUbufToGmAlignB16(__gm__ T *dst, __ubuf__ T *src, uint16_t nBurst, uint32_t lenBurst,
                                                uint16_t srcStride, uint16_t dstStride)
{
    DataCopyExtParams dataCopyParams(nBurst, // blockCount
                                    lenBurst, // blockLen
                                    srcStride, // srcStride
                                    dstStride, // dstStride
                                    0);
    LocalTensor<uint8_t> ubTensor;
    TBuffAddr ubAddr;
    ubAddr.logicPos = static_cast<uint8_t>(TPosition::VECIN);
    ubAddr.bufferAddr = reinterpret_cast<uint64_t>(src);
    ubTensor.SetAddr(ubAddr);
    GlobalTensor<uint8_t> gmTensor;
    gmTensor.SetGlobalBuffer(reinterpret_cast<GM_ADDR>(dst));
    DataCopyPad(gmTensor, ubTensor, dataCopyParams);
}

template <typename T>
FORCE_INLINE_AICORE void CopyGmToUbuf(__ubuf__ T *dst, __gm__ T *src, uint16_t nBurst, uint32_t lenBurst,
                                                uint16_t srcStride, uint16_t dstStride)
{
    DataCopyParams dataCopyParams(nBurst, // blockCount
                                    lenBurst, // blockLen
                                    srcStride, // srcStride
                                    dstStride // dstStride
    );
    LocalTensor<T> ubTensor;
    TBuffAddr ubAddr;
    ubAddr.logicPos = static_cast<uint8_t>(TPosition::VECIN);
    ubAddr.bufferAddr = reinterpret_cast<uint64_t>(dst);
    ubTensor.SetAddr(ubAddr);
    GlobalTensor<T> gmTensor;
    gmTensor.SetGlobalBuffer(src);
    DataCopy(ubTensor, gmTensor, dataCopyParams);
}

template <typename T>
FORCE_INLINE_AICORE void CopyUbufToGm(__gm__ T *dst, __ubuf__ T *src, uint16_t nBurst, uint16_t lenBurst,
                                                uint16_t srcStride, uint16_t dstStride)
{
    DataCopyParams dataCopyParams(nBurst, // blockCount
                                    lenBurst, // blockLen
                                    srcStride, // srcStride
                                    dstStride // dstStride
    );
    LocalTensor<T> ubTensor;
    TBuffAddr ubAddr;
    ubAddr.logicPos = static_cast<uint8_t>(TPosition::VECIN);
    ubAddr.bufferAddr = reinterpret_cast<uint64_t>(src);
    ubTensor.SetAddr(ubAddr);
    GlobalTensor<T> gmTensor;
    gmTensor.SetGlobalBuffer(dst);
    DataCopy(gmTensor, ubTensor, dataCopyParams);
}

template <typename T>
FORCE_INLINE_AICORE void CopyUbufToGmUnknown(bool ALIGN, __gm__ T *dst, __ubuf__ T*src, uint16_t nBurst,
                                            uint32_t lenBurst, uint16_t srcStride, uint16_t dstStride)
{
    if (ALIGN) {
        CopyUbufToGm(dst, src, nBurst, lenBurst / 32, srcStride, dstStride / 32);
    } else {
        CopyUbufToGmAlignB16(dst, src, nBurst, lenBurst, srcStride, dstStride);
    }
}

template <typename T>
FORCE_INLINE_AICORE void VectorDup(__ubuf__ T *dst, const T &src, uint8_t repeat, uint16_t dstBlockStride,
                                    uint8_t dstRepeatStride)
{
    LocalTensor<T> ubTensor = CreateLocalTensor<T>(dst);
    Duplicate<T, false>(ubTensor, src, -1, repeat, dstBlockStride, dstRepeatStride);
}

template <typename T>
struct CoCBuffAddrAndArgs {
public:
    __aicore__ inline CoCBuffAddrAndArgs(COC_ARGS_FUN(T))
    {
        GlobalTensor<int> commArgsGm;
        commArgsGm.SetGlobalBuffer(reinterpret_cast<__gm__ int *>(coc_comm_args), 2);
        rank = commArgsGm.GetValue(0);
        localRank = commArgsGm.GetValue(1);
        rankSize = commArgsGm.GetValue(2);
        localRankSize = commArgsGm.GetValue(3);
        extraFlag = commArgsGm.GetValue(4);
        RDMA = (extraFlag & ExtraFlag::RDMA) != 0;
        TOPO_910B2C = (extraFlag & ExtraFlag::TOPO_910B2C) != 0;
        TOPO_910_93 = (extraFlag & ExtraFlag::TOPO_910_93) != 0;
        DETERMINISTIC = (extraFlag & ExtraFlag::DETERMINISTIC) != 0;
        QUANT_FP16 = (extraFlag & ExtraFlag::QUANT_FP16) != 0;
        QUANT_FP32 = (extraFlag & ExtraFlag::QUANT_FP32) != 0;
        GlobalTensor<__gm__ T *> peerMemsAddrGm;
        peerMemsAddrGm.SetGlobalBuffer(&(reinterpret_cast<__gm__ CoCCommArgs<T> *>(coc_comm_args))->peerMems[0],
                                       LCAL_MAX_RANK_SIZE);
        for (int i = 0; i < rankSize; ++i) {
            buff[i] = peerMemsAddrGm.GetValue(i);
        }
    }

    int rank; // attr rank_id, global rank
    int localRank;
    int rankSize; // global rank size
    int localRankSize;
    uint32_t extraFlag;
    bool RDMA;
    bool TOPO_910B2C;
    bool TOPO_910_93;
    bool DETERMINISTIC;
    bool QUANT_FP16;
    bool QUANT_FP32;
    __gm__ T *buff[LCAL_MAX_RANK_SIZE]; // 共享内存地址列表
    //int64_t sendCountMatrix[LCAL_MAX_RANK_SIZE * LCAL_MAX_RANK_SIZE];
};

FORCE_INLINE_AICORE void CommMatrixTrunc(__gm__ int32_t* global_tokens_per_expert_matrix, __gm__ int32_t* workspace, int32_t EP, int32_t local_expert_nums, int32_t maxOutputSize)
{
    int32_t expert_nums = local_expert_nums * EP;
    for (int32_t i = 0; i < EP; i++) {
        int32_t sum_tokens = 0;
        for (int32_t local_expert_id = 0; local_expert_id < local_expert_nums; local_expert_id++) {
            int32_t expert_id = i * local_expert_nums + local_expert_id;
            for (int32_t j = 0; j < EP; j++) {
                if (sum_tokens + global_tokens_per_expert_matrix[j * expert_nums + expert_id]
                    >= maxOutputSize) {
                    workspace[j * expert_nums + expert_id] = maxOutputSize - sum_tokens;
                    sum_tokens = maxOutputSize;
                } else {
                    workspace[j * expert_nums + expert_id] = global_tokens_per_expert_matrix[j * expert_nums + expert_id];
                    sum_tokens += global_tokens_per_expert_matrix[j * expert_nums + expert_id];
                }
            }
        }
    }
}

#endif // LCAL_COC_INTERNAL_H