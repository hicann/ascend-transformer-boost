/*
* Copyright (c) 2024 Huawei Technologies Co., Ltd.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#ifdef __CCE_KT_TEST__
#include "stub_def.h"
#include "stub_fun.h"
#endif

#include "gmm_add.h"

using namespace AscendC;
using namespace matmul;

constexpr MatmulConfig matmulCFG = CFG_MDL;
constexpr uint32_t thresholdBlockNum = 8;
constexpr uint32_t thresholdDimM = 5;

template <bool Trans = false, typename Dtype = half>
using xType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, Dtype, Trans>;

template <bool Trans = false, typename Dtype = half>
using weightType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, Dtype, Trans>;

using yType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, float>;

using biasType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, float>;

template <class T> __aicore__ inline void InitPrivateGmmTilingData(const __gm__ uint8_t *tiling, T *const_data)
{
    const __gm__ uint32_t *src = (const __gm__ uint32_t *)tiling;
    uint32_t *dst = (uint32_t *)const_data;
    for (auto i = 0; i < sizeof(T) / 4; i++) {
        *(dst + i) = *(src + i);
    }
}

/** @brief store variables for core split configuration
 */
struct MNConfig {
    uint32_t m = 0;
    uint32_t k = 0;
    uint32_t n = 0;
    uint32_t baseM = 0;
    uint32_t baseN = 0;
    uint32_t mIdx = 0;
    uint32_t nIdx = 0;
    uint32_t blockDimM = 0;
    uint32_t blockDimN = 0;
    uint32_t singleM = 0;
    uint32_t singleN = 0;
    uint64_t wBaseOffset = 0;
    uint64_t nAxisBaseOffset = 0;
    uint64_t mAxisBaseOffset = 0;
    uint64_t xBaseOffset = 0;
    uint64_t yBaseOffset = 0;
    uint64_t wOutOffset = 0;
    uint64_t workSpaceOffset = 0;
};

template <typename ComputeType> class GmmAddProcess {
protected:
    ComputeType &computeOp; // inernal computation operator
    const GmmBaseParams *__restrict gmmBaseParams;
    const TCubeTiling *__restrict mmTilingData;

    uint32_t blockIdx;
    uint32_t subBlockIdx;
    uint32_t coreIdx;
    uint32_t groupNum;
    int32_t preOffset;
    GM_ADDR groupListPtr;
    GlobalTensor<int64_t> groupListGm;
    GlobalTensor<int32_t> mListGm;
    GlobalTensor<int32_t> kListGm;
    GlobalTensor<int32_t> nListGm;

public:
    __aicore__ inline GmmAddProcess(ComputeType &computeOp_) : computeOp(computeOp_) {}

    __aicore__ inline void Init(const GmmBaseParams *__restrict gmmBaseParamsIn,
                                const TCubeTiling *__restrict mmTilingDataIn, GM_ADDR groupList, GM_ADDR tiling);

    __aicore__ inline void Process();

    __aicore__ inline void SetMNConfig(const int32_t splitValue, MNConfig &mnConfig);

    __aicore__ inline void SetMKN(const int32_t splitValue, MNConfig &mnConfig);

    __aicore__ inline void UpdateMnConfig(MNConfig &mnConfig);

    __aicore__ inline void MnBlockIdxCompute(MNConfig &mnConfig, const uint32_t curBlock, const uint32_t count,
                                             const uint32_t thresholdM_dimN);
    __aicore__ inline uint32_t GreatestCommonDivisor(uint32_t a, uint32_t b);
    __aicore__ inline uint32_t LeastCommonMultiple(uint32_t a, uint32_t b);
};

template <typename ComputeType>
__aicore__ inline uint32_t GmmAddProcess<ComputeType>::GreatestCommonDivisor(uint32_t a, uint32_t b)
{
    uint32_t c = a;
    if (a < b) {
        a = b;
        b = c;
    }
    while (b != 0) {
        c = a;
        a = b;
        b = c % b;
    }
    return a;
}

template <typename ComputeType>
__aicore__ inline uint32_t GmmAddProcess<ComputeType>::LeastCommonMultiple(uint32_t a, uint32_t b) { return a * b / GreatestCommonDivisor(a, b); }

template <typename ComputeType>
__aicore__ inline void GmmAddProcess<ComputeType>::Init(const GmmBaseParams *__restrict gmmBaseParamsIn,
                                                        const TCubeTiling *__restrict mmTilingDataIn, GM_ADDR groupList,
                                                        GM_ADDR tiling)
{
    blockIdx = GetBlockIdx();
    subBlockIdx = GetSubBlockIdx();
    coreIdx = blockIdx;
    int64_t coreRation = GetTaskRation();
    if (coreRation > 1) {
        coreIdx /= coreRation;
    }
    gmmBaseParams = gmmBaseParamsIn;
    mmTilingData = mmTilingDataIn;
    groupNum = gmmBaseParams->groupNum;
    groupListPtr = groupList;
    groupListGm.SetGlobalBuffer((__gm__ int64_t *)groupList);
    GET_TILING_DATA_MEMBER_ADDR(GmmTilingData, gmmArray, gmmArrayAddr, tiling); // custom macro
    mListGm.SetGlobalBuffer((__gm__ int32_t *)gmmArrayAddr);
    kListGm.SetGlobalBuffer((__gm__ int32_t *)(gmmArrayAddr + sizeof(int32_t) * MKN_LIST_LEN));
    nListGm.SetGlobalBuffer((__gm__ int32_t *)(gmmArrayAddr + sizeof(int32_t) * MKN_LIST_LEN * 2));
}

template <typename ComputeType>
__aicore__ inline void GmmAddProcess<ComputeType>::SetMNConfig(const int32_t splitValue, MNConfig &mnConfig)
{
    SetMKN(splitValue, mnConfig);
    mnConfig.baseM = mmTilingData->baseM;
    mnConfig.baseN = mmTilingData->baseN;
    mnConfig.singleM = mnConfig.baseM;
    mnConfig.singleN = mnConfig.baseN;
}

template <typename ComputeType>
__aicore__ inline void GmmAddProcess<ComputeType>::SetMKN(const int32_t splitValue, MNConfig &mnConfig)
{
    mnConfig.m = mListGm.GetValue(0);
    mnConfig.k = splitValue;
    mnConfig.n = nListGm.GetValue(0);
    return;
}

template <typename ComputeType> __aicore__ inline void GmmAddProcess<ComputeType>::UpdateMnConfig(MNConfig &mnConfig)
{
    mnConfig.wBaseOffset += mnConfig.k * mnConfig.n;
    mnConfig.nAxisBaseOffset += mnConfig.n;
    mnConfig.mAxisBaseOffset += mnConfig.m;
    mnConfig.xBaseOffset += mnConfig.m * mnConfig.k;
    mnConfig.yBaseOffset += mnConfig.m * mnConfig.n;
}

template <typename ComputeType>
__aicore__ inline void GmmAddProcess<ComputeType>::MnBlockIdxCompute(MNConfig &mnConfig, const uint32_t curBlock,
                                                                     const uint32_t count,
                                                                     const uint32_t thresholdM_dimN)
{
    if (mnConfig.blockDimM <= thresholdDimM || thresholdDimM == 1) {
        mnConfig.mIdx = (curBlock - count) / mnConfig.blockDimN;
        mnConfig.nIdx = (curBlock - count) % mnConfig.blockDimN;
    } else {
        uint32_t relativeBlock = curBlock - count;
        uint32_t curThresholdM = relativeBlock >= AlignDown(mnConfig.blockDimM * mnConfig.blockDimN, thresholdM_dimN)
                                     ? mnConfig.blockDimM % thresholdBlockNum
                                     : thresholdBlockNum;
        uint32_t curThresholdM_thresholdN = curThresholdM * thresholdBlockNum;
        uint32_t curThresholdN =
            relativeBlock % thresholdM_dimN >= AlignDown(curThresholdM * mnConfig.blockDimN, curThresholdM_thresholdN)
                ? mnConfig.blockDimN % thresholdBlockNum
                : thresholdBlockNum;

        uint32_t localRelativeBlock = relativeBlock % thresholdM_dimN % curThresholdM_thresholdN;
        mnConfig.mIdx = localRelativeBlock % curThresholdM + relativeBlock / thresholdM_dimN * thresholdBlockNum;
        mnConfig.nIdx = (localRelativeBlock + localRelativeBlock / LeastCommonMultiple(curThresholdM, curThresholdN)) %
                            curThresholdN +
                        relativeBlock % thresholdM_dimN / curThresholdM_thresholdN * thresholdBlockNum;
    }
}

template <typename ComputeType> __aicore__ inline void GmmAddProcess<ComputeType>::Process()
{
    MNConfig mnConfig;
    MNConfig mnPreConfig;
    if (gmmBaseParams->groupType != -1) { // -1: no split
        preOffset = 0;
        if (unlikely(groupListPtr == nullptr)) {
            groupNum = 0; // not continue Process
        }
    }
    for (uint32_t groupIdx = 0, count = 0; groupIdx < groupNum; ++groupIdx) {
        int32_t splitValue = GetSplitValueFromGroupList(groupIdx, preOffset, groupListGm);
        SetMNConfig(splitValue, mnConfig);
        uint32_t dimM = Ceil(mnConfig.m, mnConfig.singleM);
        uint32_t dimN = Ceil(mnConfig.n, mnConfig.singleN);
        mnConfig.blockDimM = dimM;
        mnConfig.blockDimN = dimN;
        if constexpr (ComputeType::transposeX) {
            if (mnConfig.k == 0) {
                UpdateMnConfig(mnConfig);
                continue;
            }
        }

        uint32_t curCount = count + dimM * dimN;
        uint32_t curBlock = coreIdx >= count ? coreIdx : coreIdx + gmmBaseParams->coreNum;
        uint32_t thresholdM_dimN = thresholdBlockNum * dimN;

        while (curBlock < curCount) {
            MnBlockIdxCompute(mnConfig, curBlock, count, thresholdM_dimN);
            computeOp.MmCompute(groupIdx, mnConfig, subBlockIdx);
            curBlock += gmmBaseParams->coreNum;
            mnPreConfig = mnConfig;
        }
        UpdateMnConfig(mnConfig);
        count = curCount % gmmBaseParams->coreNum;
    }
}

template <class mmType, bool sync = false> class GmmAddCompute {
public:
    using AT = typename mmType::AT::T;
    using BT = typename mmType::BT::T;
    using CT = typename mmType::CT::T;
    constexpr static bool transposeX = mmType::AT::isTrans;
    constexpr static bool transposeW = mmType::BT::isTrans;

    /** @brief constructor */
    __aicore__ inline GmmAddCompute(typename mmType::MT &mm_) : mm(mm_) {}

    __aicore__ inline void Init(GM_ADDR x, GM_ADDR weight, GM_ADDR groupList, GM_ADDR y,
                                const GmmBaseParams *__restrict gmmBaseParams,
                                const TCubeTiling *__restrict mmTilingData, TPipe *tPipe);

    __aicore__ inline void MmCompute(uint32_t groupIdx, MNConfig &mnConfig, uint32_t subBlockIdx);

    __aicore__ inline GlobalTensor<BT> SetGlobalBufferW(uint32_t groupIdx, uint32_t tailN, MNConfig &mnConfig);

    __aicore__ inline uint64_t SetWOffset(uint32_t tailN, uint32_t k);

protected:
    TPipe *pipe;
    typename mmType::MT &mm; // matmul operator
    bool hasBias = false;
    GM_ADDR xTensorPtr;
    GM_ADDR weightTensorPtr;
    GM_ADDR yTensorPtr;
    GlobalTensor<AT> xGm;
    GlobalTensor<BT> weightGm;
    GlobalTensor<CT> yGm;

    uint32_t cubeNum; // 核上已完成的matmul次数
    uint32_t aicIdx;
    uint32_t coreNum;
    bool mmWaitStatus;
};

template <typename mmType, bool sync>
__aicore__ inline void GmmAddCompute<mmType, sync>::Init(GM_ADDR x, GM_ADDR weight, GM_ADDR groupList, GM_ADDR y,
                                                         const GmmBaseParams *__restrict gmmBaseParams,
                                                         const TCubeTiling *__restrict mmTilingData, TPipe *tPipe)
{
    xTensorPtr = x;
    weightTensorPtr = weight;
    yTensorPtr = y;
    pipe = tPipe;
    coreNum = gmmBaseParams->coreNum;
    mmWaitStatus = false;
}

template <typename mmType, bool sync>
__aicore__ inline uint64_t GmmAddCompute<mmType, sync>::SetWOffset(uint32_t tailN, uint32_t k)
{
    uint64_t wOffset = 0;
    if constexpr (transposeW) {
        wOffset = tailN * k;
    } else {
        wOffset = tailN;
    }
    return wOffset;
}

template <typename mmType, bool sync>
__aicore__ inline GlobalTensor<typename mmType::BT::T>
GmmAddCompute<mmType, sync>::SetGlobalBufferW(uint32_t groupIdx, uint32_t tailN, MNConfig &mnConfig)
{
    uint64_t wOffset = SetWOffset(tailN, mnConfig.k);
    GlobalTensor<BT> weightGmLocal;
    weightGmLocal.SetGlobalBuffer(reinterpret_cast<__gm__ BT *>(weightTensorPtr) + mnConfig.wBaseOffset + wOffset);
    if (mnConfig.blockDimM == 1) {
        weightGmLocal.SetL2CacheHint(CacheMode::CACHE_MODE_DISABLE);
    }
    return weightGmLocal;
}

template <typename mmType, bool sync>
__aicore__ inline void GmmAddCompute<mmType, sync>::MmCompute(uint32_t groupIdx, MNConfig &mnConfig,
                                                              uint32_t subBlockIdx)
{
    if (subBlockIdx != 0) {
        return;
    }
    uint32_t tailN = mnConfig.nIdx * mnConfig.singleN;
    uint32_t curSingleN = mnConfig.nIdx < mnConfig.blockDimN - 1 ? mnConfig.singleN : mnConfig.n - tailN;
    uint32_t curSingleM =
        mnConfig.mIdx < mnConfig.blockDimM - 1 ? mnConfig.singleM : mnConfig.m - mnConfig.mIdx * mnConfig.singleM;
    uint64_t xOffset = mnConfig.mIdx * mnConfig.singleM * mnConfig.k;
    if constexpr (transposeX) {
        xOffset = mnConfig.mIdx * mnConfig.singleM;
    }
    uint64_t outOffset = mnConfig.mIdx * mnConfig.singleM * mnConfig.n + tailN;
    xGm.SetGlobalBuffer(reinterpret_cast<__gm__ AT *>(xTensorPtr) + mnConfig.xBaseOffset);
    GlobalTensor<BT> weightGmLocal = SetGlobalBufferW(groupIdx, tailN, mnConfig);
    mm.SetOrgShape(mnConfig.m, mnConfig.n, mnConfig.k);
    mm.SetSingleShape(curSingleM, curSingleN, mnConfig.k);
    mm.SetTensorA(xGm[xOffset], transposeX);
    mm.SetTensorB(weightGmLocal, transposeW);
    yGm.SetGlobalBuffer(reinterpret_cast<__gm__ CT *>(yTensorPtr) + mnConfig.yBaseOffset);
    mm.template IterateAll<sync>(yGm[outOffset], 1);
}

extern "C" __global__ __aicore__ void gmm_add(__gm__ uint8_t *__restrict__ x, __gm__ uint8_t *__restrict__ weight,
                                              __gm__ uint8_t *__restrict__ groupList, __gm__ uint8_t *__restrict__ y,
                                              __gm__ uint8_t *__restrict__ yRef, __gm__ uint8_t *__restrict__ tiling)
{
    TPipe tPipe;
    AscendCUtils::SetOverflow(1);
    KERNEL_TASK_TYPE_DEFAULT(KERNEL_TYPE_AIC_ONLY);

    GmmTilingData gmmTilingData;
    GmmBaseParams &gmmBaseParams = gmmTilingData.gmmBaseParams;
    TCubeTiling &mmTilingData = gmmTilingData.mmTilingData;
    InitPrivateGmmTilingData<GmmTilingData>(tiling, &gmmTilingData);
    if (TILING_KEY_IS(0)) {
        using matmulType = MmImplType<xType<true>, weightType<false>, yType, biasType, matmulCFG>;
        matmulType::MT mm;
        mm.SetSubBlockIdx(0);
        mm.Init(&mmTilingData, &tPipe);
        GmmAddCompute<matmulType, false> computeOp(mm);
        computeOp.Init(x, weight, groupList, yRef, &gmmBaseParams, &mmTilingData, &tPipe);
        GmmAddProcess<decltype(computeOp)> op(computeOp);
        op.Init(&gmmBaseParams, &mmTilingData, groupList, tiling);
        op.Process();
    } else if (TILING_KEY_IS(1)) {
        using matmulType =
            MmImplType<xType<true, bfloat16_t>, weightType<false, bfloat16_t>, yType, biasType, matmulCFG>;
        matmulType::MT mm;
        mm.SetSubBlockIdx(0);
        mm.Init(&mmTilingData, &tPipe);
        GmmAddCompute<matmulType, false> computeOp(mm);
        computeOp.Init(x, weight, groupList, yRef, &gmmBaseParams, &mmTilingData, &tPipe);
        GmmAddProcess<decltype(computeOp)> op(computeOp);
        op.Init(&gmmBaseParams, &mmTilingData, groupList, tiling);
        op.Process();
    }
}