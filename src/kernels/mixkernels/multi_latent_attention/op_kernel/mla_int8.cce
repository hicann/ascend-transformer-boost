/*
* Copyright (c) Huawei Technologies Co., Ltd. 2025. All rights reserved.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#ifdef __DAV_C220_CUBE__
//TP1 INT8 L1 Size
constexpr uint32_t TP1_INT8_L1_Q_SIZE = 128 * 512;
constexpr uint32_t TP1_INT8_L1_Q_ROPE_SIZE = 128 * 64 * 2;
constexpr uint32_t TP1_INT8_L1_K_SIZE = 128 * 512 * 2;
constexpr uint32_t TP1_INT8_L1_K_ROPE_SIZE = 128 * 64 * 4;
constexpr uint32_t TP1_INT8_L1_P_SIZE = 128 * 512 * 2;
constexpr uint32_t TP1_INT8_L1_V_SIZE = 128 * 512 * 2;
constexpr uint32_t TP1_INT8_Q_OFFSET = 0;
constexpr uint32_t TP1_INT8_Q_ROPE_OFFSET = TP1_INT8_Q_OFFSET+TP1_INT8_L1_Q_SIZE;
constexpr uint32_t TP1_INT8_K_OFFSET = TP1_INT8_Q_ROPE_OFFSET+TP1_INT8_L1_Q_ROPE_SIZE;
constexpr uint32_t TP1_INT8_K_ROPE_OFFSET = TP1_INT8_K_OFFSET+TP1_INT8_L1_K_SIZE;
constexpr uint32_t TP1_INT8_P_OFFSET = TP1_INT8_K_ROPE_OFFSET+TP1_INT8_L1_K_ROPE_SIZE;
constexpr uint32_t TP1_INT8_V_OFFSET = TP1_INT8_P_OFFSET+TP1_INT8_L1_P_SIZE;

template <TilingKeyType tilingKeyType, typename IN_ROPE_DTYPE,  typename OUT_DTYPE, typename IN_KVDTYPE,
          InputFormat KInputType, bool flashDecoding>
class MLAttentionDecoderAic<tilingKeyType, int8_t, IN_ROPE_DTYPE,  OUT_DTYPE, IN_KVDTYPE, KInputType, flashDecoding> {
    // define dtype
    using mm1OutputType = typename AttentionType<tilingKeyType>::mm1OutputType;
    using mm1CopyType = typename AttentionType<tilingKeyType>::mm1CopyType;
    using mmBiasType = typename AttentionType<tilingKeyType>::mmBiasType;
    using mmScaleType = typename AttentionType<tilingKeyType>::mmScaleType;
    using mm2OutputType = typename AttentionType<tilingKeyType>::mm2OutputType;
    using mm2CopyType = typename AttentionType<tilingKeyType>::mm2CopyType;
    static constexpr uint32_t T_CUBE_MATRIX_SIZE = CUBE_MATRIX_SIZE_512 / sizeof(int8_t);
    static constexpr uint32_t T_BLOCK_SIZE =  BLOCK_SIZE_32 / sizeof(int8_t);
    static constexpr uint32_t T_BLOCK_OFFSET = 2 / sizeof(int8_t);
    static constexpr int32_t L1_KV_HALF_SIZE = 73728;// 2* 128 * 256
    static constexpr int32_t L1_KV_UINT8_SIZE = 73728 * 2;

public:
    __aicore__ __attribute__((always_inline)) inline MLAttentionDecoderAic() {
    }

    __aicore__ __attribute__((always_inline)) inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t *__restrict__ q_in_gm,
        __gm__ uint8_t *__restrict__ q_rope_in_gm,
        __gm__ uint8_t *__restrict__ k_in_gm,
        __gm__ uint8_t *__restrict__ k_rope_in_gm,
        __gm__ uint8_t *__restrict__ block_tables_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ s_rope_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t *__restrict__ tiling_para_gm)
    {
        SetFftsBaseAddr((uint64_t)sync);
        SetPadding<uint64_t>(0);
        SetAtomicnone();
        SetNdpara(1, 0, 0);
        SetMasknorm();

        q_gm = reinterpret_cast<__gm__ int8_t *>(q_in_gm);
        q_rope_gm = reinterpret_cast<__gm__ IN_ROPE_DTYPE *>(q_rope_in_gm);
        k_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm);
        k_rope_gm = reinterpret_cast<__gm__ IN_ROPE_DTYPE *>(k_rope_in_gm);
        block_tables_gm = reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm);
        s_gm = reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm);

        p_gm = reinterpret_cast<__gm__ int8_t *>(p_out_gm);
        o_tmp_gm = reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm);
        tiling_gm = reinterpret_cast<__gm__ uint8_t *>(tiling_para_gm);

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(q_in_gm));
        q_rope_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_ROPE_DTYPE *>(q_rope_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm));
        k_rope_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_ROPE_DTYPE *>(k_rope_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(p_out_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm));
        block_tables_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm));
        s_rope_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(s_rope_out_gm));

        l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(l1q_buf_addr_offset);
        l1q_rope_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_ROPE_DTYPE>(l1q_rope_buf_addr_offset);
        l1kv_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(l1kv_buf_addr_offset);
        l1kv_rope_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_ROPE_DTYPE>(l1kv_rope_buf_addr_offset);
        l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(l1p_buf_addr_offset);

        num_batches = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM));
        block_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MAXBLOCKS));
        kv_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVHEADS));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        cur_qn_blk_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_SIZE));
        mask_type = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MASK_TYPE_ND));
        totalTaskNum = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TASK_NUM));
        maxKVSeqLen = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MAX_KVSEQLEN));
        flashDecodingTaskNum = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + TILING_DECODINGNUM));

        num_batches_pad = RoundUp<16>(num_batches);

        stride_kv = static_cast<uint64_t>(kv_heads) * 512;
        stride_kv_rope = static_cast<uint64_t>(kv_heads) * 64;

        __k = embedding_size;
        round_k = RoundUp<T_BLOCK_SIZE>(__k);
        __v = embedding_size;
        stride_vo = static_cast<uint64_t>(kv_heads) * embedding_size;
        round_v = RoundUp<BLOCK_SIZE>(__v);
        embed_split_size_qk = 128;
        embed_split_loop_qk = (embedding_size + embed_split_size_qk - 1) / embed_split_size_qk;
    }


    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID4);
        SET_FLAG(M, MTE1, EVENT_ID5);
        SET_FLAG(M, MTE1, EVENT_ID6);
	    SET_FLAG(M, MTE1, EVENT_ID7);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(MTE1, MTE2, EVENT_ID6);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        SET_FLAG(FIX, MTE1, EVENT_ID0);
        SET_FLAG(FIX, MTE1, EVENT_ID1);
        SET_FLAG(FIX, MTE1, EVENT_ID2);
        SET_FLAG(FIX, MTE1, EVENT_ID3);
        SET_FLAG(FIX, MTE1, EVENT_ID4);
        SET_FLAG(FIX, MTE1, EVENT_ID5);
        SET_FLAG(MTE2, FIX, EVENT_ID0);


        uint64_t cur_batch = 0;

        uint32_t q_block_num_per_batch = (q_heads + cur_qn_blk_size - 1) / cur_qn_blk_size;
        uint32_t process_num = q_block_num_per_batch * num_batches;

        for (uint32_t process = block_idx; process < process_num; process += (uint32_t)block_num) {  // for task
            cur_batch = process / q_block_num_per_batch;
            if (cur_batch >= num_batches) break;

            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t start_core_idx = (cur_batch * q_block_num_per_batch) % block_num;

            uint32_t q_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;

            uint32_t start_head = (process % q_block_num_per_batch) * cur_qn_blk_size;
            uint32_t start_kv = 0;
            uint32_t cur_q_seq_len = q_seqlen;
            uint32_t cur_kv_seqlen = kv_seqlen;
            uint32_t cur_head_num = cur_qn_blk_size;

            InnerRunCubeMLA(cur_batch, start_head, cur_head_num, start_kv, cur_q_seq_len, cur_kv_seqlen,
                            offset_tiling);
        }
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID4);
        WAIT_FLAG(M, MTE1, EVENT_ID5);
        WAIT_FLAG(M, MTE1, EVENT_ID6);
        WAIT_FLAG(M, MTE1, EVENT_ID7);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        WAIT_FLAG(FIX, MTE1, EVENT_ID0);
        WAIT_FLAG(FIX, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, MTE1, EVENT_ID2);
        WAIT_FLAG(FIX, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, MTE1, EVENT_ID4);
        WAIT_FLAG(FIX, MTE1, EVENT_ID5);
        WAIT_FLAG(MTE2, FIX, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

    __aicore__ __attribute__((always_inline)) inline void RunTP1()
    {
        l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(TP1_INT8_Q_OFFSET);
        l1q_rope_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_ROPE_DTYPE>(TP1_INT8_Q_ROPE_OFFSET);
        l1kv_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(TP1_INT8_K_OFFSET);
        l1kv_rope_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_ROPE_DTYPE>(TP1_INT8_K_ROPE_OFFSET);
        l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(TP1_INT8_P_OFFSET);
        l1v_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(TP1_INT8_V_OFFSET);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID4);
        SET_FLAG(M, MTE1, EVENT_ID5);
        SET_FLAG(M, MTE1, EVENT_ID6);
	    SET_FLAG(M, MTE1, EVENT_ID7);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(MTE1, MTE2, EVENT_ID6);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        SET_FLAG(FIX, MTE1, EVENT_ID0);
        SET_FLAG(FIX, MTE1, EVENT_ID1);
        SET_FLAG(FIX, MTE1, EVENT_ID2);
        SET_FLAG(FIX, MTE1, EVENT_ID3);
        SET_FLAG(FIX, MTE1, EVENT_ID4);
        SET_FLAG(FIX, MTE1, EVENT_ID5);
        SET_FLAG(MTE2, FIX, EVENT_ID0);

        for(uint32_t process = block_idx; process < totalTaskNum; process += block_num){
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * process;
            uint32_t cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling));
            uint32_t q_row_id = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 1));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 2));
            uint32_t q_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 3));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            uint32_t start_head = q_row_id * q_heads;
            uint32_t start_kv = 0;
            uint32_t cur_q_seq_len = q_seqlen;
            uint32_t cur_kv_seqlen = kv_seqlen;
            uint32_t cur_head_num = q_heads;
            InnerRunCubeMLATP1(cur_batch, start_head, cur_head_num, start_kv, cur_q_seq_len, cur_kv_seqlen, offset_tiling);
        }

        if constexpr (flashDecoding) {
            for (uint32_t process = block_idx; process < flashDecodingTaskNum; process += (uint32_t)block_num) {  // for task
                uint32_t offset_tiling = tiling_head_size + tiling_para_size * (totalTaskNum + process);
                uint32_t cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling));
                uint32_t q_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 1));
                uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 2));
                if (kv_seqlen == 0) {
                    continue;
                }
                uint32_t start_kv = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 3));
                uint32_t prev_task = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + offset_tiling + 4));
                uint32_t start_head = prev_task * q_heads;
                uint32_t cur_q_seq_len = 1;
                uint32_t cur_kv_seqlen = kv_seqlen;
                uint32_t cur_head_num = q_heads;
                InnerRunCubeMLATP1(cur_batch, start_head, cur_head_num, start_kv, cur_q_seq_len, cur_kv_seqlen, offset_tiling);
            }
        }
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID4);
        WAIT_FLAG(M, MTE1, EVENT_ID5);
        WAIT_FLAG(M, MTE1, EVENT_ID6);
        WAIT_FLAG(M, MTE1, EVENT_ID7);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        WAIT_FLAG(FIX, MTE1, EVENT_ID0);
        WAIT_FLAG(FIX, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, MTE1, EVENT_ID2);
        WAIT_FLAG(FIX, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, MTE1, EVENT_ID4);
        WAIT_FLAG(FIX, MTE1, EVENT_ID5);
        WAIT_FLAG(MTE2, FIX, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

private:
    __aicore__ __attribute__((always_inline)) inline void InnerRunCubeMLA(uint32_t cur_batch, uint32_t start_head, uint32_t cur_head_num,
        uint32_t start_kv, uint32_t cur_q_seqlen, uint32_t cur_kv_seqlen, uint32_t offset_tiling)
    {
        uint32_t addr_q_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 2 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 3 + offset_tiling));
        uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
        uint64_t q_offset = addr_q_scalar * 512 + start_head * 512;
        uint64_t q_rope_offset = addr_q_scalar * 64 + start_head * 64;

        uint32_t pp_n_scalar = block_size;
        uint32_t sub_n_loop = pp_n_scalar / block_size;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);
        uint32_t qk_round_n_l1 = RoundUp<T_BLOCK_SIZE>(qk_n);
        uint32_t qk_round_n_2_l1 = RoundUp<T_BLOCK_SIZE>(qk_n_2);
        uint64_t hidden_size = 576;
        if constexpr(tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
            hidden_size = 512;
        }
        uint64_t k_round_n = qk_round_n;
        uint32_t row_num  = cur_head_num * cur_q_seqlen;
        m = RoundUp<16>(row_num);

        // copy Q
        if (cur_q_seqlen == 1) {
            gm_to_l1<ArchType::ASCEND_V220, int8_t, DataFormat::ND, DataFormat::NZ>(
                l1q_buf_addr_tensor,
                q_gm_tensor[q_offset],
                cur_head_num,        // nValue
                RoundUp<16>(cur_head_num),// dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                512,                   // dValue
                0,                     // dstNzMatrixStride, unused
                512                   // srcDValue
            );
        } else {
            if (q_heads < 128) {
                AscendC::DataCopy(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    AscendC::Nd2NzParams(
                        cur_q_seqlen,                // ndNum
                        cur_head_num,                 // nValue
                        512,                            // dValue
                        512 * q_heads,        // srcNdMatrixStride
                        512,    // srcDValue
                        RoundUp<16>(cur_head_num * cur_q_seqlen), // dstNzC0Stride
                        cur_q_seqlen,                   // dstNzNStride
                        16             // dstNzMatrixStride
                    )
                );
            } else {
                for (uint32_t ii =0; ii < cur_q_seqlen; ii++) {
                    AscendC::DataCopy(
                        l1q_buf_addr_tensor[ii * 16], // offset one datablock
                        q_gm_tensor[q_offset + ii * q_heads * 512],
                        AscendC::Nd2NzParams(
                            1,                // ndNum
                            cur_head_num,                 // nValue
                            512,                            // dValue
                            0,        // srcNdMatrixStride
                            512,    // srcDValue
                            RoundUp<16>(cur_q_seqlen * cur_head_num), // dstNzC0Stride
                            cur_q_seqlen,                   // dstNzNStride
                            16             // dstNzMatrixStride
                        )
                    );
                }
            }

        }
        if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
            gm_to_l1<ArchType::ASCEND_V220, IN_ROPE_DTYPE, DataFormat::ND, DataFormat::NZ>(
                l1q_rope_buf_addr_tensor,
                q_rope_gm_tensor[q_rope_offset],
                cur_head_num,        // nValue
                RoundUp<16>(cur_head_num),// dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                64,                   // dValue
                0,                     // dstNzMatrixStride, unused
                64                   // srcDValue
            );
        } else {
            AscendC::DataCopy(
                l1q_buf_addr_tensor[RoundUp<16>(cur_head_num * cur_q_seqlen) * 512],
                q_rope_gm_tensor[q_rope_offset],
                AscendC::Nd2NzParams(
                    cur_head_num,                // ndNum, 32
                    cur_q_seqlen,                 // nValue, 4
                    64,                            // dValue
                    64,                 // srcNdMatrixStride
                    64 * q_heads,                            // srcDValue
                    RoundUp<16>(cur_head_num * cur_q_seqlen),    // dstNzC0Stride
                    1,                              // dstNzNStride
                    16 * cur_q_seqlen             // dstNzMatrixStride
                )
            );
        }
        SET_FLAG(MTE2, MTE1, EVENT_ID0);
        WAIT_FLAG(MTE2, MTE1, EVENT_ID0);
        for (uint32_t n_idx = 0; n_idx < n_loop + 1; n_idx+=1) {
            if (n_idx != n_loop) {
                uint32_t l1_kv_pingpong_flag = n_idx % 2;
                if (n_idx == (n_loop - 1)) {
                    qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                    qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
                    qk_round_n_l1 = RoundUp<T_BLOCK_SIZE>(qk_n);
                }
                if constexpr(tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                    k_round_n = qk_round_n_l1;
                } else {
                    k_round_n = qk_round_n;
                }
                uint64_t hiddenSize_offset = start_head * cur_q_seqlen * embedding_size;
                uint32_t embed_split_size = 128;
                uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);

                /* ************ CUBE1 stage1  ************* */

                uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                                cur_batch * max_num_blocks_per_query + start_kv / block_size + n_idx));
                int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv;
                int64_t kv_offset_rope = (int64_t)block_table_id * block_size * stride_kv_rope;
                uint32_t q_load_coeff = 1;
                q_load_coeff = m;
                WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag);  // 等待V全部搬入L0B
                if constexpr(KInputType == InputFormat::ND_FORMAT) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 576],
                        k_gm_tensor[kv_offset],
                        qk_n,         // nValue
                        qk_round_n,             // dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        512,            // dValue
                        0,                     // dstNzMatrixStride, unused
                        stride_kv            // srcDValue
                    );
                    SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag);
                    WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);  // 等待V全部搬入L0B
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 576 + 512 * qk_round_n],
                        k_rope_gm_tensor[kv_offset_rope],
                        qk_n,         // nValue
                        qk_round_n,             // dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        64,            // dValue
                        0,                     // dstNzMatrixStride, unused
                        stride_kv_rope            // srcDValue
                    );
                } else if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 512],
                        k_gm_tensor[kv_offset],
                        qk_round_n_l1,
                        qk_round_n_l1,         // nValue
                        block_size,         // dstNzC0Stride
                        512,            // dValue
                        512,            // dValue
                        0            // srcDValue
                    );
                    SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag);
                    WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);  // 等待V全部搬入L0B
                    gm_to_l1<ArchType::ASCEND_V220, IN_ROPE_DTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1kv_rope_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 64],
                        k_rope_gm_tensor[kv_offset_rope],
                        qk_round_n,
                        qk_round_n,         // nValue
                        block_size,             // dstNzC0Stride
                        64,            // dValue
                        64,                     // dstNzMatrixStride, unused
                        0            // srcDValue
                    );
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 576],
                        k_gm_tensor[kv_offset],
                        qk_round_n,         // nValue
                        qk_round_n,         // nValue
                        block_size,             // dstNzC0Stride
                        512,            // dValue
                        512,            // dValue
                        0            // srcDValue
                    );

                    SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag);
                    WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);  // 等待V全部搬入L0B
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 576 + 512 * qk_round_n],
                        k_rope_gm_tensor[kv_offset_rope],
                        qk_round_n,         // nValue
                        qk_round_n,         // nValue
                        block_size,             // dstNzC0Stride
                        64,            // dValue
                        64,                     // dstNzMatrixStride, unused
                        0            // srcDValue
                    );
                }

                SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 2);
                uint64_t hidden_split_time = (hidden_size + 128 - 1) / 128;
                uint64_t embed_split_idx = 0;
                for (embed_split_idx = 0; embed_split_idx < hidden_split_time; ++embed_split_idx) {
                    if (embed_split_idx == 4) {
                        embed_split_size = 64;
                        round_embed_split_size = 64;
                    }
                    WAIT_FLAG(M, MTE1, embed_split_idx % 2);

                    for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                        l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf_tensor[embed_split_idx % 2 * 16384 + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                            l1q_buf_addr_tensor[embed_split_idx * m * 128 + loa_load_idx * T_CUBE_MATRIX_SIZE],
                            0,
                            round_embed_split_size / T_BLOCK_SIZE,                                 // repeat
                            0,
                            q_load_coeff / BLOCK_SIZE,                            // srcStride
                            0,
                            0                                                     // dstStride
                        );
                    }

                    SET_FLAG(MTE1, M, embed_split_idx % 2);

                    if (embed_split_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, l1_kv_pingpong_flag);
                    }
                    if (embed_split_idx == 4) {
                        WAIT_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 2);
                    }
                    WAIT_FLAG(M, MTE1, embed_split_idx % 2 + 2);
                    l1_to_l0_b<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor[embed_split_idx % 2 * 16384],
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * hidden_size + embed_split_idx * k_round_n * 128],
                        0,
                        round_embed_split_size * k_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                        // srcStride
                        0,
                        0                                        // dstStride
                    );
                    if (embed_split_idx == 4) {
                        SET_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);
                    }
                    SET_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                    WAIT_FLAG(MTE1, M, embed_split_idx % 2);
                    WAIT_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                    if (embed_split_idx == 0) {
                        WAIT_FLAG(FIX, M, l1_kv_pingpong_flag);
                    }
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                        mmad<ArchType::ASCEND_V220, int8_t, int8_t, mm1OutputType, false>(
                            mm1_l0c_buf_tensor[l1_kv_pingpong_flag * 16384],
                            l0a_buf_tensor[embed_split_idx % 2 * 16384],
                            l0b_buf_tensor[embed_split_idx % 2 * 16384],
                            m,     // m
                            qk_round_n_l1,  // n
                            embed_split_size,   // k
                            embed_split_idx == 0     // cmatrixInitVal
                        );
                    } else {
                        mmad<ArchType::ASCEND_V220, int8_t, int8_t, mm1OutputType, false>(
                            mm1_l0c_buf_tensor[l1_kv_pingpong_flag * 16384],
                            l0a_buf_tensor[embed_split_idx % 2 * 16384],
                            l0b_buf_tensor[embed_split_idx % 2 * 16384],
                            m,     // m
                            qk_n,  // n
                            embed_split_size,   // k
                            embed_split_idx == 0     // cmatrixInitVal
                        );
                    }

                    PIPE_BARRIER(M);
                    SET_FLAG(M, MTE1, embed_split_idx % 2);
                    SET_FLAG(M, MTE1, embed_split_idx % 2 + 2);

                    // copy S to gm
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                        if (embed_split_idx == 3) {
                            SET_FLAG(M, FIX, l1_kv_pingpong_flag);
                            WAIT_FLAG(M, FIX, l1_kv_pingpong_flag);

                            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + (uint64_t)(n_idx % 2) * TMP_SIZE_DECODER / 2],
                                mm1_l0c_buf_tensor[l1_kv_pingpong_flag * 16384],
                                m,           // MSize
                                qk_n,  // NSize
                                RoundUp<16>(m), // srcStride
                                qk_round_n  // dstStride_dst_D
                            );
                            SET_FLAG(FIX, M, l1_kv_pingpong_flag);
                        }
                    }
                    if (embed_split_idx == 4) {
                        SET_FLAG(M, FIX, l1_kv_pingpong_flag);
                        WAIT_FLAG(M, FIX, l1_kv_pingpong_flag);

                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                            s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + (uint64_t)(n_idx % 2) * TMP_SIZE_DECODER / 2],
                            mm1_l0c_buf_tensor[l1_kv_pingpong_flag * 16384],
                            m,           // MSize
                            qk_round_n,  // NSize
                            RoundUp<16>(m), // srcStride
                            qk_round_n  // dstStride_dst_D
                        );
                        SET_FLAG(FIX, M, l1_kv_pingpong_flag);
                    }
                }
                if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                    embed_split_idx = 4;
                    embed_split_size = 64;
                    round_embed_split_size = 64;
                    WAIT_FLAG(M, MTE1, embed_split_idx % 2);

                    for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                        l1_to_l0_a<ArchType::ASCEND_V220, IN_ROPE_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384 * 2 + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                            l1q_rope_buf_addr_tensor[loa_load_idx * CUBE_MATRIX_SIZE],
                            0,
                            round_embed_split_size / BLOCK_SIZE,                                 // repeat
                            0,
                            q_load_coeff / BLOCK_SIZE,                            // srcStride
                            0,
                            0                                                     // dstStride
                        );
                    }

                    SET_FLAG(MTE1, M, embed_split_idx % 2);

                    WAIT_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 2);
                    WAIT_FLAG(M, MTE1, embed_split_idx % 2 + 2);
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_ROPE_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384 * 2],
                        l1kv_rope_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 64],
                        0,
                        round_embed_split_size * qk_round_n / CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                        // srcStride
                        0,
                        0                                        // dstStride
                    );

                    SET_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);
                    SET_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                    WAIT_FLAG(MTE1, M, embed_split_idx % 2);
                    WAIT_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                        WAIT_FLAG(FIX, M, l1_kv_pingpong_flag);
                    }
                    mmad<ArchType::ASCEND_V220, IN_ROPE_DTYPE, IN_ROPE_DTYPE, float, false>(
                        mm1_l0c_buf_tensor.template ReinterpretCast<float>()[l1_kv_pingpong_flag * 16384],
                        l0a_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384 * 2],
                        l0b_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384 * 2],
                        m,     // m
                        qk_n,  // n
                        embed_split_size,   // k
                        1     // cmatrixInitVal
                    );
                    PIPE_BARRIER(M);
                    SET_FLAG(M, MTE1, embed_split_idx % 2);
                    SET_FLAG(M, MTE1, embed_split_idx % 2 + 2);

                    SET_FLAG(M, FIX, l1_kv_pingpong_flag);
                    WAIT_FLAG(M, FIX, l1_kv_pingpong_flag);
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                            s_rope_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + (uint64_t)(n_idx % 2) * TMP_SIZE_DECODER / 2],
                            mm1_l0c_buf_tensor.template ReinterpretCast<float>()[l1_kv_pingpong_flag * 16384],
                            m,           // MSize
                            qk_round_n,  // NSize
                            RoundUp<16>(m), // srcStride
                            qk_round_n  // dstStride_dst_D
                        );
                    } else {
                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                            s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + (uint64_t)(n_idx % 2) * TMP_SIZE_DECODER / 2],
                            mm1_l0c_buf_tensor[l1_kv_pingpong_flag * 16384],
                            m,           // MSize
                            qk_round_n,  // NSize
                            RoundUp<16>(m), // srcStride
                            qk_round_n  // dstStride_dst_D
                        );
                    }
                    SET_FLAG(FIX, M, l1_kv_pingpong_flag);
                }
                FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_DECODER);
            }
            /* ************ CUBE2 stage1  ************* */
            if (n_idx != 0) {
                if (n_idx == n_loop) {
                    qk_n_2 = (cur_kv_seqlen - (n_idx - 1) * pp_n_scalar);
                    qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);
                    qk_round_n_2_l1 = RoundUp<T_BLOCK_SIZE>(qk_n_2);
                }
                k_round_n = qk_round_n_2_l1;
                uint32_t l1_kv_pingpong_flag = (n_idx - 1) % 2;
                uint32_t l0_p_pingpong_flag = (n_idx - 1) % 2;
                uint32_t embed_split_size = 128;
                embed_split_loop_v = 4;
                uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                    uint32_t l0c_pingpong_flag = (n_idx + embed_split_idx) % 2;
                    uint32_t l0b_pingpong_flag = (embed_split_idx + 1) % 2;
                    uint64_t l1kv_offset = embed_split_idx * k_round_n * round_embed_split_size;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                    AscendC::LoadData2dTransposeParams loadDataParams;
                    loadDataParams.dstGap = 0;
                    loadDataParams.startIndex = 0;
                    loadDataParams.dstFracGap = 0;
                    if (k_round_n <= round_embed_split_size) { // Nz -> nZ
                        loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                        loadDataParams.srcStride = k_round_n / T_BLOCK_SIZE;
                        uint16_t dstGap = sizeof(int8_t) == 1 ? 1 : 0;
                        loadDataParams.dstGap = dstGap;
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < k_round_n / T_BLOCK_SIZE; ++l0b_load_idx) {
                            // 沿 embd 方向搬
                            AscendC::LoadDataWithTranspose(
                                    l0b_buf_tensor[l0b_pingpong_flag * 16384 + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                    l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * hidden_size + l1kv_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                    loadDataParams);
                        }
                    } else {
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_embed_split_size / T_BLOCK_SIZE; ++l0b_load_idx) {
                            // 沿 kv_len_blk方向搬
                            loadDataParams.repeatTimes = qk_round_n_2 / T_BLOCK_SIZE;
                            loadDataParams.srcStride = 1;
                            loadDataParams.dstGap = round_embed_split_size / BLOCK_SIZE - 1;
                            AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[l0b_pingpong_flag * 16384 + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * hidden_size + l1kv_offset + l0b_load_idx * qk_round_n_2 * T_BLOCK_SIZE],
                                loadDataParams);
                        }
                    }
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        SET_FLAG(MTE1, MTE2, l1_kv_pingpong_flag);
                    }
                    // move p from gm to l1
                    uint32_t p_move_head_num = row_num;
                    if (embed_split_idx == 0) {
                        WaitFlagDev(SOFTMAX_READY_DECODER);

                        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
                        gm_to_l1<ArchType::ASCEND_V220, int8_t, DataFormat::ND, DataFormat::NZ>(
                            l1p_buf_addr_tensor,
                            p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET + ((n_idx - 1) % 2) * TMP_SIZE * T_BLOCK_OFFSET / 2],
                            p_move_head_num,         // nValue
                            RoundUp<BLOCK_SIZE>(p_move_head_num),// dstNzC0Stride
                            0,                     // dstNzMatrixStride, unused
                            k_round_n,           // dValue
                            0,                     // dstNzMatrixStride, unused
                            qk_round_n_2 * 2 / sizeof(int8_t)           // srcDValue
                        );
                        SET_FLAG(MTE2, MTE1, EVENT_ID7);
                        WAIT_FLAG(MTE2, MTE1, EVENT_ID7);
                        // move p from l1 to l0a
                        WAIT_FLAG(M, MTE1, l0_p_pingpong_flag);
                        uint32_t p_load_coeff = RoundUp<16>(p_move_head_num);
                        if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_DATA) {
                            l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::NZ, DataFormat::ZZ>(
                                l0a_buf_tensor[l0_p_pingpong_flag * 16384], l1p_buf_addr_tensor, RoundUp<BLOCK_SIZE>(p_move_head_num),
                                qk_round_n_2_l1, // repeat
                                0,
                                0, // srcStride
                                0,
                                0 // dstStride
                            );
                        } else {
                            for (uint64_t loa_load_idx = 0; loa_load_idx < p_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                                l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                    l0a_buf_tensor[l0_p_pingpong_flag * 16384 + loa_load_idx * qk_round_n_2 * BLOCK_SIZE],
                                    l1p_buf_addr_tensor[loa_load_idx * T_CUBE_MATRIX_SIZE],
                                    0,
                                    qk_round_n_2 / T_BLOCK_SIZE,                                 // repeat
                                    0,
                                    p_load_coeff / BLOCK_SIZE,                               // srcStride
                                    0,
                                    0                                                        // dstStride
                                );
                            }
                        }
                        SET_FLAG(MTE1, MTE2, EVENT_ID7);
                    }
                    SET_FLAG(MTE1, M, l0b_pingpong_flag);
                    WAIT_FLAG(MTE1, M, l0b_pingpong_flag);
                    WAIT_FLAG(FIX, M, l0c_pingpong_flag);
                    mmad<ArchType::ASCEND_V220, int8_t, int8_t, mm2OutputType, false>(
                        mm2_l0c_buf_tensor[l0c_pingpong_flag * 16384],
                        l0a_buf_tensor[l0_p_pingpong_flag * 16384],
                        l0b_buf_tensor[l0b_pingpong_flag * 16384],
                        m,     // m
                        embed_split_size,   // n
                        qk_n_2,  // k
                        1      // cmatrixInitVal
                    );
                    SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        SET_FLAG(M, MTE1, l0_p_pingpong_flag);
                    }
                    SET_FLAG(M, FIX, l0c_pingpong_flag);
                    WAIT_FLAG(M, FIX, l0c_pingpong_flag);

                    // copy O to gm
                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm2CopyType, mm2OutputType>(
                        o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 + embed_split_idx * round_embed_split_size + ((n_idx - 1) % 2) * TMP_SIZE],
                        mm2_l0c_buf_tensor[l0c_pingpong_flag * 16384],
                        m,        // MSize
                        RoundUp<16>(embed_split_size),  // NSize 32B对齐，防止workspace补齐的位置中有脏数据
                        RoundUp<16>(m),       // srcStride
                        round_v  // dstStride_dst_D
                    );
                    SET_FLAG(FIX, M, l0c_pingpong_flag);
                }
                FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_DECODER);
            }
        }
    }

    __aicore__ __attribute__((always_inline)) inline void InnerRunCubeMLATP1(uint32_t cur_batch, uint32_t start_head, uint32_t cur_head_num,
        uint32_t start_kv, uint32_t cur_q_seqlen, uint32_t cur_kvs_seqlen, uint32_t offset_tiling)
    {
        uint32_t cur_kv_seqlen = cur_kvs_seqlen - cur_q_seqlen + 1;
        uint64_t addr_q_scalar = 0;
        uint64_t q_offset = addr_q_scalar * 512 + start_head * 512;
        uint64_t q_rope_offset = addr_q_scalar * 64 + start_head * 64;

        uint32_t pp_n_scalar = block_size;
        uint32_t sub_n_loop = pp_n_scalar / block_size;

        uint32_t n_loop = (cur_kv_seqlen + cur_q_seqlen - 1 + pp_n_scalar - 1) / pp_n_scalar;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);

        uint32_t row_num  = cur_head_num * cur_q_seqlen;

        uint32_t sv_n = n_loop == 1 ? cur_kv_seqlen : pp_n_scalar;
        m = RoundUp<16>(row_num);
        gm_to_l1<ArchType::ASCEND_V220, int8_t, DataFormat::ND, DataFormat::NZ>(
            l1q_buf_addr_tensor,
            q_gm_tensor[q_offset],
            row_num,        // nValue
            RoundUp<16>(row_num),// dstNzC0Stride
            0,                     // dstNzMatrixStride, unused
            512,                   // dValue
            0,                     // dstNzMatrixStride, unused
            512                   // srcDValue
        );
        gm_to_l1<ArchType::ASCEND_V220, IN_ROPE_DTYPE, DataFormat::ND, DataFormat::NZ>(
            l1q_rope_buf_addr_tensor,
            q_rope_gm_tensor[q_rope_offset],
            row_num,        // nValue
            RoundUp<16>(row_num),// dstNzC0Stride
            0,                     // dstNzMatrixStride, unused
            64,                   // dValue
            0,                     // dstNzMatrixStride, unused
            64                   // srcDValue
        );


        SET_FLAG(MTE2, MTE1, EVENT_ID0);
        WAIT_FLAG(MTE2, MTE1, EVENT_ID0);
        uint32_t q_load_coeff = 128;//m;
        uint32_t l0a_pingpong_size = L0AB_UINT8_BUF_SIZE / sizeof(int8_t);
        uint32_t l0b_pingpong_size = L0AB_UINT8_BUF_SIZE / sizeof(int8_t);
        uint32_t l0c_pingpong_size = L0C_FLOAT_BUF_SIZE;
        uint32_t s_block_stack = 4;
        for (uint32_t n_idx = 0; n_idx < n_loop + s_block_stack; n_idx+=s_block_stack) {
            if (n_idx < n_loop) {
                uint32_t sv_n_triu = n_loop * pp_n_scalar;
                if (n_idx + s_block_stack > n_loop - 1) {
                    sv_n = cur_kv_seqlen - n_idx * pp_n_scalar; // delete
                } else {
                    sv_n = pp_n_scalar * s_block_stack;
                }
                uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                for (uint32_t split_idx = 0; split_idx < s_block_stack && n_idx + split_idx < n_loop; split_idx++) {
                    uint32_t now_idx = n_idx + split_idx;
                    uint32_t l1_kv_pingpong_flag = now_idx % 2;
                    if (now_idx == (n_loop - 1)) {
                        qk_n = (cur_kv_seqlen - now_idx * pp_n_scalar);
                        qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
                    } else  {
                        qk_n = pp_n_scalar;
                        qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
                    }
                    bool last_split = split_idx == s_block_stack - 1 || now_idx == n_loop - 1;
                    /* ************ CUBE1 stage1  ************* */
                    uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                                    cur_batch * max_num_blocks_per_query + start_kv / block_size + now_idx));
                    int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv;
                    int64_t kv_offset_rope = (int64_t)block_table_id * block_size * stride_kv_rope;
                    int64_t now_l1_offset = 0;
                    WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag);
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 512],
                        k_gm_tensor[kv_offset],
                        qk_round_n,         // nValue
                        qk_round_n,         // nValue
                        pp_n_scalar,         // dstNzC0Stride
                        512,            // dValue
                        512,                     // dstNzMatrixStride, unused
                        0            // srcDValue
                    );

                    SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag);
                    WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);
                    gm_to_l1<ArchType::ASCEND_V220, IN_ROPE_DTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1kv_rope_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 64],
                        k_rope_gm_tensor[kv_offset_rope],
                        qk_round_n,
                        qk_round_n,         // nValue
                        pp_n_scalar,             // dstNzC0Stride
                        64,            // dValue
                        64,                     // dstNzMatrixStride, unused
                        0            // srcDValue
                    );
                    SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 2);
                    uint32_t embed_split_size = 256 / sizeof(int8_t);
                    uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    uint32_t embed_split_loop_v = (512 + round_embed_split_size - 1) / round_embed_split_size;
                    uint32_t embed_split_idx;
                    for (embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                        WAIT_FLAG(M, MTE1, embed_split_idx % 2);
                        for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                            l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0a_buf_tensor[embed_split_idx % 2 * l0a_pingpong_size + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                                l1q_buf_addr_tensor[embed_split_idx * m * round_embed_split_size + loa_load_idx * T_CUBE_MATRIX_SIZE],
                                0,
                                round_embed_split_size / T_BLOCK_SIZE,                                 // repeat
                                0,
                                q_load_coeff / BLOCK_SIZE,                            // srcStride
                                0,
                                0                                                     // dstStride
                            );
                        }
                        SET_FLAG(MTE1, M, embed_split_idx % 2);
                        now_l1_offset = l1_kv_pingpong_flag * 128 * 512 + embed_split_idx * qk_round_n * round_embed_split_size;
                        if (embed_split_idx == 0) {
                            WAIT_FLAG(MTE2, MTE1, l1_kv_pingpong_flag);
                        }
                        WAIT_FLAG(M, MTE1, embed_split_idx % 2 + 2);
                        l1_to_l0_b<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf_tensor[embed_split_idx % 2 * l0b_pingpong_size],
                            l1kv_buf_addr_tensor[now_l1_offset],
                            0,
                            round_embed_split_size * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                            0,
                            1,                                        // srcStride
                            0,
                            0                                        // dstStride
                        );
                        if(embed_split_idx == embed_split_loop_v - 1) {
                            SET_FLAG(MTE1, MTE2, l1_kv_pingpong_flag);
                        }
                        SET_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                        WAIT_FLAG(MTE1, M, embed_split_idx % 2);
                        WAIT_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                        if (embed_split_idx == 0) {
                            WAIT_FLAG(FIX, M, l1_kv_pingpong_flag);
                        }
                        mmad<ArchType::ASCEND_V220, int8_t, int8_t, mm1OutputType, false>(
                            mm1_l0c_buf_tensor[l1_kv_pingpong_flag * l0c_pingpong_size],
                            l0a_buf_tensor[embed_split_idx % 2 * l0a_pingpong_size],
                            l0b_buf_tensor[embed_split_idx % 2 * l0b_pingpong_size],
                            m,     // m
                            qk_n,  // n
                            embed_split_size,   // k
                            embed_split_idx == 0     // cmatrixInitVal
                        );
                        PIPE_BARRIER(M);
                        SET_FLAG(M, MTE1, embed_split_idx % 2);
                        SET_FLAG(M, MTE1, embed_split_idx % 2 + 2);

                        // copy S to gm
                        if (embed_split_idx == embed_split_loop_v - 1) {
                            SET_FLAG(M, FIX, l1_kv_pingpong_flag);
                            WAIT_FLAG(M, FIX, l1_kv_pingpong_flag);
                            
                            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER * 4 + ((n_idx / 4) % 2)*SP_BLOCK_SIZE*KV_FLOW_NUM +split_idx*pp_n_scalar],
                                mm1_l0c_buf_tensor[l1_kv_pingpong_flag * l0c_pingpong_size],
                                m,           // MSize
                                qk_round_n,  // NSize
                                RoundUp<16>(m), // srcStride
                                512  // dstStride_dst_D
                            );
                            SET_FLAG(FIX, M, l1_kv_pingpong_flag);
                        }
                    }
                    embed_split_idx = 4;
                    embed_split_size = 64;
                    round_embed_split_size = 64;
                    WAIT_FLAG(M, MTE1, embed_split_idx % 2);
                    for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                        l1_to_l0_a<ArchType::ASCEND_V220, IN_ROPE_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384 + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                            l1q_rope_buf_addr_tensor[loa_load_idx * CUBE_MATRIX_SIZE],
                            0,
                            round_embed_split_size / BLOCK_SIZE,                                 // repeat
                            0,
                            q_load_coeff / BLOCK_SIZE,                            // srcStride
                            0,
                            0                                                     // dstStride
                        );
                    }
                    SET_FLAG(MTE1, M, embed_split_idx % 2);

                    WAIT_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 2);
                    WAIT_FLAG(M, MTE1, embed_split_idx % 2 + 2);
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_ROPE_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384],
                        l1kv_rope_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 64],
                        0,
                        round_embed_split_size * qk_round_n / CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                        // srcStride
                        0,
                        0                                        // dstStride
                    );
                    SET_FLAG(MTE1, MTE2, l1_kv_pingpong_flag + 2);
                    SET_FLAG(MTE1, M, embed_split_idx % 2 + 2);

                    WAIT_FLAG(MTE1, M, embed_split_idx % 2);
                    WAIT_FLAG(MTE1, M, embed_split_idx % 2 + 2);
                    WAIT_FLAG(FIX, M, l1_kv_pingpong_flag);
                    mmad<ArchType::ASCEND_V220, IN_ROPE_DTYPE, IN_ROPE_DTYPE, float, false>(
                        mm1_l0c_buf_tensor.template ReinterpretCast<float>()[l1_kv_pingpong_flag * 16384],
                        l0a_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384],
                        l0b_buf_tensor.template ReinterpretCast<IN_ROPE_DTYPE>()[embed_split_idx % 2 * 16384],
                        m,     // m
                        qk_n,  // n
                        embed_split_size,   // k
                        true     // cmatrixInitVal
                    );
                    PIPE_BARRIER(M);
                    SET_FLAG(M, MTE1, embed_split_idx % 2);
                    SET_FLAG(M, MTE1, embed_split_idx % 2 + 2);
                    SET_FLAG(M, FIX, l1_kv_pingpong_flag);
                    WAIT_FLAG(M, FIX, l1_kv_pingpong_flag);
                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                    s_rope_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER * 4 + ((n_idx / 4) % 2)*SP_BLOCK_SIZE*KV_FLOW_NUM +split_idx*pp_n_scalar],
                        mm1_l0c_buf_tensor.template ReinterpretCast<float>()[l1_kv_pingpong_flag * 16384],
                        m,           // MSize
                        qk_round_n,  // NSize
                        RoundUp<16>(m), // srcStride
                        512  // dstStride_dst_D
                    );
                    SET_FLAG(FIX, M, l1_kv_pingpong_flag);
                }
                FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_DECODER);
            }
            /* ************ CUBE2 stage1  ************* */
            if (n_idx >= s_block_stack) {
                if (n_idx + s_block_stack > n_loop + s_block_stack - 1) {
                    sv_n = cur_kv_seqlen - (n_idx - s_block_stack) * pp_n_scalar; // delete
                } else {
                    sv_n = pp_n_scalar * s_block_stack;
                }
                uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                uint32_t embed_split_size = 128;
                embed_split_loop_v = 4;
                uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                uint32_t v_rows_mad = 256; 
                uint32_t l0_p_pingpong_offset = block_size * v_rows_mad;
                uint32_t l1_p_pingpong_offset = block_size * v_rows_mad;
                qk_n_2 = v_rows_mad;
                qk_round_n_2 = RoundUp<BLOCK_SIZE>(v_rows_mad);
                uint32_t v_row_loop_num = (sv_n + v_rows_mad - 1) / v_rows_mad;
                l1p_pingpong_flag = (n_idx / s_block_stack - 1) % 2;

                WaitFlagDev(SOFTMAX_READY_DECODER); 
                WAIT_FLAG(MTE1, MTE2, l1p_pingpong_flag + 4);
                gm_to_l1<ArchType::ASCEND_V220, int8_t, DataFormat::ND, DataFormat::NZ>(
                    l1p_buf_addr_tensor[l1p_pingpong_flag * P_TMP_SIZE / 2], 
                    p_gm_tensor[(uint64_t)block_idx * P_TMP_SIZE + ((n_idx / 4 - 1) % 2) * P_TMP_SIZE / 2],
                    cur_head_num,                       // nValue
                    RoundUp<BLOCK_SIZE>(cur_head_num),  // dstNzC0Stride
                    0,                                  // dstNzMatrixStride, unused
                    sv_round_n,
                    0,                                  // dstNzMatrixStride, unused
                    512 
                );
                SET_FLAG(MTE2, MTE1, l1p_pingpong_flag + 4);
                for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                    uint32_t l0c_pingpong_flag = embed_split_idx % 2;
                    for(uint32_t v_row_loop_idx = 0; v_row_loop_idx < v_row_loop_num; v_row_loop_idx++){
                        if (n_idx + s_block_stack > n_loop + s_block_stack - 1) {
                            qk_n_2 = v_row_loop_idx == v_row_loop_num - 1 ? sv_n - v_row_loop_idx * v_rows_mad : v_rows_mad;
                            qk_round_n_2 = RoundUp<T_BLOCK_SIZE>(qk_n_2);
                        }

                        uint32_t combine_loop_idx = embed_split_idx * v_row_loop_num + v_row_loop_idx; 
                        uint32_t l1_kv_pingpong_flag = combine_loop_idx % 2; 
                        uint32_t l0b_pingpong_flag = combine_loop_idx % 2;
                        uint32_t l0a_pingpong_flag = v_row_loop_idx % 2; 
                        uint32_t l1_p_pingpong_flag = v_row_loop_idx % 2;
                    
                        WAIT_FLAG(M, MTE1, l0a_pingpong_flag);

                    if(embed_split_idx == 0 || v_row_loop_idx >= 2){
                        if(v_row_loop_idx == 0) {
                            WAIT_FLAG(MTE2, MTE1, l1p_pingpong_flag+4);
                        }
                        l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::NZ, DataFormat::ZZ>(
                            l0a_buf_tensor[l0a_pingpong_flag * l0_p_pingpong_offset], 
                            l1p_buf_addr_tensor[l1p_pingpong_flag * P_TMP_SIZE / 2 + l1_p_pingpong_flag * l1_p_pingpong_offset],
                            RoundUp<BLOCK_SIZE>(cur_head_num),
                            qk_round_n_2, // repeat
                            0,
                            0, // srcStride
                            0,
                            0 // dstStride
                            );
                        SET_FLAG(MTE1, M, l1p_pingpong_flag + 4);
                        WAIT_FLAG(MTE1, M, l1p_pingpong_flag + 4);
                        if(v_row_loop_idx == v_row_loop_num - 1) {
                            SET_FLAG(MTE1, MTE2, l1p_pingpong_flag + 4);
                        }
                        SET_FLAG(MTE1, MTE2, l0a_pingpong_flag + 6);
                        WAIT_FLAG(MTE1, MTE2, l0a_pingpong_flag + 6);
                    }
                    
                    WAIT_FLAG(MTE1, MTE2, l1_kv_pingpong_flag);
                    uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                            cur_batch * max_num_blocks_per_query + start_kv / block_size + n_idx - s_block_stack + v_row_loop_idx * 2));
                    int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv; 
                    
                    uint32_t qk_n_2_block1 = qk_n_2 > 128 ? 128 : qk_n_2;
                    uint32_t qk_round_n_2_block1 = RoundUp<T_BLOCK_SIZE>(qk_n_2_block1);
                    uint32_t qk_n_2_block2 = qk_n_2 - 128;
                    uint32_t qk_round_n_2_block2 = RoundUp<T_BLOCK_SIZE>(qk_n_2_block2);
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1v_buf_addr_tensor[l1_kv_pingpong_flag * block_size * v_rows_mad], 
                        k_gm_tensor[kv_offset + embed_split_idx * block_size * block_size],
                        qk_round_n_2_block1,         // nValue    
                        qk_round_n_2_block1,         // nValue 
                        embed_split_size,                       // dstNzC0Stride
                        embed_split_size,                       // dValue
                        embed_split_size,                             // dstNzMatrixStride, unused
                        0                                       // srcDValue
                        );
                    if (qk_n_2 > 128) {
                        block_table_id = (uint32_t)(*(block_tables_gm +
                        cur_batch * max_num_blocks_per_query + start_kv / block_size + n_idx - s_block_stack + v_row_loop_idx * 2 + 1)); // hyc改
                        kv_offset = (int64_t)block_table_id * block_size * stride_kv; 
                        gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::NZ, DataFormat::NZ>(
                        l1v_buf_addr_tensor[l1_kv_pingpong_flag * block_size * v_rows_mad + qk_round_n_2_block1 * block_size],
                        k_gm_tensor[kv_offset + embed_split_idx * block_size * block_size],
                        qk_round_n_2_block2,        // nValue
                        qk_round_n_2_block2,        // nValue
                        embed_split_size,                       // dstNzC0Stride
                        embed_split_size,                       // dValue
                        embed_split_size,                             // dstNzMatrixStride, unused
                        0                                       // srcDValue
                        ); 
                    }
                    SET_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 6); 
                    WAIT_FLAG(MTE2, MTE1, l1_kv_pingpong_flag + 6);

                    AscendC::LoadData2dTransposeParams loadDataParams;
                    loadDataParams.dstGap = 0;
                    loadDataParams.startIndex = 0;
                    loadDataParams.dstFracGap = 0;
                    loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                    loadDataParams.srcStride = qk_round_n_2_block1 / T_BLOCK_SIZE;
                    uint16_t dstGap = sizeof(int8_t) == 1 ? 1 : 0;
                    loadDataParams.dstGap = dstGap;
                    uint32_t copy_loops1 = (qk_n_2_block1 + T_BLOCK_SIZE - 1) / T_BLOCK_SIZE; 
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2); 
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < copy_loops1; ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[l0b_pingpong_flag * 16384 * 2 + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                l1v_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 256 +  l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                loadDataParams); 
                    }
                    uint32_t copy_loops2 = (qk_n_2_block2 + T_BLOCK_SIZE - 1) / T_BLOCK_SIZE;
                    loadDataParams.srcStride = qk_round_n_2_block2 / T_BLOCK_SIZE;
                    if (qk_n_2 > 128) {
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < copy_loops2; ++l0b_load_idx) { 
                            AscendC::LoadDataWithTranspose(
                                    l0b_buf_tensor[l0b_pingpong_flag * 16384 * 2 + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE + block_size * block_size],
                                    l1v_buf_addr_tensor[l1_kv_pingpong_flag * 128 * 256 +  l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE + block_size * block_size],
                                    loadDataParams);
                            }
                    }
                    SET_FLAG(MTE1, MTE2, l1_kv_pingpong_flag);

                    SET_FLAG(MTE1, M, l1_kv_pingpong_flag);
                    WAIT_FLAG(MTE1, M, l1_kv_pingpong_flag);

                    WAIT_FLAG(FIX, M, l0c_pingpong_flag);
                    bool init_c = v_row_loop_idx == 0 ? true : false;
                    mmad<ArchType::ASCEND_V220, int8_t, int8_t, mm2OutputType, false>(
                        mm2_l0c_buf_tensor[l0c_pingpong_flag * 16384],
                        l0a_buf_tensor[l0a_pingpong_flag * 16384 * 2],
                        l0b_buf_tensor[l0b_pingpong_flag * 16384 * 2],
                        m,                  // m  
                        embed_split_size,   // n 
                        qk_n_2,     // k  
                        init_c              // cmatrixInitVal  
                    );
                    SET_FLAG(M, MTE1, l0a_pingpong_flag);
                    SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                
                    SET_FLAG(M, FIX, l0c_pingpong_flag);
                    WAIT_FLAG(M, FIX, l0c_pingpong_flag);
                    if(v_row_loop_idx == v_row_loop_num - 1){
                        // copy O to gm
                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm2CopyType, mm2OutputType>(
                            o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 + embed_split_idx * round_embed_split_size + ((n_idx / s_block_stack - 1) % 2) * TMP_SIZE],
                            mm2_l0c_buf_tensor[l0c_pingpong_flag * 16384],
                            m,                              // MSize
                            RoundUp<16>(embed_split_size),  // NSize 
                            RoundUp<16>(m),                 // srcStride
                            round_v                         // dstStride_dst_D 
                        );
                    }
                    SET_FLAG(FIX, M, l0c_pingpong_flag);
                }
                }

                FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_DECODER);
            }
        }
    }

private:
    __gm__ int8_t *__restrict__ q_gm{nullptr};
    __gm__ IN_ROPE_DTYPE *__restrict__ q_rope_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ ctkv_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ k_gm{nullptr};
    __gm__ IN_ROPE_DTYPE *__restrict__ k_rope_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ v_gm{nullptr};

    __gm__ mm1CopyType *__restrict__ s_gm{nullptr};
    __gm__ float *__restrict__ s_rope_gm{nullptr};
    __gm__ int8_t *__restrict__ p_gm{nullptr};
    __gm__ mm2CopyType *__restrict__ o_tmp_gm{nullptr};
    __gm__ int32_t *__restrict__ block_tables_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_gm{nullptr};

    AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<int8_t> q_gm_tensor;
    AscendC::GlobalTensor<IN_ROPE_DTYPE> q_rope_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> k_gm_tensor;
    AscendC::GlobalTensor<IN_ROPE_DTYPE> k_rope_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> v_gm_tensor;
    AscendC::GlobalTensor<mm1CopyType> s_gm_tensor;
    AscendC::GlobalTensor<float> s_rope_gm_tensor;
    AscendC::GlobalTensor<int8_t> p_gm_tensor;
    AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<int32_t> block_tables_gm_tensor;

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1q_rope_buf_addr_offset = 128*512*2;
    const uint32_t l1kv_buf_addr_offset = 128*576*2;
    const uint32_t l1kv_rope_buf_addr_offset= 128 * 576 * 2 + 128 * 512 * 2;
    const uint32_t l1p_buf_addr_offset = 128 * 576 * 6;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<int8_t> l1q_buf_addr_tensor;
    AscendC::LocalTensor<IN_ROPE_DTYPE> l1q_rope_buf_addr_tensor;
    AscendC::LocalTensor<int8_t> l1kv_buf_addr_tensor;
    AscendC::LocalTensor<IN_ROPE_DTYPE> l1kv_rope_buf_addr_tensor;
    AscendC::LocalTensor<int8_t> l1p_buf_addr_tensor;
    AscendC::LocalTensor<int8_t> l1v_buf_addr_tensor;
    AscendC::LocalTensor<int8_t> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, int8_t>(0);
    AscendC::LocalTensor<int8_t> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, int8_t>(0);
    AscendC::LocalTensor<mm1OutputType> mm1_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm1OutputType>(0);
    AscendC::LocalTensor<mm2OutputType> mm2_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm2OutputType>(0);


    uint32_t num_batches{0};
    uint32_t q_heads{0};
    uint32_t kv_heads{0};
    uint32_t embedding_size{0};
    uint32_t block_size{0};
    uint32_t max_num_blocks_per_query{0};
    uint32_t group_num{0};
    uint32_t stride_kv{0};
    uint32_t stride_kv_rope{0};
    uint32_t stride_vo{0};
    uint32_t m{0};
    uint32_t __k{0};
    uint32_t __v{0};
    uint32_t round_k{0};
    uint32_t round_v{0};
    uint32_t process_num{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t mask_type{0};
    uint32_t totalTaskNum{0};
    uint32_t flashDecodingTaskNum{0};
    uint32_t maxKVSeqLen{0};

    uint32_t cur_qn_blk_size{0};
    uint32_t num_batches_pad{0};

    uint32_t embed_split_size_qk{0};
    uint32_t embed_split_loop_qk{1};
    uint32_t embed_split_size_v{0};
    uint32_t embed_split_loop_v{1};

    uint32_t l0b_pingpong_flag = 0;
    uint32_t l0c_pingpong_flag = 0;
    uint32_t l1p_pingpong_flag = 0;
};

#endif