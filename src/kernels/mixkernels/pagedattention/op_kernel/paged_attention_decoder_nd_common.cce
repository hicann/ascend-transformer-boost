/*
* Copyright (c) 2024 Huawei Technologies Co., Ltd.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#include "kernels/utils/kernel/common.h"
#include "kernels/utils/kernel/common_func.h"
#include "kernels/utils/kernel/simd.h"
#include "kernels/utils/kernel/iterator.h"
#include "kernel_operator.h"
#include "mixkernels/pagedattention/tiling/paged_attention_tiling_dependency.h"

// define common const value

// FFTS Flag
constexpr int32_t QK_READY = 0;
constexpr int32_t SOFTMAX_READY = 1;
constexpr int32_t UPDATE_READY = 2;
constexpr int32_t QK_READY_DECODER = 3;
constexpr int32_t SOFTMAX_READY_DECODER = 4;
constexpr int32_t UPDATE_READY_DECODER = 5;
constexpr int32_t QK_READY_STAGE2 = 6;
constexpr int32_t SOFTMAX_READY_STAGE2 = 7;
constexpr int32_t UPDATE_READY_STAGE2 = 8;
constexpr uint32_t VEC_DEQ_K0_READY = 9;
constexpr uint32_t VEC_DEQ_K1_READY = 10;
constexpr uint32_t VEC_DEQ_V0_READY = 11;
constexpr uint32_t VEC_DEQ_V1_READY = 12;


constexpr int32_t BLOCK_SIZE = 16;
constexpr int32_t BLOCK_SIZE_32 = 32;
constexpr int64_t TMP_SIZE = 65536;              // 256 * 256
constexpr int32_t BIT_SHIFT = 8;

const int32_t TILING_BATCH = 0;
const int32_t TILING_NUMHEADS = 1;
const int32_t TILING_HEADDIM = 2;
const int32_t TILING_NUMBLOKS = 3;
const int32_t TILING_BLOCKSIZE = 4;
const int32_t TILING_MAXBLOCKS = 5;
const int32_t TILING_TOR = 6;
const int32_t TILING_KVHEADS = 7;
const int32_t TILING_FORMER_BATCH = 8;
const int32_t TILING_FORMER_HEAD = 9;
const int32_t TILING_TAIL_BATCH = 10;
const int32_t TILING_TAIL_HEAD = 11;
const int32_t TILING_HEADNUM_MOVE = 12;
const int32_t TILING_MASK_MAX_LEN = 13;
const int32_t TILING_BATCH_STRIDE = 14;
const int32_t TILING_HEAD_STRIDE = 15;
const int32_t TILING_KEY = 16;
const int32_t TILING_HEADSIZE = 17;
const int32_t TILING_PARASIZE = 18;
const int32_t TILING_GROUPNUM = 19;
const int32_t TILING_FORMER_GROUP_MOVE = 20;
const int32_t TILING_TAIL_GROUP_MOVE = 21;
const int32_t TILING_MAX_KVSEQLEN = 22;
const int32_t TILING_KVSPLIT = 23;
const int32_t TILING_KVCORENUM = 24;
const int32_t TILING_BLOCKSIZE_CALC = 25;
const int32_t TILING_TOTAL_BLOCK_NUM = 26;
const int32_t TILING_PREFILL_BS = 27;
const int32_t TILING_DECODER_BS = 28;
const int32_t TILING_HEADDIM_V = 29;
const int32_t TILING_MODCOEF = 30;
const int32_t TILING_DIVCOEF = 31;
const int32_t TILING_QHEADORIGINAL = 32;
const int32_t TILING_COMPRESSHEAD = 33;
const int32_t TILING_QUANTYPE = 34;
const int32_t TILING_DATA_SHAPE_TYPE = 35;
const int32_t TILING_SCALETYPE = 36;
const int32_t TILING_MASK_TYPE_ND = 37;
const int32_t TILING_HEADDIM_K_SPLIT = 38;
const int32_t TILING_HEADDIM_V_SPLIT = 39;
const int32_t TILING_HEADDIM_V_SPLIT_VECTOR_FORMER = 40;
const int32_t TILING_HEADDIM_V_SPLIT_VECTOR_TAIL = 41;
const int32_t BLOCKSIZE_CALC_256 = 256;
constexpr uint32_t CONST_16 = 16;
constexpr uint32_t KV_SEQ_STEP = 16;
constexpr uint32_t MAX_NUMEL_INST_B8 = 255 * 256;
constexpr uint32_t MAX_NUMEL_INST_B16 = 255 * 128;
constexpr uint32_t MAX_NUMEL_INST_B32 = 255 * 64;

template <typename T>
using GlobalT = AscendC::GlobalTensor<T>;

template <typename T>
using LocalT = AscendC::LocalTensor<T>;

using TilingKeyType = AtbOps::TilingKeyType;

using DataShapeType = AtbOps::DataShapeType;

using CompressType = AtbOps::CompressType;

using PagedAttnVariant = AtbOps::PagedAttnVariant;

template<TilingKeyType tilingKeyType>
struct AttentionType
{
};

template<>
struct AttentionType<TilingKeyType::TILING_QUANT_BF16OUT>
{
    using mm1OutputType = int32_t;
    using mm1CopyType = int32_t;
    using mmBiasType = int32_t;
    using mmScaleType = float;
    using mm2OutputType = int32_t;
    using mm2CopyType = int32_t;
};

template<>
struct AttentionType<TilingKeyType::TILING_QUANT_FP16OUT>
{
    using mm1OutputType = int32_t;
    using mm1CopyType = int32_t;
    using mmBiasType = int32_t;
    using mmScaleType = float;
    using mm2OutputType = int32_t;
    using mm2CopyType = int32_t;
};

template<>
struct AttentionType<TilingKeyType::TILING_INT8_CUBE_QUANT>
{
    using mm1OutputType = float;
    using mm1CopyType = float;
    using mmBiasType = int32_t;
    using mmScaleType = uint64_t;
    using mm2OutputType = float;
    using mm2CopyType = float;
};

template<>
struct AttentionType<TilingKeyType::TILING_INT8_VEC_QUANT>
{
    using mm1OutputType = float;
    using mm1CopyType = float;
    using mmBiasType = int32_t;
    using mmScaleType = float32_t;
    using mm2OutputType = float;
    using mm2CopyType = float;
};

template<>
struct AttentionType<TilingKeyType::TILING_INT8_VEC_QUANTBF16>
{
    using mm1OutputType = float;
    using mm1CopyType = float;
    using mmBiasType = int32_t;
    using mmScaleType = float32_t;
    using mm2OutputType = float;
    using mm2CopyType = float;
};

template<>
struct AttentionType<TilingKeyType::TILING_HALF_DATA>
{
    using mm1OutputType = float;
    using mm1CopyType = float;
    using mmBiasType = float;
    using mmScaleType = float;
    using mm2OutputType = float;
    using mm2CopyType = float;
};

template<>
struct AttentionType<TilingKeyType::TILING_BF16_DATA>
{
    using mm1OutputType = float;
    using mm1CopyType = float;
    using mmBiasType = float;
    using mmScaleType = float;
    using mm2OutputType = float;
    using mm2CopyType = float;
};


#ifdef __DAV_C220_CUBE__
constexpr int32_t L0AB_HALF_BUF_SIZE = 16384;    // 128 * 128 = 16K
constexpr int32_t L0AB_UINT8_BUF_SIZE = 16384 * 2;
constexpr int32_t L0C_FLOAT_BUF_SIZE = 16384;
constexpr int32_t L0C_UINT8_BUF_SIZE = 131072;
constexpr int32_t CUBE_MATRIX_SIZE = 256;        // 16 * 16
constexpr int64_t L0AB_UINT8_BLOCK_SIZE = 32768; // 128 * 128 * 2B
constexpr int32_t L1_HALF_BUF_SIZE = 65536;  // 256 * 256
constexpr int32_t L1_P_UINT8_BUF_SIZE = 32768;

constexpr int32_t TMP_SIZE_DECODER = 32768;

constexpr int32_t L1_HALF_BUF_SIZE_DECODER = 16384;
constexpr int32_t L1_UINT8_BUF_SIZE_DECODER = 16384 * 2;
constexpr int32_t L1_KV_HALF_BUF_SIZE = 65536;// 2* 128 * 256
constexpr int32_t L1_KV_UINT8_BUF_SIZE = 65536 * 2;
constexpr uint64_t L1_E_UINT8_SIZE = 1024;  // 32 * 32 * 1B
constexpr uint64_t L1_SCALE_UINT8_SIZE = 4096;  // uint64 256 * 8 * 2head
constexpr uint64_t L1_SCALE_UINT64_SIZE = L1_SCALE_UINT8_SIZE / 8;
constexpr uint64_t L1_OFFSET_UINT8_SIZE = 2048;  // int32 256 * 4 8 2head
constexpr uint64_t L1_OFFSET_INT32_SIZE = L1_OFFSET_UINT8_SIZE / 4;

//DeQuant
constexpr uint32_t L0AB_PINGPONG_BUFFER_LEN = 32768; // 32 KB
constexpr uint32_t L0C_PINGPONG_BUFFER_LEN_INT32 = 16384; // 65536 / 4
constexpr uint32_t CUBE_MATRIX_SIZE_512 = 16 * 32;       // 16 * 23
constexpr int32_t BLOCK_SIZE_16 = 16;
constexpr uint64_t CONST_4 = 4;
constexpr uint64_t CONST_32 = 32;
constexpr uint64_t CONST_64 = 64;
constexpr uint64_t CONST_128 = 128;
constexpr uint32_t EMBED_SPLIT = 256;
constexpr uint32_t ROUND_EMBED_SPLIT = 256;

#elif __DAV_C220_VEC__
constexpr uint32_t HALF_VECTOR_SIZE = 128;
constexpr uint32_t UB_ALIGN_BYTE = 32;
constexpr int32_t FLOAT_VECTOR_SIZE = 64;
constexpr int64_t UB_UINT8_BLOCK_SIZE_MLA = 16384;      // 96 * 128 * 2B // prefill/decoder diff
constexpr int64_t UB_UINT8_BLOCK_SIZE_NORM = 24576;
constexpr int64_t UB_UINT8_LINE_SIZE = 512;         // 64 * 4B，申请两倍空间防踩踏。
constexpr int64_t UB_HALF_LINE_SIZE = 256;          // UB_FLOAT_LINE_SIZE * 2
constexpr int64_t UB_FLOAT_LINE_SIZE = 128;         // 64，申请两倍空间防踩踏。

constexpr int64_t PRE_UB_UINT8_BLOCK_SIZE = 16384;  // 64 * 128 * 2B
constexpr int32_t VECTOR_SIZE = 128;                // prefill
constexpr int32_t FLOAT_BLOCK_SIZE = 8;
constexpr int32_t UB_HALF_BUF_SIZE = 8192;          // 64 * 128
constexpr int32_t TMP_SIZE_DECODER = 32768;
constexpr int32_t STAGE2_UB_UINT8_BLOCK_SIZE = 8192;
constexpr int32_t CUBE_MATRIX_SIZE = 256;
constexpr uint32_t MAX_UB_SIZE = 196608; // 192 * 1024
constexpr uint32_t EMBED_SPLIT_SM = 128;
constexpr uint32_t ROUND_EMBED_SPLIT_SM = 128;

__aicore__ __attribute__((always_inline)) void inline __set_mask(int32_t len)
{
    uint64_t mask = 0;
    uint64_t one = 1;
    uint64_t temp = len % FLOAT_VECTOR_SIZE;
    for (int64_t i = 0; i < temp; i++) {
        mask |= one << i;
    }

    if (len == VECTOR_SIZE) {
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
    } else if (len >= FLOAT_VECTOR_SIZE) {
        SetVectorMask<int8_t>(mask, (uint64_t)-1);
    } else {
        SetVectorMask<int8_t>(0x0, mask);
    }
}

template<PagedAttnVariant pagedAttnVariant>
struct UbufAlloc
{
};

template<>
struct UbufAlloc<PagedAttnVariant::MULTI_LATENT>
{
    const uint32_t ls32_ubuf_offset = 0;
    const uint32_t lp_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t lp32_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t mask_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t lo_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t mask32_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t ls16_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t lm32_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA;
    const uint32_t hm32_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 1 * UB_UINT8_LINE_SIZE;
    const uint32_t pm32_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 2 * UB_UINT8_LINE_SIZE;
    const uint32_t pm32_ubuf_stage2_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 3 * UB_UINT8_LINE_SIZE;
    const uint32_t descale1_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 4 * UB_UINT8_LINE_SIZE;
    const uint32_t descale2_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 5 * UB_UINT8_LINE_SIZE;
    const uint32_t dm32_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 6 * UB_UINT8_LINE_SIZE;
    const uint32_t dm32_ubuf_stage2_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 7 * UB_UINT8_LINE_SIZE;
    const uint32_t ll_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 9 * UB_UINT8_LINE_SIZE; 
    const uint32_t ll_ubuf_stage2_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 11 * UB_UINT8_LINE_SIZE;      
    const uint32_t gm32_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 13 * UB_UINT8_LINE_SIZE;  
    const uint32_t gl_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 15 * UB_UINT8_LINE_SIZE;          
    const uint32_t gl32_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE_MLA + 15 * UB_UINT8_LINE_SIZE;        
    const uint32_t go_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE_MLA;            
    const uint32_t go32_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE_MLA;          
    const uint32_t tv32_ubuf_offset = 10 * UB_UINT8_BLOCK_SIZE_MLA;
};

template<>
struct UbufAlloc<PagedAttnVariant::DEFAULT>
{
    const uint32_t ls32_ubuf_offset = 0;
    const uint32_t lp_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t lp32_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t mask_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t lo_ubuf_offset = 3 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t mask32_ubuf_offset = 3 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t ls16_ubuf_offset = 3 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t lm32_ubuf_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM;
    const uint32_t hm32_ubuf_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 1 * UB_UINT8_LINE_SIZE;
    const uint32_t pm32_ubuf_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 2 * UB_UINT8_LINE_SIZE;
    const uint32_t pm32_ubuf_stage2_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 3 * UB_UINT8_LINE_SIZE;
    const uint32_t descale1_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 4 * UB_UINT8_LINE_SIZE;
    const uint32_t descale2_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 5 * UB_UINT8_LINE_SIZE;
    const uint32_t dm32_ubuf_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 6 * UB_UINT8_LINE_SIZE;
    const uint32_t dm32_ubuf_stage2_offset = 5 * UB_UINT8_BLOCK_SIZE_NORM + 7 * UB_UINT8_LINE_SIZE;
    const uint32_t ll_ubuf_offset = MAX_UB_SIZE - (UB_UINT8_LINE_SIZE + UB_UINT8_LINE_SIZE * 4); // 2 * UB_UINT8_LINE_SIZE
    const uint32_t ll_ubuf_stage2_offset = MAX_UB_SIZE- UB_UINT8_LINE_SIZE * 2;      // 2 * UB_UINT8_LINE_SIZE
    const uint32_t gm32_ubuf_offset = dm32_ubuf_stage2_offset + 3 * UB_UINT8_LINE_SIZE; // 2 * UB_UINT8_LINE_SIZE
    const uint32_t gl_ubuf_offset = gm32_ubuf_offset + 2 * UB_UINT8_LINE_SIZE;          // 3 * UB_UINT8_LINE_SIZE
    const uint32_t gl32_ubuf_offset = gm32_ubuf_offset + 2 * UB_UINT8_LINE_SIZE;        // 3 * UB_UINT8_LINE_SIZE
    const uint32_t go_ubuf_offset = gl_ubuf_offset + 3 * UB_UINT8_LINE_SIZE;            // 16K
    const uint32_t go32_ubuf_offset = gl_ubuf_offset + 3 * UB_UINT8_LINE_SIZE;          // 16K
    const uint32_t tv32_ubuf_offset = go32_ubuf_offset + 2 * UB_UINT8_BLOCK_SIZE_NORM;
};
#endif

#ifdef __DAV_C220_CUBE__
// pa Parallel feature class 
template <typename IN_DATA_TYPE, typename S_DATA_TYPE>
class PagedAttentionParallelAic {
public:
    __aicore__ inline PagedAttentionParallelAic(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    __aicore__ inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t *__restrict__ q_in_gm,
        __gm__ uint8_t *__restrict__ k_in_gm,
        __gm__ uint8_t *__restrict__ v_in_gm,
        __gm__ uint8_t *__restrict__ block_tables_in_gm,
        __gm__ uint8_t *__restrict__ mask_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t *__restrict__ tiling_in_para_gm) {

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(q_in_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(k_in_gm));
        v_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(v_in_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ S_DATA_TYPE *>(s_out_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(p_out_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_temp_gm));

        block_tables_gm = block_tables_in_gm;
        tiling_para_gm = tiling_in_para_gm;

        const uint32_t l1q_buf_addr_offset = 0;
        const uint32_t l1k_buf_addr_offset = 2 * L0AB_UINT8_BLOCK_SIZE;
        const uint32_t l1p_buf_addr_offset = 4 * L0AB_UINT8_BLOCK_SIZE;
        const uint32_t l1v_buf_addr_offset = 6 * L0AB_UINT8_BLOCK_SIZE;

        l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DATA_TYPE>(l1q_buf_addr_offset);
        l1k_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DATA_TYPE>(l1k_buf_addr_offset);
        l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DATA_TYPE>(l1p_buf_addr_offset);
        l1v_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DATA_TYPE>(l1v_buf_addr_offset);

        l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, IN_DATA_TYPE>(0);
        l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, IN_DATA_TYPE>(0);
        l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, float>(0);

        batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM));
        kv_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_KVHEADS));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MAXBLOCKS));
        block_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_TOTAL_BLOCK_NUM));  // total_q_blk_num
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        mask_type = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MASK_TYPE_ND));
        max_kv = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MAX_KVSEQLEN));

        group_num = q_heads / kv_heads;
        stride_kv = static_cast<uint64_t>(kv_heads) * embedding_size;
        stride_qo = static_cast<uint64_t>(q_heads) * embedding_size;

        __k = embedding_size;
        round_k = (__k + 15) / 16 * 16;
    }

    __aicore__ inline void Run() {
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);

        uint32_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num * q_heads;
        for (uint32_t process = 0; process < process_num; process++) {
            // batch 0, cur_total_q_blk_num * q_heads 分核处理； batch 1, 任务进判断处理
            if (process >= cur_total_q_blk_num * q_heads) {
                while (1) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
                    uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
                    if(q_seqlen != 0) {
                        break;
                    }
                }
            }
            uint32_t cur_core_idx = process % block_num;
            if (is_triu_mask) {
                if ((process / block_num) % 2 == 1) { // 第二轮任务: 任务顺序反转一下
                    cur_core_idx = block_num - process % block_num - 1;
                }
            }
            if (block_idx != cur_core_idx) { // core 0, cur_core_idx其他任务跳过
                continue;
            }

            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_q_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4 + offset_tiling));
            uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 5 + offset_tiling));
            uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);

            uint32_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint32_t head_idx = process_idx % q_heads;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;  // pp_n_scalar != block_size 处理

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qk_round_m = (qk_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint64_t qk_index = 0;
            /**************** pre_load *****************/
            uint32_t qk_n = (qk_index == (n_loop - 1)) ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
            uint64_t q_offset = addr_q_scalar + head_idx * embedding_size + m_idx * pp_m_scalar * stride_qo;

            uint32_t batch_id = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8 + offset_tiling));  // 真实顺序的batch_id
            uint64_t stride_table = batch_id * max_num_blocks_per_query;

            uint32_t sv_n = pp_n_scalar;
            uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint32_t n_end = n_loop;
            if (is_triu_mask) {
                uint32_t triu_seq = mask_type == 3 ? max_kv - q_seqlen : kv_seqlen - q_seqlen;
                uint32_t n_offset = ((m_idx + 1) * pp_m_scalar + triu_seq + pp_n_scalar - 1) / pp_n_scalar;
                n_end = n_offset > n_loop ? n_loop : n_offset;
            }

            // Only need load Q once
            // gm_to_l1q
            if (qk_m == 1) {
                gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    round_k,               // lenBurst
                    0,
                    0
                );
            } else {
                gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    qk_m,        // nValue
                    qk_round_m,  // dstNzC0Stride
                    0,           // dstNzMatrixStride, unused
                    __k,         // dValue
                    0,           // dstNzMatrixStride, unused
                    stride_qo    // srcDValue
                );
            }
            SET_FLAG(MTE2, MTE1, pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, pingpong_flag);
            uint32_t KV_INC = n_end > 4 ? 2 : 1;
            uint32_t vect_mod = 2 * KV_INC;
            for (uint32_t n_idx = 0; n_idx < n_end + KV_INC; n_idx += KV_INC) {
                if (n_idx < n_end) {
                    for (uint32_t split_idx = 0; split_idx < KV_INC && n_idx + split_idx < n_end; split_idx++) {
                        int32_t block_id0 = (int32_t)(*((__gm__ int32_t *)block_tables_gm + stride_table + n_idx + split_idx));
                        int64_t k_offset = (int64_t)block_id0 * block_size * kv_heads * embedding_size + (int64_t)(head_idx / group_num) * embedding_size;
                        SET_FLAG(S, MTE2, EVENT_ID0);

                        pingpong_flag = (n_idx + split_idx) % 2;
                        offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                        if (n_idx + split_idx == (n_loop - 1)) {
                            qk_n = (kv_seqlen - (n_idx + split_idx) * pp_n_scalar);
                            qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        WAIT_FLAG(M, MTE1, pingpong_flag);
                        if (qk_m == 1) {
                            l1_to_l0_a<ArchType::ASCEND_V220, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0a_buf_tensor[offset],
                                l1q_buf_addr_tensor,
                                0,
                                (round_k + CUBE_MATRIX_SIZE - 1) / CUBE_MATRIX_SIZE,  // repeat
                                0,
                                1,                                                    // srcStride
                                0,
                                0                                                    // dstStride
                            );
                        } else {
                            for (uint32_t l0a_load_idx = 0; l0a_load_idx < qk_round_m / BLOCK_SIZE; ++l0a_load_idx) {
                                l1_to_l0_a<ArchType::ASCEND_V220, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                    l0a_buf_tensor[offset + l0a_load_idx * round_k * BLOCK_SIZE],
                                    l1q_buf_addr_tensor[l0a_load_idx * CUBE_MATRIX_SIZE],
                                    0,
                                    round_k / BLOCK_SIZE,     // repeat
                                    0,
                                    qk_round_m / BLOCK_SIZE,  // srcStride
                                    0,
                                    0                        // dstStride
                                );
                            }
                        }
                        // *** Prepare K to L1
                        WAIT_FLAG(S, MTE2, EVENT_ID0);
                        WAIT_FLAG(MTE1, MTE2, pingpong_flag);
                        // gm_to_l1k
                        gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                            l1k_buf_addr_tensor[offset],
                            k_gm_tensor[k_offset],
                            qk_n,        // nValue
                            qk_round_n,  // dstNzC0Stride
                            0,            // dstNzMatrixStride, unused
                            __k,         // dValue
                            0,            // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );

                        SET_FLAG(MTE2, MTE1, pingpong_flag);
                        WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                        l1_to_l0_b<ArchType::ASCEND_V220, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf_tensor[offset],
                            l1k_buf_addr_tensor[offset],
                            0,
                            round_k * qk_round_n / CUBE_MATRIX_SIZE,  // repeat
                            0,
                            1,                                        // srcStride
                            0,
                            0                                        // dstStride
                        );
                        SET_FLAG(MTE1, MTE2, pingpong_flag);
                        SET_FLAG(MTE1, M, pingpong_flag);
                        WAIT_FLAG(MTE1, M, pingpong_flag);
                        WAIT_FLAG(FIX, M, pingpong_flag);
                        mmad<ArchType::ASCEND_V220, half, half, float, false>(
                            l0c_buf_tensor[offset],
                            l0a_buf_tensor[offset],
                            l0b_buf_tensor[offset],
                            qk_m,  // m
                            qk_n,  // n
                            __k,   // k
                            1      // cmatrixInitVal
                        );
                        SET_FLAG(M, MTE1, pingpong_flag);
                        SET_FLAG(M, FIX, pingpong_flag);
                        WAIT_FLAG(M, FIX, pingpong_flag);
                        // copy S to gm
                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, half, float>(
                            s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx) % vect_mod * TMP_SIZE / vect_mod],
                            l0c_buf_tensor[offset],
                            qk_m,        // MSize
                            qk_round_n,  // NSize
                            qk_round_m,  // srcStride
                            qk_round_n  // dstStride_dst_D
                        );
                        SET_FLAG(FIX, M, pingpong_flag);
                    }
                    FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
                }
                uint32_t sv_n_pre = sv_n;
                uint32_t sv_round_n_pre = sv_round_n;
                for (uint32_t split_idx = 0; split_idx < KV_INC && n_idx + split_idx < n_end + KV_INC; split_idx++) {
                    pingpong_flag = (n_idx + split_idx) % 2;
                    offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                    if (n_idx + split_idx >= KV_INC) {
                        if (n_idx + split_idx == (n_loop + KV_INC - 1)) {
                            sv_n = (kv_seqlen - (n_idx + split_idx - KV_INC) * pp_n_scalar);
                            sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        int32_t block_id0 = (int32_t)(*((__gm__ int32_t *)block_tables_gm + stride_table + n_idx + split_idx - KV_INC));  // fix: block_idx
                        int64_t v_offset = (int64_t)block_id0 * block_size * kv_heads * embedding_size + (int64_t)(head_idx / group_num) * embedding_size;

                        SET_FLAG(S, MTE2, EVENT_ID0);
                        // *** Prepare V to L1
                        WAIT_FLAG(MTE1, MTE2, pingpong_flag + 2);
                        WAIT_FLAG(S, MTE2, EVENT_ID0);
                        //gm_to_l1v
                        gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                            l1v_buf_addr_tensor[offset],
                            v_gm_tensor[v_offset],
                            sv_n,        // nValue
                            sv_round_n,  // dstNzC0Stride
                            0,            // dstNzMatrixStride, unused
                            __k,         // dValue
                            0,            // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );
                        SET_FLAG(MTE2, MTE1, pingpong_flag);
                        WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                        WAIT_FLAG(M, MTE1, pingpong_flag);
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < sv_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                            l1_to_l0_b<ArchType::ASCEND_V220, half, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0b_buf_tensor[offset + l0b_load_idx * round_k * BLOCK_SIZE],
                                l1v_buf_addr_tensor[offset + l0b_load_idx * CUBE_MATRIX_SIZE],
                                0,
                                round_k / BLOCK_SIZE,     // repeat
                                0,
                                sv_round_n / BLOCK_SIZE,  // srcStride
                                0,
                                0                        // dstStride
                            );
                        }
                    }
                }
                uint32_t sv_n = sv_n_pre;
                uint32_t sv_round_n = sv_round_n_pre;
                if (n_idx >= KV_INC) {
                    WaitFlagDev(SOFTMAX_READY);  // 2
                }
                for (uint32_t split_idx = 0; split_idx < KV_INC && n_idx + split_idx < n_end + KV_INC; split_idx++) {
                    pingpong_flag = (n_idx + split_idx) % 2;
                    offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                    if (n_idx + split_idx >= KV_INC) {
                        if (n_idx + split_idx == (n_loop + KV_INC - 1)) {
                            sv_n = (kv_seqlen - (n_idx + split_idx - KV_INC) * pp_n_scalar);
                            sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        // *** Prepare P to L1
                        if (qk_m == 1) {
                            gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::ND>(
                                l1p_buf_addr_tensor[offset],
                                p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx + KV_INC) % vect_mod * TMP_SIZE / vect_mod],
                                1,
                                0,
                                0,
                                sv_round_n,               // lenBurst
                                0,
                                0
                            );
                        } else {
                            gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                                l1p_buf_addr_tensor[offset],
                                p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx + KV_INC) % vect_mod * TMP_SIZE / vect_mod],
                                qk_m,        // nValue
                                qk_round_m,  // dstNzC0Stride
                                0,            // dstNzMatrixStride, unused
                                sv_n,        // dValue
                                0,            // dstNzMatrixStride, unused
                                sv_round_n  // srcDValue
                            );
                        }
                        SET_FLAG(MTE2, MTE1, pingpong_flag);
                        WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                        if (qk_m == 1) {
                            l1_to_l0_a<ArchType::ASCEND_V220, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0a_buf_tensor[offset],
                                l1p_buf_addr_tensor[offset],
                                0,
                                (sv_round_n + CUBE_MATRIX_SIZE - 1) / CUBE_MATRIX_SIZE,  // repeat
                                0,
                                1,                                                       // srcStride
                                0,
                                0                                                       // dstStride
                            );
                        } else {
                            for (uint32_t l0a_load_idx = 0; l0a_load_idx < qk_round_m / BLOCK_SIZE; ++l0a_load_idx) {
                                l1_to_l0_a<ArchType::ASCEND_V220, half, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                    l0a_buf_tensor[offset + l0a_load_idx * sv_round_n * BLOCK_SIZE],
                                    l1p_buf_addr_tensor[offset + l0a_load_idx * CUBE_MATRIX_SIZE],
                                    0,
                                    sv_round_n / BLOCK_SIZE,  // repeat
                                    0,
                                    qk_round_m / BLOCK_SIZE,  // srcStride
                                    0,
                                    0                        // dstStride
                                );
                            }
                        }
                        SET_FLAG(MTE1, MTE2, pingpong_flag + 2);
                        SET_FLAG(MTE1, M, pingpong_flag);
                        WAIT_FLAG(MTE1, M, pingpong_flag);
                        WAIT_FLAG(FIX, M, pingpong_flag);
                        mmad<ArchType::ASCEND_V220, half, half, float, false>(
                            l0c_buf_tensor[offset],
                            l0a_buf_tensor[offset],
                            l0b_buf_tensor[offset],
                            qk_m,  // m
                            __k,   // n
                            sv_n,  // k
                            1      // cmatrixInitVal
                        );
                        SET_FLAG(M, MTE1, pingpong_flag);
                        SET_FLAG(M, FIX, pingpong_flag);
                        WAIT_FLAG(M, FIX, pingpong_flag);
                        // copy O to gm
                        l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                            o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx + KV_INC) % vect_mod * TMP_SIZE / vect_mod],
                            l0c_buf_tensor[offset],
                            qk_m,        // MSize
                            round_k,     // NSize
                            qk_round_m,  // srcStride
                            round_k     // dstStride_dst_D
                        );
                        SET_FLAG(FIX, M, pingpong_flag);
                    }
                }
                if (n_idx >= KV_INC) {
                    FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY);
                }
            }
        }
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        PIPE_BARRIER(ALL);
    }

    __aicore__ inline void RunHighPrec()
    {
        int32_t sub_block_idx = GetSubBlockidx();
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num * q_heads;
        for (uint32_t process = 0; process < process_num; process++) {
            if (process >= cur_total_q_blk_num * q_heads) {
                while (1) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
                    uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
                    if(q_seqlen != 0) {
                        break;
                    }
                }
            }
            uint32_t cur_core_idx = process % block_num;
            if (is_triu_mask) {
                if ((process / block_num) % 2 == 1) {
                    cur_core_idx = block_num - process % block_num - 1;
                }
            }
            if (block_idx != cur_core_idx) {
                continue;
            }
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_q_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4 + offset_tiling));
            uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 5 + offset_tiling));
            uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);

            uint32_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint64_t head_idx = process_idx % q_heads;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qk_round_m = (qk_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint64_t qk_index = 0;
            /**************** pre_load *****************/
            uint32_t qk_n = (qk_index == (n_loop - 1)) ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
            uint64_t q_offset = addr_q_scalar + head_idx * embedding_size + m_idx * pp_m_scalar * stride_qo;

            uint32_t batch_id = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 8 + offset_tiling));  // 真实顺序的batch_id
            uint64_t stride_table = batch_id * max_num_blocks_per_query;
            uint32_t s_pingpong_flag = 0;
            uint32_t p_pingpong_flag = 0;
            uint32_t o_pingpong_flag = 0;

            uint32_t sv_n = pp_n_scalar;
            uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint32_t n_end = n_loop;
            if (is_triu_mask) {
                uint32_t triu_seq = mask_type == 3 ? max_kv - q_seqlen : kv_seqlen - q_seqlen;
                uint32_t n_offset = ((m_idx + 1) * pp_m_scalar + triu_seq + pp_n_scalar - 1) / pp_n_scalar;
                n_end = n_offset > n_loop ? n_loop : n_offset;
            }
            // Only need load Q once
            // gm_to_l1q
            if (qk_m == 1) {
                gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    round_k,              // lenBurst
                    0,
                    0
                );
            } else {
                gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::NZ>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    qk_m,       // nValue
                    qk_round_m, // dstNzC0Stride
                    0,
                    __k,        // dValue
                    0,
                    stride_qo  // srcDValue
                );
            }
            SET_FLAG(MTE2, MTE1, pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, pingpong_flag);
            WAIT_FLAG(M, MTE1, pingpong_flag);

            LoadDataToCa(l0a_buf_tensor[offset], l1q_buf_addr_tensor, round_k, qk_round_m, qk_m);
            // *** Prepare K to L1
            int32_t block_id_k = (int32_t)(*((__gm__ int32_t *)block_tables_gm + stride_table + qk_index));
            int64_t k_offset = (int64_t)block_id_k * block_size * kv_heads * embedding_size + (int64_t)(head_idx / group_num) * embedding_size;

            SET_FLAG(S, MTE2, EVENT_ID0);
            WAIT_FLAG(S, MTE2, EVENT_ID0);
            WAIT_FLAG(MTE1, MTE2, pingpong_flag);
            // gm_to_l1k
            gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::NZ>(
                l1k_buf_addr_tensor[offset],
                k_gm_tensor[k_offset],
                qk_n,       // nValue
                qk_round_n, // dstNzC0Stride
                0,
                __k,        // dValue
                0,
                stride_kv  // srcDValue
            );
            SET_FLAG(MTE2, MTE1, pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, pingpong_flag);
            l1_to_l0_b<ArchType::ASCEND_V220, IN_DATA_TYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                l0b_buf_tensor[offset],
                l1k_buf_addr_tensor[offset],
                0,
                round_k * qk_round_n / CUBE_MATRIX_SIZE, // repeat
                0,
                1,                                       // srcStride
                0,
                0                                       // dstStride
            );
            SET_FLAG(MTE1, MTE2, pingpong_flag);
            SET_FLAG(MTE1, M, pingpong_flag);
            WAIT_FLAG(MTE1, M, pingpong_flag);
            WAIT_FLAG(FIX, M, pingpong_flag);
            mmad<ArchType::ASCEND_V220, IN_DATA_TYPE, IN_DATA_TYPE, float, false>(
                l0c_buf_tensor[offset],
                l0a_buf_tensor[offset],
                l0b_buf_tensor[offset],
                qk_m, // m
                qk_n, // n
                __k,  // k
                1     // cmatrixInitVal
            );
            SET_FLAG(M, MTE1, pingpong_flag);
            SET_FLAG(M, FIX, pingpong_flag);
            WAIT_FLAG(M, FIX, pingpong_flag);
            // copy S to gm
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + s_pingpong_flag * TMP_SIZE_DECODER / 2],
                l0c_buf_tensor[offset],
                qk_m,       // MSize
                qk_round_n, // NSize
                qk_round_m, // srcStride
                qk_round_n // dstStride_dst_D
            );
            SET_FLAG(FIX, M, pingpong_flag);
            pingpong_flag = 1 - pingpong_flag;
            offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
            FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
            s_pingpong_flag = 1 - s_pingpong_flag;
            qk_index++;


            for (uint32_t n_idx = 0; n_idx < n_end; n_idx++) {
                if (qk_index < n_end) {
                    if (qk_index == (n_loop - 1)) {
                        qk_n = (kv_seqlen - qk_index * pp_n_scalar);
                        qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    }
                    int32_t block_id_k = (int32_t)(*((__gm__ int32_t *)block_tables_gm + stride_table + qk_index));
                    int64_t k_offset = (int64_t)block_id_k * block_size * kv_heads * embedding_size + (int64_t)(head_idx / group_num) * embedding_size;

                    SET_FLAG(S, MTE2, EVENT_ID0);
                    WAIT_FLAG(M, MTE1, pingpong_flag);

                    LoadDataToCa(l0a_buf_tensor[offset], l1q_buf_addr_tensor, round_k, qk_round_m, qk_m);

                    // *** Prepare K to L1
                    WAIT_FLAG(MTE1, MTE2, pingpong_flag);
                    WAIT_FLAG(S, MTE2, EVENT_ID0);
                    // gm_to_l1k
                    gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::NZ>(
                        l1k_buf_addr_tensor[offset],
                        k_gm_tensor[k_offset],
                        qk_n,       // nValue
                        qk_round_n, // dstNzC0Stride
                        0,
                        __k,        // dValue
                        0,
                        stride_kv  // srcDValue
                    );
                    SET_FLAG(MTE2, MTE1, pingpong_flag);
                    WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_DATA_TYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor[offset],
                        l1k_buf_addr_tensor[offset],
                        0,
                        round_k * qk_round_n / CUBE_MATRIX_SIZE, // repeat
                        0,
                        1,                                       // srcStride
                        0,
                        0                                       // dstStride
                    );
                    SET_FLAG(MTE1, MTE2, pingpong_flag);
                    SET_FLAG(MTE1, M, pingpong_flag);
                    WAIT_FLAG(MTE1, M, pingpong_flag);
                    WAIT_FLAG(FIX, M, pingpong_flag);
                    mmad<ArchType::ASCEND_V220, IN_DATA_TYPE, IN_DATA_TYPE, float, false>(
                        l0c_buf_tensor[offset],
                        l0a_buf_tensor[offset],
                        l0b_buf_tensor[offset],
                        qk_m, // m
                        qk_n, // n
                        __k,  // k
                        1     // cmatrixInitVal
                    );
                    SET_FLAG(M, MTE1, pingpong_flag);
                    SET_FLAG(M, FIX, pingpong_flag);
                    WAIT_FLAG(M, FIX, pingpong_flag);
                    // copy S to gm
                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + s_pingpong_flag * TMP_SIZE_DECODER / 2],
                        l0c_buf_tensor[offset],
                        qk_m,       // MSize
                        qk_round_n, // NSize
                        qk_round_m, // srcStride
                        qk_round_n // dstStride_dst_D
                    );
                    SET_FLAG(FIX, M, pingpong_flag);
                    pingpong_flag = 1 - pingpong_flag;
                    offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                    FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
                    s_pingpong_flag = 1 - s_pingpong_flag;
                }

                if (n_idx == (n_loop - 1)) {
                    sv_n = (kv_seqlen - n_idx * pp_n_scalar);
                    sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                }
                int32_t block_id_v = (int32_t)(*((__gm__ int32_t *)block_tables_gm + stride_table + n_idx));  // fix: block_idx
                int64_t v_offset = (int64_t)block_id_v * block_size * kv_heads * embedding_size + (int64_t)(head_idx / group_num) * embedding_size;

                SET_FLAG(S, MTE2, EVENT_ID0);
                WAIT_FLAG(S, MTE2, EVENT_ID0);
                // *** Prepare V to L1
                WAIT_FLAG(MTE1, MTE2, pingpong_flag + 2);
                // gm_to_l1v
                gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::NZ>(
                    l1v_buf_addr_tensor[offset],
                    v_gm_tensor[v_offset],
                    sv_n,       // nValue
                    sv_round_n, // dstNzC0Stride
                    0,
                    __k,        // dValue
                    0,
                    stride_kv  // srcDValue
                );
                SET_FLAG(MTE2, MTE1, pingpong_flag);
                WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                WAIT_FLAG(M, MTE1, pingpong_flag);
                for (uint32_t l0b_load_idx = 0; l0b_load_idx < sv_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_DATA_TYPE, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor[offset + l0b_load_idx * round_k * BLOCK_SIZE],
                        l1v_buf_addr_tensor[offset + l0b_load_idx * CUBE_MATRIX_SIZE],
                        0,
                        round_k / BLOCK_SIZE,    // repeat
                        0,
                        sv_round_n / BLOCK_SIZE, // srcStride
                        0,
                        0                       // dstStride
                    );
                }
                WaitFlagDev(SOFTMAX_READY); // 2
                // *** Prepare P to L1
                if (qk_m == 1) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::ND>(
                        l1p_buf_addr_tensor[offset],
                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + p_pingpong_flag * TMP_SIZE / 2],
                        1,
                        0,
                        0,
                        sv_round_n,              // lenBurst
                        0,
                        0
                    );
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DATA_TYPE, DataFormat::ND, DataFormat::NZ>(
                        l1p_buf_addr_tensor[offset],
                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + p_pingpong_flag * TMP_SIZE / 2],
                        qk_m,       // nValue
                        qk_round_m, // dstNzC0Stride
                        0,
                        sv_n,       // dValue
                        0,
                        sv_round_n // srcDValue
                    );
                }
                p_pingpong_flag = 1 - p_pingpong_flag;
                SET_FLAG(MTE2, MTE1, pingpong_flag);
                WAIT_FLAG(MTE2, MTE1, pingpong_flag);

                LoadDataToCa(l0a_buf_tensor[offset], l1p_buf_addr_tensor[offset],
                             sv_round_n, qk_round_m, qk_m);

                SET_FLAG(MTE1, MTE2, pingpong_flag + 2);
                SET_FLAG(MTE1, M, pingpong_flag);
                WAIT_FLAG(MTE1, M, pingpong_flag);
                WAIT_FLAG(FIX, M, pingpong_flag);
                mmad<ArchType::ASCEND_V220, IN_DATA_TYPE, IN_DATA_TYPE, float, false>(
                    l0c_buf_tensor[offset],
                    l0a_buf_tensor[offset],
                    l0b_buf_tensor[offset],
                    qk_m, // m
                    __k,  // n
                    sv_n, // k
                    1     // cmatrixInitVal
                );
                SET_FLAG(M, MTE1, pingpong_flag);
                SET_FLAG(M, FIX, pingpong_flag);
                WAIT_FLAG(M, FIX, pingpong_flag);
                // copy O to gm
                l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>(
                    o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + o_pingpong_flag * TMP_SIZE / 2],
                    l0c_buf_tensor[offset],
                    qk_m,       // MSize
                    round_k,    // NSize
                    qk_round_m, // srcStride
                    round_k    // dstStride_dst_D
                );
                SET_FLAG(FIX, M, pingpong_flag);
                pingpong_flag = 1 - pingpong_flag;
                offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY);
                o_pingpong_flag = 1 - o_pingpong_flag;
                qk_index++;
            }
        }
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        PIPE_BARRIER(ALL);
    }
private:
    __aicore__ __attribute__((always_inline)) inline void LoadDataToCa(
        AscendC::LocalTensor<IN_DATA_TYPE> dst_tensor, AscendC::LocalTensor<IN_DATA_TYPE> src_tensor,
        uint32_t round_k, uint32_t qk_round_m, uint32_t qk_m)
    {
        if (qk_m == 1) {
            l1_to_l0_a<ArchType::ASCEND_V220, IN_DATA_TYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                dst_tensor,
                src_tensor,
                0,
                (round_k + CUBE_MATRIX_SIZE - 1) / CUBE_MATRIX_SIZE, // repeat
                0,
                1,                                                   // srcStride
                0,
                0                                                   // dstStride
            );
        } else {
            for (uint32_t l0a_load_idx = 0; l0a_load_idx < qk_round_m / BLOCK_SIZE; ++l0a_load_idx) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DATA_TYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    dst_tensor[l0a_load_idx * round_k * BLOCK_SIZE],
                    src_tensor[l0a_load_idx * CUBE_MATRIX_SIZE],
                    0,
                    round_k / BLOCK_SIZE,    // repeat
                    0,
                    qk_round_m / BLOCK_SIZE, // srcStride
                    0,
                    0                       // dstStride
                );
            }
        }
    }
private:
    __gm__ uint8_t *__restrict__ block_tables_gm;
    __gm__ uint8_t *__restrict__ tiling_para_gm;

    AscendC::GlobalTensor<IN_DATA_TYPE> q_gm_tensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> k_gm_tensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> v_gm_tensor;
    AscendC::GlobalTensor<S_DATA_TYPE> s_gm_tensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::LocalTensor<IN_DATA_TYPE> l1q_buf_addr_tensor;
    AscendC::LocalTensor<IN_DATA_TYPE> l1k_buf_addr_tensor;
    AscendC::LocalTensor<IN_DATA_TYPE> l1p_buf_addr_tensor;
    AscendC::LocalTensor<IN_DATA_TYPE> l1v_buf_addr_tensor;

    AscendC::LocalTensor<IN_DATA_TYPE> l0a_buf_tensor;
    AscendC::LocalTensor<IN_DATA_TYPE> l0b_buf_tensor;
    AscendC::LocalTensor<float> l0c_buf_tensor;

    uint32_t batch_size;
    uint32_t q_heads;
    uint32_t embedding_size;
    uint32_t kv_heads;
    uint32_t max_num_blocks_per_query;
    uint32_t block_size;
    uint32_t total_q_blk_num;
    uint32_t group_num;
    uint32_t stride_kv;
    uint32_t stride_qo;
    uint32_t __k;
    uint32_t round_k;
    uint32_t is_triu_mask = 1;
    uint32_t tiling_head_size;
    uint32_t tiling_para_size;
    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;
    uint32_t mask_type;
    uint32_t max_kv;
};
#elif __DAV_C220_VEC__
template <typename S_DATA_TYPE, typename P_DATA_TYPE, typename MASK_DATA_TYPE>
class PagedAttentionParallelAiv {
public:
    __aicore__ inline PagedAttentionParallelAiv(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    // template <typename S_DATA_TYPE, typename P_DATA_TYPE, typename MASK_DATA_TYPE>
    __aicore__ inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t *__restrict__ q_in_gm,
        __gm__ uint8_t *__restrict__ k_in_gm,
        __gm__ uint8_t *__restrict__ v_in_gm,
        __gm__ uint8_t *__restrict__ block_tables_in_gm,
        __gm__ uint8_t *__restrict__ mask_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t *__restrict__ tiling_in_para_gm)
    {

        block_tables_gm = block_tables_in_gm;
        tiling_para_gm = tiling_in_para_gm;
        mask_gm = mask_in_gm;

        mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ MASK_DATA_TYPE *>(mask_in_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ P_DATA_TYPE *>(o_out_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ S_DATA_TYPE *>(s_out_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ P_DATA_TYPE *>(p_out_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_temp_gm));

        const uint32_t ls_ubuf_offset = 0;
        const uint32_t lp_ubuf_offset = 0;
        const uint32_t ls32_ubuf_offset = 2 * PRE_UB_UINT8_BLOCK_SIZE;
        const uint32_t mask_ubuf_offset = 4 * PRE_UB_UINT8_BLOCK_SIZE;
        const uint32_t lo_ubuf_offset = 6 * PRE_UB_UINT8_BLOCK_SIZE;
        const uint32_t lm_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE;
        const uint32_t hm_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE;
        const uint32_t gm_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE;
        const uint32_t dm_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE + 4 * UB_UINT8_LINE_SIZE;
        const uint32_t ll_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE + 8 * UB_UINT8_LINE_SIZE;
        const uint32_t gl_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE + 16 * UB_UINT8_LINE_SIZE;
        const uint32_t tv_ubuf_offset = 8 * PRE_UB_UINT8_BLOCK_SIZE + 17 * UB_UINT8_LINE_SIZE;
        const uint32_t go_ubuf_offset = 9 * PRE_UB_UINT8_BLOCK_SIZE;
        const uint32_t mask16_ubuf_offset = 11 * PRE_UB_UINT8_BLOCK_SIZE;

        ls_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DATA_TYPE>(ls_ubuf_offset);
        lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, P_DATA_TYPE>(lp_ubuf_offset);
        ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_ubuf_offset);
        mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DATA_TYPE>(mask_ubuf_offset);
        lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
        lm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DATA_TYPE>(lm_ubuf_offset);
        hm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DATA_TYPE>(hm_ubuf_offset);
        gm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DATA_TYPE>(gm_ubuf_offset);
        dm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, S_DATA_TYPE>(dm_ubuf_offset);
        ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
        gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_offset);
        tv_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv_ubuf_offset);
        go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_offset);
        mask16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, MASK_DATA_TYPE>(mask16_ubuf_offset);

        batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM));
        kv_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_KVHEADS));
        max_context_len = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MASK_MAX_LEN));
        tor = (S_DATA_TYPE)(*((__gm__ float *)tiling_para_gm + TILING_TOR));
        block_size = (int32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_TOTAL_BLOCK_NUM));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        batch_stride = (uint64_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BATCH_STRIDE));
        head_stride = (uint64_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEAD_STRIDE));
        mask_type = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MASK_TYPE_ND));
        max_kv = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MAX_KVSEQLEN));
        stride_qo = q_heads * embedding_size;

        __k = embedding_size;
        round_k = (__k + 15) / 16 * 16;
    }

    __aicore__ inline void Run()
    {
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID2);
        SET_FLAG(MTE3, V, EVENT_ID0);

        int32_t sub_block_idx = GetSubBlockidx();
        uint32_t go_flag_scalar = 1;

        uint32_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num * q_heads;
        for (uint32_t process = 0; process < process_num; process++) {
            if (process >= cur_total_q_blk_num * q_heads) {
                while (1) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
                    uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
                    if(q_seqlen != 0) {
                        break;
                    }
                }
            }
            uint32_t cur_core_idx = process % block_num;
            if (is_triu_mask) {
                if ((process / block_num) % 2 == 1) {
                    cur_core_idx = block_num - process % block_num - 1;
                }
            }
            if (block_idx != cur_core_idx) {
                continue;
            }

            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);

            uint32_t mask_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10 + offset_tiling));
            uint32_t mask_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 14 + offset_tiling));
            uint64_t mask_scalar = (uint64_t)(((uint64_t)mask_high32) << 32 | mask_loww32);

            uint32_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint32_t head_idx = process_idx % q_heads;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t sub_m = (sub_block_idx == 1) ? (qk_m - qk_m / 2) : qk_m / 2;
            uint32_t sub_m_d128 = (sub_m + VECTOR_SIZE - 1) / VECTOR_SIZE;             // up aligned to 128
            uint32_t sub_m_d64 = (sub_m + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;  // up aligned to 64
            uint32_t round_sub_m = (sub_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint64_t qk_index = 0;
            /******** pre_load *******/
            uint32_t qk_n = (qk_index == (n_loop - 1)) ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * UB_HALF_BUF_SIZE;
            uint64_t mask_offset = (cur_batch * batch_stride + head_idx * head_stride) * max_context_len + m_idx * pp_m_scalar * max_context_len;
            mask_offset += mask_scalar;

            uint64_t o_offset = addr_o_scalar + head_idx * embedding_size + m_idx * pp_m_scalar * stride_qo;
            uint32_t n_end = n_loop;
            if (is_triu_mask) {
                uint32_t triu_seq = mask_type == 3 ? max_kv - q_seqlen : kv_seqlen - q_seqlen;
                uint32_t n_offset = ((m_idx + 1) * pp_m_scalar + triu_seq + pp_n_scalar - 1) / pp_n_scalar;
                n_end = n_offset > n_loop ? n_loop : n_offset;
            }
            uint32_t KV_INC = n_end > 4 ? 2 : 1;
            uint32_t vect_mod = 2 * KV_INC;
            uint32_t long_seq = 0;
            for (uint32_t n_idx = 0; n_idx < n_end + KV_INC; n_idx += KV_INC) {
                if (n_idx < n_end) {
                    if (sub_m > 0 && mask_gm != nullptr) {
                        uint32_t qk_n_temp = qk_n;
                        for (uint32_t split_idx = 0; split_idx < KV_INC && n_idx + split_idx < n_end; split_idx++) {
                            WAIT_FLAG(V, MTE2, split_idx * 2);
                            if (n_idx + split_idx == (n_loop - 1)) {
                                qk_n_temp = (kv_seqlen - (n_idx + split_idx) * pp_n_scalar);
                            }
                            if (long_seq == 0) {
                                gm_to_ub_align<ArchType::ASCEND_V220, half>(
                                    mask_ubuf_tensor[split_idx * UB_HALF_BUF_SIZE],
                                    mask_gm_tensor[mask_offset + (uint64_t)sub_block_idx * qk_m / 2 * max_context_len],
                                    0,                                 // sid
                                    sub_m,                             // nBurst
                                    qk_n_temp * 2,                     // lenBurst
                                    0,                                 // leftPaddingNum
                                    0,                                 // rightPaddingNum
                                    (max_context_len - qk_n_temp) * 2, // srcGap
                                    0                                  // dstGap
                                );
                                mask_offset += pp_n_scalar;
                            }
                        }
                    }
                    WaitFlagDev(QK_READY);
                    for (uint32_t split_idx = 0; split_idx < KV_INC && n_idx + split_idx < n_end; split_idx++) {
                        pingpong_flag = (n_idx + split_idx) % 2;
                        offset = pingpong_flag * UB_HALF_BUF_SIZE;
                        if (n_idx + split_idx == (n_loop - 1)) {
                            qk_n = (kv_seqlen - (n_idx + split_idx) * pp_n_scalar);
                            qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        if (sub_m > 0) {
                            WAIT_FLAG(MTE3, MTE2, pingpong_flag);
                            // input QK
                            gm_to_ub<ArchType::ASCEND_V220, half>(
                                ls_ubuf_tensor[offset],
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx) % vect_mod * TMP_SIZE / vect_mod +
                                    (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                0,                                // sid
                                1,                                // nBurst
                                sub_m * qk_round_n / BLOCK_SIZE,  // lenBurst
                                0,                                // srcGap
                                0                                 // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID0);
                            WAIT_FLAG(MTE2, V, EVENT_ID0);
                            // *** ls = tor * ls
                            for (uint32_t vadd_idx = 0; vadd_idx < qk_n / VECTOR_SIZE; ++vadd_idx) {
                                muls_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[offset + vadd_idx * VECTOR_SIZE],
                                    ls_ubuf_tensor[offset + vadd_idx * VECTOR_SIZE],
                                    tor,
                                    sub_m,                          // repeat
                                    1,                              // dstBlockStride
                                    1,                              // srcBlockStride
                                    qk_round_n / BLOCK_SIZE,  // dstRepeatStride
                                    qk_round_n / BLOCK_SIZE  // srcRepeatStride
                                );
                            }
                            if (qk_n % VECTOR_SIZE > 0) {
                                __set_mask(qk_n % VECTOR_SIZE);
                                muls_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[offset + qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                    ls_ubuf_tensor[offset + qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                    tor,
                                    sub_m,                          // repeat
                                    1,                              // dstBlockStride
                                    1,                              // srcBlockStride
                                    qk_round_n / BLOCK_SIZE,  // dstRepeatStride
                                    qk_round_n / BLOCK_SIZE  // srcRepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);

                            // *** ls = ls + mask
                            if (mask_gm != nullptr) {
                                if (long_seq == 0) {
                                    add_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[offset], ls_ubuf_tensor[offset],
                                        mask_ubuf_tensor[split_idx * UB_HALF_BUF_SIZE],
                                        (sub_m * qk_round_n + VECTOR_SIZE - 1) / VECTOR_SIZE, // repeat
                                        1,                                                    // dstBlockStride
                                        1,                                                    // src0BlockStride
                                        1,                                                    // src1BlockStride
                                        8,                                                    // dstRepeatStride
                                        8,                                                    // src0RepeatStride
                                        8                                                     // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                SET_FLAG(V, MTE2, split_idx * 2);
                            }
                            // *** lm = rowmax(ls)
                            if (qk_n <= VECTOR_SIZE) {
                                __set_mask(qk_n);
                                cmax_v<ArchType::ASCEND_V220, half, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_tensor,
                                    ls_ubuf_tensor[offset],
                                    sub_m,                    // repeat
                                    1,                        // dstRepeatStride
                                    1,                        // srcBlockStride
                                    qk_round_n / BLOCK_SIZE   // srcRepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                ub_to_ub<ArchType::ASCEND_V220, half>(
                                    ls32_ubuf_tensor.ReinterpretCast<half>(),
                                    ls_ubuf_tensor[offset],
                                    0,                                        // sid
                                    sub_m,                                    // nBurst
                                    VECTOR_SIZE / BLOCK_SIZE,                 // lenBurst
                                    (qk_round_n - VECTOR_SIZE) / BLOCK_SIZE,  // srcGap
                                    0                                         // dstGap
                                );
                                PIPE_BARRIER(V);
                                __set_mask(qk_n - VECTOR_SIZE);
                                max_v<ArchType::ASCEND_V220, half>(ls32_ubuf_tensor.ReinterpretCast<half>(),
                                    ls32_ubuf_tensor.ReinterpretCast<half>(),
                                    ls_ubuf_tensor[offset + VECTOR_SIZE],
                                    sub_m,                   // repeat
                                    1,                       // dstBlockStride
                                    1,                       // src0BlockStride
                                    1,                       // src1BlockStride
                                    8,                       // dstRepeatStride
                                    8,                       // src0RepeatStride
                                    qk_round_n / BLOCK_SIZE  // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                cmax_v<ArchType::ASCEND_V220, half, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_tensor,
                                    ls32_ubuf_tensor.ReinterpretCast<half>(),
                                    sub_m,      // repeat
                                    1,          // dstRepeatStride
                                    1,          // srcBlockStride
                                    8           // srcRepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            if ((n_idx + split_idx) == 0) {
                                // *** hm = lm
                                ub_to_ub<ArchType::ASCEND_V220, half>(
                                    hm_ubuf_tensor,
                                    lm_ubuf_tensor,
                                    0,                         // sid
                                    1,                         // nBurst
                                    round_sub_m / BLOCK_SIZE,  // lenBurst
                                    0,                         // srcGap
                                    0                          // dstGap
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** hm = vmax(lm, gm)
                                max_v<ArchType::ASCEND_V220, half>(hm_ubuf_tensor,
                                    lm_ubuf_tensor,
                                    gm_ubuf_tensor,
                                    sub_m_d128,  // repeat
                                    1,           // dstBlockStride
                                    1,           // src0BlockStride
                                    1,           // src1BlockStride
                                    8,           // dstRepeatStride
                                    8,           // src0RepeatStride
                                    8            // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = gm - hm
                                sub_v<ArchType::ASCEND_V220, half>(dm_ubuf_tensor[(n_idx + split_idx) % vect_mod * UB_HALF_LINE_SIZE],
                                    gm_ubuf_tensor,
                                    hm_ubuf_tensor,
                                    sub_m_d128,  // repeat
                                    1,           // dstBlockStride
                                    1,           // src0BlockStride
                                    1,           // src1BlockStride
                                    8,           // dstRepeatStride
                                    8,           // src0RepeatStride
                                    8            // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                            // *** gm = hm
                            ub_to_ub<ArchType::ASCEND_V220, half>(
                                gm_ubuf_tensor,
                                hm_ubuf_tensor,
                                0,                         // sid
                                1,                         // nBurst
                                round_sub_m / BLOCK_SIZE,  // lenBurst
                                0,                         // srcGap
                                0                          // dstGap
                            );
                            PIPE_BARRIER(V);
                            // *** hm_block = expand_to_block(hm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint16_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                hm_ubuf_tensor.template ReinterpretCast<uint16_t>(),
                                1,                              // dstBlockStride
                                8,                              // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** ls = ls - hm_block
                            for (uint32_t vsub_idx = 0; vsub_idx < qk_n / VECTOR_SIZE; ++vsub_idx) {
                                sub_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[offset + vsub_idx * VECTOR_SIZE],
                                    ls_ubuf_tensor[offset + vsub_idx * VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<half>(),
                                    sub_m,                    // repeat
                                    1,                        // dstBlockStride
                                    1,                        // src0BlockStride
                                    0,                        // src1BlockStride
                                    qk_round_n / BLOCK_SIZE,  // dstRepeatStride
                                    qk_round_n / BLOCK_SIZE,  // src0RepeatStride
                                    1                         // src1RepeatStride
                                );
                            }
                            if (qk_n % VECTOR_SIZE > 0) {
                                __set_mask(qk_n % VECTOR_SIZE);
                                sub_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[offset + qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                    ls_ubuf_tensor[offset + qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<half>(),
                                    sub_m,                    // repeat
                                    1,                        // dstBlockStride
                                    1,                        // src0BlockStride
                                    0,                        // src1BlockStride
                                    qk_round_n / BLOCK_SIZE,  // dstRepeatStride
                                    qk_round_n / BLOCK_SIZE,  // src0RepeatStride
                                    1                         // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** ls = castfp16to32(ls)
                            conv_v<ArchType::ASCEND_V220, half, float>(ls32_ubuf_tensor,
                                ls_ubuf_tensor[offset],
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                1,                                                                 // dstBlockStride
                                1,                                                                 // srcBlockStride
                                8,                                                                 // dstRepeatStride
                                4                                                                  // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** ls = exp(ls)
                            exp_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor,
                                ls32_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                1,                                                                 // dstBlockStride
                                1,                                                                 // srcBlockStride
                                8,                                                                 // dstRepeatStride
                                8                                                                  // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** lp = castfp32to16(ls)
                            conv_v<ArchType::ASCEND_V220, float, half>(lp_ubuf_tensor[offset],
                                ls32_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                1,                                                                 // dstBlockStride
                                1,                                                                 // srcBlockStride
                                4,                                                                 // dstRepeatStride
                                8                                                                  // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            SET_FLAG(V, MTE3, EVENT_ID0);
                            // *** ll = rowsum(ls32)
                            if (qk_n <= FLOAT_VECTOR_SIZE) {
                                __set_mask(qk_n);
                                cadd_v<ArchType::ASCEND_V220, float>(ll_ubuf_tensor[(n_idx + split_idx) % vect_mod * UB_FLOAT_LINE_SIZE],
                                    ls32_ubuf_tensor,
                                    sub_m,                          // repeat
                                    1,                              // dstRepeatStride
                                    1,                              // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                                        sub_m,                          // repeat
                                        1,                              // dstBlockStride
                                        1,                              // src0BlockStride
                                        1,                              // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE   // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        sub_m,                          // repeat
                                        1,                              // dstBlockStride
                                        1,                              // src0BlockStride
                                        1,                              // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE   // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);
                                cadd_v<ArchType::ASCEND_V220, float>(ll_ubuf_tensor[(n_idx + split_idx) % vect_mod * UB_FLOAT_LINE_SIZE],
                                    ls32_ubuf_tensor,
                                    sub_m,                          // repeat
                                    1,                              // dstRepeatStride
                                    1,                              // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm<ArchType::ASCEND_V220, half>(
                                p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx) % vect_mod * TMP_SIZE / vect_mod +
                                    (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                lp_ubuf_tensor[offset],
                                0,                                // sid
                                1,                                // nBurst
                                sub_m * qk_round_n / BLOCK_SIZE,  // lenBurst
                                0,                                // srcGap
                                0                                 // dstGap
                            );
                            SET_FLAG(MTE3, MTE2, pingpong_flag);
                        }
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);
                }
                if (n_idx >= KV_INC) {
                    WaitFlagDev(UPDATE_READY);  // 4
                }
                for (uint32_t split_idx = 0; split_idx < KV_INC && n_idx + split_idx < n_end + KV_INC; split_idx++) {
                    if (n_idx + split_idx >= KV_INC && sub_m > 0) {
                        WAIT_FLAG(V, MTE2, EVENT_ID1);
                        pingpong_flag = (n_idx + split_idx + KV_INC) % 2;
                        gm_to_ub<ArchType::ASCEND_V220, float>(
                            lo_ubuf_tensor,
                            o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx + split_idx + KV_INC) % vect_mod * TMP_SIZE / vect_mod +
                                (uint64_t)sub_block_idx * qk_m / 2 * round_k],
                            0,                                   // sid
                            1,                                   // nBurst
                            sub_m * round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                            0,                                   // srcGap
                            0                                    // dstGap
                        );
                        SET_FLAG(MTE2, V, EVENT_ID0);
                        WAIT_FLAG(MTE2, V, EVENT_ID0);
                        // *** 更新 L 和 O
                        if ((n_idx + split_idx) != KV_INC) {
                            // *** dm32 = castfp16to32(dm), 存放于 tv
                            conv_v<ArchType::ASCEND_V220, half, float>(tv_ubuf_tensor,
                                dm_ubuf_tensor[(n_idx + split_idx + KV_INC) % vect_mod * UB_HALF_LINE_SIZE],
                                sub_m_d64,  // repeat
                                1,          // dstBlockStride
                                1,          // srcBlockStride
                                8,          // dstRepeatStride
                                4           // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** dm_block = expand_to_block(dm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv_ubuf_tensor.ReinterpretCast<uint32_t>()[VECTOR_SIZE],
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                1,                              // dstBlockStride
                                8,                              // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** dm = exp(dm)
                            exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor,
                                tv_ubuf_tensor,
                                sub_m_d64,  // repeat
                                1,          // dstBlockStride
                                1,          // srcBlockStride
                                8,          // dstRepeatStride
                                8           // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** gl = dm * gl
                            mul_v<ArchType::ASCEND_V220, float>(gl_ubuf_tensor,
                                tv_ubuf_tensor,
                                gl_ubuf_tensor,
                                sub_m_d64,  // repeat
                                1,          // dstBlockStride
                                1,          // src0BlockStride
                                1,          // src1BlockStride
                                8,          // dstRepeatStride
                                8,          // src0RepeatStride
                                8           // src1RepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** gl = ll + gl
                            add_v<ArchType::ASCEND_V220, float>(gl_ubuf_tensor,
                                gl_ubuf_tensor,
                                ll_ubuf_tensor[(n_idx + split_idx + KV_INC) % vect_mod * UB_FLOAT_LINE_SIZE],
                                sub_m_d64,  // repeat
                                1,          // dstBlockStride
                                1,          // src0BlockStride
                                1,          // src1BlockStride
                                8,          // dstRepeatStride
                                8,          // src0RepeatStride
                                8           // src1RepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** dm_block = exp(dm_block)
                            exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor[VECTOR_SIZE],
                                tv_ubuf_tensor[VECTOR_SIZE],
                                (sub_m * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                1,                                                                       // dstBlockStride
                                1,                                                                       // srcBlockStride
                                8,                                                                       // dstRepeatStride
                                8                                                                        // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            if (go_flag_scalar == 1) {
                                WAIT_FLAG(MTE3, V, EVENT_ID0);
                                go_flag_scalar = 0;
                            }
                            // *** go = go * dm_block
                            for (uint32_t vmul_idx = 0; vmul_idx < __k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                                mul_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor[VECTOR_SIZE],
                                    sub_m,                       // repeat
                                    1,                           // dstBlockStride
                                    1,                           // src0BlockStride
                                    0,                           // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                    1                            // src1RepeatStride
                                );
                            }
                            if (__k % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(__k % FLOAT_VECTOR_SIZE);
                                mul_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor[VECTOR_SIZE],
                                    sub_m,                       // repeat
                                    1,                           // dstBlockStride
                                    1,                           // src0BlockStride
                                    0,                           // src1BlockStride
                                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                    1                            // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** go = lo + go
                            add_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor,
                                go_ubuf_tensor,
                                lo_ubuf_tensor,
                                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                1,                                                              // dstBlockStride
                                1,                                                              // src0BlockStride
                                1,                                                              // src1BlockStride
                                8,                                                              // dstRepeatStride
                                8,                                                              // src0RepeatStride
                                8                                                               // src1RepeatStride
                            );
                            PIPE_BARRIER(V);
                        } else {
                            // *** gl = ll
                            ub_to_ub<ArchType::ASCEND_V220, float>(
                                gl_ubuf_tensor,
                                ll_ubuf_tensor[(n_idx + split_idx + KV_INC) % vect_mod * UB_FLOAT_LINE_SIZE],
                                0,                               // sid
                                1,                               // nBurst
                                round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                                0,                               // srcGap
                                0                                // dstGap
                            );
                            PIPE_BARRIER(V);
                            if (go_flag_scalar == 1) {
                                WAIT_FLAG(MTE3, V, EVENT_ID0);
                                go_flag_scalar = 0;
                            }
                            // *** go = lo
                            ub_to_ub<ArchType::ASCEND_V220, float>(
                                go_ubuf_tensor,
                                lo_ubuf_tensor,
                                0,                                   // sid
                                1,                                   // nBurst
                                sub_m * round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                                0,                                   // srcGap
                                0                                    // dstGap
                            );
                            PIPE_BARRIER(V);
                        }
                        SET_FLAG(V, MTE2, EVENT_ID1);
                        if (n_idx + split_idx == (n_end + KV_INC - 1)) {
                            // *** gl = castfp32to16(gl)
                            conv_v<ArchType::ASCEND_V220, float, half>(gl_ubuf_tensor.ReinterpretCast<half>(),
                                gl_ubuf_tensor,
                                sub_m_d64,  // repeat
                                1,          // dstBlockStride
                                1,          // srcBlockStride
                                4,          // dstRepeatStride
                                8           // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** go = castfp32to16(go)
                            conv_v<ArchType::ASCEND_V220, float, half>(go_ubuf_tensor.ReinterpretCast<half>(),
                                go_ubuf_tensor,
                                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                1,                                                              // dstBlockStride
                                1,                                                              // srcBlockStride
                                4,                                                              // dstRepeatStride
                                8                                                               // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** gl_block = expand_to_block(gl), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint16_t>(tv_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                gl_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                1,                              // dstBlockStride
                                8,                              // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** go = go / gl_block
                            for (uint32_t vdiv_idx = 0; vdiv_idx < __k / VECTOR_SIZE; ++vdiv_idx) {
                                div_v<ArchType::ASCEND_V220, half>(go_ubuf_tensor.ReinterpretCast<half>()[vdiv_idx * VECTOR_SIZE],
                                    go_ubuf_tensor.ReinterpretCast<half>()[vdiv_idx * VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<half>(),
                                    sub_m,                 // repeat
                                    1,                     // dstBlockStride
                                    1,                     // src0BlockStride
                                    0,                     // src1BlockStride
                                    round_k / BLOCK_SIZE,  // dstRepeatStride
                                    round_k / BLOCK_SIZE,  // src0RepeatStride
                                    1                      // src1RepeatStride
                                );
                            }
                            if (__k % VECTOR_SIZE > 0) {
                                __set_mask(__k % VECTOR_SIZE);
                                div_v<ArchType::ASCEND_V220, half>(go_ubuf_tensor.ReinterpretCast<half>()[__k / VECTOR_SIZE * VECTOR_SIZE],
                                    go_ubuf_tensor.ReinterpretCast<half>()[__k / VECTOR_SIZE * VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<half>(),
                                    sub_m,                 // repeat
                                    1,                     // dstBlockStride
                                    1,                     // src0BlockStride
                                    0,                     // src1BlockStride
                                    round_k / BLOCK_SIZE,  // dstRepeatStride
                                    round_k / BLOCK_SIZE,  // src0RepeatStride
                                    1                      // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            // ********************* move O to GM ************************
                            SET_FLAG(V, MTE3, EVENT_ID0);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm_align<ArchType::ASCEND_V220, half>(
                                o_gm_tensor[o_offset + (uint64_t)sub_block_idx * qk_m / 2 * stride_qo],
                                go_ubuf_tensor.ReinterpretCast<half>(),
                                0,                     // sid
                                sub_m,                 // nBurst
                                __k * 2,               // lenBurst
                                0,                     // leftPaddingNum
                                0,                     // rightPaddingNum
                                0,                     // srcGap
                                (stride_qo - __k) * 2  // dstGap
                            );
                            if (go_flag_scalar == 0) {
                                SET_FLAG(MTE3, V, EVENT_ID0);
                                go_flag_scalar = 1;
                            }
                        }
                    }
                }
            }
        }
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

    __aicore__ inline void RunHighPrec()
    {
        int32_t sub_block_idx = GetSubBlockidx();
        int32_t go_flag_scalar = 1;
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID1);
        SET_FLAG(MTE3, V, EVENT_ID0);
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num * q_heads;
        for (uint32_t process = 0; process < process_num; process++) {
            if (process >= cur_total_q_blk_num * q_heads) {
                while (1) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
                    uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
                    if(q_seqlen != 0) {
                        break;
                    }
                }
            }
            uint32_t cur_core_idx = process % block_num;
            if (is_triu_mask) {
                if ((process / block_num) % 2 == 1) {
                    cur_core_idx = block_num - process % block_num - 1;
                }
            }
            if (block_idx != cur_core_idx) {
                continue;
            }

            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);

            uint32_t mask_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10 + offset_tiling));
            uint32_t mask_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 14 + offset_tiling));
            uint64_t mask_scalar = (uint64_t)(((uint64_t)mask_high32) << 32 | mask_loww32);

            uint32_t process_idx = process - pre_total_q_blk_num * q_heads;
            uint32_t m_idx = process_idx / q_heads;
            uint64_t head_idx = process_idx % q_heads;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qk_m = (m_idx == (m_loop - 1)) ? (q_seqlen - m_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t sub_m = (sub_block_idx == 1) ? (qk_m - qk_m / 2) : qk_m / 2;
            uint32_t sub_m_d128 = (sub_m + VECTOR_SIZE - 1) / VECTOR_SIZE;            // up aligned to 128
            uint32_t sub_m_d64 = (sub_m + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE; // up aligned to 64
            uint32_t round_sub_m = (sub_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint64_t qk_index = 0;
            /******** pre_load *******/
            uint32_t qk_n = (qk_index == (n_loop - 1)) ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * UB_HALF_BUF_SIZE;
            uint64_t mask_offset = (cur_batch * batch_stride + head_idx * head_stride) * max_context_len + m_idx * pp_m_scalar * max_context_len;
            mask_offset += mask_scalar;

            uint32_t s_pingpong_flag = 0;
            uint32_t p_pingpong_flag = 0;
            uint32_t o_pingpong_flag = 0;
            uint64_t o_offset = addr_o_scalar + head_idx * embedding_size + m_idx * pp_m_scalar * stride_qo;
            if (sub_m > 0 && mask_gm != nullptr) {
                WAIT_FLAG(V, MTE2, EVENT_ID1);
                gm_to_ub_align<ArchType::ASCEND_V220, MASK_DATA_TYPE>(
                    mask16_ubuf_tensor,
                    mask_gm_tensor[mask_offset + (uint64_t)sub_block_idx * qk_m / 2 * max_context_len],
                    0,                       // sid
                    sub_m,                   // nBurst
                    qk_n * 2,                // lenBurst
                    0,                       // leftPaddingNum
                    0,                       // rightPaddingNum
                    (max_context_len - qk_n) * 2, // srcGap
                    0                        // dstGap
                );
                mask_offset += pp_n_scalar;
                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);
                conv_v<ArchType::ASCEND_V220, MASK_DATA_TYPE, float>(mask_ubuf_tensor, mask16_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1, // dstBlockStride
                                1, // srcBlockStride
                                8, // dstRepeatStride
                                4  // srcRepeatStride
                );
                SET_FLAG(V, MTE2, EVENT_ID1);
                PIPE_BARRIER(V);
            }
            WaitFlagDev(QK_READY);
            if (sub_m > 0) {
                WAIT_FLAG(MTE3, MTE2, pingpong_flag);
                // input QK
                gm_to_ub<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2],
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER + s_pingpong_flag * TMP_SIZE_DECODER / 2 +
                                    (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                0,                                     // sid
                                1,                                     // nBurst
                                sub_m * qk_round_n / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                                     // srcGap
                                0                                      // dstGap
                );
                s_pingpong_flag = 1 - s_pingpong_flag;
                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);
                // *** ls = tor * ls
                for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
                    muls_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + vadd_idx * FLOAT_VECTOR_SIZE],
                        ls_ubuf_tensor[offset / 2 + vadd_idx * FLOAT_VECTOR_SIZE],
                        tor,
                        sub_m,                          // repeat
                        1,                              // dstBlockStride
                        1,                              // srcBlockStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                        qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                    );
                }
                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                    muls_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        tor,
                        sub_m,                          // repeat
                        1,                              // dstBlockStride
                        1,                              // srcBlockStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                        qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                    );
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                }
                PIPE_BARRIER(V);
                // *** ls = ls + mask
                if (mask_gm != nullptr) {
                    add_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2], ls_ubuf_tensor[offset / 2],
                            mask_ubuf_tensor,
                            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                            1,                                                                // dstBlockStride
                            1,                                                                // src0BlockStride
                            1,                                                                // src1BlockStride
                            8,                                                                // dstRepeatStride
                            8,                                                                // src0RepeatStride
                            8                                                                 // src1RepeatStride
                    );
                    PIPE_BARRIER(V);
		        }
                // *** lm = rowmax(ls)
                if (qk_n <= FLOAT_VECTOR_SIZE) {
                    __set_mask(qk_n);
                    cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_tensor, ls_ubuf_tensor[offset / 2],
                          sub_m,                         // repeat
                          1,                             // dstRepeatStride
                          1,                             // srcBlockStride
                          qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                    );
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                } else {
                    ub_to_ub<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls_ubuf_tensor[offset / 2],
                                      0,                                                   // sid
                                      sub_m,                                               // nBurst
                                      8,                                                   // lenBurst
                                      (qk_round_n - FLOAT_VECTOR_SIZE) / FLOAT_BLOCK_SIZE, // srcGap
                                      0                                                    // dstGap
                    );
                    PIPE_BARRIER(V);
                    for (uint64_t rowmax_idx = 1; rowmax_idx < (uint64_t)qk_n / FLOAT_VECTOR_SIZE; ++rowmax_idx) {
                        max_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                             ls_ubuf_tensor[offset / 2 + rowmax_idx * FLOAT_VECTOR_SIZE],
                             sub_m,                        // repeat
                             1,                            // dstBlockStride
                             1,                            // src0BlockStride
                             1,                            // src1BlockStride
                             8,                            // dstRepeatStride
                             8,                            // src0RepeatStride
                             qk_round_n / FLOAT_BLOCK_SIZE // src1RepeatStride
                        );
                    }

                    if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                        __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                        max_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                             ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                             sub_m,                        // repeat
                             1,                            // dstBlockStride
                             1,                            // src0BlockStride
                             1,                            // src1BlockStride
                             8,                            // dstRepeatStride
                             8,                            // src0RepeatStride
                             qk_round_n / FLOAT_BLOCK_SIZE // src1RepeatStride
                        );
                    }
                    PIPE_BARRIER(V);
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                    cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_tensor, ls32_ubuf_tensor,
                          sub_m,     // repeat
                          1,         // dstRepeatStride
                          1,         // srcBlockStride
                          8          // srcRepeatStride
                    );
                }
                PIPE_BARRIER(V);
                // *** hm = lm
                ub_to_ub<ArchType::ASCEND_V220, float>(hm_ubuf_tensor, lm_ubuf_tensor,
                                  0,                              // sid
                                  1,                              // nBurst
                                  round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                  0,                              // srcGap
                                  0                               // dstGap
                );

                PIPE_BARRIER(V);

                // *** gm = hm
                ub_to_ub<ArchType::ASCEND_V220, float>(gm_ubuf_tensor, hm_ubuf_tensor,
                                  0,                              // sid
                                  1,                              // nBurst
                                  round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                  0,                              // srcGap
                                  0                               // dstGap
                );
                PIPE_BARRIER(V);
                // *** hm_block = expand_to_block(hm), 存放于 tv
                brcb_v<ArchType::ASCEND_V220, uint32_t>(tv_ubuf_tensor.ReinterpretCast<uint32_t>(), hm_ubuf_tensor.template ReinterpretCast<uint32_t>(),
                      1,                             // dstBlockStride
                      8,                             // dstRepeatStride
                      round_sub_m / FLOAT_BLOCK_SIZE // repeat
                );

                PIPE_BARRIER(V);
                // *** ls = ls - hm_block
                for (uint32_t vsub_idx = 0; vsub_idx < qk_n / FLOAT_VECTOR_SIZE; ++vsub_idx) {
                    sub_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + vsub_idx * FLOAT_VECTOR_SIZE],
                         ls_ubuf_tensor[offset / 2 + vsub_idx * FLOAT_VECTOR_SIZE],
                         tv_ubuf_tensor,
                         sub_m,                         // repeat
                         1,                             // dstBlockStride
                         1,                             // src0BlockStride
                         0,                             // src1BlockStride
                         qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                         qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                         1                              // src1RepeatStride
                    );
                }
                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                    sub_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                         ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                         tv_ubuf_tensor,
                         sub_m,                         // repeat
                         1,                             // dstBlockStride
                         1,                             // src0BlockStride
                         0,                             // src1BlockStride
                         qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                         qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                         1                              // src1RepeatStride
                    );
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                }
                PIPE_BARRIER(V);
                // *** ls = exp(ls)
                exp_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls_ubuf_tensor[offset / 2],
                     (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                     1,                                                                // dstBlockStride
                     1,                                                                // srcBlockStride
                     8,                                                                // dstRepeatStride
                     8                                                                 // srcRepeatStride
                );
                PIPE_BARRIER(V);
                // *** lp = castfp32to16(ls)
                convr_v<ArchType::ASCEND_V220, float, P_DATA_TYPE>(lp_ubuf_tensor[offset], ls32_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                4,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                );
                PIPE_BARRIER(V);
                SET_FLAG(V, MTE3, EVENT_ID0);
                // *** ll = rowsum(ls32)
                if (qk_n <= FLOAT_VECTOR_SIZE) {
                    __set_mask(qk_n);
                    cadd_v<ArchType::ASCEND_V220, float>(ll_ubuf_tensor, ls32_ubuf_tensor,
                          sub_m,                         // repeat
                          1,                             // dstRepeatStride
                          1,                             // srcBlockStride
                          qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                    );
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                } else {
                    for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                        add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                             ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                             sub_m,                         // repeat
                             1,                             // dstBlockStride
                             1,                             // src0BlockStride
                             1,                             // src1BlockStride
                             qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                             qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                             qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                    }
                    if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                        __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                        add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                             ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                             sub_m,                         // repeat
                             1,                             // dstBlockStride
                             1,                             // src0BlockStride
                             1,                             // src1BlockStride
                             qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                             qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                             qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                        );
                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                    }
                    PIPE_BARRIER(V);
                    cadd_v<ArchType::ASCEND_V220, float>(ll_ubuf_tensor, ls32_ubuf_tensor,
                          sub_m,                         // repeatoffset
                          1,                             // dstRepeatStride
                          1,                             // srcBlockStride
                          qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                    );
                }
                PIPE_BARRIER(V);
                WAIT_FLAG(V, MTE3, EVENT_ID0);
                ub_to_gm<ArchType::ASCEND_V220, P_DATA_TYPE>(p_gm_tensor[(uint64_t)block_idx * TMP_SIZE +
                                    p_pingpong_flag * TMP_SIZE / 2 + (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                lp_ubuf_tensor[offset],
                                0,                               // sid
                                1,                               // nBurst
                                sub_m * qk_round_n / BLOCK_SIZE, // lenBurst
                                0,                               // srcGap
                                0                                // dstGap
                );
                SET_FLAG(MTE3, MTE2, pingpong_flag);
                offset = pingpong_flag * UB_HALF_BUF_SIZE;
                p_pingpong_flag = 1 - p_pingpong_flag;
            }
            qk_index++;
            FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);

            uint32_t n_end = n_loop;
            if (is_triu_mask) {
                uint32_t triu_seq = mask_type == 3 ? max_kv - q_seqlen : kv_seqlen - q_seqlen;
                uint32_t n_offset = ((m_idx + 1) * pp_m_scalar + triu_seq + pp_n_scalar - 1) / pp_n_scalar;
                n_end = n_offset > n_loop ? n_loop : n_offset;
            }
            for (uint32_t n_idx = 0; n_idx < n_end; n_idx++) {
                if (qk_index < n_end) {
                    if (qk_index == (n_loop - 1)) {
                        qk_n = (kv_seqlen - qk_index * pp_n_scalar);
                        qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    }
                    if (sub_m > 0 && mask_gm != nullptr) {
                        WAIT_FLAG(V, MTE2, EVENT_ID1);
                        gm_to_ub_align<ArchType::ASCEND_V220, MASK_DATA_TYPE>(mask16_ubuf_tensor,
                                                    mask_gm_tensor[mask_offset +
                                                        (uint64_t)sub_block_idx * qk_m / 2 * max_context_len],
                                                    0,                       // sid
                                                    sub_m,                   // nBurst
                                                    qk_n * 2,                // lenBurst
                                                    0,                       // leftPaddingNum
                                                    0,                       // rightPaddingNum
                                                    (max_context_len - qk_n) * 2, // srcGap
                                                    0                        // dstGap
                        );
                        mask_offset += pp_n_scalar;
                        SET_FLAG(MTE2, V, EVENT_ID0);
                        WAIT_FLAG(MTE2, V, EVENT_ID0);
                        conv_v<ArchType::ASCEND_V220, MASK_DATA_TYPE, float>(
                            mask_ubuf_tensor, mask16_ubuf_tensor,
                            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                            1,                                                                // dstBlockStride
                            1,                                                                // srcBlockStride
                            8,                                                                // dstRepeatStride
                            4                                                                 // srcRepeatStride
                        );
                        SET_FLAG(V, MTE2, EVENT_ID1);
                        PIPE_BARRIER(V);
                    }
                    WaitFlagDev(QK_READY);
                    if (sub_m > 0) {
                        WAIT_FLAG(MTE3, MTE2, pingpong_flag);
                        // input QK
                        gm_to_ub<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2],
                                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                                            s_pingpong_flag * TMP_SIZE_DECODER / 2 +
                                            (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                        0,                                     // sid
                                        1,                                     // nBurst
                                        sub_m * qk_round_n / FLOAT_BLOCK_SIZE, // lenBurst
                                        0,                                     // srcGap
                                        0                                      // dstGap
                        );
                        s_pingpong_flag = 1 - s_pingpong_flag;
                        SET_FLAG(MTE2, V, EVENT_ID0);
                        WAIT_FLAG(MTE2, V, EVENT_ID0);
                        // *** ls = tor * ls
                        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
                            muls_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + vadd_idx * FLOAT_VECTOR_SIZE],
                                ls_ubuf_tensor[offset / 2 + vadd_idx * FLOAT_VECTOR_SIZE],
                                tor,
                                sub_m,                          // repeat
                                1,                              // dstBlockStride
                                1,                              // srcBlockStride
                                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                            );
                        }
                        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                            muls_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                tor,
                                sub_m,                          // repeat
                                1,                              // dstBlockStride
                                1,                              // srcBlockStride
                                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                            );
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                        }
                        PIPE_BARRIER(V);

                        // *** ls = ls + mask
                        if (mask_gm != nullptr) {
                            add_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2], ls_ubuf_tensor[offset / 2],
                                    mask_ubuf_tensor,
                                    (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1,                                                                // dstBlockStride
                                    1,                                                                // src0BlockStride
                                    1,                                                                // src1BlockStride
                                    8,                                                                // dstRepeatStride
                                    8,                                                                // src0RepeatStride
                                    8                                                                 // src1RepeatStride
                            );
                            PIPE_BARRIER(V);
			            }
                        // *** lm = rowmax(ls)
                        if (qk_n <= FLOAT_VECTOR_SIZE) {
                            __set_mask(qk_n);
                            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_tensor, ls_ubuf_tensor[offset / 2],
                                  sub_m,                         // repeat
                                  1,                             // dstRepeatStride
                                  1,                             // srcBlockStride
                                  qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                            );
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                        } else {
                            ub_to_ub<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls_ubuf_tensor[offset / 2],
                                              0,                                                   // sid
                                              sub_m,                                               // nBurst
                                              8,                                                   // lenBurst
                                              (qk_round_n - FLOAT_VECTOR_SIZE) / FLOAT_BLOCK_SIZE, // srcGap
                                              0                                                    // dstGap
                            );
                            PIPE_BARRIER(V);
                            for (uint64_t rowmax_idx = 1; rowmax_idx < (uint64_t)qk_n / FLOAT_VECTOR_SIZE;
                                 ++rowmax_idx) {
                                max_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                                     ls_ubuf_tensor[offset / 2 + rowmax_idx * FLOAT_VECTOR_SIZE],
                                     sub_m,                        // repeat
                                     1,                            // dstBlockStride
                                     1,                            // src0BlockStride
                                     1,                            // src1BlockStride
                                     8,                            // dstRepeatStride
                                     8,                            // src0RepeatStride
                                     qk_round_n / FLOAT_BLOCK_SIZE // src1RepeatStride
                                );
                            }

                            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                max_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                                     ls_ubuf_tensor[offset / 2 +
                                         qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                     sub_m,                        // repeat
                                     1,                            // dstBlockStride
                                     1,                            // src0BlockStride
                                     1,                            // src1BlockStride
                                     8,                            // dstRepeatStride
                                     8,                            // src0RepeatStride
                                     qk_round_n / FLOAT_BLOCK_SIZE // src1RepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_tensor, ls32_ubuf_tensor,
                                  sub_m,     // repeat
                                  1,         // dstRepeatStride
                                  1,         // srcBlockStride
                                  8          // srcRepeatStride
                            );
                        }
                        PIPE_BARRIER(V);
                        // *** hm = vmax(lm, gm)
                        max_v<ArchType::ASCEND_V220, float>(hm_ubuf_tensor, lm_ubuf_tensor, gm_ubuf_tensor,
                             sub_m_d64, // repeat
                             1,         // dstBlockStride
                             1,         // src0BlockStride
                             1,         // src1BlockStride
                             8,         // dstRepeatStride
                             8,         // src0RepeatStride
                             8          // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                        // *** dm = gm - hm
                        sub_v<ArchType::ASCEND_V220, float>(dm_ubuf_tensor[qk_index % 2 * UB_FLOAT_LINE_SIZE], gm_ubuf_tensor,
                             hm_ubuf_tensor,
                             sub_m_d64, // repeat
                             1,         // dstBlockStride
                             1,         // src0BlockStride
                             1,         // src1BlockStride
                             8,         // dstRepeatStride
                             8,         // src0RepeatStride
                             8          // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                        // *** gm = hm
                        ub_to_ub<ArchType::ASCEND_V220, float>(gm_ubuf_tensor, hm_ubuf_tensor,
                                          0,                              // sid
                                          1,                              // nBurst
                                          round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                          0,                              // srcGap
                                          0                               // dstGap
                        );
                        PIPE_BARRIER(V);
                        // *** hm_block = expand_to_block(hm), 存放于 tv
                        brcb_v<ArchType::ASCEND_V220, uint32_t>(tv_ubuf_tensor.ReinterpretCast<uint32_t>(), hm_ubuf_tensor.template ReinterpretCast<uint32_t>(),
                              1,                             // dstBlockStride
                              8,                             // dstRepeatStride
                              round_sub_m / FLOAT_BLOCK_SIZE // repeat
                        );
                        PIPE_BARRIER(V);
                        // *** ls = ls - hm_block
                        for (uint32_t vsub_idx = 0; vsub_idx < qk_n / FLOAT_VECTOR_SIZE; ++vsub_idx) {
                            sub_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + vsub_idx * FLOAT_VECTOR_SIZE],
                                 ls_ubuf_tensor[offset / 2 + vsub_idx * FLOAT_VECTOR_SIZE],
                                 tv_ubuf_tensor,
                                 sub_m,                         // repeat
                                 1,                             // dstBlockStride
                                 1,                             // src0BlockStride
                                 0,                             // src1BlockStride
                                 qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                 qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                 1                              // src1RepeatStride
                            );
                        }
                        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                            sub_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                 ls_ubuf_tensor[offset / 2 + qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                 tv_ubuf_tensor,
                                 sub_m,                         // repeat
                                 1,                             // dstBlockStride
                                 1,                             // src0BlockStride
                                 0,                             // src1BlockStride
                                 qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                 qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                 1                              // src1RepeatStride
                            );
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                        }
                        PIPE_BARRIER(V);
                        // *** ls = castfp16to32(ls)
                        // *** ls = exp(ls)
                        exp_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls_ubuf_tensor[offset / 2],
                             (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                             1,                                                                // dstBlockStride
                             1,                                                                // srcBlockStride
                             8,                                                                // dstRepeatStride
                             8                                                                 // srcRepeatStride
                        );
                        PIPE_BARRIER(V);
                        // *** lp = castfp32to16(ls)
                        convr_v<ArchType::ASCEND_V220, float, P_DATA_TYPE>(lp_ubuf_tensor[offset], ls32_ubuf_tensor,
                                        (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1, // dstBlockStride
                                        1, // srcBlockStride
                                        4, // dstRepeatStride
                                        8  // srcRepeatStride
                        );
                        PIPE_BARRIER(V);
                        SET_FLAG(V, MTE3, EVENT_ID0);
                        // *** ll = rowsum(ls32)
                        if (qk_n <= FLOAT_VECTOR_SIZE) {
                            __set_mask(qk_n);
                            cadd_v<ArchType::ASCEND_V220, float>(ll_ubuf_tensor[qk_index % 2 * UB_FLOAT_LINE_SIZE],
                                  ls32_ubuf_tensor,
                                  sub_m,                         // repeat
                                  1,                             // dstRepeatStride
                                  1,                             // srcBlockStride
                                  qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                            );
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                        } else {
                            for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                                     ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                                     sub_m,                         // repeat
                                     1,                             // dstBlockStride
                                     1,                             // src0BlockStride
                                     1,                             // src1BlockStride
                                     qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                     qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                     qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor, ls32_ubuf_tensor,
                                     ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                     sub_m,                         // repeat
                                     1,                             // dstBlockStride
                                     1,                             // src0BlockStride
                                     1,                             // src1BlockStride
                                     qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                     qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                     qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            cadd_v<ArchType::ASCEND_V220, float>(ll_ubuf_tensor[qk_index % 2 * UB_FLOAT_LINE_SIZE],
                                  ls32_ubuf_tensor,
                                  sub_m,                         // repeat
                                  1,                             // dstRepeatStride
                                  1,                             // srcBlockStride
                                  qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                            );
                        }
                        PIPE_BARRIER(V);
                        WAIT_FLAG(V, MTE3, EVENT_ID0);
                        ub_to_gm<ArchType::ASCEND_V220, P_DATA_TYPE>(p_gm_tensor[(uint64_t)block_idx * TMP_SIZE +
                                            p_pingpong_flag * TMP_SIZE / 2 +
                                            (uint64_t)sub_block_idx * qk_m / 2 * qk_round_n],
                                        lp_ubuf_tensor[offset],
                                        0,                               // sid
                                        1,                               // nBurst
                                        sub_m * qk_round_n / BLOCK_SIZE, // lenBurst
                                        0,                               // srcGap
                                        0                                // dstGap
                        );
                        SET_FLAG(MTE3, MTE2, pingpong_flag);
                        offset = pingpong_flag * UB_HALF_BUF_SIZE;
                        p_pingpong_flag = 1 - p_pingpong_flag;
                    }
                    qk_index++;
                    FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);
                }

                WaitFlagDev(UPDATE_READY); // 4
                if (sub_m > 0) {
                    WAIT_FLAG(V, MTE2, EVENT_ID0);
                    gm_to_ub<ArchType::ASCEND_V220, float>(lo_ubuf_tensor,
                                    o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE +
                                        o_pingpong_flag * TMP_SIZE / 2 + (uint64_t)sub_block_idx * qk_m / 2 * round_k],
                                    0,                                  // sid
                                    1,                                  // nBurst
                                    sub_m * round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                                  // srcGap
                                    0                                   // dstGap
                    );
                    o_pingpong_flag = 1 - o_pingpong_flag;
                    SET_FLAG(MTE2, V, EVENT_ID0);
                    WAIT_FLAG(MTE2, V, EVENT_ID0);
                    // *** 更新 L 和 O
                    if (n_idx != 0) {
                        // *** dm32 = castfp16to32(dm), 存放于 tv
                        ub_to_ub<ArchType::ASCEND_V220, float>(tv_ubuf_tensor,
                                          dm_ubuf_tensor[n_idx % 2 * UB_FLOAT_LINE_SIZE],
                                          0,                              // sid
                                          1,                              // nBurst
                                          round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                          0,                              // srcGap
                                          0                               // dstGap
                        );

                        PIPE_BARRIER(V);
                        // *** dm_block = expand_to_block(dm), 存放于 tv
                        brcb_v<ArchType::ASCEND_V220, uint32_t>(tv_ubuf_tensor.ReinterpretCast<uint32_t>()[VECTOR_SIZE], tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                              1,                             // dstBlockStride
                              8,                             // dstRepeatStride
                              round_sub_m / FLOAT_BLOCK_SIZE // repeat
                        );
                        PIPE_BARRIER(V);
                        // *** dm = exp(dm)
                        exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                             sub_m_d64, // repeat
                             1,         // dstBlockStride
                             1,         // srcBlockStride
                             8,         // dstRepeatStride
                             8          // srcRepeatStride
                        );
                        PIPE_BARRIER(V);
                        // *** gl = dm * gl
                        mul_v<ArchType::ASCEND_V220, float>(gl_ubuf_tensor, tv_ubuf_tensor, gl_ubuf_tensor,
                             sub_m_d64, // repeat
                             1,         // dstBlockStride
                             1,         // src0BlockStride
                             1,         // src1BlockStride
                             8,         // dstRepeatStride
                             8,         // src0RepeatStride
                             8          // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                        // *** gl = ll + gl
                        add_v<ArchType::ASCEND_V220, float>(gl_ubuf_tensor, gl_ubuf_tensor,
                             ll_ubuf_tensor[n_idx % 2 * UB_FLOAT_LINE_SIZE],
                             sub_m_d64, // repeat
                             1,         // dstBlockStride
                             1,         // src0BlockStride
                             1,         // src1BlockStride
                             8,         // dstRepeatStride
                             8,         // src0RepeatStride
                             8          // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                        // *** dm_block = exp(dm_block)
                        exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor[VECTOR_SIZE], tv_ubuf_tensor[VECTOR_SIZE],
                             (sub_m * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                             1,                                                                      // dstBlockStride
                             1,                                                                      // srcBlockStride
                             8,                                                                      // dstRepeatStride
                             8                                                                       // srcRepeatStride
                        );
                        PIPE_BARRIER(V);
                        if (go_flag_scalar == 1) {
                            WAIT_FLAG(MTE3, V, EVENT_ID0);
                            go_flag_scalar = 0;
                        }
                        // *** go = go * dm_block
                        for (uint32_t vmul_idx = 0; vmul_idx < __k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                            mul_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                 go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                 tv_ubuf_tensor[VECTOR_SIZE],
                                 sub_m,                      // repeat
                                 1,                          // dstBlockStride
                                 1,                          // src0BlockStride
                                 0,                          // src1BlockStride
                                 round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                 round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                 1                           // src1RepeatStride
                            );
                        }
                        if (__k % FLOAT_VECTOR_SIZE > 0) {
                            __set_mask(__k % FLOAT_VECTOR_SIZE);
                            mul_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                 go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                 tv_ubuf_tensor[VECTOR_SIZE],
                                 sub_m,                      // repeat
                                 1,                          // dstBlockStride
                                 1,                          // src0BlockStride
                                 0,                          // src1BlockStride
                                 round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                 round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                 1                           // src1RepeatStride
                            );
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                        }
                        PIPE_BARRIER(V);
                        // *** go = lo + go
                        add_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor, go_ubuf_tensor, lo_ubuf_tensor,
                             (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                             1,                                                             // dstBlockStride
                             1,                                                             // src0BlockStride
                             1,                                                             // src1BlockStride
                             8,                                                             // dstRepeatStride
                             8,                                                             // src0RepeatStride
                             8                                                              // src1RepeatStride
                        );
                        PIPE_BARRIER(V);
                    } else {
                        // *** gl = ll
                        ub_to_ub<ArchType::ASCEND_V220, float>(gl_ubuf_tensor,
                                          ll_ubuf_tensor[n_idx % 2 * UB_FLOAT_LINE_SIZE],
                                          0,                              // sid
                                          1,                              // nBurst
                                          round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                          0,                              // srcGap
                                          0                               // dstGap
                        );
                        PIPE_BARRIER(V);
                        if (go_flag_scalar == 1) {
                            WAIT_FLAG(MTE3, V, EVENT_ID0);
                            go_flag_scalar = 0;
                        }
                        // *** go = lo
                        ub_to_ub<ArchType::ASCEND_V220, float>(go_ubuf_tensor, lo_ubuf_tensor,
                                          0,                                  // sid
                                          1,                                  // nBurst
                                          sub_m * round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                          0,                                  // srcGap
                                          0                                   // dstGap
                        );
                        PIPE_BARRIER(V);
                    }
                    SET_FLAG(V, MTE2, EVENT_ID0);
                    if (n_idx == n_end - 1) {
                        // *** gl_block = expand_to_block(gl), 存放于 tv
                        brcb_v<ArchType::ASCEND_V220, uint32_t>(tv_ubuf_tensor.ReinterpretCast<uint32_t>(), gl_ubuf_tensor.ReinterpretCast<uint32_t>(),
                              1,                             // dstBlockStride
                              8,                             // dstRepeatStride
                              round_sub_m / FLOAT_BLOCK_SIZE // repeat
                        );
                        PIPE_BARRIER(V);
                        // *** go = go / gl_block
                        for (uint32_t vdiv_idx = 0; vdiv_idx < __k / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                            div_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                 go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE], tv_ubuf_tensor,
                                 sub_m,                      // repeat
                                 1,                          // dstBlockStride
                                 1,                          // src0BlockStride
                                 0,                          // src1BlockStride
                                 round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                 round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                 1                           // src1RepeatStride
                            );
                        }
                        if (__k % FLOAT_VECTOR_SIZE > 0) {
                            __set_mask(__k % FLOAT_VECTOR_SIZE);
                            div_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                 go_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                 tv_ubuf_tensor,
                                 sub_m,                      // repeat
                                 1,                          // dstBlockStride
                                 1,                          // src0BlockStride
                                 0,                          // src1BlockStride
                                 round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                 round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                 1                           // src1RepeatStride
                            );
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                        }
                        PIPE_BARRIER(V);
                        convr_v<ArchType::ASCEND_V220, float, P_DATA_TYPE>(go_ubuf_tensor.ReinterpretCast<P_DATA_TYPE>(), go_ubuf_tensor,
                                        (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1, // dstBlockStride
                                        1, // srcBlockStride
                                        4, // dstRepeatStride
                                        8  // srcRepeatStride
                        );
                        // ********************* move O to GM ************************
                        SET_FLAG(V, MTE3, EVENT_ID0);
                        WAIT_FLAG(V, MTE3, EVENT_ID0);
                        ub_to_gm_align<ArchType::ASCEND_V220, P_DATA_TYPE>(
                            o_gm_tensor[o_offset + (uint64_t)sub_block_idx * qk_m / 2 * stride_qo],
                            go_ubuf_tensor.ReinterpretCast<P_DATA_TYPE>(),
                            0,                    // sid
                            sub_m,                // nBurst
                            __k * 2,              // lenBurst
                            0,                    // leftPaddingNum
                            0,                    // rightPaddingNum
                            0,                    // srcGap
                            (stride_qo - __k) * 2 // dstGap
                        );
                        if (go_flag_scalar == 0) {
                            SET_FLAG(MTE3, V, EVENT_ID0);
                            go_flag_scalar = 1;
                        }
                    }
                }
            }
        }
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

private:

    __gm__ uint8_t *__restrict__ block_tables_gm;
    __gm__ uint8_t *__restrict__ tiling_para_gm;
    __gm__ uint8_t *__restrict__ mask_gm;

    AscendC::GlobalTensor<MASK_DATA_TYPE> mask_gm_tensor;
    AscendC::GlobalTensor<P_DATA_TYPE> o_gm_tensor;
    AscendC::GlobalTensor<S_DATA_TYPE> s_gm_tensor;
    AscendC::GlobalTensor<P_DATA_TYPE> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<S_DATA_TYPE> ls_ubuf_tensor;
    AscendC::LocalTensor<P_DATA_TYPE> lp_ubuf_tensor;
    AscendC::LocalTensor<float> ls32_ubuf_tensor;
    AscendC::LocalTensor<S_DATA_TYPE> mask_ubuf_tensor;
    AscendC::LocalTensor<MASK_DATA_TYPE> mask16_ubuf_tensor;
    AscendC::LocalTensor<float> lo_ubuf_tensor;
    AscendC::LocalTensor<S_DATA_TYPE> lm_ubuf_tensor;
    AscendC::LocalTensor<S_DATA_TYPE> hm_ubuf_tensor;
    AscendC::LocalTensor<S_DATA_TYPE> gm_ubuf_tensor;
    AscendC::LocalTensor<S_DATA_TYPE> dm_ubuf_tensor;
    AscendC::LocalTensor<float> ll_ubuf_tensor;
    AscendC::LocalTensor<float> gl_ubuf_tensor;
    AscendC::LocalTensor<float> tv_ubuf_tensor;
    AscendC::LocalTensor<float> go_ubuf_tensor;

    uint32_t batch_size;
    uint32_t q_heads;
    uint32_t embedding_size;
    uint32_t kv_heads;
    uint32_t max_context_len;
    S_DATA_TYPE tor;
    uint32_t block_size;
    uint32_t total_q_blk_num;
    uint32_t is_triu_mask = 1;
    uint32_t tiling_head_size;
    uint32_t tiling_para_size;
    uint32_t batch_stride;
    uint32_t head_stride;
    uint32_t stride_qo;
    uint32_t __k;
    uint32_t round_k;
    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;
    uint32_t mask_type;
    uint32_t max_kv;
};
#endif

#ifdef __DAV_C220_CUBE__
template <bool SplitKV = false, TilingKeyType tilingKeyType = TilingKeyType::TILING_HALF_DATA, typename IN_DTYPE = half,  typename OUT_DTYPE = half, typename IN_KVDTYPE = half, PagedAttnVariant pagedAttnVariant = PagedAttnVariant::DEFAULT, DataShapeType dataShapeType = DataShapeType::BSND, CompressType compressType = CompressType::COMPRESS_TYPE_UNDEFINED>
class MLAttentionDecoderAic {
    // define dtype
    using mm1OutputType = typename AttentionType<tilingKeyType>::mm1OutputType;
    using mm1CopyType = typename AttentionType<tilingKeyType>::mm1CopyType;
    using mmBiasType = typename AttentionType<tilingKeyType>::mmBiasType;
    using mmScaleType = typename AttentionType<tilingKeyType>::mmScaleType;
    using mm2OutputType = typename AttentionType<tilingKeyType>::mm2OutputType;
    using mm2CopyType = typename AttentionType<tilingKeyType>::mm2CopyType;
    static constexpr uint32_t T_CUBE_MATRIX_SIZE = CUBE_MATRIX_SIZE_512 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_SIZE =  BLOCK_SIZE_32 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_OFFSET = 2 / sizeof(IN_DTYPE);
    static constexpr int32_t L1_KV_HALF_SIZE = 73728;// 2* 128 * 256
    static constexpr int32_t L1_KV_UINT8_SIZE = 73728 * 2;

public:
    __aicore__ __attribute__((always_inline)) inline MLAttentionDecoderAic(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    __aicore__ __attribute__((always_inline)) inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t *__restrict__ q_in_gm,
        __gm__ uint8_t *__restrict__ k_in_gm,
        __gm__ uint8_t *__restrict__ v_in_gm,
        __gm__ uint8_t *__restrict__ block_tables_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t* __restrict__ gm_k16,
        __gm__ uint8_t* __restrict__ gm_v16, 
        __gm__ uint8_t *__restrict__ tiling_para_gm,
        __gm__ uint8_t *__restrict__ razorOffset)
    {
        SetFftsBaseAddr((uint64_t)sync);
        SetPadding<uint64_t>(0);
        SetAtomicnone();
        SetNdpara(1, 0, 0);
        SetMasknorm();

        q_gm = reinterpret_cast<__gm__ IN_DTYPE *>(q_in_gm);
        k_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm);
        v_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(v_in_gm);
        block_tables_gm = reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm);
        s_gm = reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm);

        p_gm = reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm);
        o_tmp_gm = reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm);
        tiling_gm = reinterpret_cast<__gm__ uint8_t *>(tiling_para_gm);
        razor_offset_gm = reinterpret_cast<__gm__ float *>(razorOffset);

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(q_in_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm));
        v_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(v_in_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm));
        block_tables_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm));

        num_tokens = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM));
        embedding_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V));
        block_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MAXBLOCKS));
        kv_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVHEADS));
        former_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_BATCH));
        former_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_HEAD));
        tail_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_BATCH));
        tail_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_HEAD));
        head_split_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADNUM_MOVE));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        group_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_GROUPNUM));
        block_size_calc = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE_CALC));
        q_head_original = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QHEADORIGINAL));
        compressHead = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_COMPRESSHEAD));
        former_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_GROUP_MOVE));
        tail_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_GROUP_MOVE));

        kv_split_per_core = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVSPLIT));
        kv_split_core_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVCORENUM));

        former_head_split_num = (former_head_split > group_num) && (former_group_num_move == group_num) ? head_split_num : 1;
        tail_head_split_num = (tail_head_split > group_num) && (tail_group_num_move == group_num) ? head_split_num : 1;

        stride_kv = static_cast<uint64_t>(kv_heads) * embedding_size;

        if constexpr (dataShapeType == DataShapeType::BNSD) {
            stride_kv = embedding_size;
        }

        __k = embedding_size;
        round_k = RoundUp<T_BLOCK_SIZE>(__k);
        if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
            __v = embedding_size_v;
            stride_vo = static_cast<uint64_t>(kv_heads) * embedding_size_v;
            round_v = RoundUp<BLOCK_SIZE>(__v);
            embed_split_size_qk = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_K_SPLIT));
            embed_split_loop_qk = (embedding_size + embed_split_size_qk - 1) / embed_split_size_qk;
            embed_split_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT));
            embed_split_loop_v = (embedding_size_v + embed_split_size_v - 1) / embed_split_size_v;
        }
    }


    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID4);
        SET_FLAG(M, MTE1, EVENT_ID5);
	    SET_FLAG(M, MTE1, EVENT_ID7);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(MTE1, MTE2, EVENT_ID6);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        SET_FLAG(FIX, MTE1, EVENT_ID0);
        SET_FLAG(FIX, MTE1, EVENT_ID1);
        SET_FLAG(FIX, MTE1, EVENT_ID2);
        SET_FLAG(FIX, MTE1, EVENT_ID3);
        SET_FLAG(FIX, MTE1, EVENT_ID4);
        SET_FLAG(FIX, MTE1, EVENT_ID5);
        SET_FLAG(MTE2, FIX, EVENT_ID0);        
        core_per_batch = (q_heads + former_head_split - 1) / former_head_split;
        process_num = static_cast<uint64_t>(former_batch) * core_per_batch * kv_split_core_num;
        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {  // for task
            uint32_t cur_batch = process / (core_per_batch * kv_split_core_num) + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm  + 1 + offset_tiling));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            uint32_t cur_head = (process / kv_split_core_num) % core_per_batch;
            uint32_t cur_nIndx = process % kv_split_core_num;
            uint32_t start_head = cur_head * former_head_split;
            uint32_t start_kv = cur_nIndx * kv_split_per_core;
            uint32_t cur_kv_seqlen = kv_split_per_core;
            uint32_t kv_loop = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            if (cur_nIndx >= kv_loop) {
                continue;
            }
            if (cur_nIndx == (kv_loop - 1)) {
                cur_kv_seqlen = kv_seqlen - cur_nIndx * kv_split_per_core;
            }
            uint32_t cur_head_num = former_head_split;
            uint32_t former_group_num_move_real = former_group_num_move;
            if (cur_head == (core_per_batch - 1)) {
                cur_head_num = q_heads - cur_head * former_head_split;
                former_group_num_move_real = former_group_num_move <= cur_head_num ? former_group_num_move : cur_head_num;
            }
            uint32_t head_split_loop = (cur_head_num + (former_head_split_num * former_group_num_move_real) - 1) /
                                       (former_head_split_num * former_group_num_move_real);
            if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                InnerRunCubeMLA(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, former_group_num_move_real, former_head_split_num);
            }
        }
        if (tail_batch > 0) {
            core_per_batch = (q_heads + tail_head_split - 1) / tail_head_split;
            process_num = static_cast<uint64_t>(tail_batch) * core_per_batch;
            for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {  // for task
                uint32_t cur_batch = process / core_per_batch + former_batch + prefill_batch_size_;
                uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
                uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
                if (kv_seqlen == 0) {
                    continue;
                }
                uint32_t cur_kv_seqlen = kv_seqlen;
                uint32_t start_kv = 0;
                uint32_t cur_head = process % core_per_batch;
                uint32_t cur_head_num = tail_head_split;
                uint32_t tail_group_num_move_real = tail_group_num_move;
                if (cur_head == (core_per_batch - 1)) {
                    cur_head_num = q_heads - cur_head * tail_head_split;
                    tail_group_num_move_real = tail_group_num_move <= cur_head_num ? tail_group_num_move : cur_head_num;
                }
                uint32_t head_split_loop = (cur_head_num + (tail_head_split_num * tail_group_num_move_real) - 1) /
                                           (tail_head_split_num * tail_group_num_move_real);
                uint32_t start_head = (process % core_per_batch) * tail_head_split;
                if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                    InnerRunCubeMLA(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, tail_group_num_move_real, tail_head_split_num);
                }
            }
        }
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID4);
        WAIT_FLAG(M, MTE1, EVENT_ID5);
        WAIT_FLAG(M, MTE1, EVENT_ID7);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        WAIT_FLAG(FIX, MTE1, EVENT_ID0);
        WAIT_FLAG(FIX, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, MTE1, EVENT_ID2);
        WAIT_FLAG(FIX, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, MTE1, EVENT_ID4);
        WAIT_FLAG(FIX, MTE1, EVENT_ID5);
        WAIT_FLAG(MTE2, FIX, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

private:

    __attribute__((always_inline)) inline __aicore__ void LoadQToL1MLA(
        uint32_t q_offset,
        uint32_t cur_head_num)
    {
        if (is_multi_head_mmad) {
            gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                l1q_buf_addr_tensor,
                q_gm_tensor[q_offset],
                cur_head_num,        // nValue
                RoundUp<16>(cur_head_num),// dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                __k,                   // dValue
                0,                     // dstNzMatrixStride, unused
                __k                   // srcDValue
            );
        } else {
            if (embedding_size % BLOCK_SIZE == 0) {
                gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    round_k * cur_head_num,               // lenBurst
                    0,
                    0
                );
            } else {
                for (uint32_t copy_idx = 0; copy_idx < cur_head_num; copy_idx++) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1q_buf_addr_tensor[copy_idx * round_k],
                        q_gm_tensor[q_offset + copy_idx * embedding_size],
                        1,
                        0,
                        0,
                        round_k,               // lenBurst
                        0,
                        0
                    );
                }
            }
                       
        }
    }

    __attribute__((always_inline)) inline __aicore__ void LoadKVToL1MLA(
        AscendC::GlobalTensor<IN_KVDTYPE> kv_gm_tensor,
        AscendC::LocalTensor<IN_KVDTYPE> l1kv_buf_addr_tensor,
        bool move_l1b_flag,
        uint32_t head_num_move,
        uint32_t cur_batch,
        uint32_t cur_kv_seqlen,
        uint32_t start_kv,
        uint32_t qk_round_n,
        uint32_t real_n_loop,
        uint32_t sub_n_loop,
        uint32_t n_idx,
        uint32_t embed_split_size,
        uint32_t stride_kv_real,
        uint32_t l1_embed_offset,
        uint32_t embed_split_idx
    )
    {
        for (uint32_t inner_n_idx = 0; inner_n_idx < sub_n_loop; inner_n_idx++) {
            uint32_t actual_idx = n_idx * sub_n_loop + inner_n_idx;
            uint32_t sub_qk_n = block_size;
            if (actual_idx >= real_n_loop) {
                break;
            }
            uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                            cur_batch * max_num_blocks_per_query + start_kv / block_size + actual_idx));
            int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv_real;
            if (actual_idx == (real_n_loop - 1)) {
                sub_qk_n = (cur_kv_seqlen - actual_idx * block_size);
            }
            if (group_num == 1) {
                if (inner_n_idx == 0) {
                    WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2 * (embed_split_idx + 1));
                }
                gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                    l1kv_buf_addr_tensor[l1b_offset + qk_round_n * l1_embed_offset +
                                        block_size * 16 * inner_n_idx],
                    kv_gm_tensor[kv_offset],
                    sub_qk_n,                 // nValue
                    qk_round_n,           // dstNzC0Stride
                    0,                     // dstNzMatrixStride, unused
                    embed_split_size * head_num_move,  // dValue
                    0,                     // dstNzMatrixStride, unused
                    stride_kv_real            // srcDValue
                );
                if (actual_idx == real_n_loop - 1 || inner_n_idx == sub_n_loop - 1) {
                    SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                }
            } else {
                if (move_l1b_flag && inner_n_idx == 0) {
                    WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2 * (embed_split_idx + 1));
                }
                if (move_l1b_flag) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1b_offset + qk_round_n * l1_embed_offset +// 每次把对应的embed切块搬到对应的l1地址
                                            block_size * 16 * inner_n_idx],
                        kv_gm_tensor[kv_offset],
                        sub_qk_n,                 // nValue
                        qk_round_n,           // dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        embed_split_size * head_num_move,  // dValue
                        0,                     // dstNzMatrixStride, unused
                        stride_kv_real            // srcDValue
                    );
                    if (actual_idx == real_n_loop - 1 || inner_n_idx == sub_n_loop - 1) {
                        SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
            }
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ProcessQKMLA(
        AscendC::GlobalTensor<mm1CopyType> s_gm_tensor,
        uint32_t qk_n, uint32_t qk_round_n,
        uint32_t head_num_move, uint32_t group_num_move,
        uint32_t head_split_num_move, uint32_t cur_head_num_round,
        uint32_t split_idx, bool move_l1b_flag, uint32_t l1_embed_offset,
        uint32_t embed_split_size, uint32_t round_embed_split_size, uint32_t embed_split_idx,
        uint32_t head_split_loop, uint32_t n_loop, uint32_t n_idx,
        bool is_l0b_pingpong_off, bool is_l0c_pingpong_off)
    {
        uint32_t cMatrixInit = (embed_split_idx == 0) ? 1 : 0;
        uint32_t loop_mad = (group_num == 1) ? head_num_move : 1;
        for (uint32_t headdim_idx = 0; headdim_idx < loop_mad; headdim_idx++) {
            bool move_l0b_flag = move_l1b_flag;
            WAIT_FLAG(M, MTE1, l0_pingpong_flag);
            uint64_t l1q_offset = 0;
            uint32_t q_load_coeff = 1;
            if (!is_multi_head_mmad) {
                l1q_offset = split_idx * head_split_num_move * round_k + 
                             headdim_idx * round_k + l1_embed_offset;
            } else {
                l1q_offset = split_idx * group_num_move * T_BLOCK_SIZE + l1_embed_offset * cur_head_num_round;
                q_load_coeff = cur_head_num_round;
            }
            if (q_load_coeff == 1) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0a_buf_tensor[l0_offset],
                    l1q_buf_addr_tensor[l1q_offset],
                    0,
                    (round_embed_split_size + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                                    // srcStride
                    0,
                    0                                                    // dstStride
                );
            } else {
                for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[l0_offset + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                        l1q_buf_addr_tensor[l1q_offset + loa_load_idx * T_CUBE_MATRIX_SIZE],
                        0,
                        round_embed_split_size / T_BLOCK_SIZE,                                 // repeat
                        0,
                        q_load_coeff / BLOCK_SIZE,                            // srcStride
                        0,
                        0                                                     // dstStride
                    );
                }
            }
            uint32_t mad_l0b_offset = 0;
            if (group_num == 1) {
                if (headdim_idx == 0) {
                    WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                }

                if (is_l0b_pingpong_off) {
                    mad_l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else {
                    mad_l0b_offset = l0_offset;
                }
                uint64_t l1kv_offset = l1b_offset + 
                                        headdim_idx * round_embed_split_size * qk_round_n +
                                        l1_embed_offset * qk_round_n;
                l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0b_buf_tensor[mad_l0b_offset],
                    l1kv_buf_addr_tensor[l1kv_offset],
                    0,
                    round_embed_split_size * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                        // srcStride
                    0,
                    0                                        // dstStride
                );
            } else {
                if (is_l0b_pingpong_off) {
                    l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else if (move_l0b_flag) {
                    l0b_pingpong_flag = 1 - l0b_pingpong_flag;
                    l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
                if (headdim_idx == 0 && move_l1b_flag) {
                    WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                }
                if (move_l0b_flag) {
                    uint64_t l1kv_offset = l1b_offset + 
                                           headdim_idx * round_embed_split_size * qk_round_n +
                                           l1_embed_offset * qk_round_n;
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor[l0b_offset],
                        l1kv_buf_addr_tensor[l1kv_offset],
                        0,
                        round_embed_split_size * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                        // srcStride
                        0,
                        0                                        // dstStride
                    );
                }
                mad_l0b_offset = l0b_offset;
            }

            if (headdim_idx == loop_mad - 1) {
                if ((group_num != 1 && move_l1b_flag || group_num == 1) &&
                     embed_split_idx >= embed_split_loop_v) {
                    SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2 * (embed_split_idx + 1));
                }
            }

            SET_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(MTE1, M, l0_pingpong_flag);
            if (embed_split_idx == 0) {
                WAIT_FLAG(FIX, M, l0_pingpong_flag);
            }
            mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm1OutputType, false>(
                mm1_l0c_buf_tensor[l0c_offset],
                l0a_buf_tensor[l0_offset],
                l0b_buf_tensor[mad_l0b_offset],
                m,     // m
                qk_n,  // n
                embed_split_size,   // k
                cMatrixInit      // cmatrixInitVal
            );
            PIPE_BARRIER(M);
            if (is_l0b_pingpong_off) {
                SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
            } else {
                if (group_num != 1 && move_l0b_flag) {
                    SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
            }
            SET_FLAG(M, MTE1, l0_pingpong_flag);
            
            // copy S to gm
            if (embed_split_idx == embed_split_loop_qk - 1) {
                SET_FLAG(M, FIX, l0_pingpong_flag);
                WAIT_FLAG(M, FIX, l0_pingpong_flag);
                uint64_t s_gm_offset = headdim_idx * group_num_move * qk_round_n;
                l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                    s_gm_tensor[s_gm_offset],
                    mm1_l0c_buf_tensor[l0c_offset],
                    m,           // MSize
                    qk_round_n,  // NSize
                    RoundUp<16>(m), // srcStride
                    qk_round_n  // dstStride_dst_D
                );
                SET_FLAG(FIX, M, l0_pingpong_flag);
            }
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ProcessPVMLA(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor,
        AscendC::LocalTensor<IN_DTYPE> l1p_buf_addr_tensor,
        uint32_t qk_n, uint32_t qk_round_n, uint32_t head_num_move, uint32_t group_num_move,
        uint32_t head_split_num_move, uint32_t cur_head_num,
        uint32_t split_idx, bool move_l1b_flag, uint32_t softmax_ready_flag,
        uint32_t embed_split_size, uint32_t round_embed_split_size, uint32_t embed_split_idx, 
	    uint32_t l1_embed_offset, bool is_l0b_pingpong_off, bool is_l0c_pingpong_off)
    {
        uint32_t loop_mad = (group_num == 1) ? head_num_move : 1;
        // gqa场景没有多head搬移，head_num_move = 1
        for (uint32_t headdim_idx = 0; headdim_idx < loop_mad; headdim_idx++) {
            bool move_l0b_flag = move_l1b_flag;
            uint32_t mad_l0b_offset = 0;
            AscendC::LoadData2dTransposeParams loadDataParams;
            loadDataParams.dstGap = 0;
            loadDataParams.startIndex = 0;
            loadDataParams.dstFracGap = 0;
            if (group_num == 1) {
                if (is_l0b_pingpong_off) {
                    mad_l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else {
                    mad_l0b_offset = l0_offset;
                }
                uint64_t l1kv_offset = l1b_offset +
                                       headdim_idx * round_embed_split_size * qk_round_n / group_num +
                                       l1_embed_offset * qk_round_n;
		        if(qk_round_n <= round_embed_split_size || tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {// Nz -> nZ
                    loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                    loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                    uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                    loadDataParams.dstGap = dstGap;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[mad_l0b_offset + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                } else {
                    loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                    loadDataParams.dstGap = round_embed_split_size / BLOCK_SIZE - 1;
                    loadDataParams.srcStride = 1;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_embed_split_size / T_BLOCK_SIZE; ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[mad_l0b_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                }
            } else {
                if (is_l0b_pingpong_off) {
                    l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else if (move_l0b_flag) {
                    l0b_pingpong_flag = 1 - l0b_pingpong_flag;
                    l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
                if (move_l0b_flag) {
                    uint64_t l1kv_offset = l1b_offset +
                                           headdim_idx * round_embed_split_size * qk_round_n +
                                           l1_embed_offset * qk_round_n;
                    if (qk_round_n <= round_embed_split_size || tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) { // Nz -> nZ
                        loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                        loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                        uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                        loadDataParams.dstGap = dstGap;
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / T_BLOCK_SIZE; ++l0b_load_idx) {
                            // 沿 embd 方向搬
                            AscendC::LoadDataWithTranspose(
                                    l0b_buf_tensor[l0b_offset + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                    l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                    loadDataParams);
                        }
                    } else {
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_embed_split_size / T_BLOCK_SIZE; ++l0b_load_idx) {
                            // 沿 kv_len_blk方向搬
                            loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                            loadDataParams.srcStride = 1;
                            loadDataParams.dstGap = round_embed_split_size / BLOCK_SIZE - 1;
                            AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[l0b_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                                loadDataParams);
                        }
                    }
                }
                mad_l0b_offset = l0b_offset;
            }

            if (headdim_idx == loop_mad - 1) {
                if (group_num != 1 && move_l1b_flag || group_num == 1) {
                    if (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
                        if (group_num_move <= 64) {
                            SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2 * (embed_split_idx + 1));
                        } else if (embed_split_idx % 2 == 1 || embed_split_idx == embed_split_loop_v - 1) {
                            SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2 * (embed_split_idx / 2 + 1));
                        }
                    } else {
                        SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2 * (embed_split_idx + 1));
                    }
                }
            }

            // move p from gm to l1
            uint32_t p_move_head_num = group_num_move;
            if (headdim_idx == 0 && embed_split_idx == 0) {
                WaitFlagDev(softmax_ready_flag);
                WAIT_FLAG(MTE1, MTE2, l1p_pingpong_flag);
                if (!is_multi_head_mmad) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1p_buf_addr_tensor[l1p_start_offset],
                        p_gm_tensor,
                        1,
                        0,
                        0,
                        RoundUp<BLOCK_SIZE>(qk_n) * p_move_head_num * T_BLOCK_OFFSET,               // lenBurst
                        0,
                        0
                    );
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1p_buf_addr_tensor[l1p_start_offset],
                        p_gm_tensor,
                        p_move_head_num,         // nValue
                        (p_move_head_num + 15) / 16 * 16,// dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        qk_round_n,           // dValue
                        0,                     // dstNzMatrixStride, unused
                        RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET           // srcDValue
                    );
                }
            }
            // move p from l1 to l0a
            SET_FLAG(MTE2, MTE1, l0_pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
            WAIT_FLAG(M, MTE1, l0_pingpong_flag);
            uint64_t l1p_offset = l1p_start_offset;
            uint32_t p_load_coeff = 1;
            if (!is_multi_head_mmad) {
                l1p_offset += split_idx * head_split_num_move * RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET +
                     headdim_idx * RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET;

            } else {
                l1p_offset += split_idx * group_num_move * T_BLOCK_SIZE;
                p_load_coeff = RoundUp<16>(p_move_head_num);
            }
            if (p_load_coeff == 1) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0a_buf_tensor[l0_offset],
                    l1p_buf_addr_tensor[l1p_offset],
                    0,
                    (qk_round_n + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                                       // srcStride
                    0,
                    0                                                        // dstStride
                );
            } else {
                for (uint64_t loa_load_idx = 0; loa_load_idx < p_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[l0_offset + loa_load_idx * qk_round_n * BLOCK_SIZE],
                        l1p_buf_addr_tensor[l1p_offset + loa_load_idx * T_CUBE_MATRIX_SIZE],
                        0,
                        qk_round_n / T_BLOCK_SIZE,                                 // repeat
                        0,
                        p_load_coeff / BLOCK_SIZE,                               // srcStride
                        0,
                        0                                                        // dstStride
                    );
                }
            }

            if (headdim_idx == loop_mad - 1 && embed_split_idx == embed_split_loop_v - 1) {
                SET_FLAG(MTE1, MTE2, l1p_pingpong_flag);
            }

            SET_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(FIX, M, l0_pingpong_flag);
            mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm2OutputType, false>(
                mm2_l0c_buf_tensor[l0c_offset],
                l0a_buf_tensor[l0_offset],
                l0b_buf_tensor[mad_l0b_offset],
                m,     // m
                embed_split_size,   // n
                qk_n,  // k
                1      // cmatrixInitVal
            );
            if (is_l0b_pingpong_off) {
                SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
            } else {
                if (group_num != 1 && move_l0b_flag) {
                    SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
            }
            SET_FLAG(M, MTE1, l0_pingpong_flag);
            SET_FLAG(M, FIX, l0_pingpong_flag);
            WAIT_FLAG(M, FIX, l0_pingpong_flag);
            // copy O to gm
            uint64_t o_temp_gm_offset = headdim_idx * group_num_move * round_v;
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm2CopyType, mm2OutputType>(
                o_tmp_gm_tensor[o_temp_gm_offset],
                mm2_l0c_buf_tensor[l0c_offset],
                m,        // MSize
                RoundUp<16>(embed_split_size),  // NSize 32B对齐，防止workspace补齐的位置中有脏数据
                RoundUp<16>(m),       // srcStride
                round_v  // dstStride_dst_D
            );

            SET_FLAG(FIX, M, l0_pingpong_flag);
        }
    }


    __attribute__((always_inline)) inline __aicore__ void ChangeL1bPingPongFlag() {
        l1b_pingpong_flag = 1 - l1b_pingpong_flag;
        l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_SIZE / sizeof(IN_DTYPE);
    }

    __attribute__((always_inline)) inline __aicore__ void ChangePingPongFlag() {
        l1_pingpong_flag = 1 - l1_pingpong_flag;
        l1_offset = l1_pingpong_flag * L1_UINT8_BUF_SIZE_DECODER / sizeof(IN_DTYPE);
        l1_scale_offset = l1_pingpong_flag * L1_SCALE_UINT64_SIZE;
        l1_bias_offset = l1_pingpong_flag * L1_OFFSET_INT32_SIZE;
        if (pagedAttnVariant != PagedAttnVariant::MULTI_LATENT && group_num == 1) {
            ChangeL1bPingPongFlag();
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ChangeL0PingPongFlag() {
        if (is_l0c_pingpong_off) {
            l0_pingpong_flag = 0;
            l0_offset = 0;
            l0c_offset = 0;
        } else {
            l0_pingpong_flag = 1 - l0_pingpong_flag;
            l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
            l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ChangeWorkSpacePingPongFlag()
    {
        l1p_pingpong_flag = 1 - l1p_pingpong_flag;
        l1p_start_offset = l1p_pingpong_flag * L1_P_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    }

    __aicore__ __attribute__((always_inline)) inline void InnerRunCubeMLA(uint32_t cur_batch, uint32_t start_head, uint32_t cur_head_num, uint32_t head_split_loop,
                                    uint32_t start_kv, uint32_t cur_kv_seqlen, uint32_t offset_tiling, uint32_t group_num_move, uint32_t head_split_num_move)
    {
        uint32_t addr_q_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 5 + offset_tiling));
        uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
        uint64_t q_offset = addr_q_scalar + start_head * embedding_size;

        uint32_t pp_n_scalar = block_size_calc;
        uint32_t sub_n_loop = pp_n_scalar / block_size;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        uint32_t real_n_loop = (cur_kv_seqlen + block_size - 1) / block_size;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);

        uint32_t cur_head_num_round = RoundUp<16>(cur_head_num);
        m = (group_num == 1) ? 1 : group_num_move;
        is_multi_head_mmad = group_num_move > 1;
        bool is_l0b_pingpong_off = (RoundUp<T_BLOCK_SIZE>(block_size_calc) * embed_split_size_qk > (L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE))) ? 1 : 0;
        bool is_l0c_pingpong_off = (RoundUp<T_BLOCK_SIZE>(m) * embed_split_size_v > (L0C_UINT8_BUF_SIZE / 2 / sizeof(float))) ? 1 : 0;
        for (uint32_t n_idx = 0; n_idx < n_loop; n_idx+=2) {
            if (n_idx == (n_loop - 1)) {
                qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
            }
            if ((n_idx + 1) == (n_loop - 1)) {
                qk_n_2 = (cur_kv_seqlen - (n_idx + 1) * pp_n_scalar);
                qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);
            }
            for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {
                bool move_l1b_flag = 1;
                uint32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                        cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                uint64_t hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                uint32_t embed_split_size = embed_split_size_qk;
                uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                uint32_t embed_split_offset_tight = 0;
                uint32_t embed_split_offset = 0;
                /* ************ CUBE1 stage1  ************* */
                for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_qk; ++embed_split_idx) {
                    embed_split_offset_tight = embed_split_idx * embed_split_size;
                    embed_split_offset = embed_split_idx * round_embed_split_size;
                    if (embed_split_idx == embed_split_loop_qk - 1) {
                        embed_split_size = embedding_size - embed_split_offset_tight;
                    }
                    round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    if (n_idx == 0 && split_idx == 0 && embed_split_idx == 0) {
                        LoadQToL1MLA(q_offset, cur_head_num);
                    }
                    SET_FLAG(MTE2, MTE1, l0_pingpong_flag);

                    LoadKVToL1MLA(
                        k_gm_tensor[hiddenSize_offset + embed_split_offset_tight],
                        l1kv_buf_addr_tensor,
                        move_l1b_flag,
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<T_BLOCK_SIZE>(qk_round_n),
                        real_n_loop, sub_n_loop, n_idx, embed_split_size, stride_kv,
                        embed_split_offset, embed_split_idx
                    );
                    WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
                    ProcessQKMLA(
                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                        split_idx * head_split_num_move * group_num_move * RoundUp<T_BLOCK_SIZE>(qk_round_n)],
                        qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), head_num_move, group_num_move,
                        head_split_num_move, cur_head_num_round, split_idx, move_l1b_flag,
                        embed_split_offset, embed_split_size, round_embed_split_size, embed_split_idx,
                        head_split_loop, n_loop, n_idx,
                        is_l0b_pingpong_off, is_l0c_pingpong_off
                    );
                }
                ChangePingPongFlag();
                ChangeL1bPingPongFlag();
                FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_DECODER);

                /* ************ CUBE1 stage2  ************* */
                if (n_idx + 1 < n_loop) {
                    embed_split_size = embed_split_size_qk;
                    round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_qk; ++embed_split_idx) {
                        embed_split_offset_tight = embed_split_idx * embed_split_size;
                        embed_split_offset = embed_split_idx * round_embed_split_size;
                        if (embed_split_idx == embed_split_loop_qk - 1) {
                            embed_split_size = embedding_size - embed_split_offset_tight;
                        }
                        round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                        SET_FLAG(MTE2, MTE1, l0_pingpong_flag);
                        LoadKVToL1MLA(
                            k_gm_tensor[hiddenSize_offset + embed_split_offset_tight],
                            l1kv_buf_addr_tensor,
                            move_l1b_flag,
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<T_BLOCK_SIZE>(qk_round_n_2),
                            real_n_loop, sub_n_loop, (n_idx + 1), embed_split_size, stride_kv,
                            embed_split_offset, embed_split_idx
                        );
                        WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
                        ProcessQKMLA(
                            s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                                split_idx * head_split_num_move * group_num_move * RoundUp<T_BLOCK_SIZE>(qk_round_n_2) +
                                TMP_SIZE_DECODER / 2],
                            qk_n_2, RoundUp<T_BLOCK_SIZE>(qk_round_n_2), head_num_move, group_num_move,
                            head_split_num_move, cur_head_num_round, split_idx, move_l1b_flag,
                            embed_split_offset, embed_split_size, round_embed_split_size, embed_split_idx,
                            head_split_loop, n_loop, (n_idx + 1),
			                is_l0b_pingpong_off, is_l0c_pingpong_off
                        );
                    }
                    ChangePingPongFlag();
                    FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_STAGE2);
                }
                ChangeL1bPingPongFlag();

                /* ************ CUBE2 stage1  ************* */
                embed_split_size = embed_split_size_v;
                round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                    embed_split_offset_tight = embed_split_idx * embed_split_size;
                    embed_split_offset = embed_split_idx * round_embed_split_size;
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        embed_split_size = embedding_size_v - embed_split_offset_tight;
                    }
                    round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    ProcessPVMLA(
                        o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 +
                                        split_idx * head_split_num_move * group_num_move * round_v +
                                        embed_split_offset],
                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET +
                                    split_idx * group_num_move * RoundUp<T_BLOCK_SIZE>(qk_round_n)],
                        l1p_buf_addr_tensor,
                        qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), head_num_move, group_num_move,
                        head_split_num_move, cur_head_num,
                        split_idx, move_l1b_flag, SOFTMAX_READY_DECODER,
                        embed_split_size, round_embed_split_size, embed_split_idx,
                        embed_split_offset, is_l0b_pingpong_off, is_l0c_pingpong_off
                    );
                }
                ChangePingPongFlag();
            	ChangeL1bPingPongFlag();
            	ChangeWorkSpacePingPongFlag();
            	FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_DECODER);
            	/* ************ CUBE2 stage2  ************* */
            	if (n_idx + 1 < n_loop) {
                    embed_split_size = embed_split_size_v;
                    round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                        embed_split_offset_tight = embed_split_idx * embed_split_size;
                        embed_split_offset = embed_split_idx * round_embed_split_size;
                        if (embed_split_idx == embed_split_loop_v - 1) {
                            embed_split_size = embedding_size_v - embed_split_offset_tight;
                        }
                        round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                        ProcessPVMLA(
                            o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 +
                                            split_idx * head_split_num_move * group_num_move * round_v +
                                            TMP_SIZE +
                                            embed_split_offset],
                            p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET + TMP_SIZE * T_BLOCK_OFFSET / 2 +
                                        split_idx * group_num_move * RoundUp<T_BLOCK_SIZE>(qk_round_n_2)],
                            l1p_buf_addr_tensor,
                            qk_n_2, RoundUp<T_BLOCK_SIZE>(qk_round_n_2), head_num_move, group_num_move,
                            head_split_num_move, cur_head_num,
                            split_idx, move_l1b_flag, SOFTMAX_READY_STAGE2,
                            embed_split_size, round_embed_split_size, embed_split_idx,
                            embed_split_offset, is_l0b_pingpong_off, is_l0c_pingpong_off
                        );
                    }
                    ChangePingPongFlag();
                    ChangeWorkSpacePingPongFlag();
                    FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_STAGE2);
                }
                ChangeL1bPingPongFlag();
            }
        }
    }

private:
    __gm__ IN_DTYPE *__restrict__ q_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ ctkv_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ k_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ v_gm{nullptr};

    __gm__ mmScaleType *__restrict__ deq_scale1_gm{nullptr};
    __gm__ mmScaleType *__restrict__ deq_scale2_gm{nullptr};
    __gm__ mmBiasType *__restrict__ offset1_gm{nullptr};
    __gm__ mmBiasType *__restrict__ offset2_gm{nullptr};
    __gm__ int8_t *__restrict__ eye_gm{nullptr};

    __gm__ mm1CopyType *__restrict__ s_gm{nullptr};
    __gm__ IN_DTYPE *__restrict__ p_gm{nullptr};
    __gm__ mm2CopyType *__restrict__ o_tmp_gm{nullptr};
    __gm__ int32_t *__restrict__ block_tables_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_gm{nullptr};
    __gm__ float *__restrict__ razor_offset_gm{nullptr};

    AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> q_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> k_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> v_gm_tensor;
    AscendC::GlobalTensor<mm1CopyType> s_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor;
    AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<int32_t> block_tables_gm_tensor;

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1p_buf_addr_offset = 5 * L0AB_UINT8_BLOCK_SIZE;
    const uint32_t l1kv_buf_addr_offset = 7 * L0AB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<IN_DTYPE> l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1q_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l1kv_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1kv_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1p_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, IN_DTYPE>(0);
    AscendC::LocalTensor<IN_DTYPE> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, IN_DTYPE>(0);
    AscendC::LocalTensor<mm1OutputType> mm1_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm1OutputType>(0);
    AscendC::LocalTensor<mm2OutputType> mm2_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm2OutputType>(0);
    AscendC::LocalTensor<int32_t> l0c_buf_int32_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, int32_t>(0);

    
    uint32_t k_bias_flag{0};
    uint32_t v_bias_flag{0};
    uint32_t num_tokens{0};
    uint32_t q_heads{0};
    uint32_t kv_heads{0};
    uint32_t embedding_size{0};
    uint32_t embedding_size_v{0};
    uint32_t block_size{0};
    uint32_t max_num_blocks_per_query{0};
    uint32_t group_num{0};
    uint32_t former_group_num_move{1};
    uint32_t tail_group_num_move{1};
    uint32_t former_head_split_num{1};
    uint32_t tail_head_split_num{1};
    uint32_t stride_kv{0};
    uint32_t stride_vo{0};
    uint32_t m{0};
    uint32_t __k{0};
    uint32_t __v{0};
    uint32_t round_k{0};
    uint32_t round_v{0};
    uint32_t core_per_batch{0};
    uint32_t process_num{0};
    uint64_t former_batch{0};
    uint32_t former_head_split{0};
    uint64_t tail_batch{0};
    uint32_t tail_head_split{0};
    uint32_t head_split_num{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t kv_split_per_core{0};
    uint32_t kv_split_core_num{1};
    uint32_t block_size_calc{0};

    uint32_t embed_split_size_qk{0};
    uint32_t embed_split_loop_qk{1};
    uint32_t embed_split_size_v{0};
    uint32_t embed_split_loop_v{1};
    bool is_multi_head_mmad{0};
    uint32_t move_l1b_offset = 0;
    uint32_t q_head_original{0};
    uint32_t compressHead{0};

    uint32_t l1_pingpong_flag = 0;
    uint32_t l1b_pingpong_flag = 0;
    uint32_t l0_pingpong_flag = 0;
    uint32_t l0b_pingpong_flag = 0;
    uint32_t l1p_pingpong_flag = 0;

    uint32_t l1_offset = l1_pingpong_flag * L1_UINT8_BUF_SIZE_DECODER / sizeof(IN_DTYPE);
    uint32_t l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_SIZE / sizeof(IN_DTYPE);
    uint32_t l1_scale_offset = l1_pingpong_flag * L1_SCALE_UINT64_SIZE;
    uint32_t l1_bias_offset = l1_pingpong_flag * L1_OFFSET_INT32_SIZE;
    uint32_t l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
    uint32_t l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l1p_start_offset = l1p_pingpong_flag * L1_P_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    bool is_l0c_pingpong_off = 0;
    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;
};
#endif

#ifdef __DAV_C220_CUBE__
template <bool SplitKV = false, TilingKeyType tilingKeyType = TilingKeyType::TILING_HALF_DATA, typename IN_DTYPE = half,  typename OUT_DTYPE = half, typename IN_KVDTYPE = half, PagedAttnVariant pagedAttnVariant = PagedAttnVariant::DEFAULT, DataShapeType dataShapeType = DataShapeType::BSND, CompressType compressType = CompressType::COMPRESS_TYPE_UNDEFINED, bool SplitBlock = false>
class UnpadAttentionDecoderAic {
    // define dtype
    using mm1OutputType = typename AttentionType<tilingKeyType>::mm1OutputType;
    using mm1CopyType = typename AttentionType<tilingKeyType>::mm1CopyType;
    using mmBiasType = typename AttentionType<tilingKeyType>::mmBiasType;
    using mmScaleType = typename AttentionType<tilingKeyType>::mmScaleType;
    using mm2OutputType = typename AttentionType<tilingKeyType>::mm2OutputType;
    using mm2CopyType = typename AttentionType<tilingKeyType>::mm2CopyType;
    static constexpr uint32_t T_CUBE_MATRIX_SIZE = CUBE_MATRIX_SIZE_512 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_SIZE =  BLOCK_SIZE_32 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_OFFSET = 2 / sizeof(IN_DTYPE);

public:
    __aicore__ __attribute__((always_inline)) inline UnpadAttentionDecoderAic(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    __aicore__ __attribute__((always_inline)) inline void InitQuant(
        __gm__ uint8_t *__restrict__ deq_scale1_in_gm,
        __gm__ uint8_t *__restrict__ offset1_in_gm,
        __gm__ uint8_t *__restrict__ deq_scale2_in_gm,
        __gm__ uint8_t *__restrict__ offset2_in_gm,
        __gm__ uint8_t *__restrict__ eye_in_gm
    )
    {
        deq_scale1_gm = reinterpret_cast<__gm__ mmScaleType *>(deq_scale1_in_gm);
        deq_scale2_gm = reinterpret_cast<__gm__ mmScaleType *>(deq_scale2_in_gm);
        offset1_gm = reinterpret_cast<__gm__ mmBiasType *>(offset1_in_gm);
        offset2_gm = reinterpret_cast<__gm__ mmBiasType *>(offset2_in_gm);

        deq_scale1_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(deq_scale1_in_gm));
        deq_scale2_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(deq_scale2_in_gm));
        bias1_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmBiasType *>(offset1_in_gm));
        bias2_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmBiasType *>(offset2_in_gm));
        eye_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(eye_in_gm));
        if (offset1_gm != nullptr) {
            k_bias_flag = 1;
        }
        if (offset2_gm != nullptr) {
            v_bias_flag = 1;
        }
    }

    __aicore__ __attribute__((always_inline)) inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t *__restrict__ q_in_gm,
        __gm__ uint8_t *__restrict__ k_in_gm,
        __gm__ uint8_t *__restrict__ v_in_gm,
        __gm__ uint8_t *__restrict__ block_tables_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t* __restrict__ gm_k16,
        __gm__ uint8_t* __restrict__ gm_v16, 
        __gm__ uint8_t *__restrict__ tiling_para_gm,
        __gm__ uint8_t *__restrict__ razorOffset)
    {
        SetFftsBaseAddr((uint64_t)sync);
        SetPadding<uint64_t>(0);
        SetAtomicnone();
        SetNdpara(1, 0, 0);
        SetMasknorm();

        q_gm = reinterpret_cast<__gm__ IN_DTYPE *>(q_in_gm);
        k_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm);
        v_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(v_in_gm);
        block_tables_gm = reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm);
        s_gm = reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm);

        p_gm = reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm);
        o_tmp_gm = reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm);
        tiling_gm = reinterpret_cast<__gm__ uint8_t *>(tiling_para_gm);
        razor_offset_gm = reinterpret_cast<__gm__ float *>(razorOffset);

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(q_in_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm));
        v_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(v_in_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm));
        block_tables_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm));

        num_tokens = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM));
        embedding_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V));
        block_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MAXBLOCKS));
        kv_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVHEADS));
        former_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_BATCH));
        former_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_HEAD));
        tail_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_BATCH));
        tail_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_HEAD));
        head_split_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADNUM_MOVE));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        group_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_GROUPNUM));
        block_size_calc = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE_CALC));
        q_head_original = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QHEADORIGINAL));
        compressHead = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_COMPRESSHEAD));
        block_size_inner_count = block_size / block_size_calc;

        if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
            former_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_GROUP_MOVE));
            tail_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_GROUP_MOVE));
        }
        kv_split_per_core = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVSPLIT));
        kv_split_core_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVCORENUM));

        former_head_split_num = (former_head_split > group_num) && (former_group_num_move == group_num) ? head_split_num : 1;
        tail_head_split_num = (tail_head_split > group_num) && (tail_group_num_move == group_num) ? head_split_num : 1;

        stride_kv = static_cast<uint64_t>(kv_heads) * embedding_size;

        if constexpr (dataShapeType == DataShapeType::BNSD) {
            stride_kv = embedding_size;
        }

        __k = embedding_size;
        round_k = RoundUp<T_BLOCK_SIZE>(__k);
        if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
            __v = embedding_size_v;
            stride_vo = static_cast<uint64_t>(kv_heads) * embedding_size_v;
            round_v = RoundUp<BLOCK_SIZE>(__v);
            embed_split_size_qk = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_K_SPLIT));
            embed_split_loop_qk = (embedding_size + embed_split_size_qk - 1) / embed_split_size_qk;
            embed_split_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT));
            embed_split_loop_v = (embedding_size_v + embed_split_size_v - 1) / embed_split_size_v;
        }
        if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT  || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
            gm_k16_ping_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_k16) +
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_k16_pong_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_k16) +
                                    get_block_num() * block_size_calc * q_heads * embedding_size + 
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_v16_ping_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_v16) +
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_v16_pong_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_v16) +
                                    get_block_num() * block_size_calc * q_heads * embedding_size + 
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
        }
    }


    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID4);
        SET_FLAG(M, MTE1, EVENT_ID5);
        SET_FLAG(M, MTE1, EVENT_ID7);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        SET_FLAG(FIX, MTE1, EVENT_ID2);
        SET_FLAG(FIX, MTE1, EVENT_ID3);
        SET_FLAG(FIX, MTE1, EVENT_ID4);
        SET_FLAG(FIX, MTE1, EVENT_ID5);
        SET_FLAG(MTE2, FIX, EVENT_ID0);
        core_per_batch = (q_heads + former_head_split - 1) / former_head_split;
        process_num = static_cast<uint64_t>(former_batch) * core_per_batch * kv_split_core_num;

        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {  // for task
            uint32_t cur_batch = process / (core_per_batch * kv_split_core_num) + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm  + 1 + offset_tiling));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            uint32_t cur_head = (process / kv_split_core_num) % core_per_batch;
            uint32_t cur_nIndx = process % kv_split_core_num;
            uint32_t start_head = cur_head * former_head_split;
            uint32_t start_kv = cur_nIndx * kv_split_per_core;
            uint32_t cur_kv_seqlen = kv_split_per_core;
            uint32_t kv_loop = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            if (cur_nIndx >= kv_loop) {
                continue;
            }
            if (cur_nIndx == (kv_loop - 1)) {
                cur_kv_seqlen = kv_seqlen - cur_nIndx * kv_split_per_core;
            }
            uint32_t cur_head_num = former_head_split;
            if (cur_head == (core_per_batch - 1)) {
                cur_head_num = q_heads - cur_head * former_head_split;
                former_group_num_move = former_group_num_move <= cur_head_num ? former_group_num_move : cur_head_num;
            }
            uint32_t head_split_loop = (cur_head_num + (former_head_split_num * former_group_num_move) - 1) /
                                       (former_head_split_num * former_group_num_move);
            if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_VEC_QUANT && tilingKeyType != TilingKeyType::TILING_INT8_VEC_QUANTBF16 && tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    InnerRunCubeMLA(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, former_group_num_move, former_head_split_num);
                }
            } else {
                InnerRunCube(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, former_group_num_move, former_head_split_num);
            }
        }
        if (tail_batch > 0) {
            core_per_batch = (q_heads + tail_head_split - 1) / tail_head_split;
            process_num = static_cast<uint64_t>(tail_batch) * core_per_batch;
            for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {  // for task
                uint32_t cur_batch = process / core_per_batch + former_batch + prefill_batch_size_;
                uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
                uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
                if (kv_seqlen == 0) {
                    continue;
                }
                uint32_t cur_kv_seqlen = kv_seqlen;
                uint32_t start_kv = 0;
                uint32_t cur_head = process % core_per_batch;
                uint32_t cur_head_num = tail_head_split;
                if (cur_head == (core_per_batch - 1)) {
                    cur_head_num = q_heads - cur_head * tail_head_split;
                    tail_group_num_move = tail_group_num_move <= cur_head_num ? tail_group_num_move : cur_head_num;
                }
                uint32_t head_split_loop = (cur_head_num + (tail_head_split_num * tail_group_num_move) - 1) /
                                           (tail_head_split_num * tail_group_num_move);
                uint32_t start_head = (process % core_per_batch) * tail_head_split;
                if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                    if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_VEC_QUANT && tilingKeyType != TilingKeyType::TILING_INT8_VEC_QUANTBF16 && tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                        InnerRunCubeMLA(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, tail_group_num_move, tail_head_split_num);
                    }
                } else {
                    InnerRunCube(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, tail_group_num_move, tail_head_split_num);
                }
            }
        }
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID4);
        WAIT_FLAG(M, MTE1, EVENT_ID5);
        WAIT_FLAG(M, MTE1, EVENT_ID7);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        WAIT_FLAG(FIX, MTE1, EVENT_ID2);
        WAIT_FLAG(FIX, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, MTE1, EVENT_ID4);
        WAIT_FLAG(FIX, MTE1, EVENT_ID5);
        WAIT_FLAG(MTE2, FIX, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }
private:

   __attribute__((always_inline)) inline __aicore__ void DeQuant(AscendC::LocalTensor<int8_t> l1e_buf_addr_temp_tensor,
                                                                  AscendC::LocalTensor<IN_KVDTYPE> l1kv_addr_tensor,
                                                                  AscendC::LocalTensor<mmScaleType> l1kscale_int8_addr_tensor,
                                                                  AscendC::LocalTensor<mmBiasType> bias_l1_tensor,
                                                                   uint32_t qk_n, uint32_t headdim, bool bias_flag, bool pingpang)
    {
        uint32_t m_actual = qk_n; // qslen
        uint32_t kv_actual = qk_n; // qslen
        uint32_t head_actual = headdim; // headdim
        uint32_t m_round = (m_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
        uint32_t kv_round = (kv_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
        uint32_t head_round = (head_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
        const uint32_t FB_BUFF_SIZE = 1024 * 7;
        AscendC::LocalTensor<uint64_t> scale_fb_tensor;
        scale_fb_tensor.InitBuffer(0, FB_BUFF_SIZE);

        auto l0a_buf_temp_tensor = pingpang ? l0a_buf_tensor.template ReinterpretCast<int8_t>()[L0AB_PINGPONG_BUFFER_LEN]
                                            : l0a_buf_tensor.template ReinterpretCast<int8_t>();
        auto l0b_buf_temp_tensor = pingpang ? l0b_buf_tensor.template ReinterpretCast<int8_t>()[L0AB_PINGPONG_BUFFER_LEN]
                                            : l0b_buf_tensor.template ReinterpretCast<int8_t>();
        auto l0c_buf_temp_tensor = pingpang ? mm1_l0c_buf_tensor.template ReinterpretCast<int32_t>()[L0C_PINGPONG_BUFFER_LEN_INT32]
                                            : mm1_l0c_buf_tensor.template ReinterpretCast<int32_t>();

        auto event_id = pingpang ? EVENT_ID2 : EVENT_ID3;
        uint64_t bias_tb = pingpang ? 256 : 0;

        //step1: creat ca matrix and move diag matrix
        CreateCaMatrix(l0a_buf_temp_tensor.template ReinterpretCast<int16_t>(), 1, ((kv_round * head_round + 511)) / 512, 0, (int16_t)0);
        PIPE_BARRIER(MTE1);
        uint32_t m_part1_rpeat = (m_round + 31) / 32;
        uint32_t m_part2_rpeat = kv_round / BLOCK_SIZE_32;
        l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0a_buf_temp_tensor,
            l1e_buf_addr_temp_tensor,
            0,
            m_part1_rpeat,                   // repeat
            0,
            0,                   // srcStride
            0,
            kv_round / 16 // dstStride
        );
        l1_to_l0_a<ArchType::ASCEND_V220, int8_t, false, DataFormat::VECTOR, DataFormat::VECTOR>(
            l0a_buf_temp_tensor[16 * kv_round],
            l1e_buf_addr_temp_tensor[CUBE_MATRIX_SIZE_512],
            0,
            m_part2_rpeat,       // repeat
            0,
            0,                   // srcStride
            0,
            kv_round / 16          // dstStride
        );

        SET_FLAG(MTE1, M, event_id);
        WAIT_FLAG(MTE1, M, event_id);
        //step2: move b matrix
        l1_to_l0_b<ArchType::ASCEND_V220, int8_t, true, DataFormat::NZ, DataFormat::ZN>(
            l0b_buf_temp_tensor,
            l1kv_addr_tensor,
            kv_round,                                   // nTileCeil
            head_round,                                 // kPartCeil
            0,                                          // nSrcStride
            0,                                          // kSrcStride
            0,                                          // nDstStride
            kv_round / 16                               // kDstStride
        );
        SET_FLAG(MTE1, M, event_id + 2);
        WAIT_FLAG(MTE1, M, event_id + 2);

        if (bias_flag) {
            l1_to_bt<ArchType::ASCEND_V220, int32_t>(bias_tb,                                          // dst
                                                     bias_l1_tensor,                                   // src
                                                     0,                                                // convControl
                                                     1,                                                // nBurst
                                                     CeilDiv<CONST_64>(head_actual * sizeof(int32_t)), // lenBurst
                                                     0,                                                // srcGap
                                                     0);                                               // dstGap
            SET_FLAG(MTE1, M, EVENT_ID7);
            WAIT_FLAG(MTE1, M, EVENT_ID7); // wait move bias fron L1 to BT
            mmad<ArchType::ASCEND_V220, int8_t, int8_t, int32_t, false>(
                l0c_buf_temp_tensor,
                l0a_buf_temp_tensor,
                l0b_buf_temp_tensor,
                ((uint64_t)bias_tb),
                m_actual,  // m
                head_actual,  // n
                kv_actual, // k
                0          // cmatrixInitVal
            );
        } else {
            mmad<ArchType::ASCEND_V220, int8_t, int8_t, int32_t, false>(
                l0c_buf_temp_tensor,
                l0a_buf_temp_tensor,
                l0b_buf_temp_tensor,
                m_actual,  // m
                head_actual,  // n
                kv_actual, // k
                1          // cmatrixInitVal
            );
        }
        SET_FLAG(M, FIX, EVENT_ID0);
        WAIT_FLAG(M, FIX, EVENT_ID0);

        //step3: move bias
        l1_to_fb<ArchType::ASCEND_V220, mmScaleType>(scale_fb_tensor,                                       // dst
                                                     l1kscale_int8_addr_tensor,                             // src
                                                     1,                                                     // nBurst
                                                     CeilDiv<CONST_128>(head_actual * sizeof(mmScaleType)), // lenBurst
                                                     0,                                                     // srcGap
                                                     0);                                                    // dstGap
        PIPE_BARRIER(FIX);
        SetFpc<uint64_t>(scale_fb_tensor, false);
        l0c_to_l1<ArchType::ASCEND_V220, DataFormat::ZN, half, int32_t>(
            l1kv_addr_tensor.template ReinterpretCast<half>(),
            l0c_buf_temp_tensor,
            l1kscale_int8_addr_tensor,
            m_actual, // MSize
            head_actual, // NSize
            m_round,  // srcStride
            m_round // dstStride_dst_D
        );
    }


    __attribute__((always_inline)) inline __aicore__ void LoadQToL1(
        uint32_t q_offset,
        uint32_t cur_head_num)
    {
        if (is_multi_head_mmad) {
            // gm_to_l1q
            gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                l1q_buf_addr_tensor,
                q_gm_tensor[q_offset],
                cur_head_num,        // nValue
                RoundUp<16>(cur_head_num),// dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                __k,                   // dValue
                0,                     // dstNzMatrixStride, unused
                __k                   // srcDValue
            );
        } else {
            if (embedding_size % T_BLOCK_SIZE == 0) {
                gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    round_k * cur_head_num,               // lenBurst
                    0,
                    0
                );
            } else {
                for (uint32_t copy_idx = 0; copy_idx < cur_head_num; copy_idx++) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1q_buf_addr_tensor[copy_idx * round_k],
                        q_gm_tensor[q_offset + copy_idx * embedding_size],
                        1,
                        0,
                        0,
                        round_k,               // lenBurst
                        0,
                        0
                    );
                }
            }
        }
    }

    __attribute__((always_inline)) inline __aicore__ void LoadQToL1MLA(
        uint32_t q_offset,
        uint32_t cur_head_num)
    {
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        if (is_multi_head_mmad) {
            gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                l1q_buf_addr_tensor,
                q_gm_tensor[q_offset],
                cur_head_num,        // nValue
                RoundUp<16>(cur_head_num),// dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                __k,                   // dValue
                0,                     // dstNzMatrixStride, unused
                __k                   // srcDValue
            );
        } else {
            if (embedding_size % BLOCK_SIZE == 0) {
                gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    round_k * cur_head_num,               // lenBurst
                    0,
                    0
                );
            } else {
                for (uint32_t copy_idx = 0; copy_idx < cur_head_num; copy_idx++) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1q_buf_addr_tensor[copy_idx * round_k],
                        q_gm_tensor[q_offset + copy_idx * embedding_size],
                        1,
                        0,
                        0,
                        round_k,               // lenBurst
                        0,
                        0
                    );
                }
            }
                       
        }
    }

    __attribute__((always_inline)) inline __aicore__ void LoadKVToL1Int8(
        AscendC::GlobalTensor<IN_KVDTYPE> kv_gm_tensor,
        AscendC::LocalTensor<IN_KVDTYPE> l1kv_buf_int8_addr_tensor,
        AscendC::GlobalTensor<mmScaleType> deq_scale_gm_tensor,
        AscendC::LocalTensor<mmScaleType> l1scale_buf_addr_tensor,
        AscendC::GlobalTensor<mmBiasType> offset_gm_tensor,
        AscendC::LocalTensor<mmBiasType> l1bias_buf_addr_tensor,
        uint32_t head_num_move,
        uint32_t cur_batch,
        uint32_t cur_kv_seqlen,
        uint32_t start_kv,
        uint32_t qk_n,
        uint32_t real_n_loop,
        uint32_t sub_n_loop,
        uint32_t n_idx
    )
    {
        for (uint32_t inner_n_idx = 0; inner_n_idx < sub_n_loop; inner_n_idx++) {
            uint32_t actual_idx = n_idx * sub_n_loop + inner_n_idx;
            uint32_t sub_qk_n = block_size;
            if (actual_idx >= real_n_loop) {
                break;
            }
            uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                            cur_batch * max_num_blocks_per_query + start_kv / block_size + actual_idx));
            int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv;
            if (actual_idx == (real_n_loop - 1)) {
                sub_qk_n = (cur_kv_seqlen - actual_idx * block_size);
            }
            if (inner_n_idx == 0) {
                WAIT_FLAG(MTE1, MTE2, l1_pingpong_flag);
            }
            gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                l1kv_buf_int8_addr_tensor[block_size * 32 * inner_n_idx], kv_gm_tensor[kv_offset],
                sub_qk_n,              // nValue
                (qk_n + 31) / 32 * 32, // dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                __k * head_num_move,   // dValue
                0,                     // dstNzMatrixStride, unused
                stride_kv              // srcDValue
                );
        }
        move_l1b_offset = l1_offset;
        gm_to_l1<ArchType::ASCEND_V220, mmScaleType, DataFormat::ND, DataFormat::ND>(
            l1scale_buf_addr_tensor,
            deq_scale_gm_tensor,
            1,
            0,
            0,
            (__k * head_num_move), // lenBurst
            0,
            0
        );

        if (offset_gm_tensor.GetPhyAddr() != nullptr) {
            // move bias
            gm_to_l1<ArchType::ASCEND_V220, mmBiasType, DataFormat::ND, DataFormat::ND>(
                l1bias_buf_addr_tensor,
                offset_gm_tensor,
                1,
                0,
                0,
                (__k * head_num_move), // lenBurst
                0,
                0
            );
        }
    }

    __attribute__((always_inline)) inline __aicore__ void LoadKVToL1MLA(
        AscendC::GlobalTensor<IN_KVDTYPE> kv_gm_tensor,
        AscendC::LocalTensor<IN_KVDTYPE> l1kv_buf_addr_tensor,
        bool move_l1b_flag,
        uint32_t head_num_move,
        uint32_t cur_batch,
        uint32_t cur_kv_seqlen,
        uint32_t start_kv,
        uint32_t qk_round_n,
        uint32_t real_n_loop,
        uint32_t sub_n_loop,
        uint32_t n_idx,
        uint32_t embed_split_size,
        uint32_t stride_kv_real
    )
    {
        for (uint32_t inner_n_idx = 0; inner_n_idx < sub_n_loop; inner_n_idx++) {
            uint32_t actual_idx = n_idx * sub_n_loop + inner_n_idx;
            uint32_t sub_qk_n = block_size;
            if (actual_idx >= real_n_loop) {
                break;
            }
            uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                            cur_batch * max_num_blocks_per_query + start_kv / block_size + actual_idx));
            int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv_real;
            if (actual_idx == (real_n_loop - 1)) {
                sub_qk_n = (cur_kv_seqlen - actual_idx * block_size);
            }
            if (group_num == 1) {
                if (inner_n_idx == 0) {
                    WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                }
                gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                    l1kv_buf_addr_tensor[l1b_offset +
                                        block_size * T_BLOCK_SIZE * inner_n_idx],
                    kv_gm_tensor[kv_offset],
                    sub_qk_n,                 // nValue
                    qk_round_n,           // dstNzC0Stride
                    0,                     // dstNzMatrixStride, unused
                    embed_split_size * head_num_move,  // dValue
                    0,                     // dstNzMatrixStride, unused
                    stride_kv_real            // srcDValue
                );
                if (actual_idx == real_n_loop - 1 || inner_n_idx == sub_n_loop - 1) {
                    SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    move_l1b_offset = l1b_offset;
                }
            } else {
                if (move_l1b_flag && inner_n_idx == 0) {
                    l1b_pingpong_flag = 1 - l1b_pingpong_flag;
                    l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                }
                if (move_l1b_flag) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1b_offset +
                                            block_size * T_BLOCK_SIZE * inner_n_idx],
                        kv_gm_tensor[kv_offset],
                        sub_qk_n,                 // nValue
                        qk_round_n,           // dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        embed_split_size * head_num_move,  // dValue
                        0,                     // dstNzMatrixStride, unused
                        stride_kv_real            // srcDValue
                    );
                    if (actual_idx == real_n_loop - 1 || inner_n_idx == sub_n_loop - 1) {
                        SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                move_l1b_offset = l1b_offset;
            }
        }
    }

    __attribute__((always_inline)) inline __aicore__ void LoadKVToL1SplitB(
        AscendC::GlobalTensor<IN_KVDTYPE> kv_gm_tensor,
        AscendC::LocalTensor<IN_KVDTYPE> l1kv_buf_addr_tensor,
        bool move_l1b_flag,
        uint32_t head_num_move,
        uint32_t cur_batch,
        uint32_t cur_kv_seqlen,
        uint32_t start_kv,
        uint32_t qk_round_n,
        uint32_t real_n_loop,
        uint32_t sub_n_loop,
        uint32_t n_idx
    )
    {
        uint32_t actual_idx = n_idx * sub_n_loop;
        uint32_t sub_qk_n = block_size_calc;
        if (actual_idx >= real_n_loop) {
            return;
        }
        uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                        cur_batch * max_num_blocks_per_query + start_kv / block_size + actual_idx / this->block_size_inner_count));
        int64_t block_inner_stride = actual_idx % this->block_size_inner_count;
        int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv +
                    this->block_size * stride_kv * block_inner_stride / this->block_size_inner_count;
        if constexpr (dataShapeType == DataShapeType::BNSD) {
            kv_offset = (int64_t)block_table_id * block_size * stride_kv * kv_heads;
        }

        if (actual_idx == (real_n_loop - 1)) {
            sub_qk_n = (cur_kv_seqlen - actual_idx * block_size_calc);
        }
        if (group_num == 1) {
            WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
            if constexpr (dataShapeType == DataShapeType::BNSD) {
                for (int i = 0; i < head_num_move; i++) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1b_offset + i * qk_round_n * embedding_size ],
                        kv_gm_tensor[kv_offset + i * block_size_calc * embedding_size],
                        sub_qk_n,   // nValue
                        qk_round_n, // dstNzC0Stride
                        0,          // dstNzMatrixStride, unused
                        __k,        // dValue
                        0,          // dstNzMatrixStride, unused
                        stride_kv   // srcDValue
                    );
                }
            } else {
                gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                    l1kv_buf_addr_tensor[l1b_offset], kv_gm_tensor[kv_offset],
                    sub_qk_n,            // nValue
                    qk_round_n,          // dstNzC0Stride
                    0,                   // dstNzMatrixStride, unused
                    __k * head_num_move, // dValue
                    0,                   // dstNzMatrixStride, unused
                    stride_kv            // srcDValue
                );
            }
            SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
            move_l1b_offset = l1b_offset;
        } else {
            if (move_l1b_flag) {
                l1b_pingpong_flag = 1 - l1b_pingpong_flag;
                l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                if constexpr (dataShapeType == DataShapeType::BNSD) {
                    for (int i = 0; i < head_num_move; i++) {
                        gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                            l1kv_buf_addr_tensor[l1b_offset +
                                                    i * qk_round_n * embedding_size],
                            kv_gm_tensor[kv_offset + i * block_size_calc * embedding_size],
                            sub_qk_n,   // nValue
                            qk_round_n, // dstNzC0Stride
                            0,          // dstNzMatrixStride, unused
                            __k,        // dValue
                            0,          // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );
                    }
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1b_offset], kv_gm_tensor[kv_offset],
                        sub_qk_n,            // nValue
                        qk_round_n,          // dstNzC0Stride
                        0,                   // dstNzMatrixStride, unused
                        __k * head_num_move, // dValue
                        0,                   // dstNzMatrixStride, unused
                        stride_kv            // srcDValue
                    );
                }

                SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
            }
            move_l1b_offset = l1b_offset;
        }
    }

    __attribute__((always_inline)) inline __aicore__ void LoadKVToL1(
        AscendC::GlobalTensor<IN_KVDTYPE> kv_gm_tensor,
        AscendC::LocalTensor<IN_KVDTYPE> l1kv_buf_addr_tensor,
        bool move_l1b_flag,
        uint32_t head_num_move,
        uint32_t cur_batch,
        uint32_t cur_kv_seqlen,
        uint32_t start_kv,
        uint32_t qk_round_n,
        uint32_t real_n_loop,
        uint32_t sub_n_loop,
        uint32_t n_idx
    )
    {
        if constexpr (SplitBlock) {
            return LoadKVToL1SplitB(kv_gm_tensor, l1kv_buf_addr_tensor, move_l1b_flag, head_num_move, cur_batch, cur_kv_seqlen,
            start_kv, qk_round_n, real_n_loop, sub_n_loop, n_idx);
        }
        for (uint32_t inner_n_idx = 0; inner_n_idx < sub_n_loop; inner_n_idx++) {
            uint32_t actual_idx = n_idx * sub_n_loop + inner_n_idx;
            uint32_t sub_qk_n = block_size;
            if (actual_idx >= real_n_loop) {
                break;
            }
            uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                            cur_batch * max_num_blocks_per_query + start_kv / block_size + actual_idx));
            int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv;

            if constexpr (dataShapeType == DataShapeType::BNSD) {
                kv_offset = (int64_t)block_table_id * block_size * stride_kv * kv_heads;
            }

            if (actual_idx == (real_n_loop - 1)) {
                sub_qk_n = (cur_kv_seqlen - actual_idx * block_size);
            }
            if (group_num == 1) {
                if (inner_n_idx == 0) {
                    WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                }
                if constexpr (dataShapeType == DataShapeType::BNSD) {
                    for (int i = 0; i < head_num_move; i++) {
                        gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                            l1kv_buf_addr_tensor[l1b_offset + block_size * T_BLOCK_SIZE * inner_n_idx + i * qk_round_n * embedding_size ],
                            kv_gm_tensor[kv_offset + i * block_size * embedding_size],
                            sub_qk_n,   // nValue
                            qk_round_n, // dstNzC0Stride
                            0,          // dstNzMatrixStride, unused
                            __k,        // dValue
                            0,          // dstNzMatrixStride, unused
                            stride_kv   // srcDValue
                        );
                    }
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[l1b_offset + block_size * T_BLOCK_SIZE * inner_n_idx], kv_gm_tensor[kv_offset],
                        sub_qk_n,            // nValue
                        qk_round_n,          // dstNzC0Stride
                        0,                   // dstNzMatrixStride, unused
                        __k * head_num_move, // dValue
                        0,                   // dstNzMatrixStride, unused
                        stride_kv            // srcDValue
                    );
                }
                if (actual_idx == real_n_loop - 1 || inner_n_idx == sub_n_loop - 1) {
                    SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    move_l1b_offset = l1b_offset;
                }
            } else {
                if (move_l1b_flag && inner_n_idx == 0) {
                    l1b_pingpong_flag = 1 - l1b_pingpong_flag;
                    l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                }
                if (move_l1b_flag) {
                    if constexpr (dataShapeType == DataShapeType::BNSD) {
                        for (int i = 0; i < head_num_move; i++) {
                            gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                                l1kv_buf_addr_tensor[l1b_offset + block_size * T_BLOCK_SIZE * inner_n_idx +
                                                     i * qk_round_n * embedding_size],
                                kv_gm_tensor[kv_offset + i * block_size * embedding_size],
                                sub_qk_n,   // nValue
                                qk_round_n, // dstNzC0Stride
                                0,          // dstNzMatrixStride, unused
                                __k,        // dValue
                                0,          // dstNzMatrixStride, unused
                                stride_kv   // srcDValue
                            );
                        }
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                            l1kv_buf_addr_tensor[l1b_offset + block_size * T_BLOCK_SIZE * inner_n_idx], kv_gm_tensor[kv_offset],
                            sub_qk_n,            // nValue
                            qk_round_n,          // dstNzC0Stride
                            0,                   // dstNzMatrixStride, unused
                            __k * head_num_move, // dValue
                            0,                   // dstNzMatrixStride, unused
                            stride_kv            // srcDValue
                        );
                    }

                    if (actual_idx == real_n_loop - 1 || inner_n_idx == sub_n_loop - 1) {
                        SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                move_l1b_offset = l1b_offset;
            }
        }
    }

    // antiquant
    __attribute__((always_inline)) inline __aicore__ void
    LoadKVToL1(AscendC::GlobalTensor<IN_DTYPE> kv_gm_tensor,        // [seq_len, num_head, embd_size]
               AscendC::LocalTensor<IN_DTYPE> l1kv_buf_addr_tensor, // [seq_len, hidden_size]
               bool move_l1b_flag, uint32_t head_num_move, uint32_t qk_n, uint32_t qk_round_n, uint32_t num_head)
    {
        if (group_num == 1) {
            // [qk_n, cur_head_num, head_size] -> [qk_n, head_num_move, head_size]
            WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
            gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(l1kv_buf_addr_tensor[l1b_offset],
                                                                                      kv_gm_tensor,
                                                                                      qk_n,       // nValue
                                                                                      qk_round_n, // dstNzC0Stride
                                                                                      0,          // dstNzMatrixStride
                                                                                      __k * head_num_move, // dValue
                                                                                      0,        // dstNzMatrixStride
                                                                                      stride_kv // srcDValue
            );
            SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
            move_l1b_offset = l1b_offset;
        } else {
            if (move_l1b_flag) {
                l1b_pingpong_flag = 1 - l1b_pingpong_flag;
                l1b_offset = l1b_pingpong_flag * L1_KV_HALF_BUF_SIZE;
                WAIT_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(l1kv_buf_addr_tensor[l1b_offset],
                                                                                          kv_gm_tensor,
                                                                                          qk_n,       // nValue
                                                                                          qk_round_n, // dstNzC0Stride
                                                                                          0, // dstNzMatrixStride
                                                                                          __k * head_num_move, // dValue
                                                                                          0,        // dstNzMatrixStride
                                                                                          stride_kv // srcDValue
 
                );
                SET_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                move_l1b_offset = l1b_offset;
            }
        }
    } 

    __attribute__((always_inline)) inline __aicore__ void ProcessQKMLA(
        AscendC::GlobalTensor<mm1CopyType> s_gm_tensor,
        uint32_t qk_n, uint32_t qk_round_n,
        uint32_t head_num_move, uint32_t group_num_move,
        uint32_t head_split_num_move, uint32_t cur_head_num_round,
        uint32_t split_idx, bool move_l1b_flag, uint32_t l1q_embed_offset,
        uint32_t embed_split_size, uint32_t round_embed_split_size, uint32_t embed_split_idx,
        uint32_t head_split_loop, uint32_t n_loop, uint32_t n_idx,
        bool is_l0b_pingpong_off, bool is_l0c_pingpong_off)
    {
        uint32_t cMatrixInit = (embed_split_idx == 0) ? 1 : 0;
        uint32_t loop_mad = (group_num == 1) ? head_num_move : 1;
        for (uint32_t headdim_idx = 0; headdim_idx < loop_mad; headdim_idx++) {
            bool move_l0b_flag = move_l1b_flag;
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
                WAIT_FLAG(M, MTE1, l0_pingpong_flag + 2);
                uint64_t l1kv_int8_offset = l1_offset * 2 + (uint64_t)headdim_idx * round_k * qk_round_n;
                uint64_t l1kscale_int8_offset =  l1_scale_offset + (uint64_t)headdim_idx * round_k;
                uint64_t l1koffset_int8_offset = k_bias_flag ? l1_bias_offset + (uint64_t)headdim_idx * round_k : 0;
                DeQuant(l1e_buf_addr_tensor,
                        l1kv_buf_int8_addr_tensor[l1kv_int8_offset],
                        l1scale_buf_addr_tensor[l1kscale_int8_offset],
                        l1bias_buf_addr_tensor[l1koffset_int8_offset],
                        qk_n, __k, k_bias_flag, l0_pingpong_flag);
                SET_FLAG(FIX, MTE1, l0_pingpong_flag);
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag);
            }
            WAIT_FLAG(M, MTE1, l0_pingpong_flag);
            uint64_t l1q_offset = 0;
            uint32_t q_load_coeff = 1;
            if (!is_multi_head_mmad) {
                l1q_offset = split_idx * head_split_num_move * round_k + 
                             headdim_idx * round_k + l1q_embed_offset;
            } else {
                l1q_offset = split_idx * group_num_move * T_BLOCK_SIZE + l1q_embed_offset * cur_head_num_round;
                q_load_coeff = cur_head_num_round;
            }
            if (q_load_coeff == 1) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0a_buf_tensor[l0_offset],
                    l1q_buf_addr_tensor[l1q_offset],
                    0,
                    (round_embed_split_size + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                                    // srcStride
                    0,
                    0                                                    // dstStride
                );
            } else {
                for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[l0_offset + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                        l1q_buf_addr_tensor[l1q_offset + loa_load_idx * T_CUBE_MATRIX_SIZE],
                        0,
                        round_embed_split_size / T_BLOCK_SIZE,                                 // repeat
                        0,
                        q_load_coeff / BLOCK_SIZE,                            // srcStride
                        0,
                        0                                                     // dstStride
                    );
                }
            }
            if (headdim_idx == loop_mad - 1 && n_idx == n_loop - 1 &&
                split_idx == head_split_loop - 1 && embed_split_idx == embed_split_loop_qk - 1) {
                SET_FLAG(MTE1, MTE2, EVENT_ID7);
            }
            uint32_t mad_l0b_offset = 0;
            if (group_num == 1 || tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (is_l0b_pingpong_off) {
                    mad_l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else {
                    mad_l0b_offset = l0_offset;
                }
                l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0b_buf_tensor[mad_l0b_offset],
                    l1kv_buf_addr_tensor[move_l1b_offset + headdim_idx * round_embed_split_size * qk_round_n],
                    0,
                    round_embed_split_size * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                        // srcStride
                    0,
                    0                                        // dstStride
                );
            } else {
                if (is_l0b_pingpong_off) {
                    l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else if (move_l0b_flag) {
                    l0b_pingpong_flag = 1 - l0b_pingpong_flag;
                    l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0 && move_l1b_flag) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (move_l0b_flag) {
                    uint64_t l1kv_offset = move_l1b_offset + headdim_idx * round_embed_split_size * qk_round_n;
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor[l0b_offset],
                        l1kv_buf_addr_tensor[l1kv_offset],
                        0,
                        round_embed_split_size * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                        // srcStride
                        0,
                        0                                        // dstStride
                    );
                }
                mad_l0b_offset = l0b_offset;
            }

            if (headdim_idx == loop_mad - 1) {
                if ((group_num != 1 && move_l1b_flag) || group_num == 1) {
                    SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                }
            }

            SET_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(MTE1, M, l0_pingpong_flag);
            if (embed_split_idx == 0) {
                WAIT_FLAG(FIX, M, l0_pingpong_flag);
            }
            mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm1OutputType, false>(
                mm1_l0c_buf_tensor[l0c_offset],
                l0a_buf_tensor[l0_offset],
                l0b_buf_tensor[mad_l0b_offset],
                m,     // m
                qk_n,  // n
                embed_split_size,   // k
                cMatrixInit      // cmatrixInitVal
            );
            PIPE_BARRIER(M);
            if (is_l0b_pingpong_off) {
                SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
            } else {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (group_num != 1 && move_l0b_flag) {
                        SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                    }
                }
            }
            SET_FLAG(M, MTE1, l0_pingpong_flag);
            
            // copy S to gm
            if (embed_split_idx == embed_split_loop_qk - 1) {
                SET_FLAG(M, FIX, l0_pingpong_flag);
                WAIT_FLAG(M, FIX, l0_pingpong_flag);
                uint64_t s_gm_offset = headdim_idx * group_num_move * qk_round_n;
                l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                    s_gm_tensor[s_gm_offset],
                    mm1_l0c_buf_tensor[l0c_offset],
                    m,           // MSize
                    qk_round_n,  // NSize
                    RoundUp<16>(m), // srcStride
                    qk_round_n  // dstStride_dst_D
                );
                SET_FLAG(FIX, M, l0_pingpong_flag);
            }
            
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                SET_FLAG(M, MTE1, l0_pingpong_flag + 2);
                SET_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
            }
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ProcessQK(
        AscendC::GlobalTensor<mm1CopyType> s_gm_tensor,
        uint32_t qk_n, uint32_t qk_round_n,
        uint32_t head_num_move, uint32_t group_num_move,
        uint32_t head_split_num_move, uint32_t cur_head_num_round,
        uint32_t split_idx, bool move_l1b_flag, bool is_l0b_pingpong_off)
    {
        uint32_t loop_mad = (group_num == 1) ? head_num_move : 1;
        for (uint32_t headdim_idx = 0; headdim_idx < loop_mad; headdim_idx++) {
            bool move_l0b_flag = move_l1b_flag;
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
                WAIT_FLAG(M, MTE1, l0_pingpong_flag + 2);
                uint64_t l1kv_int8_offset = l1_offset * 2 + (uint64_t)headdim_idx * round_k * qk_round_n;
                uint64_t l1kscale_int8_offset =  l1_scale_offset + (uint64_t)headdim_idx * round_k;
                uint64_t l1koffset_int8_offset = k_bias_flag ? l1_bias_offset + (uint64_t)headdim_idx * round_k : 0;
                DeQuant(l1e_buf_addr_tensor,
                        l1kv_buf_int8_addr_tensor[l1kv_int8_offset],
                        l1scale_buf_addr_tensor[l1kscale_int8_offset],
                        l1bias_buf_addr_tensor[l1koffset_int8_offset],
                        qk_n, __k, k_bias_flag, l0_pingpong_flag);
                SET_FLAG(FIX, MTE1, l0_pingpong_flag);
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag);
            }
            WAIT_FLAG(M, MTE1, l0_pingpong_flag);
            uint64_t l1q_offset = 0;
            uint32_t q_load_coeff = 1;
            if (!is_multi_head_mmad) {
                l1q_offset = split_idx * head_split_num_move * round_k + headdim_idx * round_k;
            } else {
                l1q_offset = split_idx * group_num_move * T_BLOCK_SIZE;
                q_load_coeff = cur_head_num_round;
            }
            if (q_load_coeff == 1) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0a_buf_tensor[l0_offset],
                    l1q_buf_addr_tensor[l1q_offset],
                    0,
                    (round_k  + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                                    // srcStride
                    0,
                    0                                                    // dstStride
                );
            } else {
                for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[l0_offset + loa_load_idx * round_k * BLOCK_SIZE],
                        l1q_buf_addr_tensor[l1q_offset + loa_load_idx * T_CUBE_MATRIX_SIZE],
                        0,
                        round_k / T_BLOCK_SIZE,            // repeat
                        0,
                        q_load_coeff / BLOCK_SIZE,                            // srcStride
                        0,
                        0                                                     // dstStride
                    );
                }
            }
            uint32_t mad_l0b_offset = 0;
            if (group_num == 1 || tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (is_l0b_pingpong_off) {
                    mad_l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else {
                    mad_l0b_offset = l0_offset;
                }
                l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0b_buf_tensor[mad_l0b_offset],
                    l1kv_buf_addr_tensor[move_l1b_offset + headdim_idx * round_k * qk_round_n],
                    0,
                    (round_k * qk_round_n) / T_CUBE_MATRIX_SIZE,                   // repeat
                    0,
                    1,                                        // srcStride
                    0,
                    0                                        // dstStride
                );
            } else {
                if (is_l0b_pingpong_off) {
                    l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else if (move_l0b_flag) {
                    l0b_pingpong_flag = 1 - l0b_pingpong_flag;
                    l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0 && move_l1b_flag) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (move_l0b_flag) {
                    uint64_t l1kv_offset = move_l1b_offset + headdim_idx * round_k * qk_round_n;
                    l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0b_buf_tensor[l0b_offset],
                        l1kv_buf_addr_tensor[l1kv_offset],
                        0,
                        round_k * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                        // srcStride
                        0,
                        0                                        // dstStride
                    );
                }
                mad_l0b_offset = l0b_offset;
            }

            if (headdim_idx == loop_mad - 1) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if ((group_num != 1 && move_l1b_flag) || group_num == 1) {
                        SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                    }
                } else {
                    SET_FLAG(MTE1, MTE2, l1_pingpong_flag);
                }
            }

            SET_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(FIX, M, l0_pingpong_flag);
            mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm1OutputType, false>(
                mm1_l0c_buf_tensor[l0c_offset],
                l0a_buf_tensor[l0_offset],
                l0b_buf_tensor[mad_l0b_offset],
                m,     // m
                qk_n,  // n
                __k,   // k
                1      // cmatrixInitVal
            );
            if (is_l0b_pingpong_off) {
                SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
            } else {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (group_num != 1 && move_l0b_flag) {
                        SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                    }
                }
            }
            SET_FLAG(M, MTE1, l0_pingpong_flag);
            SET_FLAG(M, FIX, l0_pingpong_flag);
            WAIT_FLAG(M, FIX, l0_pingpong_flag);
            // copy S to gm
            uint64_t s_gm_offset = headdim_idx * group_num_move * qk_round_n;
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                s_gm_tensor[s_gm_offset],
                mm1_l0c_buf_tensor[l0c_offset],
                m,           // MSize
                qk_round_n,  // NSize
                RoundUp<16>(m), // srcStride
                qk_round_n  // dstStride_dst_D
            );
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                SET_FLAG(M, MTE1, l0_pingpong_flag + 2);
                SET_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
            }
            SET_FLAG(FIX, M, l0_pingpong_flag);
            l0_pingpong_flag = 1 - l0_pingpong_flag;
            l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
            l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ProcessPVMLA(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor,
        AscendC::LocalTensor<IN_DTYPE> l1p_buf_addr_tensor,
        uint32_t qk_n, uint32_t qk_round_n, uint32_t head_num_move, uint32_t group_num_move,
        uint32_t head_split_num_move, uint32_t cur_head_num, uint32_t cur_head_num_round,
        uint32_t split_idx, bool move_l1b_flag, uint32_t softmax_ready_flag,
        uint32_t embed_split_size, uint32_t round_embed_split_size, uint32_t embed_split_idx, uint32_t head_split_loop,
        bool is_l0b_pingpong_off, bool is_l0c_pingpong_off)
    {
        uint32_t loop_mad = (group_num == 1) ? head_num_move : 1;
        // gqa场景没有多head搬移，head_num_move = 1
        for (uint32_t headdim_idx = 0; headdim_idx < loop_mad; headdim_idx++) {
            bool move_l0b_flag = move_l1b_flag;
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
                WAIT_FLAG(M, MTE1, l0_pingpong_flag + 2);
                uint64_t l1kv_int8_offset = l1_offset * 2 + (uint64_t)headdim_idx * round_embed_split_size * qk_round_n / group_num;
                uint64_t l1vscale_int8_offset = l1_scale_offset + (uint64_t)headdim_idx * round_embed_split_size / group_num;
                uint64_t l1voffset_int8_offet = v_bias_flag ? l1_bias_offset + (uint64_t)headdim_idx * round_embed_split_size / group_num : 0;
                DeQuant(l1e_buf_addr_tensor,
                        l1kv_buf_int8_addr_tensor[l1kv_int8_offset],
                        l1scale_buf_addr_tensor[l1vscale_int8_offset],
                        l1bias_buf_addr_tensor[l1voffset_int8_offet],
                         qk_n, __k, v_bias_flag, l0_pingpong_flag);
                SET_FLAG(FIX, MTE1, l0_pingpong_flag);
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag);
            }
            uint32_t mad_l0b_offset = 0;
            AscendC::LoadData2dTransposeParams loadDataParams;
            loadDataParams.dstGap = 0;
            loadDataParams.startIndex = 0;
            loadDataParams.dstFracGap = 0;
            if (group_num == 1 || tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (is_l0b_pingpong_off) {
                    mad_l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else {
                    mad_l0b_offset = l0_offset;
                }
                if(qk_round_n <= round_embed_split_size || tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {// Nz -> nZ
                    loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                    loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                    uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                    loadDataParams.dstGap = dstGap;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[mad_l0b_offset + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[move_l1b_offset +
                                    headdim_idx * round_embed_split_size * qk_round_n / group_num +
                                    l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                } else {
                    loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                    loadDataParams.dstGap = round_embed_split_size / BLOCK_SIZE - 1;
                    loadDataParams.srcStride = 1;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_embed_split_size / T_BLOCK_SIZE; ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[mad_l0b_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[move_l1b_offset +
                                    headdim_idx * round_embed_split_size * qk_round_n / group_num +
                                    l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                }
            } else {
                if (is_l0b_pingpong_off) {
                    l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else if (move_l0b_flag) {
                    l0b_pingpong_flag = 1 - l0b_pingpong_flag;
                    l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0 && move_l1b_flag) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (move_l0b_flag) {
                    uint64_t l1kv_offset = move_l1b_offset + headdim_idx * round_embed_split_size * qk_round_n;
                    if (qk_round_n <= round_embed_split_size || tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) { // Nz -> nZ
                        loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                        loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                        uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                        loadDataParams.dstGap = dstGap;
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / T_BLOCK_SIZE; ++l0b_load_idx) {
                            // 沿 embd 方向搬
                            AscendC::LoadDataWithTranspose(
                                    l0b_buf_tensor[l0b_offset + l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                    l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                    loadDataParams);
                        }
                    } else {
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_embed_split_size / T_BLOCK_SIZE; ++l0b_load_idx) {
                            // 沿 kv_len_blk方向搬
                            loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                            loadDataParams.srcStride = 1;
                            loadDataParams.dstGap = round_embed_split_size / BLOCK_SIZE - 1;
                            AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[l0b_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                                loadDataParams);
                        }
                    }
                }
                mad_l0b_offset = l0b_offset;
            }

            if (headdim_idx == loop_mad - 1) {
                if (group_num != 1 && move_l1b_flag || group_num == 1) {
                    SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                }
            }

            // move p from gm to l1
            if (split_idx == 0 && headdim_idx == 0 && embed_split_idx == 0) {
                WaitFlagDev(softmax_ready_flag);
                WAIT_FLAG(MTE1, MTE2, l1p_pingpong_flag);
                if (!is_multi_head_mmad) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1p_buf_addr_tensor[l1p_start_offset],
                        p_gm_tensor,
                        1,
                        0,
                        0,
                        RoundUp<BLOCK_SIZE>(qk_n) * cur_head_num * T_BLOCK_OFFSET,               // lenBurst
                        0,
                        0
                    );
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1p_buf_addr_tensor[l1p_start_offset],
                        p_gm_tensor,
                        cur_head_num,         // nValue
                        (cur_head_num + 15) / 16 * 16,// dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        qk_round_n,           // dValue
                        0,                     // dstNzMatrixStride, unused
                        RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET           // srcDValue
                    );
                }
            }
            // move p from l1 to l0a
            SET_FLAG(MTE2, MTE1, l0_pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
            WAIT_FLAG(M, MTE1, l0_pingpong_flag);
            uint64_t l1p_offset = l1p_start_offset;
            uint32_t p_load_coeff = 1;
            if (!is_multi_head_mmad) {
                l1p_offset += split_idx * head_split_num_move * RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET +
                     headdim_idx * RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET;

            } else {
                l1p_offset += split_idx * group_num_move * T_BLOCK_SIZE;
                p_load_coeff = cur_head_num_round;
            }
            if (p_load_coeff == 1) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0a_buf_tensor[l0_offset],
                    l1p_buf_addr_tensor[l1p_offset],
                    0,
                    (qk_round_n + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                                       // srcStride
                    0,
                    0                                                        // dstStride
                );
            } else {
                for (uint64_t loa_load_idx = 0; loa_load_idx < p_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[l0_offset + loa_load_idx * qk_round_n * BLOCK_SIZE],
                        l1p_buf_addr_tensor[l1p_offset + loa_load_idx * T_CUBE_MATRIX_SIZE],
                        0,
                        qk_round_n / T_BLOCK_SIZE,                                 // repeat
                        0,
                        p_load_coeff / BLOCK_SIZE,                               // srcStride
                        0,
                        0                                                        // dstStride
                    );
                }
            }

            if (split_idx == head_split_loop - 1 && headdim_idx == loop_mad - 1 && embed_split_idx == embed_split_loop_v - 1) {
                SET_FLAG(MTE1, MTE2, l1p_pingpong_flag);
            }

            SET_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(FIX, M, l0_pingpong_flag);
            mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm2OutputType, false>(
                mm2_l0c_buf_tensor[l0c_offset],
                l0a_buf_tensor[l0_offset],
                l0b_buf_tensor[mad_l0b_offset],
                m,     // m
                embed_split_size,   // n
                qk_n,  // k
                1      // cmatrixInitVal
            );
            if (is_l0b_pingpong_off) {
                SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
            } else {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (group_num != 1 && move_l0b_flag) {
                        SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                    }
                }
            }
            SET_FLAG(M, MTE1, l0_pingpong_flag);
            SET_FLAG(M, FIX, l0_pingpong_flag);
            WAIT_FLAG(M, FIX, l0_pingpong_flag);
            // copy O to gm
            uint64_t o_temp_gm_offset = headdim_idx * group_num_move * round_v;
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm2CopyType, mm2OutputType>(
                o_tmp_gm_tensor[o_temp_gm_offset],
                mm2_l0c_buf_tensor[l0c_offset],
                m,        // MSize
                RoundUp<16>(embed_split_size),  // NSize 32B对齐，防止workspace补齐的位置中有脏数据
                RoundUp<16>(m),       // srcStride
                round_v  // dstStride_dst_D
            );

            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                SET_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
                SET_FLAG(M, MTE1, l0_pingpong_flag + 2);
            }
            SET_FLAG(FIX, M, l0_pingpong_flag);
        }
    }


    __attribute__((always_inline)) inline __aicore__ void ProcessPV(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor,
        AscendC::LocalTensor<IN_DTYPE> l1p_buf_addr_tensor,
        uint32_t qk_n, uint32_t qk_round_n, uint32_t head_num_move, uint32_t group_num_move,
        uint32_t head_split_num_move, uint32_t cur_head_num, uint32_t cur_head_num_round,
        uint32_t split_idx, bool move_l1b_flag, uint32_t softmax_ready_flag, bool is_l0b_pingpong_off)
    {
        uint32_t loop_mad = (group_num == 1) ? head_num_move : 1;
        for (uint32_t headdim_idx = 0; headdim_idx < loop_mad; headdim_idx++) {
            bool move_l0b_flag = move_l1b_flag;
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
                WAIT_FLAG(M, MTE1, l0_pingpong_flag + 2);
                uint64_t l1kv_int8_offset = l1_offset * 2 + (uint64_t)headdim_idx * round_k * qk_round_n / group_num;
                uint64_t l1vscale_int8_offset = l1_scale_offset + (uint64_t)headdim_idx * round_k / group_num;
                uint64_t l1voffset_int8_offet = v_bias_flag ? l1_bias_offset + (uint64_t)headdim_idx * round_k / group_num : 0;
                DeQuant(l1e_buf_addr_tensor,
                        l1kv_buf_int8_addr_tensor[l1kv_int8_offset],
                        l1scale_buf_addr_tensor[l1vscale_int8_offset],
                        l1bias_buf_addr_tensor[l1voffset_int8_offet],
                         qk_n, __k, v_bias_flag, l0_pingpong_flag);
                SET_FLAG(FIX, MTE1, l0_pingpong_flag);
                WAIT_FLAG(FIX, MTE1, l0_pingpong_flag);
            }
            WAIT_FLAG(M, MTE1, l0_pingpong_flag);
            uint32_t mad_l0b_offset = 0;
            AscendC::LoadData2dTransposeParams loadDataParams;
            loadDataParams.dstGap = 0;
            loadDataParams.startIndex = 0;
            loadDataParams.dstFracGap = 0;
            if (group_num == 1 || tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (is_l0b_pingpong_off) {
                    mad_l0b_offset = 0;
                    WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                } else {
                    mad_l0b_offset = l0_offset;
                }
                if(qk_round_n <= round_k || tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {// Nz -> nZ
                    loadDataParams.repeatTimes = round_k / T_BLOCK_SIZE;
                    loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                    uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                    loadDataParams.dstGap = dstGap;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / (T_BLOCK_SIZE); ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[mad_l0b_offset + l0b_load_idx * RoundUp<16>(__k) * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[move_l1b_offset +
                                    headdim_idx * round_k * qk_round_n / group_num +
                                    l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                } else {
                    loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                    loadDataParams.dstGap = round_k / BLOCK_SIZE - 1;
                    loadDataParams.srcStride = 1;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_k / T_BLOCK_SIZE; ++l0b_load_idx) {
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[mad_l0b_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[move_l1b_offset +
                                    headdim_idx * round_k * qk_round_n / group_num +
                                    l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                }
            } else {
                if (is_l0b_pingpong_off) {
                        l0b_offset = 0;
                        WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                 } else if (move_l0b_flag) {
                        l0b_pingpong_flag = 1 - l0b_pingpong_flag;
                        l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
                        WAIT_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                }
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (headdim_idx == 0 && move_l1b_flag) {
                        WAIT_FLAG(MTE2, MTE1, l1b_pingpong_flag + 2);
                    }
                }
                if (move_l0b_flag) {
                    uint64_t l1kv_offset = move_l1b_offset + headdim_idx * round_k * qk_round_n;
                    if (qk_round_n <= round_k || tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {// Nz -> nZ
                        loadDataParams.repeatTimes = round_k / T_BLOCK_SIZE;
                        loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                        uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                        loadDataParams.dstGap = dstGap;
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / T_BLOCK_SIZE; ++l0b_load_idx) {
                            AscendC::LoadDataWithTranspose(
                                    l0b_buf_tensor[l0b_offset + l0b_load_idx * RoundUp<16>(__k) * T_BLOCK_SIZE],
                                    l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                    loadDataParams);
                        }
                    } else {
                        for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_k / T_BLOCK_SIZE; ++l0b_load_idx) {
                        loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                        loadDataParams.srcStride = 1;
                        loadDataParams.dstGap = round_k / BLOCK_SIZE - 1;
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[l0b_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                                loadDataParams);
                        }
                    }
                }
                mad_l0b_offset = l0b_offset;
            }

            if (split_idx == 0 && headdim_idx == 0) {
                WaitFlagDev(softmax_ready_flag);
                if (!is_multi_head_mmad) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1p_buf_addr_tensor,
                        p_gm_tensor,
                        1,
                        0,
                        0,
                        RoundUp<BLOCK_SIZE>(qk_n) * cur_head_num * T_BLOCK_OFFSET,               // lenBurst
                        0,
                        0
                    );
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1p_buf_addr_tensor,
                        p_gm_tensor,
                        cur_head_num,         // nValue
                        (cur_head_num + 15) / 16 * 16,// dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        qk_round_n,           // dValue
                        0,                     // dstNzMatrixStride, unused
                        RoundUp<BLOCK_SIZE>(qk_n) * T_BLOCK_OFFSET           // srcDValue
                    );
                }
            }

            SET_FLAG(MTE2, MTE1, l0_pingpong_flag);
            WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
            uint64_t l1p_offset = 0;
            uint32_t p_load_coeff = 1;
            if (!is_multi_head_mmad) {
                 l1p_offset =  split_idx * head_split_num_move * RoundUp<BLOCK_SIZE>(qk_n)  * T_BLOCK_OFFSET +
                     headdim_idx * RoundUp<BLOCK_SIZE>(qk_n)  * T_BLOCK_OFFSET;
            } else {
                l1p_offset = split_idx * group_num_move * T_BLOCK_SIZE;
                p_load_coeff = cur_head_num_round;
            }
            if (p_load_coeff == 1) {
                l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0a_buf_tensor[l0_offset],
                    l1p_buf_addr_tensor[l1p_offset],
                    0,
                    (qk_round_n + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                                       // srcStride
                    0,
                    0                                                        // dstStride
                );
            } else {
                for (uint64_t loa_load_idx = 0; loa_load_idx < p_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[l0_offset + loa_load_idx * qk_round_n * BLOCK_SIZE],
                        l1p_buf_addr_tensor[l1p_offset + loa_load_idx * T_CUBE_MATRIX_SIZE],
                        0,
                        qk_round_n / T_BLOCK_SIZE,                                 // repeat
                        0,
                        p_load_coeff / BLOCK_SIZE,                               // srcStride
                        0,
                        0                                                        // dstStride
                    );
                }
            }

            if (headdim_idx == loop_mad - 1) {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (group_num != 1 && move_l1b_flag || group_num == 1) {
                        SET_FLAG(MTE1, MTE2, l1b_pingpong_flag + 2);
                    }
                } else {
                    SET_FLAG(MTE1, MTE2, l1_pingpong_flag);
                }
            }

            SET_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(MTE1, M, l0_pingpong_flag);
            WAIT_FLAG(FIX, M, l0_pingpong_flag);
            mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm2OutputType, false>(
                mm2_l0c_buf_tensor[l0c_offset],
                l0a_buf_tensor[l0_offset],
                l0b_buf_tensor[mad_l0b_offset],
                m,     // m
                __k,   // n
                qk_n,  // k
                1      // cmatrixInitVal
            );

            if (is_l0b_pingpong_off) {
                SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
            } else {
                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if (group_num != 1 && move_l0b_flag) {
                        SET_FLAG(M, MTE1, l0b_pingpong_flag + 2);
                    }
                }
            }
            SET_FLAG(M, MTE1, l0_pingpong_flag);
            SET_FLAG(M, FIX, l0_pingpong_flag);
            WAIT_FLAG(M, FIX, l0_pingpong_flag);
            // copy O to gm
            uint64_t o_temp_gm_offset = headdim_idx * group_num_move * RoundUp<16>(__k);
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm2CopyType, mm2OutputType>(
                o_tmp_gm_tensor[o_temp_gm_offset],
                mm2_l0c_buf_tensor[l0c_offset],
                m,        // MSize
                RoundUp<16>(__k),  // NSize
                RoundUp<16>(m),       // srcStride
                RoundUp<16>(__k)  // dstStride_dst_D
            );

            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                SET_FLAG(FIX, MTE1, l0_pingpong_flag + 2);
                SET_FLAG(M, MTE1, l0_pingpong_flag + 2);
            }
            SET_FLAG(FIX, M, l0_pingpong_flag);
            ChangeL0PingPongFlag();
        }
    }



    __attribute__((always_inline)) inline __aicore__ void ChangePingPongFlag() {
        l1_pingpong_flag = 1 - l1_pingpong_flag;
        l1_offset = l1_pingpong_flag * L1_UINT8_BUF_SIZE_DECODER / sizeof(IN_DTYPE);
        l1_scale_offset = l1_pingpong_flag * L1_SCALE_UINT64_SIZE;
        l1_bias_offset = l1_pingpong_flag * L1_OFFSET_INT32_SIZE;
        if (group_num == 1) {
            l1b_pingpong_flag = 1 - l1b_pingpong_flag;
            l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ChangeL0PingPongFlag() {
        if (is_l0c_pingpong_off) {
            l0_pingpong_flag = 0;
            l0_offset = 0;
            l0c_offset = 0;
        } else {
            l0_pingpong_flag = 1 - l0_pingpong_flag;
            l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
            l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
        }

    }
    __attribute__((always_inline)) inline __aicore__ void ChangeWorkSpacePingPongFlag()
    {
        l1p_pingpong_flag = 1 - l1p_pingpong_flag;
        l1p_start_offset = l1p_pingpong_flag * L1_P_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    }
   __aicore__ __attribute__((always_inline)) inline void InnerRunCubeMLA(uint32_t cur_batch, uint32_t start_head, uint32_t cur_head_num, uint32_t head_split_loop,
                                    uint32_t start_kv, uint32_t cur_kv_seqlen, uint32_t offset_tiling, uint32_t group_num_move, uint32_t head_split_num_move)
    {
        uint32_t addr_q_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 5 + offset_tiling));
        uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
        uint64_t q_offset = addr_q_scalar + start_head * embedding_size;

        uint32_t pp_n_scalar = block_size_calc;
        uint32_t sub_n_loop = pp_n_scalar / block_size;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        uint32_t real_n_loop = (cur_kv_seqlen + block_size - 1) / block_size;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);

        uint32_t cur_head_num_round = (cur_head_num + 15) / 16 * 16;
        m = (group_num == 1) ? 1 : group_num_move;
        is_multi_head_mmad = (group_num_move > 1) && (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT);
        bool is_l0b_pingpong_off = (RoundUp<T_BLOCK_SIZE>(block_size_calc) * embed_split_size_qk > (L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE))) ? 1 : 0;
        bool is_l0c_pingpong_off = (RoundUp<T_BLOCK_SIZE>(m) * embed_split_size_v > (L0C_UINT8_BUF_SIZE / 2 / sizeof(float))) ? 1 : 0;
        for (uint32_t n_idx = 0; n_idx < n_loop; n_idx+=2) {
            if (n_idx == (n_loop - 1)) {
                qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
            }
            if ((n_idx + 1) == (n_loop - 1)) {
                qk_n_2 = (cur_kv_seqlen - (n_idx + 1) * pp_n_scalar);
                qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);
            }
            /* ************ CUBE1 stage1  ************* */
            for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {
                bool move_l1b_flag = 1;
                uint32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                        cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                uint64_t hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                uint32_t embed_split_size = embed_split_size_qk;
                uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_qk; ++embed_split_idx) {
                    uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                    uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                    if (embed_split_idx == embed_split_loop_qk - 1) {
                        embed_split_size = embedding_size - embed_split_offset_tight;
                        round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    }
                    uint32_t q_embed_offset = q_offset + embed_split_offset_tight;
                    if (n_idx == 0 && split_idx == 0 && embed_split_idx == 0) {
                        LoadQToL1MLA(q_offset, cur_head_num);
                    }
                    SET_FLAG(MTE2, MTE1, l0_pingpong_flag);

                    LoadKVToL1MLA(
                        k_gm_tensor[hiddenSize_offset + embed_split_offset_tight],
                        l1kv_buf_addr_tensor,
                        move_l1b_flag,
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, qk_round_n,
                        real_n_loop, sub_n_loop, n_idx, embed_split_size, stride_kv
                    );
                    WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
                    ProcessQKMLA(
                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                        split_idx * head_split_num_move * group_num_move * qk_round_n],
                        qk_n, qk_round_n, head_num_move, group_num_move,
                        head_split_num_move, cur_head_num_round, split_idx, move_l1b_flag,
                        embed_split_offset,embed_split_size, round_embed_split_size, embed_split_idx,
                        head_split_loop, n_loop, n_idx,
                        is_l0b_pingpong_off, is_l0c_pingpong_off
                    );
                }
                ChangePingPongFlag();
            }
            FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_DECODER);
            /* ************ CUBE1 stage2  ************* */
            if (n_idx + 1 < n_loop) {
                for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {  // for head
                    bool move_l1b_flag = 1;
                    uint32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                            cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                    uint64_t hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                    uint32_t embed_split_size = embed_split_size_qk;
                    uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_qk; ++embed_split_idx) {
                        uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                        uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                        if (embed_split_idx == embed_split_loop_qk - 1) {
                            embed_split_size = embedding_size - embed_split_offset_tight;
                            round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                        }
                        uint32_t q_embed_offset = q_offset + embed_split_offset_tight;
                        SET_FLAG(MTE2, MTE1, l0_pingpong_flag);
                        LoadKVToL1MLA(
                            k_gm_tensor[hiddenSize_offset + embed_split_offset_tight],
                            l1kv_buf_addr_tensor,
                            move_l1b_flag,
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv, qk_round_n_2,
                            real_n_loop, sub_n_loop, (n_idx + 1), embed_split_size, stride_kv
                        );
                        WAIT_FLAG(MTE2, MTE1, l0_pingpong_flag);
                        ProcessQKMLA(
                            s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                                split_idx * head_split_num_move * group_num_move * qk_round_n_2 +
                                TMP_SIZE_DECODER / 2],
                            qk_n_2, qk_round_n_2, head_num_move, group_num_move,
                            head_split_num_move, cur_head_num_round, split_idx, move_l1b_flag,
                            embed_split_offset, 
                            embed_split_size, round_embed_split_size, embed_split_idx,
                            head_split_loop, n_loop, (n_idx + 1), is_l0b_pingpong_off, is_l0c_pingpong_off
                        );
                    }
                    ChangePingPongFlag();
                }
                FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_STAGE2);
            }
            /* ************ CUBE2 stage1  ************* */
            for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {
                int32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                        cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                bool move_l1b_flag = 1;
                uint64_t hiddenSize_offset = 
                        (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                uint32_t embed_split_size = embed_split_size_v;
                uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                    uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                    uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        embed_split_size = embedding_size_v - embed_split_offset_tight;
                        round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    }
                    LoadKVToL1MLA(
                        k_gm_tensor[hiddenSize_offset + embed_split_offset_tight],
                        l1kv_buf_addr_tensor,
                        move_l1b_flag,
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<T_BLOCK_SIZE>(qk_round_n),
                        real_n_loop, sub_n_loop, n_idx, embed_split_size, stride_kv
                    );
                    ProcessPVMLA(
                        o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 +
                                        split_idx * head_split_num_move * group_num_move * round_v +
                                        embed_split_offset],
                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET],
                        l1p_buf_addr_tensor,
                        qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), head_num_move, group_num_move,
                        head_split_num_move, cur_head_num, cur_head_num_round,
                        split_idx, move_l1b_flag, SOFTMAX_READY_DECODER,
                        embed_split_size, round_embed_split_size, embed_split_idx,
                            head_split_loop, is_l0b_pingpong_off, is_l0c_pingpong_off
                    );
                }
                ChangePingPongFlag();
            }
            ChangeWorkSpacePingPongFlag();
            FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_DECODER);
            /* ************ CUBE2 stage2  ************* */
            if (n_idx + 1 < n_loop) {
                for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {
                    int32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                            cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                    bool move_l1b_flag = 1;
                    uint64_t hiddenSize_offset =
                            (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                    uint32_t embed_split_size = embed_split_size_v;
                    uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                    for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {              
                        uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                        uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                        if (embed_split_idx == embed_split_loop_v - 1) {
                            embed_split_size = embedding_size_v - embed_split_offset_tight;
                            round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                        }
                        LoadKVToL1MLA(
                            k_gm_tensor[hiddenSize_offset + embed_split_offset_tight],
                            l1kv_buf_addr_tensor,
                            move_l1b_flag,
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<T_BLOCK_SIZE>(qk_round_n_2),
                            real_n_loop, sub_n_loop, (n_idx + 1), embed_split_size, stride_kv
                        );
                        ProcessPVMLA(
                            o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 +
                                        split_idx * head_split_num_move * group_num_move * round_v +
                                        TMP_SIZE +
                                        embed_split_offset],
                            p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET  + TMP_SIZE * T_BLOCK_OFFSET / 2],
                            l1p_buf_addr_tensor,
                            qk_n_2, RoundUp<T_BLOCK_SIZE>(qk_round_n_2), head_num_move, group_num_move,
                            head_split_num_move, cur_head_num, cur_head_num_round,
                            split_idx, move_l1b_flag, SOFTMAX_READY_STAGE2,
                            embed_split_size, round_embed_split_size, embed_split_idx,
                            head_split_loop, is_l0b_pingpong_off, is_l0c_pingpong_off
                        );
                    }
                    ChangePingPongFlag();
                }
                ChangeWorkSpacePingPongFlag();
                FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_STAGE2);
            }
        }
    }



    __aicore__ __attribute__((always_inline)) inline void InnerRunCube(uint32_t cur_batch, uint32_t start_head, uint32_t cur_head_num, uint32_t head_split_loop,
                                    uint32_t start_kv, uint32_t cur_kv_seqlen, uint32_t offset_tiling, uint32_t group_num_move, uint32_t head_split_num_move)
    {
        uint32_t addr_q_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 5 + offset_tiling));
        uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
        uint64_t q_offset = addr_q_scalar + start_head * embedding_size;

        uint32_t pp_n_scalar = block_size_calc;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        sub_n_loop = pp_n_scalar / block_size;
        real_n_loop = (cur_kv_seqlen + block_size - 1) / block_size;
        
        if constexpr (SplitBlock) {
            sub_n_loop = (pp_n_scalar + block_size - 1) / block_size;
            real_n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        }
        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);

        uint32_t cur_head_num_round = (cur_head_num + 15) / 16 * 16;
        m = (group_num == 1) ? 1 : group_num_move;
        is_multi_head_mmad = (group_num_move > 1) && (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT);
        bool is_l0b_pingpong_off = (RoundUp<T_BLOCK_SIZE>(block_size_calc) * round_k > (L0AB_UINT8_BUF_SIZE  /  sizeof(IN_DTYPE))) ? 1 : 0;
        for (uint32_t n_idx = 0; n_idx < n_loop; n_idx += 2) {  // for k_seqlen
            if (n_idx == (n_loop - 1)) {
                qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
            }
            if ((n_idx + 1) == (n_loop - 1)) {
                qk_n_2 = (cur_kv_seqlen - (n_idx + 1) * pp_n_scalar);
                qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);
            }
            /* ************ CUBE1 stage1  ************* */
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                WaitFlagDev(VEC_DEQ_K0_READY);
            }
            for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {  // for head
                bool move_l1b_flag = (split_idx == 0) || ((start_head + split_idx * group_num_move) % group_num) == 0;
                // Only need load Q once
                uint32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                        cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                if (n_idx == 0 && split_idx == 0) {
                    LoadQToL1(q_offset, cur_head_num);
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                        // move diag matrix
                        gm_to_l1<ArchType::ASCEND_V220, int8_t, DataFormat::ND, DataFormat::ND>(
                            l1e_buf_addr_tensor, eye_gm_tensor, 1, 0, 0, 32 * 32, 0, 0);
                    }
                }

                if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                   SET_FLAG(MTE2, MTE1, l1_pingpong_flag);
                }
                // *** Prepare K to L1
                uint64_t hiddenSize_offset =
                    (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;


                uint64_t deq_scale1_hiddenSize_offset = hiddenSize_offset;
                uint64_t offset1_hiddenSize = k_bias_flag ? hiddenSize_offset : 0;
                if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
                        uint32_t razor_start_head = (cur_batch * q_heads + start_head) % q_head_original;
                        deq_scale1_hiddenSize_offset = (razor_start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                        offset1_hiddenSize = k_bias_flag ? deq_scale1_hiddenSize_offset : 0;
                    }
                    LoadKVToL1Int8(
                        k_gm_tensor[hiddenSize_offset],
                        l1kv_buf_int8_addr_tensor[l1_offset * 2],
                        deq_scale1_gm_tensor[deq_scale1_hiddenSize_offset],
                        l1scale_buf_addr_tensor[l1_scale_offset],
                        bias1_gm_tensor[offset1_hiddenSize],
                        l1bias_buf_addr_tensor[l1_bias_offset],
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<32>(qk_n),
                        real_n_loop, sub_n_loop, n_idx
                    );
                    SET_FLAG(MTE2, MTE1, l1_pingpong_flag);
                } else if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                    // [qk_n, cur_head_num, head_size] -> [qk_n, head_num_move, head_size]
                    LoadKVToL1(gm_k16_ping_[hiddenSize_offset], // src
                               l1kv_buf_addr_tensor,            // dst
                               move_l1b_flag,                   // for gqa
                               head_num_move,                   // num_head
                               qk_n,                            // seq_len
                               qk_round_n,                      // seq_len_round
                               cur_head_num);
                } else {
                    if constexpr (dataShapeType == DataShapeType::BNSD) {
                        hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) /
                                            group_num * embedding_size * block_size;
                    }
                    LoadKVToL1(
                        k_gm_tensor[hiddenSize_offset],
                        l1kv_buf_addr_tensor,
                        move_l1b_flag,
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, qk_round_n,
                        real_n_loop, sub_n_loop, n_idx
                    );
                }
                WAIT_FLAG(MTE2, MTE1, l1_pingpong_flag);

                ProcessQK(
                    s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                         split_idx * head_split_num_move * group_num_move * qk_round_n],
                    qk_n, qk_round_n, head_num_move, group_num_move,
                    head_split_num_move, cur_head_num_round,split_idx, move_l1b_flag, is_l0b_pingpong_off);
                ChangePingPongFlag();
            }
            FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_DECODER);

            /* ************ CUBE1 stage2  ************* */
            if (n_idx + 1 < n_loop) {
                if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                    WaitFlagDev(VEC_DEQ_K1_READY);
                }
                for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {  // for head
                    bool move_l1b_flag = (split_idx == 0) || ((start_head + split_idx * group_num_move) % group_num) == 0;
                    // Only need load Q once
                    uint32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                            cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;

                    if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
                        SET_FLAG(MTE2, MTE1, l1_pingpong_flag);
                    }
                    // *** Prepare K to L1
                    uint64_t hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                    uint64_t deq_scale1_hiddenSize_offset = hiddenSize_offset;
                    uint64_t offset1_hiddenSize = k_bias_flag ? hiddenSize_offset : 0;
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                        if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
                            uint32_t razor_start_head = (cur_batch * q_heads + start_head) % q_head_original;
                            deq_scale1_hiddenSize_offset = (razor_start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                            offset1_hiddenSize = k_bias_flag ? deq_scale1_hiddenSize_offset : 0;
                        }
                        LoadKVToL1Int8(
                            k_gm_tensor[hiddenSize_offset],
                            l1kv_buf_int8_addr_tensor[l1_offset * 2],
                            deq_scale1_gm_tensor[deq_scale1_hiddenSize_offset],
                            l1scale_buf_addr_tensor[l1_scale_offset],
                            bias1_gm_tensor[offset1_hiddenSize],
                            l1bias_buf_addr_tensor[l1_bias_offset],
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<32>(qk_n_2),
                            real_n_loop, sub_n_loop, (n_idx + 1)
                        );
                        SET_FLAG(MTE2, MTE1, l1_pingpong_flag);
                    } else if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                        LoadKVToL1(gm_k16_pong_[hiddenSize_offset], // src
                                   l1kv_buf_addr_tensor,            // dst
                                   move_l1b_flag,                   // for gqa
                                   head_num_move,                   // num_head
                                   qk_n_2,                          // seq_len
                                   qk_round_n_2,                    // seq_len_round
                                   cur_head_num);
                    } else {
                        if constexpr (dataShapeType == DataShapeType::BNSD) {
                            hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) /
                                                group_num * embedding_size * block_size;
                        }
                        LoadKVToL1(
                            k_gm_tensor[hiddenSize_offset],
                            l1kv_buf_addr_tensor,
                            move_l1b_flag,
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv, qk_round_n_2,
                            real_n_loop, sub_n_loop, (n_idx + 1)
                        );
                    }
                    WAIT_FLAG(MTE2, MTE1, l1_pingpong_flag);
                    // s_gm ping pong 不共用
                    ProcessQK(
                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER +
                            split_idx * head_split_num_move * group_num_move * qk_round_n_2 +
                            TMP_SIZE_DECODER / 2],
                        qk_n_2, qk_round_n_2, head_num_move, group_num_move,
                        head_split_num_move, cur_head_num_round, split_idx, move_l1b_flag, is_l0b_pingpong_off);
                    ChangePingPongFlag();
                }
                FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_STAGE2);
            }

            /* ************ CUBE2 stage1  ************* */
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                WaitFlagDev(VEC_DEQ_V0_READY);
            }            
            for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {
                int32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                        cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                bool move_l1b_flag = (split_idx == 0) || ((start_head + split_idx * group_num_move) % group_num) == 0;
                // *** Prepare V to L1
                uint64_t hiddenSize_offset =
                    (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;

                uint64_t deq_scale2_hiddenSize_offset = hiddenSize_offset;
                uint64_t offset2_hiddenSize = v_bias_flag ? hiddenSize_offset : 0;
                if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                    if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
                        uint32_t razor_start_head = (cur_batch * q_heads + start_head) % q_head_original;
                        deq_scale2_hiddenSize_offset = (razor_start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                        offset2_hiddenSize = v_bias_flag ? deq_scale2_hiddenSize_offset : 0;
                    }
                    LoadKVToL1Int8(
                        v_gm_tensor[hiddenSize_offset],
                        l1kv_buf_int8_addr_tensor[l1_offset * 2],
                        deq_scale2_gm_tensor[deq_scale2_hiddenSize_offset],
                        l1scale_buf_addr_tensor[l1_scale_offset],
                        bias2_gm_tensor[offset2_hiddenSize],
                        l1bias_buf_addr_tensor[l1_bias_offset],
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<32>(qk_n),
                        real_n_loop, sub_n_loop, n_idx
                    );
                } else if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                    LoadKVToL1(gm_v16_ping_[hiddenSize_offset], // src
                               l1kv_buf_addr_tensor,            // dst
                               move_l1b_flag,                   // for gqa
                               head_num_move,                   // num_head
                               qk_n,                            // seq_len
                               qk_round_n,                      // seq_len_round
                               cur_head_num);
                } else {
                    if constexpr (dataShapeType == DataShapeType::BNSD) {
                        hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) /
                                            group_num * embedding_size * block_size;
                    }
                    LoadKVToL1(
                        v_gm_tensor[hiddenSize_offset],
                        l1kv_buf_addr_tensor,
                        move_l1b_flag,
                        head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<T_BLOCK_SIZE>(qk_round_n),
                        real_n_loop, sub_n_loop, n_idx
                    );
                }
                SET_FLAG(MTE2, MTE1, l1_pingpong_flag);
                WAIT_FLAG(MTE2, MTE1, l1_pingpong_flag);
                ProcessPV(
                    o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE +
                        split_idx * head_split_num_move * group_num_move * RoundUp<16>(__k)],
                    p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET],
                    l1p_buf_addr_tensor,
                    qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), head_num_move, group_num_move,
                    head_split_num_move, cur_head_num, cur_head_num_round,
                    split_idx, move_l1b_flag, SOFTMAX_READY_DECODER, is_l0b_pingpong_off);
                ChangePingPongFlag();
            }
            FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_DECODER);

            /* ************ CUBE2 stage2  ************* */
            if (n_idx + 1 < n_loop) {
                if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                    WaitFlagDev(VEC_DEQ_V1_READY);
                }
                for (uint32_t split_idx = 0; split_idx < head_split_loop; split_idx++) {
                    int32_t head_num_move = ((group_num_move == 1) && (split_idx == (head_split_loop - 1))) ?
                            cur_head_num - head_split_num_move * split_idx * group_num_move : head_split_num_move;
                    bool move_l1b_flag = (split_idx == 0) || ((start_head + split_idx * group_num_move) % group_num) == 0;
                    // *** Prepare V to L1
                    uint64_t hiddenSize_offset =
                        (start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;

                    uint64_t deq_scale2_hiddenSize_offset = hiddenSize_offset;
                    uint64_t offset2_hiddenSize = v_bias_flag ? hiddenSize_offset : 0;
                    if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) {
                        if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
                            uint32_t razor_start_head = (cur_batch * q_heads + start_head) % q_head_original;
                            deq_scale2_hiddenSize_offset = (razor_start_head + split_idx * head_split_num_move * group_num_move) / group_num * embedding_size;
                            offset2_hiddenSize = v_bias_flag ? deq_scale2_hiddenSize_offset : 0;
                        }
                        LoadKVToL1Int8(
                            v_gm_tensor[hiddenSize_offset],
                            l1kv_buf_int8_addr_tensor[l1_offset * 2],
                            deq_scale2_gm_tensor[deq_scale2_hiddenSize_offset],
                            l1scale_buf_addr_tensor[l1_scale_offset],
                            bias2_gm_tensor[offset2_hiddenSize],
                            l1bias_buf_addr_tensor[l1_bias_offset],
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv, RoundUp<32>(qk_n_2),
                            real_n_loop, sub_n_loop, (n_idx + 1)
                        );
                    } else if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                        LoadKVToL1(gm_v16_pong_[hiddenSize_offset], // src
                                   l1kv_buf_addr_tensor,            // dst
                                   move_l1b_flag,                   // for gqa
                                   head_num_move,                   // num_head
                                   qk_n_2,                          // seq_len
                                   qk_round_n_2,                    // seq_len_round
                                   cur_head_num);
                    } else {
                        if constexpr (dataShapeType == DataShapeType::BNSD) {
                            hiddenSize_offset = (start_head + split_idx * head_split_num_move * group_num_move) /
                                                group_num * embedding_size * block_size;
                        }
                        LoadKVToL1(
                            v_gm_tensor[hiddenSize_offset],
                            l1kv_buf_addr_tensor,
                            move_l1b_flag,
                            head_num_move, cur_batch, cur_kv_seqlen, start_kv,  RoundUp<T_BLOCK_SIZE>(qk_round_n_2),
                            real_n_loop, sub_n_loop, (n_idx + 1)
                        );
                    }
                    SET_FLAG(MTE2, MTE1, l1_pingpong_flag);
                    WAIT_FLAG(MTE2, MTE1, l1_pingpong_flag);
                    ProcessPV(
                        o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE +
                            split_idx * head_split_num_move * group_num_move * RoundUp<16>(__k) +
                            TMP_SIZE / 2],
                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET  + TMP_SIZE * T_BLOCK_OFFSET / 2],
                        l1p_buf_addr_tensor[qk_round_n * cur_head_num_round],
                        qk_n_2, RoundUp<T_BLOCK_SIZE>(qk_round_n_2), head_num_move, group_num_move,
                        head_split_num_move, cur_head_num, cur_head_num_round,
                        split_idx, move_l1b_flag, SOFTMAX_READY_STAGE2, is_l0b_pingpong_off);
                    ChangePingPongFlag();
                }
                FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_STAGE2);
            }
        }
    }


private:
    __gm__ IN_DTYPE *__restrict__ q_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ ctkv_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ k_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ v_gm{nullptr};

    __gm__ mmScaleType *__restrict__ deq_scale1_gm{nullptr};
    __gm__ mmScaleType *__restrict__ deq_scale2_gm{nullptr};
    __gm__ mmBiasType *__restrict__ offset1_gm{nullptr};
    __gm__ mmBiasType *__restrict__ offset2_gm{nullptr};
    __gm__ int8_t *__restrict__ eye_gm{nullptr};

    __gm__ mm1CopyType *__restrict__ s_gm{nullptr};
    __gm__ IN_DTYPE *__restrict__ p_gm{nullptr};
    __gm__ mm2CopyType *__restrict__ o_tmp_gm{nullptr};
    __gm__ int32_t *__restrict__ block_tables_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_gm{nullptr};
    __gm__ float *__restrict__ razor_offset_gm{nullptr};

    AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> q_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> k_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> v_gm_tensor;
    AscendC::GlobalTensor<mm1CopyType> s_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor;
    AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<mmScaleType> deq_scale1_gm_tensor;
    AscendC::GlobalTensor<mmScaleType> deq_scale2_gm_tensor;
    AscendC::GlobalTensor<mmBiasType> bias1_gm_tensor;
    AscendC::GlobalTensor<mmBiasType> bias2_gm_tensor;
    AscendC::GlobalTensor<int8_t> eye_gm_tensor;
    AscendC::GlobalTensor<int32_t> block_tables_gm_tensor;

    AscendC::GlobalTensor<IN_DTYPE> gm_k16_ping_;
    AscendC::GlobalTensor<IN_DTYPE> gm_k16_pong_;
    AscendC::GlobalTensor<IN_DTYPE> gm_v16_ping_;
    AscendC::GlobalTensor<IN_DTYPE> gm_v16_pong_;

    const uint32_t l1q_buf_addr_offset = (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) ? (L1_E_UINT8_SIZE + L1_SCALE_UINT8_SIZE * 2 + L1_OFFSET_UINT8_SIZE * 2) : 0;

    const uint32_t l1p_buf_addr_offset = (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) ? (L1_E_UINT8_SIZE + L1_SCALE_UINT8_SIZE * 2 + L1_OFFSET_UINT8_SIZE * 2 + 2 * L0AB_UINT8_BLOCK_SIZE)
                                                     : (5 * L0AB_UINT8_BLOCK_SIZE);
    const uint32_t l1kv_buf_int8_addr_offset = L1_E_UINT8_SIZE + L1_SCALE_UINT8_SIZE*2 + L1_OFFSET_UINT8_SIZE*2 + 4 * L0AB_UINT8_BLOCK_SIZE;
    const uint32_t l1kv_buf_addr_offset = (tilingKeyType == TilingKeyType::TILING_INT8_CUBE_QUANT) ? (L1_E_UINT8_SIZE + L1_SCALE_UINT8_SIZE * 2 + L1_OFFSET_UINT8_SIZE * 2 + 4 * L0AB_UINT8_BLOCK_SIZE)
                                                      : (7 * L0AB_UINT8_BLOCK_SIZE);
    const uint32_t l1e_buf_addr_offset = 0;
    const uint32_t l1scale_buf_addr_offset = L1_E_UINT8_SIZE;
    const uint32_t l1bias_buf_addr_offset = L1_E_UINT8_SIZE + L1_SCALE_UINT8_SIZE * 2;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<IN_DTYPE> l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1q_buf_addr_offset);

    AscendC::LocalTensor<IN_KVDTYPE> l1kv_buf_int8_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_KVDTYPE>(l1kv_buf_int8_addr_offset);
    AscendC::LocalTensor<IN_DTYPE> l1kv_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1kv_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1p_buf_addr_offset);
    AscendC::LocalTensor<int8_t> l1e_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, int8_t>(l1e_buf_addr_offset);
    AscendC::LocalTensor<mmScaleType> l1scale_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mmScaleType>(l1scale_buf_addr_offset);
    AscendC::LocalTensor<mmBiasType> l1bias_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mmBiasType>(l1bias_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, IN_DTYPE>(0);
    AscendC::LocalTensor<IN_DTYPE> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, IN_DTYPE>(0);
    AscendC::LocalTensor<mm1OutputType> mm1_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm1OutputType>(0);
    AscendC::LocalTensor<mm2OutputType> mm2_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm2OutputType>(0);
    AscendC::LocalTensor<int32_t> l0c_buf_int32_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, int32_t>(0);

    
    uint32_t k_bias_flag{0};
    uint32_t v_bias_flag{0};
    uint32_t num_tokens{0};
    uint32_t q_heads{0};
    uint32_t kv_heads{0};
    uint32_t embedding_size{0};
    uint32_t embedding_size_v{0};
    uint32_t block_size{0};
    uint32_t max_num_blocks_per_query{0};
    uint32_t group_num{0};
    uint32_t former_group_num_move{1};
    uint32_t tail_group_num_move{1};
    uint32_t former_head_split_num{1};
    uint32_t tail_head_split_num{1};
    uint32_t stride_kv{0};
    uint32_t stride_vo{0};
    uint32_t m{0};
    uint32_t __k{0};
    uint32_t __v{0};
    uint32_t round_k{0};
    uint32_t round_v{0};
    uint32_t core_per_batch{0};
    uint32_t process_num{0};
    uint64_t former_batch{0};
    uint32_t former_head_split{0};
    uint64_t tail_batch{0};
    uint32_t tail_head_split{0};
    uint32_t head_split_num{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t kv_split_per_core{0};
    uint32_t kv_split_core_num{1};
    uint32_t block_size_calc{0};

    uint32_t embed_split_size_qk{0};
    uint32_t embed_split_loop_qk{1};
    uint32_t embed_split_size_v{0};
    uint32_t embed_split_loop_v{1};
    bool is_multi_head_mmad{0};
    uint32_t move_l1b_offset = 0;
    uint32_t q_head_original{0};
    uint32_t compressHead{0};

    uint32_t l1_pingpong_flag = 0;
    uint32_t l1b_pingpong_flag = 0;
    uint32_t l0_pingpong_flag = 0;
    uint32_t l0b_pingpong_flag = 0;
    uint32_t l1p_pingpong_flag = 0;
    uint32_t block_size_inner_count{0};
    uint32_t sub_n_loop{0};
    uint32_t real_n_loop{0};

    uint32_t l1_offset = l1_pingpong_flag * L1_UINT8_BUF_SIZE_DECODER / sizeof(IN_DTYPE);
    uint32_t l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l1_scale_offset = l1_pingpong_flag * L1_SCALE_UINT64_SIZE;
    uint32_t l1_bias_offset = l1_pingpong_flag * L1_OFFSET_INT32_SIZE;
    uint32_t l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
    uint32_t l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l1p_start_offset = l1p_pingpong_flag * L1_P_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    bool is_l0c_pingpong_off = 0;
    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;

};
#elif __DAV_C220_VEC__
enum class ScaleType {
        SCALE_TOR = 0,
        SCALE_LOGN = 1,
        SCALE_LOGN_FP32 = 2
};

template <TilingKeyType tilingKeyType = TilingKeyType::TILING_HALF_DATA, typename IN_DTYPE = half, typename OUT_DTYPE = half, bool SplitKV = false, PagedAttnVariant pagedAttnVariant = PagedAttnVariant::DEFAULT, CompressType compressType = CompressType::COMPRESS_TYPE_UNDEFINED>
class UnpadAttentionDecoderAiv{
public:
    using mm1OutputType = typename AttentionType<tilingKeyType>::mm1OutputType;
    using mm1CopyType = typename AttentionType<tilingKeyType>::mm1CopyType;
    using mmBiasType = typename AttentionType<tilingKeyType>::mmBiasType;
    using mmScaleType = typename AttentionType<tilingKeyType>::mmScaleType;
    using mm2OutputType = typename AttentionType<tilingKeyType>::mm2OutputType;
    using mm2CopyType = typename AttentionType<tilingKeyType>::mm2CopyType;
    static constexpr uint32_t T_BLOCK_SIZE =  BLOCK_SIZE_32 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_OFFSET = 2 / sizeof(IN_DTYPE);

    __aicore__ __attribute__((always_inline)) inline UnpadAttentionDecoderAiv(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    __aicore__ __attribute__((always_inline)) inline void InitQuant(
        __gm__ uint8_t *__restrict__ deq_scale1_in_gm,
        __gm__ uint8_t *__restrict__ offset1_in_gm,
        __gm__ uint8_t *__restrict__ deq_scale2_in_gm,
        __gm__ uint8_t *__restrict__ offset2_in_gm,
        __gm__ uint8_t *__restrict__ scale_in_gm
    )
    {
        deq_scale1_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(deq_scale1_in_gm));
        deq_scale2_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(deq_scale2_in_gm));
        scale_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(scale_in_gm));
        if (pQuantType == 3) {
            pQuantOnline = 1;
        }
    }

    __aicore__ __attribute__((always_inline)) inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t* __restrict__ gm_k8,
        __gm__ uint8_t* __restrict__ gm_v8,
        __gm__ uint8_t* __restrict__ gm_scale1,
        __gm__ uint8_t* __restrict__ gm_offset1,
        __gm__ uint8_t* __restrict__ gm_scale2,
        __gm__ uint8_t* __restrict__ gm_offset2,
        __gm__ uint8_t* __restrict__ gm_block_table,
        __gm__ uint8_t *__restrict__ mask_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t *__restrict__ globalo_gm,
        __gm__ uint8_t *__restrict__ o_core_out_tmp_gm,
        __gm__ uint8_t *__restrict__ l_in_gm,
        __gm__ uint8_t* __restrict__ gm_k16,
        __gm__ uint8_t* __restrict__ gm_v16,
        __gm__ uint8_t *__restrict__ tiling_para_gm,
        __gm__ uint8_t *__restrict__ razorOffset,
        __gm__ uint8_t *__restrict__ logN_in_gm)
    {
        SetFftsBaseAddr((uint64_t)sync);
        sub_block_idx = static_cast<uint64_t>(GetSubBlockidx());
        SetAtomicnone();
        SetMasknorm();
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);

        mask_gm = reinterpret_cast<__gm__ OUT_DTYPE *>(mask_in_gm);
        o_gm = reinterpret_cast<__gm__ OUT_DTYPE *>(o_out_gm);
        s_gm = reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm);
        p_gm = reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm);
        o_tmp_gm = reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm);
        go_gm = reinterpret_cast<__gm__ float *>(globalo_gm);
        o_core_tmp_gm = reinterpret_cast<__gm__ float *>(o_core_out_tmp_gm);
        tiling_gm = reinterpret_cast<__gm__ uint8_t *>(tiling_para_gm);
        l_gm = reinterpret_cast<__gm__ float *>(l_in_gm);
        razor_offset_gm = reinterpret_cast<__gm__ float *>(razorOffset);
        gm_block_tables_ = reinterpret_cast<__gm__ int32_t*>(gm_block_table);
        logN_gm = reinterpret_cast<__gm__ float *>(logN_in_gm);
        mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ OUT_DTYPE *>(mask_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ OUT_DTYPE *>(o_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1CopyType *>(s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2CopyType *>(o_tmp_gm));
        go_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(go_gm));
        o_core_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_core_tmp_gm));
        l_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(l_gm));
        razor_offset_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(razor_offset_gm));

        num_tokens = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM));
        embedding_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V));
        block_size = (int32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ uint32_t*)tiling_para_gm + TILING_MAXBLOCKS));
        tor = (float)(*((__gm__ float *)tiling_para_gm + TILING_TOR));
        num_kv_heads = (uint32_t)(*((__gm__ uint32_t*)tiling_para_gm + TILING_KVHEADS));
        former_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_BATCH));
        former_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_HEAD));
        tail_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_BATCH));
        tail_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_HEAD));
        max_context_len = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MASK_MAX_LEN));
        batch_stride = (uint64_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BATCH_STRIDE));
        head_stride = (uint64_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEAD_STRIDE));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        group_num = (uint32_t)(*((__gm__ uint32_t*)tiling_para_gm + TILING_GROUPNUM));

        kv_split_per_core = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVSPLIT));
        kv_split_core_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVCORENUM));
        block_size_calc = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE_CALC));
        q_head_original = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QHEADORIGINAL));
        compressHead = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_COMPRESSHEAD));
        pQuantType = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QUANTYPE));
        scaleType = (ScaleType)(*((__gm__ uint32_t *)tiling_para_gm + TILING_SCALETYPE));
        
        if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
            former_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_GROUP_MOVE));
            tail_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_GROUP_MOVE));
        }

        go_flag_scalar = 1;
        gl_flag_scalar = 1;

        modCoef = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MODCOEF));
        divCoef = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_DIVCOEF));

        __k = embedding_size;
        round_k = RoundUp<T_BLOCK_SIZE>(__k);
        if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
            __v = embedding_size_v;
            round_v = RoundUp<BLOCK_SIZE>(__v);
            embed_split_size_v_former = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT_VECTOR_FORMER));
            embed_split_loop_v_former = (embedding_size_v + embed_split_size_v_former - 1) / embed_split_size_v_former;
            embed_split_size_v_tail = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT_VECTOR_TAIL));
            embed_split_loop_v_tail = (embedding_size_v + embed_split_size_v_tail - 1) / embed_split_size_v_tail;
        }

        core_per_batch = (q_heads + split_size - 1) / split_size;
        process_num = num_tokens * core_per_batch;
        if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
            gm_k8_.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(gm_k8));
            gm_v8_.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(gm_v8));
            gm_k16_ping_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_k16) +
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_k16_pong_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_k16) +
                                    get_block_num() * block_size_calc * q_heads * embedding_size + 
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_v16_ping_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_v16) +
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_v16_pong_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_v16) + 
                                    get_block_num() * block_size_calc * q_heads * embedding_size + 
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_block_tables_ = reinterpret_cast<__gm__ int32_t*>(gm_block_table);
            gm_scale1_.SetGlobalBuffer(reinterpret_cast<__gm__ float*>(gm_scale1));
            gm_offset1_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t*>(gm_offset1));
            gm_scale2_.SetGlobalBuffer(reinterpret_cast<__gm__ float*>(gm_scale2));
            gm_offset2_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t*>(gm_offset2));
            if (gm_offset1 != nullptr) {
                k_bias_flag = 1;
            }
            if (gm_offset2 != nullptr) {
                v_bias_flag = 1;
            }
        }
    }

    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        SET_FLAG(MTE3, V, EVENT_ID0);
        SET_FLAG(MTE3, V, EVENT_ID1);
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID2);
        SET_FLAG(MTE3, MTE2, EVENT_ID3);
        SET_FLAG(MTE3, MTE2, EVENT_ID4);
        SET_FLAG(V, MTE2, EVENT_ID4);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, V, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID2);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT){
            SET_FLAG(MTE2, V, EVENT_ID3);
            SET_FLAG(MTE2, V, EVENT_ID4);
        }
        core_per_batch = (q_heads + former_head_split - 1) / former_head_split;
        process_num = static_cast<uint64_t>(former_batch) * core_per_batch * kv_split_core_num;
        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {
            uint32_t cur_batch = process / (core_per_batch * kv_split_core_num) + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
            tor = (float)(*((__gm__ float *)tiling_gm + TILING_TOR));
            if(scaleType == ScaleType::SCALE_LOGN_FP32) {
                float tor_logN = (float)(*((__gm__ float *)logN_gm + cur_batch));
                tor = tor * tor_logN;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t cur_head = (process / kv_split_core_num) % core_per_batch;
            uint32_t cur_nIndx = process % kv_split_core_num;
            uint32_t start_head = cur_head * former_head_split;
            uint32_t cur_kv_seqlen = kv_split_per_core;
            uint32_t kv_loop = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            if (cur_nIndx >= kv_loop) {
                continue;
            }
            if (cur_nIndx == (kv_loop - 1)) {
                cur_kv_seqlen = kv_seqlen - cur_nIndx * kv_split_per_core;
            }
            uint32_t cur_head_num = former_head_split;
            if (cur_head == (core_per_batch - 1)) {
                cur_head_num = q_heads - cur_head * former_head_split;
            }
            InnerRunVector(batch_idx, start_head, cur_nIndx, cur_kv_seqlen, cur_head_num, offset_tiling, kv_seqlen, embed_split_size_v_former, embed_split_loop_v_former);
        }
        if (tail_batch > 0) {
            core_per_batch = (q_heads + tail_head_split - 1) / tail_head_split;
            process_num = static_cast<uint64_t>(tail_batch) * core_per_batch;
            for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {
                uint32_t cur_batch = process / core_per_batch + former_batch + prefill_batch_size_;
                uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
                uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
                if (kv_seqlen == 0) {
                    continue;
                }
                tor = (float)(*((__gm__ float *)tiling_gm + TILING_TOR));
                if(scaleType == ScaleType::SCALE_LOGN_FP32) {
                    float tor_logN = (float)(*((__gm__ float *)logN_gm + cur_batch));
                    tor = tor * tor_logN;
                }
                uint32_t cur_kv_seqlen = kv_seqlen;
                uint32_t cur_nIndx = 0;
                uint32_t cur_head = process % core_per_batch;
                uint32_t cur_head_num = tail_head_split;
                if (cur_head == (core_per_batch - 1)) {
                    cur_head_num = q_heads - cur_head * tail_head_split;
                }
                uint32_t start_head = (process % core_per_batch) * tail_head_split;
                InnerRunVector(batch_idx, start_head, cur_nIndx, cur_kv_seqlen, cur_head_num, offset_tiling, kv_seqlen, embed_split_size_v_tail, embed_split_loop_v_tail);
            }
        }
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        WAIT_FLAG(MTE3, V, EVENT_ID1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID4);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE3, V, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT){
            WAIT_FLAG(MTE2, V, EVENT_ID3);
            WAIT_FLAG(MTE2, V, EVENT_ID4);
        }
        PIPE_BARRIER(ALL);
        if (SplitKV) {
            int reduce_flag_id = 3;
            FftsCrossCoreSync<PIPE_MTE3, 0>(reduce_flag_id);
            WaitFlagDev(3);
            CombineScale(decoder_batch_size_, q_heads, kv_split_core_num, embedding_size);
        }
    }


private:

    __aicore__ __attribute__((always_inline)) inline void CopyScale(uint32_t sub_m, uint32_t l_offset, uint32_t o_offset)
    {
        SET_FLAG(V, MTE3, EVENT_ID2);
        WAIT_FLAG(V, MTE3, EVENT_ID2);
        ub_to_gm_align<ArchType::ASCEND_V220, float>(
            l_gm_tensor[(int64_t)l_offset],
            tv32_ubuf_tensor,
            0,               // sid
            sub_m,           // nBurst
            4,               // lenBurst
            0,               // leftPaddingNum
            0,               // rightPaddingNum
            0,                 // srcGap
            (kv_split_core_num - 1) * 4 // dstGap
        );
        if (gl_flag_scalar == 0) {
            SET_FLAG(MTE3, V, EVENT_ID2);
            gl_flag_scalar = 1;
        }
        uint32_t src_gap = ((__k % 16 <= 8) && (__k % 16 > 0))? 1 : 0;
        ub_to_gm_align<ArchType::ASCEND_V220, float>(
            o_core_tmp_gm_tensor[(int64_t)o_offset],
            go32_ubuf_tensor,
            0,        // sid
            sub_m,    // nBurst
            __k * 4,  // lenBurst
            0,        // leftPaddingNum
            0,        // rightPaddingNum
            src_gap,   // srcGap
            (kv_split_core_num - 1) * __k * 4  // dstGap
        );
    }
    __aicore__ __attribute__((always_inline)) inline void CombineScale(uint32_t num_tokens, uint32_t q_heads, uint32_t kv_split_core_num, uint32_t embedding_size)
    {
        SetAtomicnone();
        SetMasknorm();
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        const uint32_t ll_ubuf_stage2_offset = 0;  // 1 块，存放 存放local L fp32
        const uint32_t lm_ubuf_stage2_offset = 1 * STAGE2_UB_UINT8_BLOCK_SIZE;  //  1 块，存放 l max, fp32
        const uint32_t tl_ubuf_stage2_offset = 1 * STAGE2_UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE; // 1 块，存放中间结果tmp l, fp32
        const uint32_t rs_ubuf_stage2_offset = 2 * STAGE2_UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE; // 存放中间结果row sum, fp32
        const uint32_t ts_ubuf_stage2_offset = 2 * STAGE2_UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE; // 存放中间结果row sum, fp32
        const uint32_t gl_ubuf_stage2_offset = 2 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE; // 存放gloal scale, fp32
        const uint32_t lo_ubuf_stage2_offset = 4 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
        const uint32_t to_ubuf_stage2_offset = 8 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
        const uint32_t go_ubuf_stage2_offset = 12 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
        const uint32_t go16_ubuf_stage2_offset = 16 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;

        AscendC::LocalTensor<float> ll_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_stage2_offset);  // 1 块，存放 存放local L fp32
        AscendC::LocalTensor<float> lm_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm_ubuf_stage2_offset);  //  1 块，存放 l max, fp32
        AscendC::LocalTensor<float> tl_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tl_ubuf_stage2_offset); // 1 块，存放中间结果tmp l, fp32
        AscendC::LocalTensor<float> rs_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(rs_ubuf_stage2_offset); // 存放中间结果row sum, fp32
        AscendC::LocalTensor<float> ts_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ts_ubuf_stage2_offset); // 存放中间结果row sum, fp32
        AscendC::LocalTensor<float> gl_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_stage2_offset); // 存放gloal scale, fp32
        AscendC::LocalTensor<float> lo_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_stage2_offset);
        AscendC::LocalTensor<float> to_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(to_ubuf_stage2_offset);
        AscendC::LocalTensor<float> go_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_stage2_offset);
        AscendC::LocalTensor<OUT_DTYPE> go16_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(go16_ubuf_stage2_offset);

        uint32_t batch_size = num_tokens;
        uint32_t split_block = 1;
        uint32_t __k0 = embedding_size;
        uint32_t roundk_64 = (__k0 + 63) / 64 * 64;
        uint32_t roundk_8 = (__k0 + 7) / 8 * 8;
        uint32_t core_per_batch = (q_heads + split_block - 1) / split_block;

        uint32_t process_num = core_per_batch * batch_size;
        SET_FLAG(MTE3,MTE2,EVENT_ID0);
        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)){
            uint32_t cur_batch = process / core_per_batch + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 6 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 7 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
            uint32_t addr_o_fd_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 15 + offset_tiling));
            uint32_t addr_o_fd_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 16 + offset_tiling));
            uint64_t addr_o_fd_scalar = (uint64_t)(((uint64_t)addr_o_fd_high32) << 32 | addr_o_fd_loww32);
            uint32_t addr_l_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 11 + offset_tiling));
            uint32_t addr_l_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 12 + offset_tiling));
            uint64_t addr_l_scalar = (uint64_t)(((uint64_t)addr_l_high32) << 32 | addr_l_loww32);

            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            uint32_t m_split = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            uint32_t cur_core = process % core_per_batch;
            uint32_t cur_head_num = split_block; // 每次计算的head数量
            if (cur_core == (core_per_batch - 1)){
                cur_head_num = q_heads - cur_core * split_block;
            }
            uint32_t start_head = cur_core * split_block;
            uint64_t addr_l_offset = addr_l_scalar;
            uint64_t addr_o_offset = addr_o_fd_scalar * kv_split_core_num;
            uint32_t l_remain = m_split % FLOAT_BLOCK_SIZE;
            WAIT_FLAG(MTE3,MTE2,EVENT_ID0);
            gm_to_ub_align<ArchType::ASCEND_V220, float>(
                ll_ubuf_stage2_tensor,
                l_gm_tensor[addr_l_offset + start_head * kv_split_core_num],
                0,                            // sid
                1,                            // nBurst
                m_split * 4,                  // lenBurst
                0,                           // leftPaddingNum
                FLOAT_BLOCK_SIZE - l_remain,  // rightPaddingNum
                0,                           // srcGap
                0   // dstGap
            );

            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);

            __set_mask(m_split);
            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_stage2_tensor,
                ll_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,                              // repeat
                1,                                           // dstRepeatStride
                1,                                           // srcBlockStride
                8                                            // srcRepeatStride
            );
            PIPE_BARRIER(V);

            // lse_accum - lse_max
            SET_FLAG(V, S, EVENT_ID3);
            WAIT_FLAG(V, S, EVENT_ID3);
            float lse_max = -(float)(*((__ubuf__ float*)lm_ubuf_stage2_tensor.GetPhyAddr()));
            SET_FLAG(S, V, EVENT_ID2);
            WAIT_FLAG(S, V, EVENT_ID2);
            adds_v<ArchType::ASCEND_V220, float>(tl_ubuf_stage2_tensor,
                ll_ubuf_stage2_tensor,
                lse_max,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                               // src0RepeatStride
            );
            PIPE_BARRIER(V);

            // expf
            exp_v<ArchType::ASCEND_V220, float>(tl_ubuf_stage2_tensor,
                tl_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);

            // rowsum lse_sum
            cadd_v<ArchType::ASCEND_V220, float>(rs_ubuf_stage2_tensor,
                tl_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,    // repeat
                1,                                           // dstRepeatStride
                1,                                           // srcBlockStride
                8                                            // srcRepeatStride
            );
            PIPE_BARRIER(V);
            __set_mask(cur_head_num);
            ln_v<ArchType::ASCEND_V220, float>(rs_ubuf_stage2_tensor,
                rs_ubuf_stage2_tensor,
                (cur_head_num + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,     // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);

            // logf(lse_sum) + lse_max
            add_v<ArchType::ASCEND_V220, float>(ts_ubuf_stage2_tensor,
                rs_ubuf_stage2_tensor,
                lm_ubuf_stage2_tensor,
                (cur_head_num + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,      // repeat
                1,                                // dstBlockStride
                1,                                // src0BlockStride
                1,                                // src1BlockStride
                8,                                // dstRepeatStride
                8,                                // src0RepeatStride
                8                                 // src1RepeatStride
            );
            PIPE_BARRIER(V);

            // scale = expf(lse_accum(l) - lse_logsum)
            __set_mask(m_split);
            SET_FLAG(V, S, EVENT_ID3);
            WAIT_FLAG(V, S, EVENT_ID3);
            float log_sum = -(float)(*((__ubuf__ float*)ts_ubuf_stage2_tensor.GetPhyAddr()));
            SET_FLAG(S, V, EVENT_ID2);
            WAIT_FLAG(S, V, EVENT_ID2);
            adds_v<ArchType::ASCEND_V220, float>(gl_ubuf_stage2_tensor,
                ll_ubuf_stage2_tensor,
                log_sum,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,           // dstBlockStride
                1,           // src0BlockStride
                8,           // dstRepeatStride
                8            // src0RepeatStride
            );
            PIPE_BARRIER(V);

            __set_mask(m_split);
            exp_v<ArchType::ASCEND_V220, float>(gl_ubuf_stage2_tensor,
                gl_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);
            // msplit * 1 * embedding
            gm_to_ub_align<ArchType::ASCEND_V220, float>(
                lo_ubuf_stage2_tensor,
                o_core_tmp_gm_tensor[addr_o_offset + start_head * kv_split_core_num * __k0],
                0,                                           // sid
                m_split,                                     // nBurst
                __k0 * 4,                                    // lenBurst
                0,                                           // leftPaddingNum
                0,                                           // rightPaddingNum
                0,                                           // srcGap
                0                                            // dstGap
            );
            SET_FLAG(MTE2, V, EVENT_ID1);
            WAIT_FLAG(MTE2, V, EVENT_ID1);
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            for (uint32_t n_idx = 0; n_idx < m_split; n_idx++){
                SET_FLAG(V, S, EVENT_ID3);
                WAIT_FLAG(V, S, EVENT_ID3);
                float scale = (float)(*((__ubuf__ float*)gl_ubuf_stage2_tensor.GetPhyAddr() + n_idx));
                SET_FLAG(S, V, EVENT_ID2);
                WAIT_FLAG(S, V, EVENT_ID2);

                muls_v<ArchType::ASCEND_V220, float>(to_ubuf_stage2_tensor,
                    lo_ubuf_stage2_tensor[n_idx * roundk_8],
                    scale,
                    (roundk_64 + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                    1,                                                   // dstBlockStride
                    1,                                                   // srcBlockStride
                    8,                                                               // dstRepeatStride
                    8                                                                // srcRepeatStride
                );
                PIPE_BARRIER(V);

                if (n_idx == 0){
                    adds_v<ArchType::ASCEND_V220, float>(go_ubuf_stage2_tensor,
                        to_ubuf_stage2_tensor,
                        0,
                        roundk_64 / FLOAT_VECTOR_SIZE,  // repeat
                        1,           // dstBlockStride
                        1,           // src0BlockStride
                        8,           // dstRepeatStride
                        8           // src0RepeatStride
                    );
                    PIPE_BARRIER(V);

                }
                else{
                    add_v<ArchType::ASCEND_V220, float>(go_ubuf_stage2_tensor,
                        to_ubuf_stage2_tensor,
                        go_ubuf_stage2_tensor,
                        roundk_64 / FLOAT_VECTOR_SIZE, // repeat
                        1,                          // dstBlockStride
                        1,                          // src0BlockStride
                        1,                          // src1BlockStride
                        8,                          // dstRepeatStride
                        8,                          // src0RepeatStride
                        8                           // src1RepeatStride
                    );
                    PIPE_BARRIER(V);

                }
            }
            conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(go16_ubuf_stage2_tensor,
                go_ubuf_stage2_tensor,
                roundk_64 / FLOAT_VECTOR_SIZE,   // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                4,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);
            SET_FLAG(V, MTE3, EVENT_ID1);
            WAIT_FLAG(V, MTE3, EVENT_ID1);
            ub_to_gm_align<ArchType::ASCEND_V220, OUT_DTYPE>(
                o_gm_tensor[addr_o_scalar + start_head * __k0],
                go16_ubuf_stage2_tensor,
                0,                       // sid
                1,                       // nBurst
                __k0 * 2,                // lenBurst
                0,                       // leftPaddingNum
                0,                       // rightPaddingNum
                0,                       // srcGap
                0                        // dstGap
            );
            SET_FLAG(MTE3,MTE2,EVENT_ID0);
        }
        WAIT_FLAG(MTE3,MTE2,EVENT_ID0);
    }

    __aicore__ __attribute__((always_inline)) inline void AddMask(
        AscendC::GlobalTensor<OUT_DTYPE> mask_gm_tensor,
        AscendC::LocalTensor<OUT_DTYPE> mask_ubuf_tensor,
        AscendC::LocalTensor<float> mask32_ubuf_tensor,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t mask_offset)
    {
        uint32_t mask_repeat_stride = head_stride == 0 ? 0 : qk_round_n / FLOAT_BLOCK_SIZE;
        uint32_t mask_nburst = head_stride == 0 ? 1 : sub_m;
        gm_to_ub_align<ArchType::ASCEND_V220, OUT_DTYPE>(
            mask_ubuf_tensor,
            mask_gm_tensor,
            0,                                 // sid
            mask_nburst,                             // nBurst
            qk_n * 2,                          // lenBurst
            0,                                 // leftPaddingNum
            0,                                 // rightPaddingNum
            (max_context_len - qk_n) * 2,      // srcGap
            0                                  // dstGap
        );
        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        conv_v<ArchType::ASCEND_V220, OUT_DTYPE, float>(mask32_ubuf_tensor,
            mask_ubuf_tensor,
            (mask_nburst * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
            1,                                                         // dstBlockStride
            1,                                                         // srcBlockStride
            8,                                                         // dstRepeatStride
            4                                                          // srcRepeatStride
        );
        PIPE_BARRIER(V);
        // *** ls = ls + mask
        if (qk_round_n  > FLOAT_BLOCK_SIZE * 255) {
            for (uint32_t vadd_idx = 0; vadd_idx < sub_m; ++vadd_idx){
                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * qk_round_n],
                    ls32_ubuf_tensor[vadd_idx * qk_round_n],
                    mask32_ubuf_tensor[vadd_idx * mask_repeat_stride * FLOAT_BLOCK_SIZE],
                    qk_n / FLOAT_VECTOR_SIZE,       // repeat
                    1,                              // dstBlockStride
                    1,                              // src0BlockStride
                    1,                              // src1BlockStride
                    8,                              // dstRepeatStride
                    8,                               // src0RepeatStride
                    8                               // src1RepeatStride
                );
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                uint32_t offset = qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE;
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                for (uint32_t vadd_idx = 0; vadd_idx < sub_m; ++vadd_idx) {
                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * qk_round_n + offset],
                        ls32_ubuf_tensor[vadd_idx * qk_round_n + offset],
                        mask32_ubuf_tensor[vadd_idx * qk_round_n + offset],
                        1,                               // repeat
                        1,                              // dstBlockStride
                        1,                              // src0BlockStride
                        1,                              // src1BlockStride
                        1,                              // dstRepeatStride
                        1,                              // src0RepeatStride
                        1                               // src1RepeatStride
                    );
                }
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        } else {
            for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                    ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                    mask32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                    sub_m,                          // repeat
                    1,                              // dstBlockStride
                    1,                              // src0BlockStride
                    1,                              // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    mask_repeat_stride                               // src1RepeatStride
                );
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    mask32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    sub_m,                          // repeat
                    1,                              // dstBlockStride
                    1,                              // src0BlockStride
                    1,                              // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    mask_repeat_stride                               // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        }
        PIPE_BARRIER(V);
    }

   __aicore__ __attribute__((always_inline)) inline void ReduceMaxRepeatN(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& tempTensor,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        ub_to_ub<ArchType::ASCEND_V220, float>(
            tempTensor,
            src,
            0,                                             // sid
            sub_m,                                         // nBurst
            HALF_VECTOR_SIZE / BLOCK_SIZE,                 // lenBurst
            (qk_round_n - FLOAT_VECTOR_SIZE) / FLOAT_BLOCK_SIZE,  // srcGap
            0                                              // dstGap
        );
        PIPE_BARRIER(V);
       
        for (uint32_t rowmax_idx = 0; rowmax_idx < sub_m; ++rowmax_idx) {
            max_v<ArchType::ASCEND_V220, float>(
                tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                src[FLOAT_VECTOR_SIZE + rowmax_idx * qk_round_n],
                tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                (qk_n - FLOAT_VECTOR_SIZE) / FLOAT_VECTOR_SIZE , // repeat
                1,                             // dstBlockStride
                1,                             // src0BlockStride
                1,                             // src1BlockStride
                0,                             // dstRepeatStride
                8,                             // src0RepeatStride
                0                              // src1RepeatStride
            );

        }
        PIPE_BARRIER(V);
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            for (uint32_t rowmax_idx = 0; rowmax_idx < sub_m ; ++rowmax_idx) {
                max_v<ArchType::ASCEND_V220, float>(
                    tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                    src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE + rowmax_idx * qk_round_n],
                    tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                    1,                              // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    1,                             // dstRepeatStride
                    1,                             // src0RepeatStride
                    1                               // src1RepeatStride
                );
            }
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
        cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(
            dst,
            tempTensor,
            sub_m,      // repeat
            1,          // dstRepeatStride
            1,          // srcBlockStride
            8           // srcRepeatStride
        );
        PIPE_BARRIER(V);
    }

   __aicore__ __attribute__((always_inline)) inline void ReduceMaxRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& tempTensor,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        if (qk_n <= FLOAT_VECTOR_SIZE) {
            __set_mask(qk_n);
            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(dst,
                src,
                sub_m,                    // repeat
                1,                        // dstRepeatStride
                1,                        // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
            );
        } else {
            ub_to_ub<ArchType::ASCEND_V220, float>(
                tempTensor,
                src,
                0,                                             // sid
                sub_m,                                         // nBurst
                HALF_VECTOR_SIZE / BLOCK_SIZE,                 // lenBurst
                (qk_round_n - FLOAT_VECTOR_SIZE) / FLOAT_BLOCK_SIZE,  // srcGap
                0                                              // dstGap
            );
            PIPE_BARRIER(V);
            for (uint32_t rowmax_idx = 1; rowmax_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowmax_idx) {
                max_v<ArchType::ASCEND_V220, float>(
                    tempTensor,
                    tempTensor,
                    src[rowmax_idx * FLOAT_VECTOR_SIZE],
                    sub_m,                         // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    8,                             // dstRepeatStride
                    8,                             // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                );
            PIPE_BARRIER(V);
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                max_v<ArchType::ASCEND_V220, float>(
                    tempTensor,
                    tempTensor,
                    src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    sub_m,                         // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    8,                             // dstRepeatStride
                    8,                             // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                );
            }
            PIPE_BARRIER(V);
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(
                dst,
                tempTensor,
                sub_m,      // repeat
                1,          // dstRepeatStride
                1,          // srcBlockStride
                8           // srcRepeatStride
            );
        }
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void ReduceSumRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        if (qk_n <= FLOAT_VECTOR_SIZE) {
            __set_mask(qk_n);
            cadd_v<ArchType::ASCEND_V220, float>(
                dst,
                src,
                sub_m,           // repeat
                1,               // dstRepeatStride
                1,               // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        } else {
            for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                add_v<ArchType::ASCEND_V220, float>(
                    src,
                    src,
                    src[rowsum_idx * FLOAT_VECTOR_SIZE],
                    sub_m,           // repeat
                    1,               // dstBlockStride
                    1,               // src0BlockStride
                    1,               // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE   // src1RepeatStride
                );
                PIPE_BARRIER(V);
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                add_v<ArchType::ASCEND_V220, float>(
                    src,
                    src,
                    src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    sub_m,           // repeat
                    1,               // dstBlockStride
                    1,               // src0BlockStride
                    1,               // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE   // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
            
            cadd_v<ArchType::ASCEND_V220, float>(
                dst,
                src,
                sub_m,           // repeat
                1,               // dstRepeatStride
                1,               // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
            );
        }
    }

    __aicore__ __attribute__((always_inline)) inline void TensorSubValueRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& MaxTensor,
        const AscendC::LocalTensor<float>& tempMaxTensor,
        uint32_t sub_m,
        uint32_t round_sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        brcb_v<ArchType::ASCEND_V220, uint32_t>(
            tempMaxTensor.ReinterpretCast<uint32_t>(),
            MaxTensor.ReinterpretCast<uint32_t>(),
            1,               // dstBlockStride
            8,               // dstRepeatStride
            round_sub_m / FLOAT_BLOCK_SIZE  // repeat
        );
        PIPE_BARRIER(V);
        for (uint32_t sub_v_idx = 0; sub_v_idx < qk_n / FLOAT_VECTOR_SIZE; ++sub_v_idx) {
            sub_v<ArchType::ASCEND_V220, float>(dst[sub_v_idx * FLOAT_VECTOR_SIZE],
                src[sub_v_idx * FLOAT_VECTOR_SIZE],
                tempMaxTensor,
                sub_m,                    // repeat
                1,                        // dstBlockStride
                1,                        // src0BlockStride
                0,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                1                         // src1RepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            sub_v<ArchType::ASCEND_V220, float>(dst[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                tempMaxTensor,
                sub_m,                    // repeat
                1,                        // dstBlockStride
                1,                        // src0BlockStride
                0,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                1                         // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void TensorDivRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& src1,
        uint32_t sub_m, uint32_t qk_n, uint32_t qk_round_n)
    {
        PIPE_BARRIER(V);
        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
            div_v<ArchType::ASCEND_V220, float>(dst[vadd_idx * FLOAT_VECTOR_SIZE],
                src[vadd_idx * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                  // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                0,                                     // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // src0RepeatStride
                1                                       // src1RepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            div_v<ArchType::ASCEND_V220, float>(dst[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                   // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                0,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,         // src0RepeatStride
                1                                      // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void TensorMulRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& src1,
        uint32_t sub_m, uint32_t qk_n, uint32_t qk_round_n, uint32_t src1BlockStride
    ) {
        PIPE_BARRIER(V);
        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
            mul_v<ArchType::ASCEND_V220, float>(dst[vadd_idx * FLOAT_VECTOR_SIZE],
                src[vadd_idx * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                  // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                src1BlockStride,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // src0RepeatStride
                1                                       // src1RepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            mul_v<ArchType::ASCEND_V220, float>(dst[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                   // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                src1BlockStride,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,         // src0RepeatStride
                1                                      // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void QuantPerTokenImpl(
        const AscendC::LocalTensor<IN_DTYPE>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& scale,
        uint32_t sub_m, uint32_t qk_n, uint32_t qk_round_n, uint32_t pQuantOnline)
    {
        
        if (pQuantOnline) {
            // scr / scale 提函数
            TensorDivRepeatM(dst.template ReinterpretCast<float>(), src, scale, sub_m, qk_n, qk_round_n);
        } else {
            // scr * scale
            TensorMulRepeatM(dst.template ReinterpretCast<float>(), src, scale, sub_m, qk_n, qk_round_n, 0);
        }
        // src fp32 -> casttofp16 -> casttoint8
        uint32_t count = sub_m * qk_round_n;
        uint32_t repeat_times = (count + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;
        if (repeat_times < 255) {
            conv_v<ArchType::ASCEND_V220, float, half>(
                dst.template ReinterpretCast<half>(), // dst
                dst.template ReinterpretCast<float>(), // src
                repeat_times,                  // repeat_times
                1,                            // dstBlockStride
                1,                            // srcBlockStride
                4,                            // dstRepeatStride
                8                             // srcRepeatStride
            );
        } else {
            for (uint64_t vconv_idx = 0; vconv_idx < 2; ++vconv_idx) {   // 一次迭代做一半，循环防止超出 repeat 范围（<=255)
                conv_v<ArchType::ASCEND_V220, float, half>(
                    dst.template ReinterpretCast<half>()[vconv_idx * count / 2], // dst
                    dst.template ReinterpretCast<float>()[vconv_idx * count / 2], // src
                    (count / 2 + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,             // repeat_times
                    1,                                                                   // dstBlockStride
                    1,                                                                   // srcBlockStride
                    4,                                                                   // dstRepeatStride
                    8                                                                    // srcRepeatStride
                );
            }
        }
        PIPE_BARRIER(V);
        for (uint32_t row_idx = 0; row_idx < qk_n / HALF_VECTOR_SIZE; ++row_idx) {
            AscendC::Cast<int8_t, half, false>(dst.template ReinterpretCast<int8_t>()[row_idx * HALF_VECTOR_SIZE],
                                               dst.template ReinterpretCast<half>()[row_idx * HALF_VECTOR_SIZE], AscendC::RoundMode::CAST_RINT,
                                               (uint64_t)0, sub_m, {1, 1, (uint8_t)((qk_round_n) / BLOCK_SIZE), (uint8_t)(qk_round_n / BLOCK_SIZE)});
        }
        if (qk_n % HALF_VECTOR_SIZE > 0) {
            __set_mask(qk_n % HALF_VECTOR_SIZE);
            AscendC::Cast<int8_t, half, false>(dst.template ReinterpretCast<int8_t>()[qk_n / HALF_VECTOR_SIZE * HALF_VECTOR_SIZE],
                                               dst.template ReinterpretCast<half>()[qk_n / HALF_VECTOR_SIZE * HALF_VECTOR_SIZE], AscendC::RoundMode::CAST_RINT,
                                               (uint64_t)0, sub_m, {1, 1, (uint8_t)((qk_round_n) / BLOCK_SIZE), (uint8_t)(qk_round_n / BLOCK_SIZE)});
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }
    
    __aicore__ __attribute__((always_inline)) inline void DeQuantPerHeadImpl(
        const AscendC::GlobalTensor<mmScaleType>& deScaleGm,
        const AscendC::GlobalTensor<int32_t>& src,
        AscendC::LocalTensor<float> dst,
        AscendC::LocalTensor<int32_t> temp,
        AscendC::LocalTensor<mmScaleType> deScaleUb,
        AscendC::LocalTensor<mmScaleType> tempScale,
        AscendC::LocalTensor<float> quantScale,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n,
        bool online,
        bool move_tensor
    ){        
        gm_to_ub_align<ArchType::ASCEND_V220, mmScaleType>(deScaleUb,
                                                        deScaleGm,
                                                        0,                                      // sid
                                                        1,                                      // nBurst
                                                        sub_m * sizeof(mmScaleType),             // lenBurst
                                                        0,                                      // leftPaddingNum
                                                        0,                                      // rightPaddingNum
                                                        0,                                      // srcGap
                                                        0                                       // dstGap
        );
        if (online) {
            // if dequant online need mul p quant scale
            SET_FLAG(MTE2, V, EVENT_ID2);
            WAIT_FLAG(MTE2, V, EVENT_ID2);
            TensorMulRepeatM(deScaleUb, deScaleUb, quantScale, 1, sub_m, RoundUp<16>(sub_m), 1);
        }

        if (move_tensor) {
            gm_to_ub<ArchType::ASCEND_V220, int32_t>(
                temp,
                src,
                0,                        // sid
                1,                        // nBurst
                CeilDiv<FLOAT_BLOCK_SIZE>(sub_m * qk_round_n),  // lenBurst
                0,                        // srcGap
                0                         // dstGap
            );
        }
        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);
        brcb_v<ArchType::ASCEND_V220, uint32_t>(
            tempScale.template ReinterpretCast<uint32_t>(),
            deScaleUb.template ReinterpretCast<uint32_t>(),
            1,               // dstBlockStrides
            8,               // dstRepeatStride
            RoundUp<16>(sub_m) / FLOAT_BLOCK_SIZE  // repeat
        );
        PIPE_BARRIER(V);
        uint32_t count = sub_m * qk_round_n;
        uint32_t repeat_times = (count + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;
        if (repeat_times < 255) {
            conv_v<ArchType::ASCEND_V220, int32_t, float>(
                dst, // dst
                temp, // src
                repeat_times,                  // repeat_times
                1,                            // dstBlockStride
                1,                            // srcBlockStride
                8,                            // dstRepeatStride
                8                             // srcRepeatStride
            );
        } else {
            for (uint64_t vconv_idx = 0; vconv_idx < 2; ++vconv_idx) {   // 一次迭代做一半，循环防止超出 repeat 范围（<=255)
                conv_v<ArchType::ASCEND_V220, int32_t, float>(
                    dst[vconv_idx * count / 2], // dst
                    temp[vconv_idx * count / 2], // src
                    (count / 2 + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,             // repeat_times
                    1,                                                                   // dstBlockStride
                    1,                                                                   // srcBlockStride
                    8,                                                                   // dstRepeatStride
                    8                                                                    // srcRepeatStride
                );
            }
        }
        TensorMulRepeatM(dst, dst, tempScale, sub_m, qk_n, qk_round_n, 0);
        PIPE_BARRIER(V);
    }
   __aicore__ __attribute__((always_inline)) inline void SoftmaxStage1(
        AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor,
        AscendC::GlobalTensor<mm1CopyType> s_gm_tensor,
        AscendC::GlobalTensor<OUT_DTYPE> mask_gm_tensor,
        AscendC::LocalTensor<float> dm32_ubuf_tensor,
        AscendC::LocalTensor<float> ll_ubuf_tensor,
        AscendC::LocalTensor<float> pm32_ubuf_tensor,
        uint32_t n_idx,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t sub_m,
        uint32_t mask_offset,
        const uint32_t sub_n_loop,
        const uint32_t cur_batch,
        const uint32_t start_kv,
        const uint32_t real_n_loop,
	    const uint32_t head_idx,
        const uint32_t pm_flag_scalar
    )
    {
        uint32_t sub_m_d128 = (sub_m + 127) / 128;  // up aligned to 128
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 128
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;
        float quantMax = (float)1 / (float)127;
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
            if constexpr (SplitKV) { // softmaxstage1 wait softmaxstage2 
                if (n_idx == 0) {
                    if (gl_flag_scalar == 1) {
                        WAIT_FLAG(MTE3, V, EVENT_ID2);
                        gl_flag_scalar = 0;
                    }
                }
            }
            DeQuantPerHeadImpl(
                deq_scale1_gm_tensor[head_idx],
                s_gm_tensor,
                ls32_ubuf_tensor, lsint32_ubuf_tensor, 
                descale1_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, qk_n, qk_round_n, 0, 1);
        } else {
            gm_to_ub<ArchType::ASCEND_V220, mm1CopyType>(
                ls32_ubuf_tensor.template ReinterpretCast<mm1CopyType>(),
                s_gm_tensor,
                0,                        // sid
                1,                        // nBurst
                sub_m * qk_round_n / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                        // srcGap
                0                         // dstGap
            );
        }

        if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
            if (razor_offset_gm != nullptr) {
                WAIT_FLAG(V, MTE2, EVENT_ID0);
                WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                for (uint32_t ni = 0; ni < sub_n_loop; ++ni) {
                    
                    uint32_t actual_idx = n_idx * sub_n_loop + ni;
                    if (actual_idx >= real_n_loop) {
                        break;
                    }
                    uint32_t block_table_id = (uint32_t)(*(gm_block_tables_ + cur_batch * max_num_blocks_per_query +
                                                    start_kv / block_size + actual_idx));
                    SET_FLAG(S, MTE2, EVENT_ID0);
                    WAIT_FLAG(S, MTE2, EVENT_ID0);
                    gm_to_ub_align<ArchType::ASCEND_V220, float>(
                        mask32_ubuf_tensor[ni * block_size],
                        razor_offset_gm_tensor[(uint64_t)block_table_id * block_size],
                        0,                          // sid
                        1,                          // nBurst
                        block_size * 4,             // lenBurst
                        0,                          // leftPaddingNum
                        0,                          // rightPaddingNum
                        0,                          // srcGap
                        0                           // dstGap
                    );
                }

                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);

                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);

                for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                        ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                        mask32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                        sub_m,                          // repeat
                        1,                              // dstBlockStride
                        1,                              // src0BlockStride
                        1,                              // src1BlockStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                        0                               // src1RepeatStride
                    );
                }
                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        mask32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        sub_m,                          // repeat
                        1,                              // dstBlockStride
                        1,                              // src0BlockStride
                        1,                              // src1BlockStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                        0   // src1RepeatStride
                    );
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                }

                SET_FLAG(V, MTE2, EVENT_ID0);
                SET_FLAG(MTE3, MTE2, EVENT_ID0);
                PIPE_BARRIER(V);
            }
        }

        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);

        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
            muls_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                tor,
                sub_m,                          // repeat
                1,                              // dstBlockStride
                1,                              // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            muls_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                tor,
                sub_m,                          // repeat
                1,                              // dstBlockStride
                1,                              // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);

        if (mask_gm != nullptr) {
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
            AddMask(mask_gm_tensor, mask_ubuf_tensor, mask32_ubuf_tensor, sub_m, qk_n, qk_round_n, mask_offset);
            PIPE_BARRIER(V);
            SET_FLAG(V, MTE2, EVENT_ID0);
        }

        // *** lm = rowmax(ls)
        WAIT_FLAG(MTE3, V, EVENT_ID1);
        ReduceMaxRepeatM(lm32_ubuf_tensor, ls32_ubuf_tensor, lp32_ubuf_tensor, sub_m, qk_n, qk_round_n);
        if (n_idx != 0) {
            // *** hm = vmax(lm, gm)
            max_v<ArchType::ASCEND_V220, float>(hm32_ubuf_tensor,
                lm32_ubuf_tensor,
                gm32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,           // dstBlockStride
                1,           // src0BlockStride
                1,           // src1BlockStride
                8,           // dstRepeatStride
                8,           // src0RepeatStride
                8            // src1RepeatStride
            );
            PIPE_BARRIER(V);
            // *** dm = gm - hm
            sub_v<ArchType::ASCEND_V220, float>(dm32_ubuf_tensor,
                gm32_ubuf_tensor,
                hm32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,           // dstBlockStride
                1,           // src0BlockStride
                1,           // src1BlockStride
                8,           // dstRepeatStride
                8,           // src0RepeatStride
                8            // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            // *** hm = lm
            ub_to_ub<ArchType::ASCEND_V220, float>(
                hm32_ubuf_tensor,
                lm32_ubuf_tensor,
                0,                         // sid
                1,                         // nBurst
                round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                         // srcGap
                0                          // dstGap
            );
            PIPE_BARRIER(V);
        }
        // *** gm = hm
        ub_to_ub<ArchType::ASCEND_V220, float>(
            gm32_ubuf_tensor,
            hm32_ubuf_tensor,
            0,                         // sid
            1,                         // nBurst
            round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
            0,                         // srcGap
            0                          // dstGap
        );
        PIPE_BARRIER(V);
        // *** hm_block = expand_to_block(hm), 存放于 tv
        if constexpr (SplitKV && tilingKeyType != TilingKeyType::TILING_QUANT_FP16OUT && tilingKeyType != TilingKeyType::TILING_QUANT_BF16OUT) {
            if (n_idx == 0) {
                if (gl_flag_scalar == 1) {
                    WAIT_FLAG(MTE3, V, EVENT_ID2);
                    gl_flag_scalar = 0;
                }
            }
        }
        // *** ls = ls - hm_block
        TensorSubValueRepeatM(ls32_ubuf_tensor, ls32_ubuf_tensor,
                           hm32_ubuf_tensor, tv32_ubuf_tensor,
                           sub_m, round_sub_m, qk_n, qk_round_n);
        // *** ls = exp(ls)
        exp_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor,
            ls32_ubuf_tensor,
            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
            1,                               // dstBlockStride
            1,                               // srcBlockStride
            8,                               // dstRepeatStride
            8                                // srcRepeatStride
        );
        PIPE_BARRIER(V);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT){
            if (pm_flag_scalar == 1) {
                WAIT_FLAG(MTE2, V, EVENT_ID3);
            } else {
                WAIT_FLAG(MTE2, V, EVENT_ID4);
            }
            if (pQuantOnline) {
                sub_v<ArchType::ASCEND_V220, float>(pm32_ubuf_tensor,
                    lm32_ubuf_tensor,
                    hm32_ubuf_tensor,
                    sub_m_d64,   // repeat
                    1,           // dstBlockStride
                    1,           // src0BlockStride
                    1,           // src1BlockStride
                    8,           // dstRepeatStride
                    8,           // src0RepeatStride
                    8            // src1RepeatStride
                );
                PIPE_BARRIER(V);
                exp_v<ArchType::ASCEND_V220, float>(pm32_ubuf_tensor,
                    pm32_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,                               // dstBlockStride
                    1,                               // srcBlockStride
                    8,                               // dstRepeatStride
                    8                                // srcRepeatStride
                );
                PIPE_BARRIER(V);
                muls_v<ArchType::ASCEND_V220, float>(pm32_ubuf_tensor,
                    pm32_ubuf_tensor,
                    quantMax,
                    sub_m_d64,              // repeat
                    1,                      // dstBlockStride
                    1,                      // srcBlockStride
                    8,                      // dstRepeatStride
                    8                        // srcRepeatStride
                );
            } else {
                gm_to_ub_align<ArchType::ASCEND_V220, mmScaleType>(pm32_ubuf_tensor,
                                                                scale_gm_tensor[head_idx],
                                                                0,                                      // sid
                                                                1,                                      // nBurst
                                                                sub_m * sizeof(mmScaleType),             // lenBurst
                                                                0,                                      // leftPaddingNum
                                                                0,                                      // rightPaddingNum
                                                                0,                                      // srcGap
                                                                0                                       // dstGap
                );
                SET_FLAG(MTE2, V, EVENT_ID2);
                WAIT_FLAG(MTE2, V, EVENT_ID2);
            }
            PIPE_BARRIER(V);
            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                pm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            QuantPerTokenImpl(lp_ubuf_tensor, ls32_ubuf_tensor, tv32_ubuf_tensor, sub_m, qk_n, qk_round_n, pQuantOnline);
        } else {
            // *** lp = castfp32to16(ls)
            conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(lp_ubuf_tensor,
                ls32_ubuf_tensor,
                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                4,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
        }
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE3, EVENT_ID0);
        WAIT_FLAG(V, MTE3, EVENT_ID0);
        ub_to_gm<ArchType::ASCEND_V220, IN_DTYPE>(
            p_gm_tensor,
            lp_ubuf_tensor,
            0,                        // sid
            1,                        // nBurst
            sub_m * qk_round_n * T_BLOCK_OFFSET / T_BLOCK_SIZE,  // lenBurst
            0,                        // srcGap
            0                         // dstGap
        );
        SET_FLAG(MTE3, V, EVENT_ID1);
        if (mask_gm != nullptr){
            SET_FLAG(MTE3, MTE2, EVENT_ID0);
        }
        // *** ll = rowsum(ls32)
        ReduceSumRepeatM(ll_ubuf_tensor, ls32_ubuf_tensor, sub_m, qk_n, qk_round_n);
        SET_FLAG(V, MTE2, EVENT_ID2);
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void SoftmaxStage2MLA(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<float> go_gm_tensor,
        AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor,
        AscendC::LocalTensor<float> dm32_ubuf_tensor,
        AscendC::LocalTensor<float> ll_ubuf_tensor,
        AscendC::LocalTensor<float> pm32_ubuf_tensor,
        uint32_t n_idx,
        uint32_t n_loop,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t sub_m,
        uint64_t l_offset,
        uint64_t o_offset,
        uint32_t head_idx,
        uint32_t embed_split_size,
        uint32_t round_embed_split_size,       
        uint32_t embed_split_idx,
        uint32_t embed_split_loop_v,
        uint32_t pm_flag_scalar)
    {
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 64
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        if (n_idx != 0) {
            gm_to_ub<ArchType::ASCEND_V220, mm2CopyType>(
                lo_ubuf_tensor.template ReinterpretCast<mm2CopyType>(),
                o_tmp_gm_tensor,
                0,                    // sid
                sub_m,                    // nBurst
                round_embed_split_size / FLOAT_BLOCK_SIZE,  // lenBurst
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE,                    // srcGap
                0                     // dstGap
            );
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);
            if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
                DeQuantPerHeadImpl(
                    deq_scale2_gm_tensor[head_idx],
                    o_tmp_gm_tensor,
                    lo_ubuf_tensor, loint32_ubuf_tensor,// loint32_ubuf_tensor  lo_ubuf_tensor use the same ptr
                    descale2_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, embed_split_size, round_embed_split_size, pQuantOnline, 0);
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        if (pm_flag_scalar == 1) {
                            SET_FLAG(MTE2, V, EVENT_ID3);
                        } else {
                            SET_FLAG(MTE2, V, EVENT_ID4);
                        }
                    }
            }
        }

        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID4);
        if (n_idx != 0) {
            if (embed_split_idx == 0) {
                // *** dm = exp(dm)
                exp_v<ArchType::ASCEND_V220, float>(dm32_ubuf_tensor,
                    dm32_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,          // dstBlockStride
                    1,          // srcBlockStride
                    8,          // dstRepeatStride
                    8           // srcRepeatStride
                );
                PIPE_BARRIER(V);
                // *** gl = dm * gl
                mul_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                    dm32_ubuf_tensor,
                    gl32_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,          // dstBlockStride
                    1,          // src0BlockStride
                    1,          // src1BlockStride
                    8,          // dstRepeatStride
                    8,          // src0RepeatStride
                    8           // src1RepeatStride
                );
                PIPE_BARRIER(V);
                // *** gl = ll + gl
                add_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                    gl32_ubuf_tensor,
                    ll_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,          // dstBlockStride
                    1,          // src0BlockStride
                    1,          // src1BlockStride
                    8,          // dstRepeatStride
                    8,          // src0RepeatStride
                    8           // src1RepeatStride
                );
                PIPE_BARRIER(V);
            }
            // *** dm_block = expand_to_block(dm), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                dm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);

            gm_to_ub<ArchType::ASCEND_V220, float>(
                go32_ubuf_tensor,
                go_gm_tensor,
                0,
                sub_m,
                round_embed_split_size / FLOAT_BLOCK_SIZE,
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE,
                0
            );
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);

            // *** go = go * dm_block
            for (uint32_t vmul_idx = 0; vmul_idx < embed_split_size / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
            }
            if (embed_split_size % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(embed_split_size % FLOAT_VECTOR_SIZE);
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
            // *** go = lo + go
            add_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor,
                go32_ubuf_tensor,
                lo_ubuf_tensor,
                (sub_m * round_embed_split_size + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                            // dstBlockStride
                1,                            // src0BlockStride
                1,                            // src1BlockStride
                8,                            // dstRepeatStride
                8,                            // src0RepeatStride
                8                             // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            if (embed_split_idx == 0) {
                // *** gl = ll
                ub_to_ub<ArchType::ASCEND_V220, float>(
                    gl32_ubuf_tensor,
                    ll_ubuf_tensor,
                    0,                // sid
                    1,                // nBurst
                    round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                    0,                // srcGap
                    0                 // dstGap
                );
                PIPE_BARRIER(V);
            }
            gm_to_ub<ArchType::ASCEND_V220, mm2CopyType>(
                go32_ubuf_tensor.template ReinterpretCast<mm2CopyType>(),
                o_tmp_gm_tensor,
                0,                    // sid
                sub_m,                    // nBurst
                round_embed_split_size / FLOAT_BLOCK_SIZE,  // lenBurst
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE,                    // srcGap
                0                     // dstGap
            );
            if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
                DeQuantPerHeadImpl(
                    deq_scale2_gm_tensor[head_idx],
                    o_tmp_gm_tensor,
                    go32_ubuf_tensor, go32_ubuf_tensor.template ReinterpretCast<mm2CopyType>(),
                    descale2_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, embed_split_size, round_embed_split_size, pQuantOnline, 0);
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        SET_FLAG(MTE2, V, EVENT_ID3);
                    }
            } else {
                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);
            }
        }
        SET_FLAG(V, MTE2, EVENT_ID0);

        if (n_idx == n_loop - 1) {
            // *** gl_block = expand_to_block(gl), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                gl32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);
            // *** go = go / gl_block
            for (uint32_t vdiv_idx = 0; vdiv_idx < embed_split_size / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
            }
            if (embed_split_size % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(embed_split_size % FLOAT_VECTOR_SIZE);
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);  // fix hidden_size=96
            }
            PIPE_BARRIER(V);
            if constexpr (SplitKV) {
                // log（l）
                ln_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                    tv32_ubuf_tensor,
                    sub_m, // repeat
                    1,       // dstBlockStride
                    1,       // srcBlockStride
                    8,       // dstRepeatStride
                    8        // srcRepeatStride
                );
                PIPE_BARRIER(V);
                brcb_v<ArchType::ASCEND_V220, uint32_t>(hm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                    gm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                    1,               // dstBlockStride
                    8,               // dstRepeatStride
                    round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                );
                PIPE_BARRIER(V);
                // logf(lse_sum) + lse_max
                add_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                    tv32_ubuf_tensor,
                    hm32_ubuf_tensor,
                    sub_m,                        // repeat
                    1,                                // dstBlockStride
                    1,                                // src0BlockStride
                    1,                                // src1BlockStride
                    8,                                // dstRepeatStride
                    8,                                // src0RepeatStride
                    8                                 // src1RepeatStride
                );
                CopyScale(sub_m, l_offset, o_offset);
            } else {
                // *** go = castfp32to16(go)
                conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(go_ubuf_tensor,
                    go32_ubuf_tensor,
                    (sub_m * round_embed_split_size + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                    1,                            // dstBlockStride
                    1,                            // srcBlockStride
                    4,                            // dstRepeatStride
                    8                             // srcRepeatStride
                );
                SET_FLAG(V, MTE3, EVENT_ID0);
                WAIT_FLAG(V, MTE3, EVENT_ID0);
                ub_to_gm_align<ArchType::ASCEND_V220, OUT_DTYPE>(
                    o_gm_tensor,
                    go_ubuf_tensor,
                    0,        // sid
                    sub_m,    // nBurst
                    embed_split_size * 2,  // lenBurst
                    0,        // leftPaddingNum
                    0,        // rightPaddingNum
                    0,        // srcGap
                    (__v - embed_split_size) * 2        // dstGap
                );
            }
            // ********************* move O to GM ************************
        } else {
            SET_FLAG(V, MTE3, EVENT_ID5);
            WAIT_FLAG(V, MTE3, EVENT_ID5);                
            ub_to_gm<ArchType::ASCEND_V220, float>(
                go_gm_tensor,
                go32_ubuf_tensor,
                0,
                sub_m,
                round_embed_split_size / FLOAT_BLOCK_SIZE,
                0,
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE
            );
        }
        SET_FLAG(MTE3, MTE2, EVENT_ID4);
    }

    __aicore__ __attribute__((always_inline)) inline void SoftmaxStage2(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor,
        AscendC::LocalTensor<float> dm32_ubuf_tensor,
        AscendC::LocalTensor<float> ll_ubuf_tensor,
        AscendC::LocalTensor<float> pm32_ubuf_tensor,
        uint32_t n_idx,
        uint32_t n_loop,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t sub_m,
        uint64_t l_offset,
        uint64_t o_offset,
        uint32_t head_idx,
        uint32_t pm_flag_scalar)
    {
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 64
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;
        uint32_t round_k =  RoundUp<16>(__k);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            DeQuantPerHeadImpl(
                deq_scale2_gm_tensor[head_idx],
                o_tmp_gm_tensor,
                lo_ubuf_tensor, loint32_ubuf_tensor,
                descale2_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, __k, round_k, pQuantOnline, 1);
            if (pm_flag_scalar == 1) {
                SET_FLAG(MTE2, V, EVENT_ID3);
            } else {
                SET_FLAG(MTE2, V, EVENT_ID4);
            }
        } else {
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            gm_to_ub<ArchType::ASCEND_V220, mm2CopyType>(
                lo_ubuf_tensor,
                o_tmp_gm_tensor,
                0,                    // sid
                1,                    // nBurst
                sub_m * round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                    // srcGap
                0                     // dstGap
            );
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);
        }

        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        // *** 更新 L 和 O
        if (n_idx != 0) {
            // *** dm = exp(dm)
            exp_v<ArchType::ASCEND_V220, float>(dm32_ubuf_tensor,
                dm32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,          // dstBlockStride
                1,          // srcBlockStride
                8,          // dstRepeatStride
                8           // srcRepeatStride
            );
            PIPE_BARRIER(V);
            // *** gl = dm * gl
            mul_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                dm32_ubuf_tensor,
                gl32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,          // dstBlockStride
                1,          // src0BlockStride
                1,          // src1BlockStride
                8,          // dstRepeatStride
                8,          // src0RepeatStride
                8           // src1RepeatStride
            );
            PIPE_BARRIER(V);
            // *** gl = ll + gl
            add_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                gl32_ubuf_tensor,
                ll_ubuf_tensor,
                sub_m_d64,  // repeat
                1,          // dstBlockStride
                1,          // src0BlockStride
                1,          // src1BlockStride
                8,          // dstRepeatStride
                8,          // src0RepeatStride
                8           // src1RepeatStride
            );
            PIPE_BARRIER(V);
            // *** dm_block = expand_to_block(dm), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                dm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);
            if (go_flag_scalar == 1) {
                WAIT_FLAG(MTE3, V, EVENT_ID0);
                go_flag_scalar = 0;
            }
            // *** go = go * dm_block
            for (uint32_t vmul_idx = 0; vmul_idx < __k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
            }
            if (__k % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(__k % FLOAT_VECTOR_SIZE);
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
            // *** go = lo + go
            add_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor,
                go32_ubuf_tensor,
                lo_ubuf_tensor,
                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                            // dstBlockStride
                1,                            // src0BlockStride
                1,                            // src1BlockStride
                8,                            // dstRepeatStride
                8,                            // src0RepeatStride
                8                             // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            // *** gl = ll
            ub_to_ub<ArchType::ASCEND_V220, float>(
                gl32_ubuf_tensor,
                ll_ubuf_tensor,
                0,                // sid
                1,                // nBurst
                round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                // srcGap
                0                 // dstGap
            );
            PIPE_BARRIER(V);
            if (go_flag_scalar == 1) {
                WAIT_FLAG(MTE3, V, EVENT_ID0);
                go_flag_scalar = 0;
            }
            // *** go = lo
            ub_to_ub<ArchType::ASCEND_V220, float>(
                go32_ubuf_tensor,
                lo_ubuf_tensor,
                0,                    // sid
                1,                    // nBurst
                sub_m * round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                    // srcGap
                0                     // dstGap
            );
            PIPE_BARRIER(V);
        }

        SET_FLAG(V, MTE2, EVENT_ID0);

        if (n_idx == n_loop - 1) {
            // *** gl_block = expand_to_block(gl), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                gl32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);
            // *** go = go / gl_block
            for (uint32_t vdiv_idx = 0; vdiv_idx < __k / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
            }
            if (__k % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(__k % FLOAT_VECTOR_SIZE);
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);  // fix hidden_size=96
            }
            PIPE_BARRIER(V);

            if constexpr (SplitKV) {
                // log（l）
                    ln_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                        tv32_ubuf_tensor,
                        sub_m, // repeat
                        1,       // dstBlockStride
                        1,       // srcBlockStride
                        8,       // dstRepeatStride
                        8        // srcRepeatStride
                    );
                    PIPE_BARRIER(V);
                    brcb_v<ArchType::ASCEND_V220, uint32_t>(hm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                        gm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                        1,               // dstBlockStride
                        8,               // dstRepeatStride
                        round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                    );
                    PIPE_BARRIER(V);
                    // logf(lse_sum) + lse_max
                    add_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                        tv32_ubuf_tensor,
                        hm32_ubuf_tensor,
                        sub_m,                        // repeat
                        1,                                // dstBlockStride
                        1,                                // src0BlockStride
                        1,                                // src1BlockStride
                        8,                                // dstRepeatStride
                        8,                                // src0RepeatStride
                        8                                 // src1RepeatStride
                    );
                    CopyScale(sub_m, l_offset, o_offset);
            } else {

                // *** go = castfp32to16(go)
                conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(go_ubuf_tensor,
                    go32_ubuf_tensor,
                    (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                    1,                            // dstBlockStride
                    1,                            // srcBlockStride
                    4,                            // dstRepeatStride
                    8                             // srcRepeatStride
                );
                SET_FLAG(V, MTE3, EVENT_ID0);
                WAIT_FLAG(V, MTE3, EVENT_ID0);
                ub_to_gm_align<ArchType::ASCEND_V220, OUT_DTYPE>(
                    o_gm_tensor[(int64_t)o_offset],
                    go_ubuf_tensor,
                    0,        // sid
                    sub_m,    // nBurst
                    __k * 2,  // lenBurst
                    0,        // leftPaddingNum
                    0,        // rightPaddingNum
                    0,        // srcGap
                    0         // dstGap
                );
            }
            // ********************* move O to GM ************************
            if (go_flag_scalar == 0) {
                SET_FLAG(MTE3, V, EVENT_ID0);
                go_flag_scalar = 1;
            }
        }
    }

    __aicore__ inline void DequantKV(GlobalT<IN_DTYPE> dst,       // [qk_n, sub_m, embedding_size]
                                     GlobalT<int8_t> src,         // [num_blocks, block_size, hidden_size]
                                     GlobalT<int32_t> deq_offset, // [hidden_size,]
                                     GlobalT<float> deq_scale,    // [hidden_size,]
                                     const uint32_t hidden_size, const uint32_t batch_idx, const uint32_t n_idx,
                                     const uint32_t kv_seq_len, const uint32_t sub_m, const uint32_t hiddenSizeOffset,
                                     const uint32_t real_n_loop, const uint32_t sub_n_loop, const uint32_t start_kv,
                                     const uint32_t bias_flag)
    {
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        uint32_t start_seq = 0;

        // [qk_n, sub_m, head_size]
        SET_FLAG(V, MTE2, EVENT_ID5);
        SET_FLAG(MTE3, V, EVENT_ID5);
        SET_FLAG(V, MTE2, EVENT_ID6);
        SET_FLAG(MTE3, V, EVENT_ID6);
        uint32_t sub_hiddensize = sub_m * embedding_size;
        uint32_t sub_hidden_d32 = RoundUp<32>(sub_hiddensize);
        uint32_t sub_hidden_d64 = RoundUp<64>(sub_hiddensize);
        uint32_t num_deq_kv = FLOAT_VECTOR_SIZE * UB_FLOAT_LINE_SIZE;
        uint32_t kv_seq_step = num_deq_kv / sub_hidden_d32;
        for (uint32_t ni = 0; ni < sub_n_loop; ++ni) {
            uint32_t actual_idx = n_idx * sub_n_loop + ni;
            if (actual_idx >= real_n_loop) {
                break;
            }
            uint32_t sub_qk_n = (actual_idx != real_n_loop - 1) ? block_size : kv_seq_len - actual_idx * block_size;
            uint32_t page_idx = (uint32_t)(*(gm_block_tables_ + batch_idx * max_num_blocks_per_query +
                                             start_kv / block_size + actual_idx));

            uint32_t dequant_ping_pang = 0;
            // [sub_qk_n, sub_m, head_size]
            for (uint32_t si = 0; si < sub_qk_n; si += kv_seq_step) {
                // copy src from gm to ub
                uint32_t seq_len_frag = Min(sub_qk_n - si, kv_seq_step);
                WAIT_FLAG(V, MTE2, EVENT_ID5 + dequant_ping_pang);
                gm_to_ub_align<ArchType::ASCEND_V220, int8_t>(ub_kv_int8_[dequant_ping_pang * num_deq_kv],
                                                              src[(page_idx * block_size + si) * hidden_size + hiddenSizeOffset],
                                                              0,                              // sid
                                                              seq_len_frag,                   // nBurst
                                                              sub_hiddensize,                 // lenBurst
                                                              0,                              // leftPaddingNum
                                                              0,                              // rightPaddingNum
                                                              (hidden_size - sub_hiddensize), // srcGap
                                                              0                               // dstGap
                    );

                if (si == 0 && ni == 0) {
                    if (bias_flag) {
                        // copy deq_offset from gm to ub
                        gm_to_ub_align<ArchType::ASCEND_V220, int32_t>(ub_offset_, deq_offset,
                                                                       0,                  // sid
                                                                       1,                  // nBurst
                                                                       sub_hiddensize * 4, // lenBurst
                                                                       0,                  // leftPaddingNum
                                                                       0,                  // rightPaddingNum
                                                                       0,                  // srcGap
                                                                       0                   // dstGap
                        );
                        SET_FLAG(MTE2, V, EVENT_ID4);
                        WAIT_FLAG(MTE2, V, EVENT_ID4);
                        conv_v<ArchType::ASCEND_V220, int32_t, float>(ub_offset_f32,                      // dst
                                                                      ub_offset_,                         // src
                                                                      sub_hidden_d64 / FLOAT_VECTOR_SIZE, // repeat
                                                                      1, // dstBlockStride
                                                                      1, // srcBlockStride
                                                                      8, // dstRepeatStride
                                                                      8  // srcRepeatStride
                        );
                    }
                    gm_to_ub_align<ArchType::ASCEND_V220, float>(ub_scale_, deq_scale,
                                                                 0,                  // sid
                                                                 1,                  // nBurst
                                                                 sub_hiddensize * 4, // lenBurst
                                                                 0,                  // leftPaddingNum
                                                                 0,                  // rightPaddingNum
                                                                 0,                  // srcGap
                                                                 0                   // dstGap
                    );
                }
                SET_FLAG(MTE2, V, EVENT_ID5 + dequant_ping_pang);
                WAIT_FLAG(MTE2, V, EVENT_ID5 + dequant_ping_pang);

                // cast src(int8) -> src(fp16) -> src(int32)
                uint32_t numel_kv = seq_len_frag * sub_hidden_d32;
                uint32_t count = numel_kv / MAX_NUMEL_INST_B16;

                WAIT_FLAG(MTE3, V, EVENT_ID5 + dequant_ping_pang);
                for (uint32_t i = 0; i < count; ++i) {
                    conv_v<ArchType::ASCEND_V220, int8_t, half>(
                        ub_kv_fp16_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B16]
                            .template ReinterpretCast<half>(),                                // dst
                        ub_kv_int8_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B16], // src
                        255,                                                                  // repeat
                        1,                                                                    // dstBlockStride
                        1,                                                                    // srcBlockStride
                        8,                                                                    // dstRepeatStride
                        4                                                                     // srcRepeatStride
                    );
                }
                conv_v<ArchType::ASCEND_V220, int8_t, half>(
                    ub_kv_fp16_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B16]
                        .template ReinterpretCast<half>(),                                    // dst
                    ub_kv_int8_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B16], // src
                    CeilDiv<uint32_t>((numel_kv - count * MAX_NUMEL_INST_B16), HALF_VECTOR_SIZE),// repeat
                    1,                                                                        // dstBlockStride
                    1,                                                                        // srcBlockStride
                    8,                                                                        // dstRepeatStride
                    4                                                                         // srcRepeatStride
                );
                SET_FLAG(V, MTE2, EVENT_ID5 + dequant_ping_pang);
                // cast src(fp16) -> src(float)
                PIPE_BARRIER(V);
                count = numel_kv / MAX_NUMEL_INST_B32;
                for (uint32_t i = 0; i < count; ++i) {
                    conv_v<ArchType::ASCEND_V220, half, float>(
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32], // dst
                        ub_kv_fp16_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32]
                            .template ReinterpretCast<half>(), // src
                        255,                                   // repeat
                        1,                                     // dstBlockStride
                        1,                                     // srcBlockStride
                        8,                                     // dstRepeatStride
                        4                                      // srcRepeatStride
                    );
                }
                conv_v<ArchType::ASCEND_V220, half, float>(
                    ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32], // dst
                    ub_kv_fp16_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32]
                        .template ReinterpretCast<half>(),                       // src
                    CeilDiv<uint32_t>((numel_kv - count * MAX_NUMEL_INST_B32), FLOAT_VECTOR_SIZE),// repeat
                    1,                                                           // dstBlockStride
                    1,                                                           // srcBlockStride
                    8,                                                           // dstRepeatStride
                    4                                                            // srcRepeatStride
                );
                if (bias_flag) {
                    // src(float) <- src(float) + offset(float)
                    PIPE_BARRIER(V);
                    count = sub_hiddensize / FLOAT_VECTOR_SIZE;
                    for (uint32_t i = 0; i < count; ++i) {
                        add_v<ArchType::ASCEND_V220, float>(
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // dst
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // src0
                            ub_offset_f32[i * FLOAT_VECTOR_SIZE],                                // src1
                            seq_len_frag,                                                        // repeat
                            1,                                                                   // dstBlockStride
                            1,                                                                   // src0BlockStride
                            1,                                                                   // src1BlockStride
                            sub_hidden_d32 / 8,                                                  // dstRepeatStride
                            sub_hidden_d32 / 8,                                                  // src0RepeatStride
                            0                                                                    // src1RepeatStride
                        );
                    }
                    if (sub_hiddensize % FLOAT_VECTOR_SIZE > 0) {
                        __set_mask(sub_hiddensize % FLOAT_VECTOR_SIZE);
                        add_v<ArchType::ASCEND_V220, float>(
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // dst
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // src0
                            ub_offset_f32[count * FLOAT_VECTOR_SIZE],                                // src1
                            seq_len_frag,                                                            // repeat
                            1,                                                                       // dstBlockStride
                            1,                                                                       // src0BlockStride
                            1,                                                                       // src1BlockStride
                            sub_hidden_d32 / 8,                                                      // dstRepeatStride
                            sub_hidden_d32 / 8,                                                      // src0RepeatStride
                            0                                                                        // src1RepeatStride
                        );
                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                    }
                }
                // src(float) <- src(float) * scale(float)
                PIPE_BARRIER(V);
                count = sub_hiddensize / FLOAT_VECTOR_SIZE;
                for (uint32_t i = 0; i < count; ++i) {
                    mul_v<ArchType::ASCEND_V220, float>(
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // dst
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // src0
                        ub_scale_[i * FLOAT_VECTOR_SIZE],                                    // src1
                        seq_len_frag,                                                        // repeat
                        1,                                                                   // dstBlockStride
                        1,                                                                   // src0BlockStride
                        1,                                                                   // src1BlockStride
                        sub_hidden_d32 / 8,                                                  // dstRepeatStride
                        sub_hidden_d32 / 8,                                                  // src0RepeatStride
                        0                                                                    // src1RepeatStride
                    );
                }
                // 非对齐场景处理：偏移的数据量按照对齐偏移sub_hidden_d32
                if (sub_hiddensize % FLOAT_VECTOR_SIZE > 0) {
                    __set_mask(sub_hiddensize % FLOAT_VECTOR_SIZE);
                    mul_v<ArchType::ASCEND_V220, float>(
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // dst
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // src0
                        ub_scale_[count * FLOAT_VECTOR_SIZE],                                    // src1
                        seq_len_frag,                                                            // repeat
                        1,                                                                       // dstBlockStride
                        1,                                                                       // src0BlockStride
                        1,                                                                       // src1BlockStride
                        sub_hidden_d32 / 8,                                                      // dstRepeatStride
                        sub_hidden_d32 / 8,                                                      // src0RepeatStride
                        0                                                                        // src1RepeatStride
                    );
                }
                // // cast src(float) -> src(half)
                count = numel_kv / MAX_NUMEL_INST_B32;
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                PIPE_BARRIER(V);
                for (uint32_t i = 0; i < count; ++i) {
                    conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(
                        ub_kv_fp16_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32], // dst
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32], // src
                        255,                                                                  // repeat
                        1,                                                                    // dstBlockStride
                        1,                                                                    // srcBlockStride
                        4,                                                                    // dstRepeatStride
                        8                                                                     // srcRepeatStride
                    );
                }
                conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(
                    ub_kv_fp16_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32], // dst
                    ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32], // src
                    CeilDiv<uint32_t>((numel_kv - count * MAX_NUMEL_INST_B32), FLOAT_VECTOR_SIZE),// repeat
                    1,                                                                        // dstBlockStride
                    1,                                                                        // srcBlockStride
                    4,                                                                        // dstRepeatStride
                    8                                                                         // srcRepeatStride
                );
                SET_FLAG(V, MTE3, EVENT_ID0 + dequant_ping_pang);
                WAIT_FLAG(V, MTE3, EVENT_ID0 + dequant_ping_pang);

                // 非对齐场景处理：非对齐headsize需要依照情况手动偏移dummydata
                uint32_t align_size = (sub_m * embedding_size % 32);
                uint32_t padd_gap = (align_size > 0) ? ((UB_ALIGN_BYTE - align_size) >= CONST_16 ? 1 : 0) : 0;
                ub_to_gm_align<ArchType::ASCEND_V220, IN_DTYPE>(dst[(start_seq + si) * hidden_size],
                                                            ub_kv_fp16_[dequant_ping_pang * num_deq_kv],
                                                                0,                          // sid
                                                                seq_len_frag,               // nBurst
                                                                sub_m * embedding_size * 2, // lenBurst
                                                                0,                          // leftPaddingNum
                                                                0,                          // rightPaddingNum
                                                                padd_gap,                   // srcGap
                                                                (hidden_size - sub_hiddensize) * 2  // dstGap
                );
                SET_FLAG(MTE3, V, EVENT_ID5 + dequant_ping_pang);
                dequant_ping_pang = 1 - dequant_ping_pang;
            }
            start_seq += sub_qk_n;
        }
        WAIT_FLAG(MTE3, V, EVENT_ID5);
        WAIT_FLAG(V, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE3, V, EVENT_ID6);
        WAIT_FLAG(V, MTE2, EVENT_ID6);
    }
    
    __aicore__ __attribute__((always_inline)) inline void InnerRunVector(uint32_t cur_batch, uint32_t start_head, uint32_t cur_nIndx, uint32_t cur_kv_seqlen, uint32_t cur_head_num,
                                                                         uint32_t offset_tiling, uint32_t kv_seqlen, uint32_t embed_split_size_v, uint32_t embed_split_loop_v)
    {
        uint32_t kv_start_head = start_head / group_num; //30 ~ 32
        uint32_t kv_end_head = (start_head + cur_head_num + group_num - 1) / group_num;
        uint32_t cur_kvhead_num = kv_end_head - kv_start_head;
        uint32_t kv_head_idx = kv_start_head + sub_block_idx * cur_kvhead_num / 2;
        uint32_t head_idx = start_head + sub_block_idx * cur_head_num / 2;
        uint32_t addr_o_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 6 + offset_tiling));
        uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 7 + offset_tiling));
        uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
        uint32_t mask_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_gm + 10 + offset_tiling));
        uint32_t mask_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_gm + 14 + offset_tiling));
        uint64_t mask_scalar = (uint64_t)(((uint64_t)mask_high32) << 32 | mask_loww32);
        uint32_t addr_l_high32 = 0;
        uint32_t addr_l_loww32 = 0;
        uint64_t addr_l_scalar = 0;
        uint64_t o_offset = 0;
        uint32_t l_offset = 0;
        // o #((num_tokens, num_heads, kvsplit, head_size))
        // l  (numt_tokens, num_heads, kvsplit)
        if constexpr (SplitKV) {
            addr_l_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 11 + offset_tiling));
            addr_l_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 12 + offset_tiling));
            addr_l_scalar = (uint64_t)(((uint64_t)addr_l_high32) << 32 | addr_l_loww32);
            uint32_t addr_o_fd_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 15 + offset_tiling));
            uint32_t addr_o_fd_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 16 + offset_tiling));
            uint64_t addr_o_fd_scalar = (uint64_t)(((uint64_t)addr_o_fd_high32) << 32 | addr_o_fd_loww32);
            o_offset = addr_o_fd_scalar * kv_split_core_num + head_idx * __k * kv_split_core_num + cur_nIndx * __k;
            l_offset = addr_l_scalar + head_idx * kv_split_core_num + cur_nIndx;
        } else {
            if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                o_offset = addr_o_scalar + head_idx * embedding_size_v;
            } else {
                o_offset = addr_o_scalar + head_idx * embedding_size;
            }
        }
        uint32_t pp_n_scalar = block_size_calc;
        uint32_t sub_n_loop = pp_n_scalar / block_size;
        uint32_t real_n_loop = (cur_kv_seqlen + block_size - 1) / block_size;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        uint64_t mask_offset = cur_batch % modCoef / divCoef * batch_stride + head_idx * head_stride + (uint64_t)cur_nIndx * kv_split_per_core;
        mask_offset += mask_scalar;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);

        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n);

        uint32_t sub_m = (sub_block_idx == 1) ? (cur_head_num - cur_head_num / 2) : cur_head_num / 2;
        uint32_t sub_m_d128 = (sub_m + 127) / 128;  // up aligned to 128
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 128
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;

        uint32_t start_kv = cur_nIndx * kv_split_per_core;


        uint32_t hiddenSizeOffset = kv_head_idx * embedding_size;
        uint32_t gm_scale_hidden_size = kv_head_idx * embedding_size;
        uint32_t hiddenSizeOffset1 = k_bias_flag ? hiddenSizeOffset : 0;
        uint32_t hiddenSizeOffset2 = v_bias_flag ? hiddenSizeOffset : 0;
        uint32_t sub_m_kv = (sub_block_idx == 1) ? (cur_kvhead_num - cur_kvhead_num / 2) : cur_kvhead_num / 2;
        
        if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
            uint32_t razor_start_head = (cur_batch * q_heads + start_head) % q_head_original;
            uint32_t razor_head_idx = razor_start_head + sub_block_idx * cur_head_num / 2;
            gm_scale_hidden_size = razor_head_idx * embedding_size;
            hiddenSizeOffset1 = k_bias_flag ? gm_scale_hidden_size : 0;
            hiddenSizeOffset2 = v_bias_flag ? gm_scale_hidden_size : 0;
        }

        for (uint32_t n_idx = 0; n_idx < n_loop; n_idx+=2) {
            if (n_idx == (n_loop - 1)) {
                qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                qk_round_n = RoundUp<16>(qk_n);
            }
            if ((n_idx + 1) == (n_loop - 1)) {
                qk_n_2 = (cur_kv_seqlen - (n_idx + 1) * pp_n_scalar);
                qk_round_n_2 = RoundUp<16>(qk_n_2);
            }
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT ||
                          tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                // [qk_n, sub_m, head_size]
                if (sub_m_kv > 0) {
                    DequantKV(gm_k16_ping_[hiddenSizeOffset], // dst
                              gm_k8_,                                                           // src
                              gm_offset1_[hiddenSizeOffset1],                                    // deq_offset
                              gm_scale1_[hiddenSizeOffset],                                            // deq_scale
                              num_kv_heads * embedding_size,                                     // hidden_size
                              cur_batch,                                                         // batch_idx
                              n_idx,                                                             // n_idx
                              cur_kv_seqlen,                                                     // kv_seq_len
                              sub_m_kv,                                                             // sub_m
                              hiddenSizeOffset,                                                      // hiddenSizeOffset
                              real_n_loop,                                                       // real_n_loop
                              sub_n_loop,                                                        // sub_n_loop
                              start_kv,                                                          // start_kv
                              k_bias_flag);
                }
                FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_K0_READY);
                if ((n_idx + 1) < n_loop) {
                    if (sub_m_kv > 0) {
                        DequantKV(gm_k16_pong_[hiddenSizeOffset], // dst
                                  gm_k8_,                                                // src
                                  gm_offset1_[hiddenSizeOffset1],                                    // deq_offset
                                  gm_scale1_[hiddenSizeOffset],                                            // deq_scale
                                  num_kv_heads * embedding_size,                                     // hidden_size
                                  cur_batch,                                                         // batch_idx
                                  n_idx + 1,                                                         // seq_idx
                                  cur_kv_seqlen,                                                     // kv_seq_len
                                  sub_m_kv,                                                             // sub_m
                                  hiddenSizeOffset,                                                      // hiddenSizeOffset
                                  real_n_loop,                                                       // real_n_loop
                                  sub_n_loop,                                                        // sub_n_loop
                                  start_kv,                                                          // start_kv
                                  k_bias_flag);
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_K1_READY);
                }
            }
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT ||
                          tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                if (sub_m_kv > 0) {
                    DequantKV(gm_v16_ping_[hiddenSizeOffset], // dst
                              gm_v8_,                                                // src
                              gm_offset2_[hiddenSizeOffset2],                                    // deq_offset
                              gm_scale2_[hiddenSizeOffset],                                            // deq_scale
                              num_kv_heads * embedding_size,                                     // hidden_size
                              cur_batch,                                                         // batch_idx
                              n_idx,                                                             // n_idx
                              cur_kv_seqlen,                                                     // kv_seq_len
                              sub_m_kv,                                                             // sub_m
                              hiddenSizeOffset,                                                      // hiddenSizeOffset
                              real_n_loop,                                                       // real_n_loop
                              sub_n_loop,                                                        // sub_n_loop
                              start_kv,                                                          // start_kv
                              v_bias_flag);
                    SET_FLAG(V, MTE2, EVENT_ID6);
                    WAIT_FLAG(V, MTE2, EVENT_ID6);
                }
                FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_V0_READY);
            }
            WaitFlagDev(QK_READY_DECODER);
            /* ************ softmax1 stage1  ************* */
            WAIT_FLAG(MTE3, MTE2, EVENT_ID3);
            if (sub_m > 0) {
                // input QK shape (sub_m, qk_round_n)
                SoftmaxStage1(
                    p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET +
                        (uint64_t)sub_block_idx * cur_head_num / 2 * qk_round_n * T_BLOCK_OFFSET],
                    s_gm_tensor[(int64_t)block_idx * TMP_SIZE_DECODER +
                        (int64_t)sub_block_idx * cur_head_num / 2 * qk_round_n],
                    mask_gm_tensor[mask_offset + (uint64_t)n_idx * pp_n_scalar],
                    dm32_ubuf_tensor, ll_ubuf_tensor, pm32_ubuf_tensor,
                    n_idx, qk_n, qk_round_n, sub_m, mask_offset, sub_n_loop, cur_batch, start_kv, real_n_loop, head_idx, pm_flag_scalar1
                );
               // input QK shape (sub_m, qk_round_n)
            }
            FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY_DECODER);
            SET_FLAG(MTE3, MTE2, EVENT_ID3);
            WAIT_FLAG(MTE3, MTE2, EVENT_ID3);
            /* ************ softmax1 stage2  ************* */
            if (n_idx + 1 < n_loop) {
                if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT ||
                              tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                    if (sub_m_kv > 0) {
                        DequantKV(gm_v16_pong_[hiddenSizeOffset], // dst
                                  gm_v8_,                                                   // src
                                  gm_offset2_[hiddenSizeOffset2],                                       // deq_offset
                                  gm_scale2_[hiddenSizeOffset],                                               // deq_scale
                                  num_kv_heads * embedding_size,                                        // hidden_size
                                  cur_batch,                                                            // batch_idx
                                  n_idx + 1,                                                            // seq_idx
                                  cur_kv_seqlen,                                                        // kv_seq_len
                                  sub_m_kv,                                                                // sub_m
                                  hiddenSizeOffset,                                                         // hiddenSizeOffset
                                  real_n_loop,                                                          // real_n_loop
                                  sub_n_loop,                                                           // sub_n_loop
                                  start_kv,                                                             // start_kv
                                  v_bias_flag);
                        SET_FLAG(V, MTE2, EVENT_ID6);
                        WAIT_FLAG(V, MTE2, EVENT_ID6);
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_V1_READY);
                }
                WaitFlagDev(QK_READY_STAGE2);
                if (sub_m > 0) {
                    SoftmaxStage1(
                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET  +
                            (uint64_t)sub_block_idx * cur_head_num / 2 * qk_round_n_2 * T_BLOCK_OFFSET +
                            TMP_SIZE * T_BLOCK_OFFSET / 2],
                        s_gm_tensor[(int64_t)block_idx * TMP_SIZE_DECODER +
                            (int64_t)sub_block_idx * cur_head_num / 2 * qk_round_n_2 +
                            TMP_SIZE_DECODER / 2],
                        mask_gm_tensor[mask_offset + (uint64_t)(n_idx + 1) * pp_n_scalar],
                        dm32_stage2_ubuf_tensor, ll_stage2_ubuf_tensor, pm32_ubuf_stage2_tensor,
                        (n_idx + 1), qk_n_2, qk_round_n_2, sub_m, mask_offset, sub_n_loop, cur_batch, start_kv, real_n_loop, head_idx, pm_flag_scalar2
                    );

                }
                FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY_STAGE2);
            }
            SET_FLAG(MTE3, MTE2, EVENT_ID3);
            /* ************ softmax2 stage1  ************* */
            WaitFlagDev(UPDATE_READY_DECODER);
            uint32_t embed_split_size = embed_split_size_v;
            uint32_t round_embed_split_size = RoundUp<BLOCK_SIZE>(embed_split_size);
            if (sub_m > 0) {
                if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                    for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {              
                        uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                        uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                        if (embed_split_idx == embed_split_loop_v - 1) {
                            embed_split_size = embedding_size_v - embed_split_offset_tight;
                            round_embed_split_size = RoundUp<BLOCK_SIZE>(embed_split_size);
                        }
                        /* ************ softmax2 stage1  ************* */
                        SoftmaxStage2MLA(
                            o_tmp_gm_tensor[(int64_t)(block_idx * TMP_SIZE * 2 +
                                sub_block_idx * cur_head_num / 2 * round_v + embed_split_offset)],
                            go_gm_tensor[(int64_t)(block_idx * TMP_SIZE +
                                sub_block_idx * cur_head_num / 2 * round_v + embed_split_offset)],
                            o_gm_tensor[(int64_t)(o_offset + embed_split_offset_tight)], 
                                dm32_ubuf_tensor, ll_ubuf_tensor, pm32_ubuf_tensor,
                            n_idx, n_loop, qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), sub_m, l_offset, o_offset, head_idx,
                            embed_split_size, round_embed_split_size, embed_split_idx, embed_split_loop_v, pm_flag_scalar1);
                    }
                } else {
                    SoftmaxStage2(
                        o_tmp_gm_tensor[(int64_t)block_idx * TMP_SIZE +
                        sub_block_idx * cur_head_num / 2 * RoundUp<16>(__k)],
                        o_gm_tensor, dm32_ubuf_tensor, ll_ubuf_tensor, pm32_ubuf_tensor,
                        n_idx, n_loop, qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), sub_m, l_offset, o_offset, head_idx, pm_flag_scalar1);
                }
            }
            /* ************ softmax2 stage2  ************* */
            embed_split_size = embed_split_size_v;
            round_embed_split_size = RoundUp<BLOCK_SIZE>(embed_split_size);
            if (n_idx + 1 < n_loop) {
                WaitFlagDev(UPDATE_READY_STAGE2);
                if (sub_m > 0) {
                    if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                        for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {              
                            uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                            uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                            if (embed_split_idx == embed_split_loop_v - 1) {
                                embed_split_size = embedding_size_v - embed_split_offset_tight;
                                round_embed_split_size = RoundUp<BLOCK_SIZE>(embed_split_size);
                            }                                  
                            SoftmaxStage2MLA(
                                o_tmp_gm_tensor[(int64_t)(block_idx * TMP_SIZE * 2 +
                                        sub_block_idx * cur_head_num / 2 * round_v + TMP_SIZE + embed_split_offset)],
                                go_gm_tensor[(int64_t)(block_idx * TMP_SIZE +
                                        sub_block_idx * cur_head_num / 2 * round_v + embed_split_offset)],
                                o_gm_tensor[(int64_t)(o_offset + embed_split_offset_tight)], 
                                dm32_stage2_ubuf_tensor, ll_stage2_ubuf_tensor, pm32_ubuf_stage2_tensor,
                                (n_idx + 1), n_loop, qk_n_2, RoundUp<T_BLOCK_SIZE>(qk_round_n_2), sub_m, l_offset, o_offset, head_idx,
                                embed_split_size, round_embed_split_size, embed_split_idx, embed_split_loop_v, pm_flag_scalar2);
                        }
                    } else {
                        SoftmaxStage2(
                            o_tmp_gm_tensor[(int64_t)block_idx * TMP_SIZE +
                            sub_block_idx * cur_head_num / 2 * RoundUp<16>(__k) +
                                TMP_SIZE / 2],
                            o_gm_tensor, dm32_stage2_ubuf_tensor, ll_stage2_ubuf_tensor, pm32_ubuf_stage2_tensor,
                            (n_idx + 1), n_loop, qk_n_2, RoundUp<T_BLOCK_SIZE>(qk_round_n_2), sub_m, l_offset, o_offset, head_idx, pm_flag_scalar2);
                    }
                }
            }
        }
    }

private:

    __gm__ mm1CopyType *__restrict__ s_gm{nullptr};
    __gm__ IN_DTYPE *__restrict__ p_gm{nullptr};
    __gm__ mm2CopyType *__restrict__ o_tmp_gm{nullptr};
    __gm__ float *__restrict__ go_gm{nullptr};
    __gm__ float *__restrict__ o_core_tmp_gm{nullptr};
    __gm__ float *__restrict__ l_gm{nullptr};
    __gm__ int32_t* __restrict__ gm_block_tables_{nullptr};

    __gm__ OUT_DTYPE *__restrict__ o_gm{nullptr};
    __gm__ OUT_DTYPE *__restrict__ mask_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_gm{nullptr};
    __gm__ float *__restrict__ razor_offset_gm{nullptr};
    __gm__ float *__restrict__ logN_gm{nullptr};

    UbufAlloc<pagedAttnVariant> UbAllocator;
    const uint32_t ls32_ubuf_offset = UbAllocator.ls32_ubuf_offset;
    const uint32_t lp_ubuf_offset = UbAllocator.lp_ubuf_offset;
    const uint32_t lp32_ubuf_offset = UbAllocator.lp32_ubuf_offset;
    const uint32_t mask_ubuf_offset = UbAllocator.mask_ubuf_offset;
    const uint32_t lo_ubuf_offset = UbAllocator.lo_ubuf_offset;
    const uint32_t mask32_ubuf_offset = UbAllocator.mask32_ubuf_offset;
    const uint32_t ls16_ubuf_offset = UbAllocator.ls16_ubuf_offset;
    
    const uint32_t lm32_ubuf_offset = UbAllocator.lm32_ubuf_offset;
    const uint32_t hm32_ubuf_offset = UbAllocator.hm32_ubuf_offset;
    const uint32_t pm32_ubuf_offset = UbAllocator.pm32_ubuf_offset;
    const uint32_t pm32_ubuf_stage2_offset = UbAllocator.pm32_ubuf_stage2_offset;
    const uint32_t descale1_offset = UbAllocator.descale1_offset;
    const uint32_t descale2_offset = UbAllocator.descale2_offset;
    const uint32_t dm32_ubuf_offset = UbAllocator.dm32_ubuf_offset;
    const uint32_t dm32_ubuf_stage2_offset = UbAllocator.dm32_ubuf_stage2_offset;
    const uint32_t ll_ubuf_offset = UbAllocator.ll_ubuf_offset; 
    const uint32_t ll_ubuf_stage2_offset = UbAllocator.ll_ubuf_stage2_offset;      
    const uint32_t gm32_ubuf_offset = UbAllocator.gm32_ubuf_offset;  
    const uint32_t gl_ubuf_offset = UbAllocator.gl_ubuf_offset;          
    const uint32_t gl32_ubuf_offset = UbAllocator.gl32_ubuf_offset;        
    const uint32_t go_ubuf_offset = UbAllocator.go_ubuf_offset;            
    const uint32_t go32_ubuf_offset = UbAllocator.go32_ubuf_offset;          
    const uint32_t tv32_ubuf_offset = UbAllocator.tv32_ubuf_offset;       

    const uint32_t addr_kv8 = 0;
    const uint32_t addr_kv16 = addr_kv8 + 32 * 4 * 128 * sizeof(int8_t);
    const uint32_t addr_kv32 = addr_kv16 + 32 * 4 * 128 * sizeof(half);
    const uint32_t addr_offset = addr_kv32 + 32 * 4 * 128 * sizeof(float);
    const uint32_t addr_scale = addr_offset + 8 * 128 * sizeof(int32_t);
    const uint32_t addr_offset32 = addr_scale + 8 * 128 * sizeof(int32_t);

    


    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<float> ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_ubuf_offset);
    AscendC::LocalTensor<half> ls16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(ls32_ubuf_offset);
    AscendC::LocalTensor<int32_t> lsint32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(ls32_ubuf_offset);
    AscendC::LocalTensor<IN_DTYPE> lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DTYPE>(lp_ubuf_offset);
    AscendC::LocalTensor<float> lp32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lp32_ubuf_offset);
    AscendC::LocalTensor<OUT_DTYPE> mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(mask_ubuf_offset);
    AscendC::LocalTensor<float> lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
    AscendC::LocalTensor<int32_t> loint32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(lo_ubuf_offset);
    AscendC::LocalTensor<float> mask32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(mask32_ubuf_offset);
    AscendC::LocalTensor<float> lm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm32_ubuf_offset);
    AscendC::LocalTensor<float> hm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(hm32_ubuf_offset);
    AscendC::LocalTensor<float> pm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(pm32_ubuf_offset);
    AscendC::LocalTensor<float> pm32_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(pm32_ubuf_stage2_offset);
    AscendC::LocalTensor<float> descale1_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(descale1_offset);
    AscendC::LocalTensor<float> descale2_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(descale2_offset);
    AscendC::LocalTensor<float> gm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gm32_ubuf_offset);
    AscendC::LocalTensor<float> dm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm32_ubuf_offset);

    AscendC::LocalTensor<float> dm32_stage2_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm32_ubuf_stage2_offset);
    AscendC::LocalTensor<float> ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
    AscendC::LocalTensor<float> ll_stage2_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_stage2_offset);
    AscendC::LocalTensor<OUT_DTYPE> gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(gl_ubuf_offset);
    AscendC::LocalTensor<float> gl32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl32_ubuf_offset);
    AscendC::LocalTensor<float> tv32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv32_ubuf_offset);
    AscendC::LocalTensor<OUT_DTYPE> go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(go_ubuf_offset);
    AscendC::LocalTensor<float> go32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go32_ubuf_offset);
    AscendC::LocalTensor<int32_t> goint32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(go32_ubuf_offset);

    AscendC::GlobalTensor<mmScaleType> deq_scale1_gm_tensor;
    AscendC::GlobalTensor<mmScaleType> deq_scale2_gm_tensor;
    AscendC::GlobalTensor<mmScaleType> scale_gm_tensor;
    AscendC::GlobalTensor<mmBiasType> bias1_gm_tensor;
    AscendC::GlobalTensor<mmBiasType> bias2_gm_tensor;

    AscendC::GlobalTensor<OUT_DTYPE> mask_gm_tensor;
    AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<float> razor_offset_gm_tensor;
    AscendC::GlobalTensor<mm1CopyType> s_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor;
    AscendC::GlobalTensor<mm2OutputType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<float> go_gm_tensor;
    AscendC::GlobalTensor<float> o_core_tmp_gm_tensor;
    AscendC::GlobalTensor<float> l_gm_tensor;

    GlobalT<int8_t> gm_k8_;
    GlobalT<int8_t> gm_v8_;
    GlobalT<OUT_DTYPE> gm_k16_ping_;
    GlobalT<OUT_DTYPE> gm_k16_pong_;
    GlobalT<OUT_DTYPE> gm_v16_ping_;
    GlobalT<OUT_DTYPE> gm_v16_pong_;
    GlobalT<int32_t> gm_offset1_;
    GlobalT<int32_t> gm_offset2_;
    GlobalT<float> gm_scale1_;
    GlobalT<float> gm_scale2_;
    LocalT<int8_t> ub_kv_int8_ = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(addr_kv8);
    LocalT<OUT_DTYPE> ub_kv_fp16_ = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(addr_kv16);
    LocalT<int32_t> ub_kv_int32_ = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(addr_kv32);
    LocalT<float> ub_kv_fp32_ = buf.GetBuffer<BufferType::ASCEND_UB, float>(addr_kv32);
    LocalT<int32_t> ub_offset_ = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(addr_offset);
    LocalT<float> ub_offset_f32 = buf.GetBuffer<BufferType::ASCEND_UB, float>(addr_offset32);
    LocalT<float> ub_scale_ = buf.GetBuffer<BufferType::ASCEND_UB, float>(addr_scale);

    uint32_t k_bias_flag{0};
    uint32_t v_bias_flag{0};
    uint32_t pQuantOnline{0};
    uint32_t pQuantType{0};

    uint32_t go_flag_scalar{1};
    uint32_t gl_flag_scalar{1};
    uint32_t pm_flag_scalar1{1};
    uint32_t pm_flag_scalar2{0};
    ScaleType scaleType = ScaleType::SCALE_TOR;
    float tor_logN{0};
    uint32_t num_tokens{0};
    uint32_t q_heads{0};
    uint32_t num_kv_heads{0};
    uint32_t embedding_size{0};
    uint32_t embedding_size_v{0};
    uint32_t block_size{0};
    uint32_t max_context_len{0};
    uint32_t start_head{0};
    uint32_t cur_head_num{0};
    uint32_t __k{0};
    uint32_t round_k{0};
    uint32_t __v{0};
    uint32_t round_v{0};
    uint32_t cur_batch{0};
    float tor{0};
    uint64_t sub_block_idx{0};
    uint32_t batch_stride{0};
    uint32_t head_stride{0};
    uint64_t former_batch{0};
    uint32_t former_head_split{0};
    uint32_t split_size{0};
    uint64_t tail_batch{0};
    uint32_t tail_head_split{0};
    uint32_t core_per_batch{0};
    uint32_t process_num{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t kv_split_per_core{0};
    uint32_t kv_split_core_num{0};
    uint32_t block_size_calc{0};
    uint32_t former_group_num_move{1};
    uint32_t tail_group_num_move{1};
    uint32_t embed_split_size_v_former{0};
    uint32_t embed_split_loop_v_former{1};
    uint32_t embed_split_size_v_tail{0};
    uint32_t embed_split_loop_v_tail{1};


    uint32_t modCoef{0xffffffff}; // 对batch_idx取模的参数，适用于多头自适应压缩场景
    uint32_t divCoef{1}; // 对batch_idx做除法的参数，适用于多头自适应压缩场景
    uint32_t q_head_original{0};
    uint32_t compressHead{0};

    uint32_t max_num_blocks_per_query{0};
    uint32_t group_num{0};

    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;
};
#endif

#ifdef __DAV_C220_CUBE__
template <bool SplitKV = false, TilingKeyType tilingKeyType = TilingKeyType::TILING_HALF_DATA, typename IN_DTYPE = half,  typename OUT_DTYPE = half, typename IN_KVDTYPE = half, PagedAttnVariant pagedAttnVariant = PagedAttnVariant::DEFAULT, DataShapeType dataShapeType = DataShapeType::BSND, CompressType compressType = CompressType::COMPRESS_TYPE_UNDEFINED>
class MLAttentionDecoderAicBlockSize256 {
    // define dtype
    using mm1OutputType = typename AttentionType<tilingKeyType>::mm1OutputType;
    using mm1CopyType = typename AttentionType<tilingKeyType>::mm1CopyType;
    using mmBiasType = typename AttentionType<tilingKeyType>::mmBiasType;
    using mmScaleType = typename AttentionType<tilingKeyType>::mmScaleType;
    using mm2OutputType = typename AttentionType<tilingKeyType>::mm2OutputType;
    using mm2CopyType = typename AttentionType<tilingKeyType>::mm2CopyType;
    static constexpr uint32_t T_CUBE_MATRIX_SIZE = CUBE_MATRIX_SIZE_512 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_SIZE =  BLOCK_SIZE_32 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_OFFSET = 2 / sizeof(IN_DTYPE);
    static constexpr int32_t L1_KV_HALF_SIZE = 73728;// 2* 128 * 256
    static constexpr int32_t L1_KV_UINT8_SIZE = 73728 * 2;

public:
    __aicore__ __attribute__((always_inline)) inline MLAttentionDecoderAicBlockSize256(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    __aicore__ __attribute__((always_inline)) inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t *__restrict__ q_in_gm,
        __gm__ uint8_t *__restrict__ k_in_gm,
        __gm__ uint8_t *__restrict__ v_in_gm,
        __gm__ uint8_t *__restrict__ block_tables_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t* __restrict__ gm_k16,
        __gm__ uint8_t* __restrict__ gm_v16, 
        __gm__ uint8_t *__restrict__ tiling_para_gm,
        __gm__ uint8_t *__restrict__ razorOffset)
    {
        SetFftsBaseAddr((uint64_t)sync);
        SetPadding<uint64_t>(0);
        SetAtomicnone();
        SetNdpara(1, 0, 0);
        SetMasknorm();

        q_gm = reinterpret_cast<__gm__ IN_DTYPE *>(q_in_gm);
        k_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm);
        v_gm = reinterpret_cast<__gm__ IN_KVDTYPE *>(v_in_gm);
        block_tables_gm = reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm);
        s_gm = reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm);

        p_gm = reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm);
        o_tmp_gm = reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm);
        tiling_gm = reinterpret_cast<__gm__ uint8_t *>(tiling_para_gm);
        razor_offset_gm = reinterpret_cast<__gm__ float *>(razorOffset);

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(q_in_gm));
        k_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(k_in_gm));
        v_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_KVDTYPE *>(v_in_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm));
        block_tables_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(block_tables_in_gm));

        num_tokens = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM));
        embedding_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V));
        block_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MAXBLOCKS));
        kv_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVHEADS));
        former_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_BATCH));
        former_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_HEAD));
        tail_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_BATCH));
        tail_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_HEAD));
        head_split_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADNUM_MOVE));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        group_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_GROUPNUM));
        block_size_calc = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE_CALC));
        q_head_original = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QHEADORIGINAL));
        compressHead = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_COMPRESSHEAD));
        former_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_GROUP_MOVE));
        tail_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_GROUP_MOVE));

        kv_split_per_core = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVSPLIT));
        kv_split_core_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVCORENUM));

        former_head_split_num = (former_head_split > group_num) && (former_group_num_move == group_num) ? head_split_num : 1;
        tail_head_split_num = (tail_head_split > group_num) && (tail_group_num_move == group_num) ? head_split_num : 1;

        stride_kv = static_cast<uint64_t>(kv_heads) * embedding_size;

        if constexpr (dataShapeType == DataShapeType::BNSD) {
            stride_kv = embedding_size;
        }

        __k = embedding_size;
        round_k = RoundUp<T_BLOCK_SIZE>(__k);
        if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
            __v = embedding_size_v;
            stride_vo = static_cast<uint64_t>(kv_heads) * embedding_size_v;
            round_v = RoundUp<BLOCK_SIZE>(__v);
            // embed_split_size_qk = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_K_SPLIT));
            embed_split_size_qk = 128;
            embed_split_loop_qk = (embedding_size + embed_split_size_qk - 1) / embed_split_size_qk;
            embed_split_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT));
            embed_split_loop_v = (embedding_size_v + embed_split_size_v - 1) / embed_split_size_v;
        }
    }


    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID4);
        SET_FLAG(M, MTE1, EVENT_ID5);
        SET_FLAG(M, MTE1, EVENT_ID6);
	    SET_FLAG(M, MTE1, EVENT_ID7);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(MTE1, MTE2, EVENT_ID6);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        SET_FLAG(FIX, MTE1, EVENT_ID0);
        SET_FLAG(FIX, MTE1, EVENT_ID1);
        SET_FLAG(FIX, MTE1, EVENT_ID2);
        SET_FLAG(FIX, MTE1, EVENT_ID3);
        SET_FLAG(FIX, MTE1, EVENT_ID4);
        SET_FLAG(FIX, MTE1, EVENT_ID5);
        SET_FLAG(MTE2, FIX, EVENT_ID0);        
        core_per_batch = (q_heads + former_head_split - 1) / former_head_split;
        process_num = static_cast<uint64_t>(former_batch) * core_per_batch * kv_split_core_num;
        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {  // for task
            uint32_t cur_batch = process / (core_per_batch * kv_split_core_num) + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm  + 1 + offset_tiling));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            uint32_t cur_head = (process / kv_split_core_num) % core_per_batch;
            uint32_t cur_nIndx = process % kv_split_core_num;
            uint32_t start_head = cur_head * former_head_split;
            uint32_t start_kv = cur_nIndx * kv_split_per_core;
            uint32_t cur_kv_seqlen = kv_split_per_core;
            uint32_t kv_loop = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            if (cur_nIndx >= kv_loop) {
                continue;
            }
            if (cur_nIndx == (kv_loop - 1)) {
                cur_kv_seqlen = kv_seqlen - cur_nIndx * kv_split_per_core;
            }
            uint32_t cur_head_num = former_head_split;
            uint32_t former_group_num_move_real = former_group_num_move;
            if (cur_head == (core_per_batch - 1)) {
                cur_head_num = q_heads - cur_head * former_head_split;
                former_group_num_move_real = former_group_num_move <= cur_head_num ? former_group_num_move : cur_head_num;
            }
            uint32_t head_split_loop = (cur_head_num + (former_head_split_num * former_group_num_move_real) - 1) /
                                       (former_head_split_num * former_group_num_move_real);
            if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                InnerRunCubeMLA(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, former_group_num_move_real, former_head_split_num);
            }
        }
        if (tail_batch > 0) {
            core_per_batch = (q_heads + tail_head_split - 1) / tail_head_split;
            process_num = static_cast<uint64_t>(tail_batch) * core_per_batch;
            for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {  // for task
                uint32_t cur_batch = process / core_per_batch + former_batch + prefill_batch_size_;
                uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
                uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
                if (kv_seqlen == 0) {
                    continue;
                }
                uint32_t cur_kv_seqlen = kv_seqlen;
                uint32_t start_kv = 0;
                uint32_t cur_head = process % core_per_batch;
                uint32_t cur_head_num = tail_head_split;
                uint32_t tail_group_num_move_real = tail_group_num_move;
                if (cur_head == (core_per_batch - 1)) {
                    cur_head_num = q_heads - cur_head * tail_head_split;
                    tail_group_num_move_real = tail_group_num_move <= cur_head_num ? tail_group_num_move : cur_head_num;
                }
                uint32_t head_split_loop = (cur_head_num + (tail_head_split_num * tail_group_num_move_real) - 1) /
                                           (tail_head_split_num * tail_group_num_move_real);
                uint32_t start_head = (process % core_per_batch) * tail_head_split;
                if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                    InnerRunCubeMLA(batch_idx, start_head, cur_head_num, head_split_loop, start_kv, cur_kv_seqlen, offset_tiling, tail_group_num_move_real, tail_head_split_num);
                }
            }
        }
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID4);
        WAIT_FLAG(M, MTE1, EVENT_ID5);
        WAIT_FLAG(M, MTE1, EVENT_ID6);
        WAIT_FLAG(M, MTE1, EVENT_ID7);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        WAIT_FLAG(FIX, MTE1, EVENT_ID0);
        WAIT_FLAG(FIX, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, MTE1, EVENT_ID2);
        WAIT_FLAG(FIX, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, MTE1, EVENT_ID4);
        WAIT_FLAG(FIX, MTE1, EVENT_ID5);
        WAIT_FLAG(MTE2, FIX, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }

private:
    __attribute__((always_inline)) inline __aicore__ void ChangeL1bPingPongFlag() {
        l1b_pingpong_flag = 1 - l1b_pingpong_flag;
        l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_SIZE / sizeof(IN_DTYPE);
    }

    __attribute__((always_inline)) inline __aicore__ void ChangeL0CPingPongFlag() {
        // l1b_pingpong_flag = 1 - l1b_pingpong_flag;
        // l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_SIZE / sizeof(IN_DTYPE);
        l0c_pingpong_flag = 1 - l0c_pingpong_flag;
        l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
    }

    __attribute__((always_inline)) inline __aicore__ void ChangePingPongFlag() {
        l1_pingpong_flag = 1 - l1_pingpong_flag;
        l1_offset = l1_pingpong_flag * L1_UINT8_BUF_SIZE_DECODER / sizeof(IN_DTYPE);
        l1_scale_offset = l1_pingpong_flag * L1_SCALE_UINT64_SIZE;
        l1_bias_offset = l1_pingpong_flag * L1_OFFSET_INT32_SIZE;
        if (pagedAttnVariant != PagedAttnVariant::MULTI_LATENT && group_num == 1) {
            ChangeL1bPingPongFlag();
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ChangeL0PingPongFlag() {
        if (is_l0c_pingpong_off) {
            l0_pingpong_flag = 0;
            l0_offset = 0;
            l0c_offset = 0;
        } else {
            l0_pingpong_flag = 1 - l0_pingpong_flag;
            l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
            // l0c_offset = l0_pingpong_flag * L0C_FLOAT_BUF_SIZE;
        }
    }

    __attribute__((always_inline)) inline __aicore__ void ChangeWorkSpacePingPongFlag()
    {
        l1p_pingpong_flag = 1 - l1p_pingpong_flag;
        l1p_start_offset = l1p_pingpong_flag * L1_P_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    }

    __aicore__ __attribute__((always_inline)) inline void InnerRunCubeMLA(uint32_t cur_batch, uint32_t start_head, uint32_t cur_head_num, uint32_t head_split_loop,
                                    uint32_t start_kv, uint32_t cur_kv_seqlen, uint32_t offset_tiling, uint32_t group_num_move, uint32_t head_split_num_move)
    {
        uint32_t addr_q_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 4 + offset_tiling));
        uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 5 + offset_tiling));
        uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);
        uint64_t q_offset = addr_q_scalar + start_head * embedding_size;

        uint32_t pp_n_scalar = block_size_calc;
        uint32_t sub_n_loop = pp_n_scalar / block_size;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        uint32_t real_n_loop = (cur_kv_seqlen + block_size - 1) / block_size;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n_2);

        uint32_t cur_head_num_round = RoundUp<16>(cur_head_num);
        m = (group_num == 1) ? 1 : group_num_move;
        is_multi_head_mmad = group_num_move > 1;

        if (is_multi_head_mmad) {
            gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                l1q_buf_addr_tensor,
                q_gm_tensor[q_offset],
                cur_head_num,        // nValue
                RoundUp<16>(cur_head_num),// dstNzC0Stride
                0,                     // dstNzMatrixStride, unused
                __k,                   // dValue
                0,                     // dstNzMatrixStride, unused
                __k                   // srcDValue
            );
        } else {
            if (embedding_size % BLOCK_SIZE == 0) {
                gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                    l1q_buf_addr_tensor,
                    q_gm_tensor[q_offset],
                    1,
                    0,
                    0,
                    round_k * cur_head_num,               // lenBurst
                    0,
                    0
                );
            } else {
                for (uint32_t copy_idx = 0; copy_idx < cur_head_num; copy_idx++) {
                    gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                        l1q_buf_addr_tensor[copy_idx * round_k],
                        q_gm_tensor[q_offset + copy_idx * embedding_size],
                        1,
                        0,
                        0,
                        round_k,               // lenBurst
                        0,
                        0
                    );
                }
            }
                       
        }

        SET_FLAG(MTE2, MTE1, EVENT_ID0);
        WAIT_FLAG(MTE2, MTE1, EVENT_ID0);
        uint32_t sub_qk_n = block_size;
        for (uint32_t n_idx = 0; n_idx < n_loop; n_idx+=1) {
            if (n_idx == (n_loop - 1)) {
                qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);
                sub_qk_n = (cur_kv_seqlen - n_idx * block_size);
            }
            uint32_t head_num_move = head_split_num_move;
            uint64_t hiddenSize_offset = (start_head) / group_num * embedding_size;
            uint32_t embed_split_size = 128;
            uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);

            /* ************ CUBE1 stage1  ************* */
            uint32_t block_table_id = (uint32_t)(*(block_tables_gm +
                            cur_batch * max_num_blocks_per_query + start_kv / block_size + n_idx));
            int64_t kv_offset = (int64_t)block_table_id * block_size * stride_kv;
            uint32_t q_load_coeff = 1;
            if (is_multi_head_mmad) {
                q_load_coeff = cur_head_num_round;
            }

            for (uint32_t embed_split_idx = 0; embed_split_idx < 5; ++embed_split_idx) {
                if (embed_split_idx == 4) {
                    embed_split_size = 64;
                    round_embed_split_size = 64;
                }

                if (embed_split_idx != 0 || n_idx == 0) {
                    WAIT_FLAG(MTE1, MTE2, embed_split_idx);  // 等待V全部搬入L0B
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[embed_split_idx * 128 * qk_round_n],
                        k_gm_tensor[kv_offset + embed_split_idx * 128],
                        qk_n,         // nValue
                        qk_round_n,             // dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        embed_split_size,            // dValue
                        0,                     // dstNzMatrixStride, unused
                        stride_kv            // srcDValue
                    );
                    SET_FLAG(MTE2, MTE1, embed_split_idx);
                }
                WAIT_FLAG(M, MTE1, embed_split_idx % 2);
                if (q_load_coeff == 1) {
                    l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                        l0a_buf_tensor[embed_split_idx % 2 * 16384],
                        l1q_buf_addr_tensor[embed_split_idx * 128],
                        0,
                        (round_embed_split_size + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                        0,
                        1,                                                    // srcStride
                        0,
                        0                                                    // dstStride
                    );
                } else {
                    for (uint64_t loa_load_idx = 0; loa_load_idx < q_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                        l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf_tensor[embed_split_idx % 2 * 16384 + loa_load_idx * round_embed_split_size * BLOCK_SIZE],
                            l1q_buf_addr_tensor[embed_split_idx * cur_head_num_round * 128 + loa_load_idx * T_CUBE_MATRIX_SIZE],
                            0,
                            round_embed_split_size / T_BLOCK_SIZE,                                 // repeat
                            0,
                            q_load_coeff / BLOCK_SIZE,                            // srcStride
                            0,
                            0                                                     // dstStride
                        );
                    }
                }
                SET_FLAG(MTE1, M, embed_split_idx % 2);
                WAIT_FLAG(MTE2, MTE1, embed_split_idx);
                WAIT_FLAG(M, MTE1, EVENT_ID2);
                l1_to_l0_b<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                    l0b_buf_tensor,
                    l1kv_buf_addr_tensor[embed_split_idx * qk_round_n * 128],
                    0,
                    round_embed_split_size * qk_round_n / T_CUBE_MATRIX_SIZE,  // repeat
                    0,
                    1,                                        // srcStride
                    0,
                    0                                        // dstStride
                );
                if (embed_split_idx == 4) {
                    SET_FLAG(MTE1, MTE2, embed_split_idx);  // 等待V全部搬入L0B
                }
                SET_FLAG(MTE1, M, EVENT_ID2);
                WAIT_FLAG(MTE1, M, embed_split_idx % 2);
                WAIT_FLAG(MTE1, M, EVENT_ID2);
                if (embed_split_idx == 0) {
                    WAIT_FLAG(FIX, M, EVENT_ID0);
                    WAIT_FLAG(FIX, M, EVENT_ID1);
                }
                mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm1OutputType, false>(
                    mm1_l0c_buf_tensor,
                    l0a_buf_tensor[embed_split_idx % 2 * 16384],
                    l0b_buf_tensor,
                    m,     // m
                    qk_n,  // n
                    embed_split_size,   // k
                    embed_split_idx == 0     // cmatrixInitVal
                );
                PIPE_BARRIER(M);
                SET_FLAG(M, MTE1, embed_split_idx % 2);
                SET_FLAG(M, MTE1, EVENT_ID2);

                // copy S to gm
                if (embed_split_idx == 4) {
                    SET_FLAG(M, FIX, EVENT_ID0);
                    WAIT_FLAG(M, FIX, EVENT_ID0);
                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm1CopyType, mm1OutputType>(
                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE_DECODER],
                        mm1_l0c_buf_tensor,
                        m,           // MSize
                        qk_round_n,  // NSize
                        RoundUp<16>(m), // srcStride
                        qk_round_n  // dstStride_dst_D
                    );
                    SET_FLAG(FIX, M, EVENT_ID0);
                    SET_FLAG(FIX, M, EVENT_ID1);
                }
            }
            FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY_DECODER);
            /* ************ CUBE2 stage1  ************* */
            embed_split_size = 128;
            embed_split_loop_v = 4;
            round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
            uint32_t l0_pp_flag = 0;
            uint32_t l0_pp_offset = l0_pp_flag * 128 * 128;
            for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {
                uint64_t l1kv_offset = embed_split_idx * qk_round_n * round_embed_split_size;
                WAIT_FLAG(M, MTE1, EVENT_ID2);
                AscendC::LoadData2dTransposeParams loadDataParams;
                loadDataParams.dstGap = 0;
                loadDataParams.startIndex = 0;
                loadDataParams.dstFracGap = 0;
                if (qk_round_n <= round_embed_split_size) { // Nz -> nZ
                    loadDataParams.repeatTimes = round_embed_split_size / T_BLOCK_SIZE;
                    loadDataParams.srcStride = qk_round_n / T_BLOCK_SIZE;
                    uint16_t dstGap = sizeof(IN_DTYPE) == 1 ? 1 : 0;
                    loadDataParams.dstGap = dstGap;
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / T_BLOCK_SIZE; ++l0b_load_idx) {
                        // 沿 embd 方向搬
                        AscendC::LoadDataWithTranspose(
                                l0b_buf_tensor[l0b_load_idx * RoundUp<16>(embed_split_size) * T_BLOCK_SIZE],
                                l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                                loadDataParams);
                    }
                } else {
                    for (uint32_t l0b_load_idx = 0; l0b_load_idx < round_embed_split_size / T_BLOCK_SIZE; ++l0b_load_idx) {
                        // 沿 kv_len_blk方向搬
                        loadDataParams.repeatTimes = qk_round_n / T_BLOCK_SIZE;
                        loadDataParams.srcStride = 1;
                        loadDataParams.dstGap = round_embed_split_size / BLOCK_SIZE - 1;
                        AscendC::LoadDataWithTranspose(
                            l0b_buf_tensor[l0b_load_idx * T_BLOCK_SIZE * T_BLOCK_SIZE],
                            l1kv_buf_addr_tensor[l1kv_offset + l0b_load_idx * qk_round_n * T_BLOCK_SIZE],
                            loadDataParams);
                    }
                }

                // 同步下一块K可以搬入L1
                SET_FLAG(MTE1, MTE2, embed_split_idx);
                if (embed_split_idx == 0 && n_idx != n_loop - 1) {
                    uint32_t qk_n_last = qk_n;
                    uint32_t qk_round_n_last = qk_round_n;
                    if (n_idx == n_loop - 2) {
                        qk_n_last = cur_kv_seqlen - (n_idx + 1) * block_size;
                        qk_round_n_last = RoundUp<BLOCK_SIZE>(qk_n_last);
                    }
                    WAIT_FLAG(MTE1, MTE2, embed_split_idx);  // 等待V全部搬入L0B
                    block_table_id = (uint32_t)(*(block_tables_gm +
                        cur_batch * max_num_blocks_per_query + start_kv / block_size + n_idx + 1));
                    kv_offset = (int64_t)block_table_id * block_size * stride_kv;
                    gm_to_l1<ArchType::ASCEND_V220, IN_KVDTYPE, DataFormat::ND, DataFormat::NZ>(
                        l1kv_buf_addr_tensor[embed_split_idx * 128 * qk_round_n_last],
                        k_gm_tensor[kv_offset + embed_split_idx * 128],
                        qk_n_last,         // nValue
                        qk_round_n_last,             // dstNzC0Stride
                        0,                     // dstNzMatrixStride, unused
                        embed_split_size,            // dValue
                        0,                     // dstNzMatrixStride, unused
                        stride_kv            // srcDValue
                    );
                    SET_FLAG(MTE2, MTE1, embed_split_idx);
                }
                // move p from gm to l1
                uint32_t p_move_head_num = group_num_move;
                if (embed_split_idx == 0) {
                    WaitFlagDev(SOFTMAX_READY_DECODER);
                    WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
                    if (!is_multi_head_mmad) {
                        gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::ND>(
                            l1p_buf_addr_tensor,
                            p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET],
                            1,
                            0,
                            0,
                            RoundUp<BLOCK_SIZE>(qk_n) * p_move_head_num * T_BLOCK_OFFSET,               // lenBurst
                            0,
                            0
                        );
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, IN_DTYPE, DataFormat::ND, DataFormat::NZ>(
                            l1p_buf_addr_tensor,
                            p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET],
                            p_move_head_num,         // nValue
                            (p_move_head_num + 15) / 16 * 16,// dstNzC0Stride
                            0,                     // dstNzMatrixStride, unused
                            qk_round_n,           // dValue
                            0,                     // dstNzMatrixStride, unused
                            qk_round_n           // srcDValue
                        );
                    }

                    SET_FLAG(MTE2, MTE1, EVENT_ID7);
                    WAIT_FLAG(MTE2, MTE1, EVENT_ID7);
                    // move p from l1 to l0a
                    WAIT_FLAG(M, MTE1, EVENT_ID0);
                    WAIT_FLAG(M, MTE1, EVENT_ID1);
                    uint32_t p_load_coeff = 1;
                    if (is_multi_head_mmad) {
                        p_load_coeff = RoundUp<16>(p_move_head_num);
                    }
                    if (p_load_coeff == 1) {
                        l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf_tensor,
                            l1p_buf_addr_tensor,
                            0,
                            (qk_round_n + T_CUBE_MATRIX_SIZE - 1) / T_CUBE_MATRIX_SIZE,  // repeat
                            0,
                            1,                                                       // srcStride
                            0,
                            0                                                        // dstStride
                        );
                    } else {
                        for (uint64_t loa_load_idx = 0; loa_load_idx < p_load_coeff / BLOCK_SIZE; ++loa_load_idx) {
                            l1_to_l0_a<ArchType::ASCEND_V220, IN_DTYPE, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0a_buf_tensor[loa_load_idx * qk_round_n * BLOCK_SIZE],
                                l1p_buf_addr_tensor[loa_load_idx * T_CUBE_MATRIX_SIZE],
                                0,
                                qk_round_n / T_BLOCK_SIZE,                                 // repeat
                                0,
                                p_load_coeff / BLOCK_SIZE,                               // srcStride
                                0,
                                0                                                        // dstStride
                            );
                        }
                    }
                    SET_FLAG(MTE1, MTE2, EVENT_ID7);
                }
                SET_FLAG(MTE1, M, EVENT_ID7);
                WAIT_FLAG(MTE1, M, EVENT_ID7);
                WAIT_FLAG(FIX, M, l0_pp_flag);
                mmad<ArchType::ASCEND_V220, IN_DTYPE, IN_DTYPE, mm2OutputType, false>(
                    mm2_l0c_buf_tensor[l0_pp_offset],
                    l0a_buf_tensor,
                    l0b_buf_tensor,
                    m,     // m
                    embed_split_size,   // n
                    qk_n,  // k
                    1      // cmatrixInitVal
                );
                SET_FLAG(M, MTE1, EVENT_ID2);
                if (embed_split_idx == embed_split_loop_v - 1) {
                    SET_FLAG(M, MTE1, EVENT_ID0);
                    SET_FLAG(M, MTE1, EVENT_ID1);
                }
                SET_FLAG(M, FIX, l0_pp_flag);
                WAIT_FLAG(M, FIX, l0_pp_flag);
                // copy O to gm
                l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mm2CopyType, mm2OutputType>(
                    o_tmp_gm_tensor[(uint64_t)block_idx * TMP_SIZE * 2 + embed_split_idx * round_embed_split_size],
                    mm2_l0c_buf_tensor[l0_pp_offset],
                    m,        // MSize
                    RoundUp<16>(embed_split_size),  // NSize 32B对齐，防止workspace补齐的位置中有脏数据
                    RoundUp<16>(m),       // srcStride
                    round_v  // dstStride_dst_D
                );
                SET_FLAG(FIX, M, l0_pp_flag);
                l0_pp_flag = 1 - l0_pp_flag;
                l0_pp_offset = l0_pp_flag * 128 * 128;
            }
            
            FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY_DECODER);
        }
    }

private:
    __gm__ IN_DTYPE *__restrict__ q_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ ctkv_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ k_gm{nullptr};
    __gm__ IN_KVDTYPE *__restrict__ v_gm{nullptr};

    __gm__ mmScaleType *__restrict__ deq_scale1_gm{nullptr};
    __gm__ mmScaleType *__restrict__ deq_scale2_gm{nullptr};
    __gm__ mmBiasType *__restrict__ offset1_gm{nullptr};
    __gm__ mmBiasType *__restrict__ offset2_gm{nullptr};
    __gm__ int8_t *__restrict__ eye_gm{nullptr};

    __gm__ mm1CopyType *__restrict__ s_gm{nullptr};
    __gm__ IN_DTYPE *__restrict__ p_gm{nullptr};
    __gm__ mm2CopyType *__restrict__ o_tmp_gm{nullptr};
    __gm__ int32_t *__restrict__ block_tables_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_gm{nullptr};
    __gm__ float *__restrict__ razor_offset_gm{nullptr};

    AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> q_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> k_gm_tensor;
    AscendC::GlobalTensor<IN_KVDTYPE> v_gm_tensor;
    AscendC::GlobalTensor<mm1CopyType> s_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor;
    AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<int32_t> block_tables_gm_tensor;

    const uint32_t l1q_buf_addr_offset = 0;
    const uint32_t l1kv_buf_addr_offset = 147456;
    const uint32_t l1p_buf_addr_offset = 442368;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<IN_DTYPE> l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1q_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l1kv_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1kv_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, IN_DTYPE>(l1p_buf_addr_offset);

    AscendC::LocalTensor<IN_DTYPE> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, IN_DTYPE>(0);
    AscendC::LocalTensor<IN_DTYPE> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, IN_DTYPE>(0);
    AscendC::LocalTensor<mm1OutputType> mm1_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm1OutputType>(0);
    AscendC::LocalTensor<mm2OutputType> mm2_l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mm2OutputType>(0);
    AscendC::LocalTensor<int32_t> l0c_buf_int32_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, int32_t>(0);

    
    uint32_t k_bias_flag{0};
    uint32_t v_bias_flag{0};
    uint32_t num_tokens{0};
    uint32_t q_heads{0};
    uint32_t kv_heads{0};
    uint32_t embedding_size{0};
    uint32_t embedding_size_v{0};
    uint32_t block_size{0};
    uint32_t max_num_blocks_per_query{0};
    uint32_t group_num{0};
    uint32_t former_group_num_move{1};
    uint32_t tail_group_num_move{1};
    uint32_t former_head_split_num{1};
    uint32_t tail_head_split_num{1};
    uint32_t stride_kv{0};
    uint32_t stride_vo{0};
    uint32_t m{0};
    uint32_t __k{0};
    uint32_t __v{0};
    uint32_t round_k{0};
    uint32_t round_v{0};
    uint32_t core_per_batch{0};
    uint32_t process_num{0};
    uint64_t former_batch{0};
    uint32_t former_head_split{0};
    uint64_t tail_batch{0};
    uint32_t tail_head_split{0};
    uint32_t head_split_num{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t kv_split_per_core{0};
    uint32_t kv_split_core_num{1};
    uint32_t block_size_calc{0};

    uint32_t embed_split_size_qk{0};
    uint32_t embed_split_loop_qk{1};
    uint32_t embed_split_size_v{0};
    uint32_t embed_split_loop_v{1};
    bool is_multi_head_mmad{0};
    uint32_t move_l1b_offset = 0;
    uint32_t q_head_original{0};
    uint32_t compressHead{0};

    uint32_t l1_pingpong_flag = 0;
    uint32_t l1b_pingpong_flag = 0;
    uint32_t l0_pingpong_flag = 0;
    uint32_t l0b_pingpong_flag = 0;
    uint32_t l0c_pingpong_flag = 0;
    uint32_t l1p_pingpong_flag = 0;

    uint32_t l1_offset = l1_pingpong_flag * L1_UINT8_BUF_SIZE_DECODER / sizeof(IN_DTYPE);
    uint32_t l1b_offset = l1b_pingpong_flag * L1_KV_UINT8_SIZE / sizeof(IN_DTYPE);
    uint32_t l1_scale_offset = l1_pingpong_flag * L1_SCALE_UINT64_SIZE;
    uint32_t l1_bias_offset = l1_pingpong_flag * L1_OFFSET_INT32_SIZE;
    uint32_t l0_offset = l0_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l0c_offset = l0c_pingpong_flag * L0C_FLOAT_BUF_SIZE;
    uint32_t l0b_offset = l0b_pingpong_flag * L0AB_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    uint32_t l1p_start_offset = l1p_pingpong_flag * L1_P_UINT8_BUF_SIZE / sizeof(IN_DTYPE);
    bool is_l0c_pingpong_off = 0;
    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;
};

#elif __DAV_C220_VEC__
template <TilingKeyType tilingKeyType = TilingKeyType::TILING_HALF_DATA, typename IN_DTYPE = half, typename OUT_DTYPE = half, bool SplitKV = false, PagedAttnVariant pagedAttnVariant = PagedAttnVariant::DEFAULT, CompressType compressType = CompressType::COMPRESS_TYPE_UNDEFINED>
class UnpadAttentionDecoderAivBlockSize256{
public:
    using mm1OutputType = typename AttentionType<tilingKeyType>::mm1OutputType;
    using mm1CopyType = typename AttentionType<tilingKeyType>::mm1CopyType;
    using mmBiasType = typename AttentionType<tilingKeyType>::mmBiasType;
    using mmScaleType = typename AttentionType<tilingKeyType>::mmScaleType;
    using mm2OutputType = typename AttentionType<tilingKeyType>::mm2OutputType;
    using mm2CopyType = typename AttentionType<tilingKeyType>::mm2CopyType;
    static constexpr uint32_t T_BLOCK_SIZE =  BLOCK_SIZE_32 / sizeof(IN_DTYPE);
    static constexpr uint32_t T_BLOCK_OFFSET = 2 / sizeof(IN_DTYPE);

    __aicore__ __attribute__((always_inline)) inline UnpadAttentionDecoderAivBlockSize256(uint32_t prefill_batch_size, uint32_t decoder_batch_size) {
        prefill_batch_size_ = prefill_batch_size;
        decoder_batch_size_ = decoder_batch_size;
    }

    __aicore__ __attribute__((always_inline)) inline void InitQuant(
        __gm__ uint8_t *__restrict__ deq_scale1_in_gm,
        __gm__ uint8_t *__restrict__ offset1_in_gm,
        __gm__ uint8_t *__restrict__ deq_scale2_in_gm,
        __gm__ uint8_t *__restrict__ offset2_in_gm,
        __gm__ uint8_t *__restrict__ scale_in_gm
    )
    {
        deq_scale1_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(deq_scale1_in_gm));
        deq_scale2_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(deq_scale2_in_gm));
        scale_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmScaleType *>(scale_in_gm));
        if (pQuantType == 3) {
            pQuantOnline = 1;
        }
    }

    __aicore__ __attribute__((always_inline)) inline void SetArgs(
        __gm__ uint8_t *__restrict__ sync,
        __gm__ uint8_t* __restrict__ gm_k8,
        __gm__ uint8_t* __restrict__ gm_v8,
        __gm__ uint8_t* __restrict__ gm_scale1,
        __gm__ uint8_t* __restrict__ gm_offset1,
        __gm__ uint8_t* __restrict__ gm_scale2,
        __gm__ uint8_t* __restrict__ gm_offset2,
        __gm__ uint8_t* __restrict__ gm_block_table,
        __gm__ uint8_t *__restrict__ mask_in_gm,
        __gm__ uint8_t *__restrict__ o_out_gm,
        __gm__ uint8_t *__restrict__ s_out_gm,
        __gm__ uint8_t *__restrict__ p_out_gm,
        __gm__ uint8_t *__restrict__ o_temp_gm,
        __gm__ uint8_t *__restrict__ globalo_gm,
        __gm__ uint8_t *__restrict__ o_core_out_tmp_gm,
        __gm__ uint8_t *__restrict__ l_in_gm,
        __gm__ uint8_t* __restrict__ gm_k16,
        __gm__ uint8_t* __restrict__ gm_v16,
        __gm__ uint8_t *__restrict__ tiling_para_gm,
        __gm__ uint8_t *__restrict__ razorOffset,
        __gm__ uint8_t *__restrict__ logN_in_gm)
    {
        SetFftsBaseAddr((uint64_t)sync);
        sub_block_idx = static_cast<uint64_t>(GetSubBlockidx());
        SetAtomicnone();
        SetMasknorm();
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);

        mask_gm = reinterpret_cast<__gm__ OUT_DTYPE *>(mask_in_gm);
        o_gm = reinterpret_cast<__gm__ OUT_DTYPE *>(o_out_gm);
        s_gm = reinterpret_cast<__gm__ mm1CopyType *>(s_out_gm);
        p_gm = reinterpret_cast<__gm__ IN_DTYPE *>(p_out_gm);
        o_tmp_gm = reinterpret_cast<__gm__ mm2CopyType *>(o_temp_gm);
        go_gm = reinterpret_cast<__gm__ float *>(globalo_gm);
        o_core_tmp_gm = reinterpret_cast<__gm__ float *>(o_core_out_tmp_gm);
        tiling_gm = reinterpret_cast<__gm__ uint8_t *>(tiling_para_gm);
        l_gm = reinterpret_cast<__gm__ float *>(l_in_gm);
        razor_offset_gm = reinterpret_cast<__gm__ float *>(razorOffset);
        gm_block_tables_ = reinterpret_cast<__gm__ int32_t*>(gm_block_table);
        logN_gm = reinterpret_cast<__gm__ float *>(logN_in_gm);
        mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ OUT_DTYPE *>(mask_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ OUT_DTYPE *>(o_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1CopyType *>(s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2CopyType *>(o_tmp_gm));
        go_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(go_gm));
        o_core_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_core_tmp_gm));
        l_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(l_gm));
        razor_offset_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(razor_offset_gm));

        num_tokens = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm));
        q_heads = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_NUMHEADS));
        embedding_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM));
        embedding_size_v = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V));
        block_size = (int32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        max_num_blocks_per_query = (uint32_t)(*((__gm__ uint32_t*)tiling_para_gm + TILING_MAXBLOCKS));
        tor = (float)(*((__gm__ float *)tiling_para_gm + TILING_TOR));
        num_kv_heads = (uint32_t)(*((__gm__ uint32_t*)tiling_para_gm + TILING_KVHEADS));
        former_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_BATCH));
        former_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_HEAD));
        tail_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_BATCH));
        tail_head_split = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_HEAD));
        max_context_len = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MASK_MAX_LEN));
        batch_stride = (uint64_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BATCH_STRIDE));
        head_stride = (uint64_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEAD_STRIDE));
        tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        group_num = (uint32_t)(*((__gm__ uint32_t*)tiling_para_gm + TILING_GROUPNUM));

        kv_split_per_core = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVSPLIT));
        kv_split_core_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_KVCORENUM));
        block_size_calc = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_BLOCKSIZE_CALC));
        q_head_original = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QHEADORIGINAL));
        compressHead = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_COMPRESSHEAD));
        pQuantType = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_QUANTYPE));
        scaleType = (ScaleType)(*((__gm__ uint32_t *)tiling_para_gm + TILING_SCALETYPE));
        
        if constexpr (tilingKeyType != TilingKeyType::TILING_INT8_CUBE_QUANT) {
            former_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_FORMER_GROUP_MOVE));
            tail_group_num_move = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_TAIL_GROUP_MOVE));
        }

        go_flag_scalar = 1;
        gl_flag_scalar = 1;

        modCoef = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MODCOEF));
        divCoef = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_DIVCOEF));

        __k = embedding_size;
        round_k = RoundUp<T_BLOCK_SIZE>(__k);
        if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
            __v = embedding_size_v;
            round_v = RoundUp<BLOCK_SIZE>(__v);
            embed_split_size_v_former = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT_VECTOR_FORMER));
            embed_split_loop_v_former = (embedding_size_v + embed_split_size_v_former - 1) / embed_split_size_v_former;
            embed_split_size_v_tail = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADDIM_V_SPLIT_VECTOR_TAIL));
            embed_split_loop_v_tail = (embedding_size_v + embed_split_size_v_tail - 1) / embed_split_size_v_tail;
        }

        core_per_batch = (q_heads + split_size - 1) / split_size;
        process_num = num_tokens * core_per_batch;
        if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT || tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
            gm_k8_.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(gm_k8));
            gm_v8_.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(gm_v8));
            gm_k16_ping_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_k16) +
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_k16_pong_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_k16) +
                                    get_block_num() * block_size_calc * q_heads * embedding_size + 
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_v16_ping_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_v16) +
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_v16_pong_.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE*>(gm_v16) + 
                                    get_block_num() * block_size_calc * q_heads * embedding_size + 
                                    get_block_idx() * block_size_calc * q_heads * embedding_size);
            gm_block_tables_ = reinterpret_cast<__gm__ int32_t*>(gm_block_table);
            gm_scale1_.SetGlobalBuffer(reinterpret_cast<__gm__ float*>(gm_scale1));
            gm_offset1_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t*>(gm_offset1));
            gm_scale2_.SetGlobalBuffer(reinterpret_cast<__gm__ float*>(gm_scale2));
            gm_offset2_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t*>(gm_offset2));
            if (gm_offset1 != nullptr) {
                k_bias_flag = 1;
            }
            if (gm_offset2 != nullptr) {
                v_bias_flag = 1;
            }
        }
    }

    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        SET_FLAG(MTE3, V, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID2);
        SET_FLAG(MTE3, MTE2, EVENT_ID3);
        SET_FLAG(MTE3, MTE2, EVENT_ID4);
        SET_FLAG(V, MTE2, EVENT_ID4);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, V, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID2);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT){
            SET_FLAG(MTE2, V, EVENT_ID3);
            SET_FLAG(MTE2, V, EVENT_ID4);
        }
        core_per_batch = (q_heads + former_head_split - 1) / former_head_split;
        process_num = static_cast<uint64_t>(former_batch) * core_per_batch * kv_split_core_num;
        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {
            uint32_t cur_batch = process / (core_per_batch * kv_split_core_num) + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
            tor = (float)(*((__gm__ float *)tiling_gm + TILING_TOR));
            if(scaleType == ScaleType::SCALE_LOGN_FP32) {
                float tor_logN = (float)(*((__gm__ float *)logN_gm + cur_batch));
                tor = tor * tor_logN;
            }
            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t cur_head = (process / kv_split_core_num) % core_per_batch;
            uint32_t cur_nIndx = process % kv_split_core_num;
            uint32_t start_head = cur_head * former_head_split;
            uint32_t cur_kv_seqlen = kv_split_per_core;
            uint32_t kv_loop = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            if (cur_nIndx >= kv_loop) {
                continue;
            }
            if (cur_nIndx == (kv_loop - 1)) {
                cur_kv_seqlen = kv_seqlen - cur_nIndx * kv_split_per_core;
            }
            uint32_t cur_head_num = former_head_split;
            if (cur_head == (core_per_batch - 1)) {
                cur_head_num = q_heads - cur_head * former_head_split;
            }
            InnerRunVector(batch_idx, start_head, cur_nIndx, cur_kv_seqlen, cur_head_num, offset_tiling, kv_seqlen, embed_split_size_v_former, embed_split_loop_v_former);
        }
        if (tail_batch > 0) {
            core_per_batch = (q_heads + tail_head_split - 1) / tail_head_split;
            process_num = static_cast<uint64_t>(tail_batch) * core_per_batch;
            for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)) {
                uint32_t cur_batch = process / core_per_batch + former_batch + prefill_batch_size_;
                uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                uint32_t batch_idx = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 8 + offset_tiling));
                uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
                if (kv_seqlen == 0) {
                    continue;
                }
                tor = (float)(*((__gm__ float *)tiling_gm + TILING_TOR));
                if(scaleType == ScaleType::SCALE_LOGN_FP32) {
                    float tor_logN = (float)(*((__gm__ float *)logN_gm + cur_batch));
                    tor = tor * tor_logN;
                }
                uint32_t cur_kv_seqlen = kv_seqlen;
                uint32_t cur_nIndx = 0;
                uint32_t cur_head = process % core_per_batch;
                uint32_t cur_head_num = tail_head_split;
                if (cur_head == (core_per_batch - 1)) {
                    cur_head_num = q_heads - cur_head * tail_head_split;
                }
                uint32_t start_head = (process % core_per_batch) * tail_head_split;
                InnerRunVector(batch_idx, start_head, cur_nIndx, cur_kv_seqlen, cur_head_num, offset_tiling, kv_seqlen, embed_split_size_v_tail, embed_split_loop_v_tail);
            }
        }
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID4);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE3, V, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT){
            WAIT_FLAG(MTE2, V, EVENT_ID3);
            WAIT_FLAG(MTE2, V, EVENT_ID4);
        }
        PIPE_BARRIER(ALL);
        if (SplitKV) {
            int reduce_flag_id = 3;
            FftsCrossCoreSync<PIPE_MTE3, 0>(reduce_flag_id);
            WaitFlagDev(3);
            CombineScale(decoder_batch_size_, q_heads, kv_split_core_num, embedding_size);
        }
    }


private:

    __aicore__ __attribute__((always_inline)) inline void CopyScale(uint32_t sub_m, uint32_t l_offset, uint32_t o_offset)
    {
        SET_FLAG(V, MTE3, EVENT_ID2);
        WAIT_FLAG(V, MTE3, EVENT_ID2);
        ub_to_gm_align<ArchType::ASCEND_V220, float>(
            l_gm_tensor[(int64_t)l_offset],
            tv32_ubuf_tensor,
            0,               // sid
            sub_m,           // nBurst
            4,               // lenBurst
            0,               // leftPaddingNum
            0,               // rightPaddingNum
            0,                 // srcGap
            (kv_split_core_num - 1) * 4 // dstGap
        );
        if (gl_flag_scalar == 0) {
            SET_FLAG(MTE3, V, EVENT_ID2);
            gl_flag_scalar = 1;
        }
        uint32_t src_gap = ((__k % 16 <= 8) && (__k % 16 > 0))? 1 : 0;
        ub_to_gm_align<ArchType::ASCEND_V220, float>(
            o_core_tmp_gm_tensor[(int64_t)o_offset],
            go32_ubuf_tensor,
            0,        // sid
            sub_m,    // nBurst
            __k * 4,  // lenBurst
            0,        // leftPaddingNum
            0,        // rightPaddingNum
            src_gap,   // srcGap
            (kv_split_core_num - 1) * __k * 4  // dstGap
        );
    }
    __aicore__ __attribute__((always_inline)) inline void CombineScale(uint32_t num_tokens, uint32_t q_heads, uint32_t kv_split_core_num, uint32_t embedding_size)
    {
        SetAtomicnone();
        SetMasknorm();
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        const uint32_t ll_ubuf_stage2_offset = 0;  // 1 块，存放 存放local L fp32
        const uint32_t lm_ubuf_stage2_offset = 1 * STAGE2_UB_UINT8_BLOCK_SIZE;  //  1 块，存放 l max, fp32
        const uint32_t tl_ubuf_stage2_offset = 1 * STAGE2_UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE; // 1 块，存放中间结果tmp l, fp32
        const uint32_t rs_ubuf_stage2_offset = 2 * STAGE2_UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE; // 存放中间结果row sum, fp32
        const uint32_t ts_ubuf_stage2_offset = 2 * STAGE2_UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE; // 存放中间结果row sum, fp32
        const uint32_t gl_ubuf_stage2_offset = 2 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE; // 存放gloal scale, fp32
        const uint32_t lo_ubuf_stage2_offset = 4 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
        const uint32_t to_ubuf_stage2_offset = 8 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
        const uint32_t go_ubuf_stage2_offset = 12 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;
        const uint32_t go16_ubuf_stage2_offset = 16 * STAGE2_UB_UINT8_BLOCK_SIZE + 3 * UB_UINT8_LINE_SIZE;

        AscendC::LocalTensor<float> ll_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_stage2_offset);  // 1 块，存放 存放local L fp32
        AscendC::LocalTensor<float> lm_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm_ubuf_stage2_offset);  //  1 块，存放 l max, fp32
        AscendC::LocalTensor<float> tl_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tl_ubuf_stage2_offset); // 1 块，存放中间结果tmp l, fp32
        AscendC::LocalTensor<float> rs_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(rs_ubuf_stage2_offset); // 存放中间结果row sum, fp32
        AscendC::LocalTensor<float> ts_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ts_ubuf_stage2_offset); // 存放中间结果row sum, fp32
        AscendC::LocalTensor<float> gl_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_stage2_offset); // 存放gloal scale, fp32
        AscendC::LocalTensor<float> lo_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_stage2_offset);
        AscendC::LocalTensor<float> to_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(to_ubuf_stage2_offset);
        AscendC::LocalTensor<float> go_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_stage2_offset);
        AscendC::LocalTensor<OUT_DTYPE> go16_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(go16_ubuf_stage2_offset);

        uint32_t batch_size = num_tokens;
        uint32_t split_block = 1;
        uint32_t __k0 = embedding_size;
        uint32_t roundk_64 = (__k0 + 63) / 64 * 64;
        uint32_t roundk_8 = (__k0 + 7) / 8 * 8;
        uint32_t core_per_batch = (q_heads + split_block - 1) / split_block;

        uint32_t process_num = core_per_batch * batch_size;
        SET_FLAG(MTE3,MTE2,EVENT_ID0);
        for (uint32_t process = block_idx; process < process_num; process += uint32_t(block_num)){
            uint32_t cur_batch = process / core_per_batch + prefill_batch_size_;
            uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            cur_batch = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 13 + offset_tiling));
            offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 1 + offset_tiling));
            if (kv_seqlen == 0) {
                continue;
            }
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 6 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 7 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
            uint32_t addr_o_fd_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 15 + offset_tiling));
            uint32_t addr_o_fd_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 16 + offset_tiling));
            uint64_t addr_o_fd_scalar = (uint64_t)(((uint64_t)addr_o_fd_high32) << 32 | addr_o_fd_loww32);
            uint32_t addr_l_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 11 + offset_tiling));
            uint32_t addr_l_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 12 + offset_tiling));
            uint64_t addr_l_scalar = (uint64_t)(((uint64_t)addr_l_high32) << 32 | addr_l_loww32);

            uint32_t kv_seqlen_align = (kv_seqlen + block_size - 1) / block_size * block_size;
            uint32_t m_split = (kv_seqlen_align + kv_split_per_core - 1) /  kv_split_per_core;
            uint32_t cur_core = process % core_per_batch;
            uint32_t cur_head_num = split_block; // 每次计算的head数量
            if (cur_core == (core_per_batch - 1)){
                cur_head_num = q_heads - cur_core * split_block;
            }
            uint32_t start_head = cur_core * split_block;
            uint64_t addr_l_offset = addr_l_scalar;
            uint64_t addr_o_offset = addr_o_fd_scalar * kv_split_core_num;
            uint32_t l_remain = m_split % FLOAT_BLOCK_SIZE;
            WAIT_FLAG(MTE3,MTE2,EVENT_ID0);
            gm_to_ub_align<ArchType::ASCEND_V220, float>(
                ll_ubuf_stage2_tensor,
                l_gm_tensor[addr_l_offset + start_head * kv_split_core_num],
                0,                            // sid
                1,                            // nBurst
                m_split * 4,                  // lenBurst
                0,                           // leftPaddingNum
                FLOAT_BLOCK_SIZE - l_remain,  // rightPaddingNum
                0,                           // srcGap
                0   // dstGap
            );

            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);

            __set_mask(m_split);
            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(lm_ubuf_stage2_tensor,
                ll_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,                              // repeat
                1,                                           // dstRepeatStride
                1,                                           // srcBlockStride
                8                                            // srcRepeatStride
            );
            PIPE_BARRIER(V);

            // lse_accum - lse_max
            SET_FLAG(V, S, EVENT_ID3);
            WAIT_FLAG(V, S, EVENT_ID3);
            float lse_max = -(float)(*((__ubuf__ float*)lm_ubuf_stage2_tensor.GetPhyAddr()));
            SET_FLAG(S, V, EVENT_ID2);
            WAIT_FLAG(S, V, EVENT_ID2);
            adds_v<ArchType::ASCEND_V220, float>(tl_ubuf_stage2_tensor,
                ll_ubuf_stage2_tensor,
                lse_max,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                               // src0RepeatStride
            );
            PIPE_BARRIER(V);

            // expf
            exp_v<ArchType::ASCEND_V220, float>(tl_ubuf_stage2_tensor,
                tl_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);

            // rowsum lse_sum
            cadd_v<ArchType::ASCEND_V220, float>(rs_ubuf_stage2_tensor,
                tl_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,    // repeat
                1,                                           // dstRepeatStride
                1,                                           // srcBlockStride
                8                                            // srcRepeatStride
            );
            PIPE_BARRIER(V);
            __set_mask(cur_head_num);
            ln_v<ArchType::ASCEND_V220, float>(rs_ubuf_stage2_tensor,
                rs_ubuf_stage2_tensor,
                (cur_head_num + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,     // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);

            // logf(lse_sum) + lse_max
            add_v<ArchType::ASCEND_V220, float>(ts_ubuf_stage2_tensor,
                rs_ubuf_stage2_tensor,
                lm_ubuf_stage2_tensor,
                (cur_head_num + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,      // repeat
                1,                                // dstBlockStride
                1,                                // src0BlockStride
                1,                                // src1BlockStride
                8,                                // dstRepeatStride
                8,                                // src0RepeatStride
                8                                 // src1RepeatStride
            );
            PIPE_BARRIER(V);

            // scale = expf(lse_accum(l) - lse_logsum)
            __set_mask(m_split);
            SET_FLAG(V, S, EVENT_ID3);
            WAIT_FLAG(V, S, EVENT_ID3);
            float log_sum = -(float)(*((__ubuf__ float*)ts_ubuf_stage2_tensor.GetPhyAddr()));
            SET_FLAG(S, V, EVENT_ID2);
            WAIT_FLAG(S, V, EVENT_ID2);
            adds_v<ArchType::ASCEND_V220, float>(gl_ubuf_stage2_tensor,
                ll_ubuf_stage2_tensor,
                log_sum,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,           // dstBlockStride
                1,           // src0BlockStride
                8,           // dstRepeatStride
                8            // src0RepeatStride
            );
            PIPE_BARRIER(V);

            __set_mask(m_split);
            exp_v<ArchType::ASCEND_V220, float>(gl_ubuf_stage2_tensor,
                gl_ubuf_stage2_tensor,
                (m_split + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                8,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);
            // msplit * 1 * embedding
            gm_to_ub_align<ArchType::ASCEND_V220, float>(
                lo_ubuf_stage2_tensor,
                o_core_tmp_gm_tensor[addr_o_offset + start_head * kv_split_core_num * __k0],
                0,                                           // sid
                m_split,                                     // nBurst
                __k0 * 4,                                    // lenBurst
                0,                                           // leftPaddingNum
                0,                                           // rightPaddingNum
                0,                                           // srcGap
                0                                            // dstGap
            );
            SET_FLAG(MTE2, V, EVENT_ID1);
            WAIT_FLAG(MTE2, V, EVENT_ID1);
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            for (uint32_t n_idx = 0; n_idx < m_split; n_idx++){
                SET_FLAG(V, S, EVENT_ID3);
                WAIT_FLAG(V, S, EVENT_ID3);
                float scale = (float)(*((__ubuf__ float*)gl_ubuf_stage2_tensor.GetPhyAddr() + n_idx));
                SET_FLAG(S, V, EVENT_ID2);
                WAIT_FLAG(S, V, EVENT_ID2);

                muls_v<ArchType::ASCEND_V220, float>(to_ubuf_stage2_tensor,
                    lo_ubuf_stage2_tensor[n_idx * roundk_8],
                    scale,
                    (roundk_64 + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                    1,                                                   // dstBlockStride
                    1,                                                   // srcBlockStride
                    8,                                                               // dstRepeatStride
                    8                                                                // srcRepeatStride
                );
                PIPE_BARRIER(V);

                if (n_idx == 0){
                    adds_v<ArchType::ASCEND_V220, float>(go_ubuf_stage2_tensor,
                        to_ubuf_stage2_tensor,
                        0,
                        roundk_64 / FLOAT_VECTOR_SIZE,  // repeat
                        1,           // dstBlockStride
                        1,           // src0BlockStride
                        8,           // dstRepeatStride
                        8           // src0RepeatStride
                    );
                    PIPE_BARRIER(V);

                }
                else{
                    add_v<ArchType::ASCEND_V220, float>(go_ubuf_stage2_tensor,
                        to_ubuf_stage2_tensor,
                        go_ubuf_stage2_tensor,
                        roundk_64 / FLOAT_VECTOR_SIZE, // repeat
                        1,                          // dstBlockStride
                        1,                          // src0BlockStride
                        1,                          // src1BlockStride
                        8,                          // dstRepeatStride
                        8,                          // src0RepeatStride
                        8                           // src1RepeatStride
                    );
                    PIPE_BARRIER(V);

                }
            }
            conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(go16_ubuf_stage2_tensor,
                go_ubuf_stage2_tensor,
                roundk_64 / FLOAT_VECTOR_SIZE,   // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                4,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
            PIPE_BARRIER(V);
            SET_FLAG(V, MTE3, EVENT_ID1);
            WAIT_FLAG(V, MTE3, EVENT_ID1);
            ub_to_gm_align<ArchType::ASCEND_V220, OUT_DTYPE>(
                o_gm_tensor[addr_o_scalar + start_head * __k0],
                go16_ubuf_stage2_tensor,
                0,                       // sid
                1,                       // nBurst
                __k0 * 2,                // lenBurst
                0,                       // leftPaddingNum
                0,                       // rightPaddingNum
                0,                       // srcGap
                0                        // dstGap
            );
            SET_FLAG(MTE3,MTE2,EVENT_ID0);
        }
        WAIT_FLAG(MTE3,MTE2,EVENT_ID0);
    }

    __aicore__ __attribute__((always_inline)) inline void AddMask(
        AscendC::GlobalTensor<OUT_DTYPE> mask_gm_tensor,
        AscendC::LocalTensor<OUT_DTYPE> mask_ubuf_tensor,
        AscendC::LocalTensor<float> mask32_ubuf_tensor,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t mask_offset)
    {
        uint32_t mask_repeat_stride = head_stride == 0 ? 0 : qk_round_n / FLOAT_BLOCK_SIZE;
        uint32_t mask_nburst = head_stride == 0 ? 1 : sub_m;
        gm_to_ub_align<ArchType::ASCEND_V220, OUT_DTYPE>(
            mask_ubuf_tensor,
            mask_gm_tensor,
            0,                                 // sid
            mask_nburst,                             // nBurst
            qk_n * 2,                          // lenBurst
            0,                                 // leftPaddingNum
            0,                                 // rightPaddingNum
            (max_context_len - qk_n) * 2,      // srcGap
            0                                  // dstGap
        );
        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        conv_v<ArchType::ASCEND_V220, OUT_DTYPE, float>(mask32_ubuf_tensor,
            mask_ubuf_tensor,
            (mask_nburst * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
            1,                                                         // dstBlockStride
            1,                                                         // srcBlockStride
            8,                                                         // dstRepeatStride
            4                                                          // srcRepeatStride
        );
        PIPE_BARRIER(V);
        // *** ls = ls + mask
        if (qk_round_n  > FLOAT_BLOCK_SIZE * 255) {
            for (uint32_t vadd_idx = 0; vadd_idx < sub_m; ++vadd_idx){
                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * qk_round_n],
                    ls32_ubuf_tensor[vadd_idx * qk_round_n],
                    mask32_ubuf_tensor[vadd_idx * mask_repeat_stride * FLOAT_BLOCK_SIZE],
                    qk_n / FLOAT_VECTOR_SIZE,       // repeat
                    1,                              // dstBlockStride
                    1,                              // src0BlockStride
                    1,                              // src1BlockStride
                    8,                              // dstRepeatStride
                    8,                               // src0RepeatStride
                    8                               // src1RepeatStride
                );
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                uint32_t offset = qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE;
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                for (uint32_t vadd_idx = 0; vadd_idx < sub_m; ++vadd_idx) {
                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * qk_round_n + offset],
                        ls32_ubuf_tensor[vadd_idx * qk_round_n + offset],
                        mask32_ubuf_tensor[vadd_idx * qk_round_n + offset],
                        1,                               // repeat
                        1,                              // dstBlockStride
                        1,                              // src0BlockStride
                        1,                              // src1BlockStride
                        1,                              // dstRepeatStride
                        1,                              // src0RepeatStride
                        1                               // src1RepeatStride
                    );
                }
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        } else {
            for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                    ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                    mask32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                    sub_m,                          // repeat
                    1,                              // dstBlockStride
                    1,                              // src0BlockStride
                    1,                              // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    mask_repeat_stride                               // src1RepeatStride
                );
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    mask32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    sub_m,                          // repeat
                    1,                              // dstBlockStride
                    1,                              // src0BlockStride
                    1,                              // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    mask_repeat_stride                               // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
        }
        PIPE_BARRIER(V);
    }

   __aicore__ __attribute__((always_inline)) inline void ReduceMaxRepeatN(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& tempTensor,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        ub_to_ub<ArchType::ASCEND_V220, float>(
            tempTensor,
            src,
            0,                                             // sid
            sub_m,                                         // nBurst
            HALF_VECTOR_SIZE / BLOCK_SIZE,                 // lenBurst
            (qk_round_n - FLOAT_VECTOR_SIZE) / FLOAT_BLOCK_SIZE,  // srcGap
            0                                              // dstGap
        );
        PIPE_BARRIER(V);
       
        for (uint32_t rowmax_idx = 0; rowmax_idx < sub_m; ++rowmax_idx) {
            max_v<ArchType::ASCEND_V220, float>(
                tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                src[FLOAT_VECTOR_SIZE + rowmax_idx * qk_round_n],
                tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                (qk_n - FLOAT_VECTOR_SIZE) / FLOAT_VECTOR_SIZE , // repeat
                1,                             // dstBlockStride
                1,                             // src0BlockStride
                1,                             // src1BlockStride
                0,                             // dstRepeatStride
                8,                             // src0RepeatStride
                0                              // src1RepeatStride
            );

        }
        PIPE_BARRIER(V);
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            for (uint32_t rowmax_idx = 0; rowmax_idx < sub_m ; ++rowmax_idx) {
                max_v<ArchType::ASCEND_V220, float>(
                    tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                    src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE + rowmax_idx * qk_round_n],
                    tempTensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                    1,                              // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    1,                             // dstRepeatStride
                    1,                             // src0RepeatStride
                    1                               // src1RepeatStride
                );
            }
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
        cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(
            dst,
            tempTensor,
            sub_m,      // repeat
            1,          // dstRepeatStride
            1,          // srcBlockStride
            8           // srcRepeatStride
        );
        PIPE_BARRIER(V);
    }

   __aicore__ __attribute__((always_inline)) inline void ReduceMaxRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& tempTensor,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        if (qk_n <= FLOAT_VECTOR_SIZE) {
            __set_mask(qk_n);
            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(dst,
                src,
                sub_m,                    // repeat
                1,                        // dstRepeatStride
                1,                        // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
            );
        } else {
            ub_to_ub<ArchType::ASCEND_V220, float>(
                tempTensor,
                src,
                0,                                             // sid
                sub_m,                                         // nBurst
                HALF_VECTOR_SIZE / BLOCK_SIZE,                 // lenBurst
                (qk_round_n - FLOAT_VECTOR_SIZE) / FLOAT_BLOCK_SIZE,  // srcGap
                0                                              // dstGap
            );
            PIPE_BARRIER(V);
            for (uint32_t rowmax_idx = 1; rowmax_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowmax_idx) {
                max_v<ArchType::ASCEND_V220, float>(
                    tempTensor,
                    tempTensor,
                    src[rowmax_idx * FLOAT_VECTOR_SIZE],
                    sub_m,                         // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    8,                             // dstRepeatStride
                    8,                             // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                );
            PIPE_BARRIER(V);
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                max_v<ArchType::ASCEND_V220, float>(
                    tempTensor,
                    tempTensor,
                    src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    sub_m,                         // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    8,                             // dstRepeatStride
                    8,                             // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                );
            }
            PIPE_BARRIER(V);
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            cmax_v<ArchType::ASCEND_V220, float, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(
                dst,
                tempTensor,
                sub_m,      // repeat
                1,          // dstRepeatStride
                1,          // srcBlockStride
                8           // srcRepeatStride
            );
        }
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void ReduceSumRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        if (qk_n <= FLOAT_VECTOR_SIZE) {
            __set_mask(qk_n);
            cadd_v<ArchType::ASCEND_V220, float>(
                dst,
                src,
                sub_m,           // repeat
                1,               // dstRepeatStride
                1,               // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        } else {
            for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                add_v<ArchType::ASCEND_V220, float>(
                    src,
                    src,
                    src[rowsum_idx * FLOAT_VECTOR_SIZE],
                    sub_m,           // repeat
                    1,               // dstBlockStride
                    1,               // src0BlockStride
                    1,               // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE   // src1RepeatStride
                );
                PIPE_BARRIER(V);
            }
            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                add_v<ArchType::ASCEND_V220, float>(
                    src,
                    src,
                    src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    sub_m,           // repeat
                    1,               // dstBlockStride
                    1,               // src0BlockStride
                    1,               // src1BlockStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    qk_round_n / FLOAT_BLOCK_SIZE   // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
            
            cadd_v<ArchType::ASCEND_V220, float>(
                dst,
                src,
                sub_m,           // repeat
                1,               // dstRepeatStride
                1,               // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE   // srcRepeatStride
            );
        }
    }

    __aicore__ __attribute__((always_inline)) inline void TensorSubValueRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& MaxTensor,
        const AscendC::LocalTensor<float>& tempMaxTensor,
        uint32_t sub_m,
        uint32_t round_sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n)
    {
        brcb_v<ArchType::ASCEND_V220, uint32_t>(
            tempMaxTensor.ReinterpretCast<uint32_t>(),
            MaxTensor.ReinterpretCast<uint32_t>(),
            1,               // dstBlockStride
            8,               // dstRepeatStride
            round_sub_m / FLOAT_BLOCK_SIZE  // repeat
        );
        PIPE_BARRIER(V);
        for (uint32_t sub_v_idx = 0; sub_v_idx < qk_n / FLOAT_VECTOR_SIZE; ++sub_v_idx) {
            sub_v<ArchType::ASCEND_V220, float>(dst[sub_v_idx * FLOAT_VECTOR_SIZE],
                src[sub_v_idx * FLOAT_VECTOR_SIZE],
                tempMaxTensor,
                sub_m,                    // repeat
                1,                        // dstBlockStride
                1,                        // src0BlockStride
                0,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                1                         // src1RepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            sub_v<ArchType::ASCEND_V220, float>(dst[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                tempMaxTensor,
                sub_m,                    // repeat
                1,                        // dstBlockStride
                1,                        // src0BlockStride
                0,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                1                         // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void TensorDivRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& src1,
        uint32_t sub_m, uint32_t qk_n, uint32_t qk_round_n)
    {
        PIPE_BARRIER(V);
        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
            div_v<ArchType::ASCEND_V220, float>(dst[vadd_idx * FLOAT_VECTOR_SIZE],
                src[vadd_idx * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                  // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                0,                                     // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // src0RepeatStride
                1                                       // src1RepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            div_v<ArchType::ASCEND_V220, float>(dst[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                   // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                0,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,         // src0RepeatStride
                1                                      // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void TensorMulRepeatM(
        const AscendC::LocalTensor<float>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& src1,
        uint32_t sub_m, uint32_t qk_n, uint32_t qk_round_n, uint32_t src1BlockStride
    ) {
        PIPE_BARRIER(V);
        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
            mul_v<ArchType::ASCEND_V220, float>(dst[vadd_idx * FLOAT_VECTOR_SIZE],
                src[vadd_idx * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                  // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                src1BlockStride,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // src0RepeatStride
                1                                       // src1RepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            mul_v<ArchType::ASCEND_V220, float>(dst[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                src1,
                sub_m,                                   // repeat
                1,                                      // dstBlockStride
                1,                                      // src0BlockStride
                src1BlockStride,                        // src1BlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,          // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE,         // src0RepeatStride
                1                                      // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void QuantPerTokenImpl(
        const AscendC::LocalTensor<IN_DTYPE>& dst,
        const AscendC::LocalTensor<float>& src,
        const AscendC::LocalTensor<float>& scale,
        uint32_t sub_m, uint32_t qk_n, uint32_t qk_round_n, uint32_t pQuantOnline)
    {
        
        if (pQuantOnline) {
            // scr / scale 提函数
            TensorDivRepeatM(dst.template ReinterpretCast<float>(), src, scale, sub_m, qk_n, qk_round_n);
        } else {
            // scr * scale
            TensorMulRepeatM(dst.template ReinterpretCast<float>(), src, scale, sub_m, qk_n, qk_round_n, 0);
        }
        // src fp32 -> casttofp16 -> casttoint8
        uint32_t count = sub_m * qk_round_n;
        uint32_t repeat_times = (count + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;
        if (repeat_times < 255) {
            conv_v<ArchType::ASCEND_V220, float, half>(
                dst.template ReinterpretCast<half>(), // dst
                dst.template ReinterpretCast<float>(), // src
                repeat_times,                  // repeat_times
                1,                            // dstBlockStride
                1,                            // srcBlockStride
                4,                            // dstRepeatStride
                8                             // srcRepeatStride
            );
        } else {
            for (uint64_t vconv_idx = 0; vconv_idx < 2; ++vconv_idx) {   // 一次迭代做一半，循环防止超出 repeat 范围（<=255)
                conv_v<ArchType::ASCEND_V220, float, half>(
                    dst.template ReinterpretCast<half>()[vconv_idx * count / 2], // dst
                    dst.template ReinterpretCast<float>()[vconv_idx * count / 2], // src
                    (count / 2 + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,             // repeat_times
                    1,                                                                   // dstBlockStride
                    1,                                                                   // srcBlockStride
                    4,                                                                   // dstRepeatStride
                    8                                                                    // srcRepeatStride
                );
            }
        }
        PIPE_BARRIER(V);
        for (uint32_t row_idx = 0; row_idx < qk_n / HALF_VECTOR_SIZE; ++row_idx) {
            AscendC::Cast<int8_t, half, false>(dst.template ReinterpretCast<int8_t>()[row_idx * HALF_VECTOR_SIZE],
                                               dst.template ReinterpretCast<half>()[row_idx * HALF_VECTOR_SIZE], AscendC::RoundMode::CAST_RINT,
                                               (uint64_t)0, sub_m, {1, 1, (uint8_t)((qk_round_n) / BLOCK_SIZE), (uint8_t)(qk_round_n / BLOCK_SIZE)});
        }
        if (qk_n % HALF_VECTOR_SIZE > 0) {
            __set_mask(qk_n % HALF_VECTOR_SIZE);
            AscendC::Cast<int8_t, half, false>(dst.template ReinterpretCast<int8_t>()[qk_n / HALF_VECTOR_SIZE * HALF_VECTOR_SIZE],
                                               dst.template ReinterpretCast<half>()[qk_n / HALF_VECTOR_SIZE * HALF_VECTOR_SIZE], AscendC::RoundMode::CAST_RINT,
                                               (uint64_t)0, sub_m, {1, 1, (uint8_t)((qk_round_n) / BLOCK_SIZE), (uint8_t)(qk_round_n / BLOCK_SIZE)});
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }
    
    __aicore__ __attribute__((always_inline)) inline void DeQuantPerHeadImpl(
        const AscendC::GlobalTensor<mmScaleType>& deScaleGm,
        const AscendC::GlobalTensor<int32_t>& src,
        AscendC::LocalTensor<float> dst,
        AscendC::LocalTensor<int32_t> temp,
        AscendC::LocalTensor<mmScaleType> deScaleUb,
        AscendC::LocalTensor<mmScaleType> tempScale,
        AscendC::LocalTensor<float> quantScale,
        uint32_t sub_m,
        uint32_t qk_n,
        uint32_t qk_round_n,
        bool online,
        bool move_tensor
    ){        
        gm_to_ub_align<ArchType::ASCEND_V220, mmScaleType>(deScaleUb,
                                                        deScaleGm,
                                                        0,                                      // sid
                                                        1,                                      // nBurst
                                                        sub_m * sizeof(mmScaleType),             // lenBurst
                                                        0,                                      // leftPaddingNum
                                                        0,                                      // rightPaddingNum
                                                        0,                                      // srcGap
                                                        0                                       // dstGap
        );
        if (online) {
            // if dequant online need mul p quant scale
            SET_FLAG(MTE2, V, EVENT_ID2);
            WAIT_FLAG(MTE2, V, EVENT_ID2);
            TensorMulRepeatM(deScaleUb, deScaleUb, quantScale, 1, sub_m, RoundUp<16>(sub_m), 1);
        }

        if (move_tensor) {
            gm_to_ub<ArchType::ASCEND_V220, int32_t>(
                temp,
                src,
                0,                        // sid
                1,                        // nBurst
                CeilDiv<FLOAT_BLOCK_SIZE>(sub_m * qk_round_n),  // lenBurst
                0,                        // srcGap
                0                         // dstGap
            );
        }
        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);
        brcb_v<ArchType::ASCEND_V220, uint32_t>(
            tempScale.template ReinterpretCast<uint32_t>(),
            deScaleUb.template ReinterpretCast<uint32_t>(),
            1,               // dstBlockStrides
            8,               // dstRepeatStride
            RoundUp<16>(sub_m) / FLOAT_BLOCK_SIZE  // repeat
        );
        PIPE_BARRIER(V);
        uint32_t count = sub_m * qk_round_n;
        uint32_t repeat_times = (count + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;
        if (repeat_times < 255) {
            conv_v<ArchType::ASCEND_V220, int32_t, float>(
                dst, // dst
                temp, // src
                repeat_times,                  // repeat_times
                1,                            // dstBlockStride
                1,                            // srcBlockStride
                8,                            // dstRepeatStride
                8                             // srcRepeatStride
            );
        } else {
            for (uint64_t vconv_idx = 0; vconv_idx < 2; ++vconv_idx) {   // 一次迭代做一半，循环防止超出 repeat 范围（<=255)
                conv_v<ArchType::ASCEND_V220, int32_t, float>(
                    dst[vconv_idx * count / 2], // dst
                    temp[vconv_idx * count / 2], // src
                    (count / 2 + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,             // repeat_times
                    1,                                                                   // dstBlockStride
                    1,                                                                   // srcBlockStride
                    8,                                                                   // dstRepeatStride
                    8                                                                    // srcRepeatStride
                );
            }
        }
        TensorMulRepeatM(dst, dst, tempScale, sub_m, qk_n, qk_round_n, 0);
        PIPE_BARRIER(V);
    }
   __aicore__ __attribute__((always_inline)) inline void SoftmaxStage1(
        AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor,
        AscendC::GlobalTensor<mm1CopyType> s_gm_tensor,
        AscendC::GlobalTensor<OUT_DTYPE> mask_gm_tensor,
        AscendC::LocalTensor<float> dm32_ubuf_tensor,
        AscendC::LocalTensor<float> ll_ubuf_tensor,
        AscendC::LocalTensor<float> pm32_ubuf_tensor,
        uint32_t n_idx,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t sub_m,
        uint32_t mask_offset,
        const uint32_t sub_n_loop,
        const uint32_t cur_batch,
        const uint32_t start_kv,
        const uint32_t real_n_loop,
	    const uint32_t head_idx,
        const uint32_t pm_flag_scalar
    )
    {
        uint32_t sub_m_d128 = (sub_m + 127) / 128;  // up aligned to 128
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 128
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;
        float quantMax = (float)1 / (float)127;
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
            if constexpr (SplitKV) { // softmaxstage1 wait softmaxstage2 
                if (n_idx == 0) {
                    if (gl_flag_scalar == 1) {
                        WAIT_FLAG(MTE3, V, EVENT_ID2);
                        gl_flag_scalar = 0;
                    }
                }
            }
            DeQuantPerHeadImpl(
                deq_scale1_gm_tensor[head_idx],
                s_gm_tensor,
                ls32_ubuf_tensor, lsint32_ubuf_tensor, 
                descale1_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, qk_n, qk_round_n, 0, 1);
        } else {
            gm_to_ub<ArchType::ASCEND_V220, mm1CopyType>(
                ls32_ubuf_tensor.template ReinterpretCast<mm1CopyType>(),
                s_gm_tensor,
                0,                        // sid
                1,                        // nBurst
                sub_m * qk_round_n / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                        // srcGap
                0                         // dstGap
            );
        }

        if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
            if (razor_offset_gm != nullptr) {
                WAIT_FLAG(V, MTE2, EVENT_ID0);
                WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                for (uint32_t ni = 0; ni < sub_n_loop; ++ni) {
                    
                    uint32_t actual_idx = n_idx * sub_n_loop + ni;
                    if (actual_idx >= real_n_loop) {
                        break;
                    }
                    uint32_t block_table_id = (uint32_t)(*(gm_block_tables_ + cur_batch * max_num_blocks_per_query +
                                                    start_kv / block_size + actual_idx));
                    SET_FLAG(S, MTE2, EVENT_ID0);
                    WAIT_FLAG(S, MTE2, EVENT_ID0);
                    gm_to_ub_align<ArchType::ASCEND_V220, float>(
                        mask32_ubuf_tensor[ni * block_size],
                        razor_offset_gm_tensor[(uint64_t)block_table_id * block_size],
                        0,                          // sid
                        1,                          // nBurst
                        block_size * 4,             // lenBurst
                        0,                          // leftPaddingNum
                        0,                          // rightPaddingNum
                        0,                          // srcGap
                        0                           // dstGap
                    );
                }

                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);

                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);

                for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                        ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                        mask32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                        sub_m,                          // repeat
                        1,                              // dstBlockStride
                        1,                              // src0BlockStride
                        1,                              // src1BlockStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                        0                               // src1RepeatStride
                    );
                }
                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                    add_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        mask32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                        sub_m,                          // repeat
                        1,                              // dstBlockStride
                        1,                              // src0BlockStride
                        1,                              // src1BlockStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                        qk_round_n / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                        0   // src1RepeatStride
                    );
                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                }

                SET_FLAG(V, MTE2, EVENT_ID0);
                SET_FLAG(MTE3, MTE2, EVENT_ID0);
                PIPE_BARRIER(V);
            }
        }

        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);

        for (uint32_t vadd_idx = 0; vadd_idx < qk_n / FLOAT_VECTOR_SIZE; ++vadd_idx) {
            muls_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                ls32_ubuf_tensor[vadd_idx * FLOAT_VECTOR_SIZE],
                tor,
                sub_m,                          // repeat
                1,                              // dstBlockStride
                1,                              // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
            );
        }
        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
            muls_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                tor,
                sub_m,                          // repeat
                1,                              // dstBlockStride
                1,                              // srcBlockStride
                qk_round_n / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);

        if (mask_gm != nullptr) {
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
            AddMask(mask_gm_tensor, mask_ubuf_tensor, mask32_ubuf_tensor, sub_m, qk_n, qk_round_n, mask_offset);
            PIPE_BARRIER(V);
            SET_FLAG(V, MTE2, EVENT_ID0);
        }

        // *** lm = rowmax(ls)
        ReduceMaxRepeatM(lm32_ubuf_tensor, ls32_ubuf_tensor, lp32_ubuf_tensor, sub_m, qk_n, qk_round_n);
        if (n_idx != 0) {
            // *** hm = vmax(lm, gm)
            max_v<ArchType::ASCEND_V220, float>(hm32_ubuf_tensor,
                lm32_ubuf_tensor,
                gm32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,           // dstBlockStride
                1,           // src0BlockStride
                1,           // src1BlockStride
                8,           // dstRepeatStride
                8,           // src0RepeatStride
                8            // src1RepeatStride
            );
            PIPE_BARRIER(V);
            // *** dm = gm - hm
            sub_v<ArchType::ASCEND_V220, float>(dm32_ubuf_tensor,
                gm32_ubuf_tensor,
                hm32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,           // dstBlockStride
                1,           // src0BlockStride
                1,           // src1BlockStride
                8,           // dstRepeatStride
                8,           // src0RepeatStride
                8            // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            // *** hm = lm
            ub_to_ub<ArchType::ASCEND_V220, float>(
                hm32_ubuf_tensor,
                lm32_ubuf_tensor,
                0,                         // sid
                1,                         // nBurst
                round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                         // srcGap
                0                          // dstGap
            );
            PIPE_BARRIER(V);
        }
        // *** gm = hm
        ub_to_ub<ArchType::ASCEND_V220, float>(
            gm32_ubuf_tensor,
            hm32_ubuf_tensor,
            0,                         // sid
            1,                         // nBurst
            round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
            0,                         // srcGap
            0                          // dstGap
        );
        PIPE_BARRIER(V);
        // *** hm_block = expand_to_block(hm), 存放于 tv
        if constexpr (SplitKV && tilingKeyType != TilingKeyType::TILING_QUANT_FP16OUT && tilingKeyType != TilingKeyType::TILING_QUANT_BF16OUT) {
            if (n_idx == 0) {
                if (gl_flag_scalar == 1) {
                    WAIT_FLAG(MTE3, V, EVENT_ID2);
                    gl_flag_scalar = 0;
                }
            }
        }
        // *** ls = ls - hm_block
        TensorSubValueRepeatM(ls32_ubuf_tensor, ls32_ubuf_tensor,
                           hm32_ubuf_tensor, tv32_ubuf_tensor,
                           sub_m, round_sub_m, qk_n, qk_round_n);
        // *** ls = exp(ls)
        exp_v<ArchType::ASCEND_V220, float>(ls32_ubuf_tensor,
            ls32_ubuf_tensor,
            (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
            1,                               // dstBlockStride
            1,                               // srcBlockStride
            8,                               // dstRepeatStride
            8                                // srcRepeatStride
        );
        PIPE_BARRIER(V);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT){
            if (pm_flag_scalar == 1) {
                WAIT_FLAG(MTE2, V, EVENT_ID3);
            } else {
                WAIT_FLAG(MTE2, V, EVENT_ID4);
            }
            if (pQuantOnline) {
                sub_v<ArchType::ASCEND_V220, float>(pm32_ubuf_tensor,
                    lm32_ubuf_tensor,
                    hm32_ubuf_tensor,
                    sub_m_d64,   // repeat
                    1,           // dstBlockStride
                    1,           // src0BlockStride
                    1,           // src1BlockStride
                    8,           // dstRepeatStride
                    8,           // src0RepeatStride
                    8            // src1RepeatStride
                );
                PIPE_BARRIER(V);
                exp_v<ArchType::ASCEND_V220, float>(pm32_ubuf_tensor,
                    pm32_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,                               // dstBlockStride
                    1,                               // srcBlockStride
                    8,                               // dstRepeatStride
                    8                                // srcRepeatStride
                );
                PIPE_BARRIER(V);
                muls_v<ArchType::ASCEND_V220, float>(pm32_ubuf_tensor,
                    pm32_ubuf_tensor,
                    quantMax,
                    sub_m_d64,              // repeat
                    1,                      // dstBlockStride
                    1,                      // srcBlockStride
                    8,                      // dstRepeatStride
                    8                        // srcRepeatStride
                );
            } else {
                gm_to_ub_align<ArchType::ASCEND_V220, mmScaleType>(pm32_ubuf_tensor,
                                                                scale_gm_tensor[head_idx],
                                                                0,                                      // sid
                                                                1,                                      // nBurst
                                                                sub_m * sizeof(mmScaleType),             // lenBurst
                                                                0,                                      // leftPaddingNum
                                                                0,                                      // rightPaddingNum
                                                                0,                                      // srcGap
                                                                0                                       // dstGap
                );
                SET_FLAG(MTE2, V, EVENT_ID2);
                WAIT_FLAG(MTE2, V, EVENT_ID2);
            }
            PIPE_BARRIER(V);
            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                pm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            QuantPerTokenImpl(lp_ubuf_tensor, ls32_ubuf_tensor, tv32_ubuf_tensor, sub_m, qk_n, qk_round_n, pQuantOnline);
        } else {
            // *** lp = castfp32to16(ls)
            conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(lp_ubuf_tensor,
                ls32_ubuf_tensor,
                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                               // dstBlockStride
                1,                               // srcBlockStride
                4,                               // dstRepeatStride
                8                                // srcRepeatStride
            );
        }
        PIPE_BARRIER(V);
        SET_FLAG(V, MTE3, EVENT_ID0);
        WAIT_FLAG(V, MTE3, EVENT_ID0);
        ub_to_gm<ArchType::ASCEND_V220, IN_DTYPE>(
            p_gm_tensor,
            lp_ubuf_tensor,
            0,                        // sid
            1,                        // nBurst
            sub_m * qk_round_n * T_BLOCK_OFFSET / T_BLOCK_SIZE,  // lenBurst
            0,                        // srcGap
            0                         // dstGap
        );
        if (mask_gm != nullptr){
            SET_FLAG(MTE3, MTE2, EVENT_ID0);
        }
        // *** ll = rowsum(ls32)
        ReduceSumRepeatM(ll_ubuf_tensor, ls32_ubuf_tensor, sub_m, qk_n, qk_round_n);
        SET_FLAG(V, MTE2, EVENT_ID2);
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void SoftmaxStage2MLA(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<float> go_gm_tensor,
        AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor,
        AscendC::LocalTensor<float> dm32_ubuf_tensor,
        AscendC::LocalTensor<float> ll_ubuf_tensor,
        AscendC::LocalTensor<float> pm32_ubuf_tensor,
        uint32_t n_idx,
        uint32_t n_loop,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t sub_m,
        uint64_t l_offset,
        uint64_t o_offset,
        uint32_t head_idx,
        uint32_t embed_split_size,
        uint32_t round_embed_split_size,       
        uint32_t embed_split_idx,
        uint32_t embed_split_loop_v,
        uint32_t pm_flag_scalar)
    {
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 64
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        if (n_idx != 0) {
            gm_to_ub<ArchType::ASCEND_V220, mm2CopyType>(
                lo_ubuf_tensor.template ReinterpretCast<mm2CopyType>(),
                o_tmp_gm_tensor,
                0,                    // sid
                sub_m,                    // nBurst
                round_embed_split_size / FLOAT_BLOCK_SIZE,  // lenBurst
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE,                    // srcGap
                0                     // dstGap
            );
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);
            if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
                DeQuantPerHeadImpl(
                    deq_scale2_gm_tensor[head_idx],
                    o_tmp_gm_tensor,
                    lo_ubuf_tensor, loint32_ubuf_tensor,// loint32_ubuf_tensor  lo_ubuf_tensor use the same ptr
                    descale2_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, embed_split_size, round_embed_split_size, pQuantOnline, 0);
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        if (pm_flag_scalar == 1) {
                            SET_FLAG(MTE2, V, EVENT_ID3);
                        } else {
                            SET_FLAG(MTE2, V, EVENT_ID4);
                        }
                    }
            }
        }

        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID4);
        if (n_idx != 0) {
            if (embed_split_idx == 0) {
                // *** dm = exp(dm)
                exp_v<ArchType::ASCEND_V220, float>(dm32_ubuf_tensor,
                    dm32_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,          // dstBlockStride
                    1,          // srcBlockStride
                    8,          // dstRepeatStride
                    8           // srcRepeatStride
                );
                PIPE_BARRIER(V);
                // *** gl = dm * gl
                mul_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                    dm32_ubuf_tensor,
                    gl32_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,          // dstBlockStride
                    1,          // src0BlockStride
                    1,          // src1BlockStride
                    8,          // dstRepeatStride
                    8,          // src0RepeatStride
                    8           // src1RepeatStride
                );
                PIPE_BARRIER(V);
                // *** gl = ll + gl
                add_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                    gl32_ubuf_tensor,
                    ll_ubuf_tensor,
                    sub_m_d64,  // repeat
                    1,          // dstBlockStride
                    1,          // src0BlockStride
                    1,          // src1BlockStride
                    8,          // dstRepeatStride
                    8,          // src0RepeatStride
                    8           // src1RepeatStride
                );
                PIPE_BARRIER(V);
            }
            // *** dm_block = expand_to_block(dm), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                dm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);

            gm_to_ub<ArchType::ASCEND_V220, float>(
                go32_ubuf_tensor,
                go_gm_tensor,
                0,
                sub_m,
                round_embed_split_size / FLOAT_BLOCK_SIZE,
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE,
                0
            );
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);

            // *** go = go * dm_block
            for (uint32_t vmul_idx = 0; vmul_idx < embed_split_size / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
            }
            if (embed_split_size % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(embed_split_size % FLOAT_VECTOR_SIZE);
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
            // *** go = lo + go
            add_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor,
                go32_ubuf_tensor,
                lo_ubuf_tensor,
                (sub_m * round_embed_split_size + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                            // dstBlockStride
                1,                            // src0BlockStride
                1,                            // src1BlockStride
                8,                            // dstRepeatStride
                8,                            // src0RepeatStride
                8                             // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            if (embed_split_idx == 0) {
                // *** gl = ll
                ub_to_ub<ArchType::ASCEND_V220, float>(
                    gl32_ubuf_tensor,
                    ll_ubuf_tensor,
                    0,                // sid
                    1,                // nBurst
                    round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                    0,                // srcGap
                    0                 // dstGap
                );
                PIPE_BARRIER(V);
            }
            gm_to_ub<ArchType::ASCEND_V220, mm2CopyType>(
                go32_ubuf_tensor.template ReinterpretCast<mm2CopyType>(),
                o_tmp_gm_tensor,
                0,                    // sid
                sub_m,                    // nBurst
                round_embed_split_size / FLOAT_BLOCK_SIZE,  // lenBurst
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE,                    // srcGap
                0                     // dstGap
            );
            if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
                DeQuantPerHeadImpl(
                    deq_scale2_gm_tensor[head_idx],
                    o_tmp_gm_tensor,
                    go32_ubuf_tensor, go32_ubuf_tensor.template ReinterpretCast<mm2CopyType>(),
                    descale2_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, embed_split_size, round_embed_split_size, pQuantOnline, 0);
                    if (embed_split_idx == embed_split_loop_v - 1) {
                        SET_FLAG(MTE2, V, EVENT_ID3);
                    }
            } else {
                SET_FLAG(MTE2, V, EVENT_ID0);
                WAIT_FLAG(MTE2, V, EVENT_ID0);
            }
        }
        SET_FLAG(V, MTE2, EVENT_ID0);

        if (n_idx == n_loop - 1) {
            // *** gl_block = expand_to_block(gl), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                gl32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);
            // *** go = go / gl_block
            for (uint32_t vdiv_idx = 0; vdiv_idx < embed_split_size / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
            }
            if (embed_split_size % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(embed_split_size % FLOAT_VECTOR_SIZE);
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[embed_split_size / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_embed_split_size / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);  // fix hidden_size=96
            }
            PIPE_BARRIER(V);
            if constexpr (SplitKV) {
                // log（l）
                ln_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                    tv32_ubuf_tensor,
                    sub_m, // repeat
                    1,       // dstBlockStride
                    1,       // srcBlockStride
                    8,       // dstRepeatStride
                    8        // srcRepeatStride
                );
                PIPE_BARRIER(V);
                brcb_v<ArchType::ASCEND_V220, uint32_t>(hm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                    gm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                    1,               // dstBlockStride
                    8,               // dstRepeatStride
                    round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                );
                PIPE_BARRIER(V);
                // logf(lse_sum) + lse_max
                add_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                    tv32_ubuf_tensor,
                    hm32_ubuf_tensor,
                    sub_m,                        // repeat
                    1,                                // dstBlockStride
                    1,                                // src0BlockStride
                    1,                                // src1BlockStride
                    8,                                // dstRepeatStride
                    8,                                // src0RepeatStride
                    8                                 // src1RepeatStride
                );
                CopyScale(sub_m, l_offset, o_offset);
            } else {
                // *** go = castfp32to16(go)
                conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(go_ubuf_tensor,
                    go32_ubuf_tensor,
                    (sub_m * round_embed_split_size + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                    1,                            // dstBlockStride
                    1,                            // srcBlockStride
                    4,                            // dstRepeatStride
                    8                             // srcRepeatStride
                );
                SET_FLAG(V, MTE3, EVENT_ID0);
                WAIT_FLAG(V, MTE3, EVENT_ID0);
                ub_to_gm_align<ArchType::ASCEND_V220, OUT_DTYPE>(
                    o_gm_tensor,
                    go_ubuf_tensor,
                    0,        // sid
                    sub_m,    // nBurst
                    embed_split_size * 2,  // lenBurst
                    0,        // leftPaddingNum
                    0,        // rightPaddingNum
                    0,        // srcGap
                    (__v - embed_split_size) * 2        // dstGap
                );
            }
            // ********************* move O to GM ************************
        } else {
            SET_FLAG(V, MTE3, EVENT_ID5);
            WAIT_FLAG(V, MTE3, EVENT_ID5);                
            ub_to_gm<ArchType::ASCEND_V220, float>(
                go_gm_tensor,
                go32_ubuf_tensor,
                0,
                sub_m,
                round_embed_split_size / FLOAT_BLOCK_SIZE,
                0,
                (round_v - round_embed_split_size) / FLOAT_BLOCK_SIZE
            );
        }
        SET_FLAG(MTE3, MTE2, EVENT_ID4);
    }

    __aicore__ __attribute__((always_inline)) inline void SoftmaxStage2(
        AscendC::GlobalTensor<mm2CopyType> o_tmp_gm_tensor,
        AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor,
        AscendC::LocalTensor<float> dm32_ubuf_tensor,
        AscendC::LocalTensor<float> ll_ubuf_tensor,
        AscendC::LocalTensor<float> pm32_ubuf_tensor,
        uint32_t n_idx,
        uint32_t n_loop,
        uint32_t qk_n,
        uint32_t qk_round_n,
        uint32_t sub_m,
        uint64_t l_offset,
        uint64_t o_offset,
        uint32_t head_idx,
        uint32_t pm_flag_scalar)
    {
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 64
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;
        uint32_t round_k =  RoundUp<16>(__k);
        if constexpr (tilingKeyType == TilingKeyType::TILING_QUANT_FP16OUT || tilingKeyType == TilingKeyType::TILING_QUANT_BF16OUT) {
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            DeQuantPerHeadImpl(
                deq_scale2_gm_tensor[head_idx],
                o_tmp_gm_tensor,
                lo_ubuf_tensor, loint32_ubuf_tensor,
                descale2_ubuf_tensor, tv32_ubuf_tensor, pm32_ubuf_tensor, sub_m, __k, round_k, pQuantOnline, 1);
            if (pm_flag_scalar == 1) {
                SET_FLAG(MTE2, V, EVENT_ID3);
            } else {
                SET_FLAG(MTE2, V, EVENT_ID4);
            }
        } else {
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            gm_to_ub<ArchType::ASCEND_V220, mm2CopyType>(
                lo_ubuf_tensor,
                o_tmp_gm_tensor,
                0,                    // sid
                1,                    // nBurst
                sub_m * round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                    // srcGap
                0                     // dstGap
            );
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);
        }

        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        // *** 更新 L 和 O
        if (n_idx != 0) {
            // *** dm = exp(dm)
            exp_v<ArchType::ASCEND_V220, float>(dm32_ubuf_tensor,
                dm32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,          // dstBlockStride
                1,          // srcBlockStride
                8,          // dstRepeatStride
                8           // srcRepeatStride
            );
            PIPE_BARRIER(V);
            // *** gl = dm * gl
            mul_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                dm32_ubuf_tensor,
                gl32_ubuf_tensor,
                sub_m_d64,  // repeat
                1,          // dstBlockStride
                1,          // src0BlockStride
                1,          // src1BlockStride
                8,          // dstRepeatStride
                8,          // src0RepeatStride
                8           // src1RepeatStride
            );
            PIPE_BARRIER(V);
            // *** gl = ll + gl
            add_v<ArchType::ASCEND_V220, float>(gl32_ubuf_tensor,
                gl32_ubuf_tensor,
                ll_ubuf_tensor,
                sub_m_d64,  // repeat
                1,          // dstBlockStride
                1,          // src0BlockStride
                1,          // src1BlockStride
                8,          // dstRepeatStride
                8,          // src0RepeatStride
                8           // src1RepeatStride
            );
            PIPE_BARRIER(V);
            // *** dm_block = expand_to_block(dm), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                dm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);
            if (go_flag_scalar == 1) {
                WAIT_FLAG(MTE3, V, EVENT_ID0);
                go_flag_scalar = 0;
            }
            // *** go = go * dm_block
            for (uint32_t vmul_idx = 0; vmul_idx < __k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
            }
            if (__k % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(__k % FLOAT_VECTOR_SIZE);
                mul_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,        // repeat
                    1,            // dstBlockStride
                    1,            // src0BlockStride
                    0,            // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1             // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            }
            PIPE_BARRIER(V);
            // *** go = lo + go
            add_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor,
                go32_ubuf_tensor,
                lo_ubuf_tensor,
                (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                1,                            // dstBlockStride
                1,                            // src0BlockStride
                1,                            // src1BlockStride
                8,                            // dstRepeatStride
                8,                            // src0RepeatStride
                8                             // src1RepeatStride
            );
            PIPE_BARRIER(V);
        } else {
            // *** gl = ll
            ub_to_ub<ArchType::ASCEND_V220, float>(
                gl32_ubuf_tensor,
                ll_ubuf_tensor,
                0,                // sid
                1,                // nBurst
                round_sub_m / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                // srcGap
                0                 // dstGap
            );
            PIPE_BARRIER(V);
            if (go_flag_scalar == 1) {
                WAIT_FLAG(MTE3, V, EVENT_ID0);
                go_flag_scalar = 0;
            }
            // *** go = lo
            ub_to_ub<ArchType::ASCEND_V220, float>(
                go32_ubuf_tensor,
                lo_ubuf_tensor,
                0,                    // sid
                1,                    // nBurst
                sub_m * round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                0,                    // srcGap
                0                     // dstGap
            );
            PIPE_BARRIER(V);
        }

        SET_FLAG(V, MTE2, EVENT_ID0);

        if (n_idx == n_loop - 1) {
            // *** gl_block = expand_to_block(gl), 存放于 tv
            brcb_v<ArchType::ASCEND_V220, uint32_t>(tv32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                gl32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                1,               // dstBlockStride
                8,               // dstRepeatStride
                round_sub_m / FLOAT_BLOCK_SIZE  // repeat
            );
            PIPE_BARRIER(V);
            // *** go = go / gl_block
            for (uint32_t vdiv_idx = 0; vdiv_idx < __k / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
            }
            if (__k % FLOAT_VECTOR_SIZE > 0) {
                __set_mask(__k % FLOAT_VECTOR_SIZE);
                div_v<ArchType::ASCEND_V220, float>(go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    go32_ubuf_tensor[__k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                    tv32_ubuf_tensor,
                    sub_m,                 // repeat
                    1,                     // dstBlockStride
                    1,                     // src0BlockStride
                    0,                     // src1BlockStride
                    round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                    round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                    1                      // src1RepeatStride
                );
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);  // fix hidden_size=96
            }
            PIPE_BARRIER(V);

            if constexpr (SplitKV) {
                // log（l）
                    ln_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                        tv32_ubuf_tensor,
                        sub_m, // repeat
                        1,       // dstBlockStride
                        1,       // srcBlockStride
                        8,       // dstRepeatStride
                        8        // srcRepeatStride
                    );
                    PIPE_BARRIER(V);
                    brcb_v<ArchType::ASCEND_V220, uint32_t>(hm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                        gm32_ubuf_tensor.ReinterpretCast<uint32_t>(),
                        1,               // dstBlockStride
                        8,               // dstRepeatStride
                        round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                    );
                    PIPE_BARRIER(V);
                    // logf(lse_sum) + lse_max
                    add_v<ArchType::ASCEND_V220, float>(tv32_ubuf_tensor,
                        tv32_ubuf_tensor,
                        hm32_ubuf_tensor,
                        sub_m,                        // repeat
                        1,                                // dstBlockStride
                        1,                                // src0BlockStride
                        1,                                // src1BlockStride
                        8,                                // dstRepeatStride
                        8,                                // src0RepeatStride
                        8                                 // src1RepeatStride
                    );
                    CopyScale(sub_m, l_offset, o_offset);
            } else {

                // *** go = castfp32to16(go)
                conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(go_ubuf_tensor,
                    go32_ubuf_tensor,
                    (sub_m * round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                    1,                            // dstBlockStride
                    1,                            // srcBlockStride
                    4,                            // dstRepeatStride
                    8                             // srcRepeatStride
                );
                SET_FLAG(V, MTE3, EVENT_ID0);
                WAIT_FLAG(V, MTE3, EVENT_ID0);
                ub_to_gm_align<ArchType::ASCEND_V220, OUT_DTYPE>(
                    o_gm_tensor[(int64_t)o_offset],
                    go_ubuf_tensor,
                    0,        // sid
                    sub_m,    // nBurst
                    __k * 2,  // lenBurst
                    0,        // leftPaddingNum
                    0,        // rightPaddingNum
                    0,        // srcGap
                    0         // dstGap
                );
            }
            // ********************* move O to GM ************************
            if (go_flag_scalar == 0) {
                SET_FLAG(MTE3, V, EVENT_ID0);
                go_flag_scalar = 1;
            }
        }
    }

    __aicore__ inline void DequantKV(GlobalT<IN_DTYPE> dst,       // [qk_n, sub_m, embedding_size]
                                     GlobalT<int8_t> src,         // [num_blocks, block_size, hidden_size]
                                     GlobalT<int32_t> deq_offset, // [hidden_size,]
                                     GlobalT<float> deq_scale,    // [hidden_size,]
                                     const uint32_t hidden_size, const uint32_t batch_idx, const uint32_t n_idx,
                                     const uint32_t kv_seq_len, const uint32_t sub_m, const uint32_t hiddenSizeOffset,
                                     const uint32_t real_n_loop, const uint32_t sub_n_loop, const uint32_t start_kv,
                                     const uint32_t bias_flag)
    {
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        uint32_t start_seq = 0;

        // [qk_n, sub_m, head_size]
        SET_FLAG(V, MTE2, EVENT_ID5);
        SET_FLAG(MTE3, V, EVENT_ID5);
        SET_FLAG(V, MTE2, EVENT_ID6);
        SET_FLAG(MTE3, V, EVENT_ID6);
        uint32_t sub_hiddensize = sub_m * embedding_size;
        uint32_t sub_hidden_d32 = RoundUp<32>(sub_hiddensize);
        uint32_t sub_hidden_d64 = RoundUp<64>(sub_hiddensize);
        uint32_t num_deq_kv = FLOAT_VECTOR_SIZE * UB_FLOAT_LINE_SIZE;
        uint32_t kv_seq_step = num_deq_kv / sub_hidden_d32;
        for (uint32_t ni = 0; ni < sub_n_loop; ++ni) {
            uint32_t actual_idx = n_idx * sub_n_loop + ni;
            if (actual_idx >= real_n_loop) {
                break;
            }
            uint32_t sub_qk_n = (actual_idx != real_n_loop - 1) ? block_size : kv_seq_len - actual_idx * block_size;
            uint32_t page_idx = (uint32_t)(*(gm_block_tables_ + batch_idx * max_num_blocks_per_query +
                                             start_kv / block_size + actual_idx));

            uint32_t dequant_ping_pang = 0;
            // [sub_qk_n, sub_m, head_size]
            for (uint32_t si = 0; si < sub_qk_n; si += kv_seq_step) {
                // copy src from gm to ub
                uint32_t seq_len_frag = Min(sub_qk_n - si, kv_seq_step);
                WAIT_FLAG(V, MTE2, EVENT_ID5 + dequant_ping_pang);
                gm_to_ub_align<ArchType::ASCEND_V220, int8_t>(ub_kv_int8_[dequant_ping_pang * num_deq_kv],
                                                              src[(page_idx * block_size + si) * hidden_size + hiddenSizeOffset],
                                                              0,                              // sid
                                                              seq_len_frag,                   // nBurst
                                                              sub_hiddensize,                 // lenBurst
                                                              0,                              // leftPaddingNum
                                                              0,                              // rightPaddingNum
                                                              (hidden_size - sub_hiddensize), // srcGap
                                                              0                               // dstGap
                    );

                if (si == 0 && ni == 0) {
                    if (bias_flag) {
                        // copy deq_offset from gm to ub
                        gm_to_ub_align<ArchType::ASCEND_V220, int32_t>(ub_offset_, deq_offset,
                                                                       0,                  // sid
                                                                       1,                  // nBurst
                                                                       sub_hiddensize * 4, // lenBurst
                                                                       0,                  // leftPaddingNum
                                                                       0,                  // rightPaddingNum
                                                                       0,                  // srcGap
                                                                       0                   // dstGap
                        );
                        SET_FLAG(MTE2, V, EVENT_ID4);
                        WAIT_FLAG(MTE2, V, EVENT_ID4);
                        conv_v<ArchType::ASCEND_V220, int32_t, float>(ub_offset_f32,                      // dst
                                                                      ub_offset_,                         // src
                                                                      sub_hidden_d64 / FLOAT_VECTOR_SIZE, // repeat
                                                                      1, // dstBlockStride
                                                                      1, // srcBlockStride
                                                                      8, // dstRepeatStride
                                                                      8  // srcRepeatStride
                        );
                    }
                    gm_to_ub_align<ArchType::ASCEND_V220, float>(ub_scale_, deq_scale,
                                                                 0,                  // sid
                                                                 1,                  // nBurst
                                                                 sub_hiddensize * 4, // lenBurst
                                                                 0,                  // leftPaddingNum
                                                                 0,                  // rightPaddingNum
                                                                 0,                  // srcGap
                                                                 0                   // dstGap
                    );
                }
                SET_FLAG(MTE2, V, EVENT_ID5 + dequant_ping_pang);
                WAIT_FLAG(MTE2, V, EVENT_ID5 + dequant_ping_pang);

                // cast src(int8) -> src(fp16) -> src(int32)
                uint32_t numel_kv = seq_len_frag * sub_hidden_d32;
                uint32_t count = numel_kv / MAX_NUMEL_INST_B16;

                WAIT_FLAG(MTE3, V, EVENT_ID5 + dequant_ping_pang);
                for (uint32_t i = 0; i < count; ++i) {
                    conv_v<ArchType::ASCEND_V220, int8_t, half>(
                        ub_kv_fp16_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B16]
                            .template ReinterpretCast<half>(),                                // dst
                        ub_kv_int8_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B16], // src
                        255,                                                                  // repeat
                        1,                                                                    // dstBlockStride
                        1,                                                                    // srcBlockStride
                        8,                                                                    // dstRepeatStride
                        4                                                                     // srcRepeatStride
                    );
                }
                conv_v<ArchType::ASCEND_V220, int8_t, half>(
                    ub_kv_fp16_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B16]
                        .template ReinterpretCast<half>(),                                    // dst
                    ub_kv_int8_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B16], // src
                    CeilDiv<uint32_t>((numel_kv - count * MAX_NUMEL_INST_B16), HALF_VECTOR_SIZE),// repeat
                    1,                                                                        // dstBlockStride
                    1,                                                                        // srcBlockStride
                    8,                                                                        // dstRepeatStride
                    4                                                                         // srcRepeatStride
                );
                SET_FLAG(V, MTE2, EVENT_ID5 + dequant_ping_pang);
                // cast src(fp16) -> src(float)
                PIPE_BARRIER(V);
                count = numel_kv / MAX_NUMEL_INST_B32;
                for (uint32_t i = 0; i < count; ++i) {
                    conv_v<ArchType::ASCEND_V220, half, float>(
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32], // dst
                        ub_kv_fp16_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32]
                            .template ReinterpretCast<half>(), // src
                        255,                                   // repeat
                        1,                                     // dstBlockStride
                        1,                                     // srcBlockStride
                        8,                                     // dstRepeatStride
                        4                                      // srcRepeatStride
                    );
                }
                conv_v<ArchType::ASCEND_V220, half, float>(
                    ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32], // dst
                    ub_kv_fp16_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32]
                        .template ReinterpretCast<half>(),                       // src
                    CeilDiv<uint32_t>((numel_kv - count * MAX_NUMEL_INST_B32), FLOAT_VECTOR_SIZE),// repeat
                    1,                                                           // dstBlockStride
                    1,                                                           // srcBlockStride
                    8,                                                           // dstRepeatStride
                    4                                                            // srcRepeatStride
                );
                if (bias_flag) {
                    // src(float) <- src(float) + offset(float)
                    PIPE_BARRIER(V);
                    count = sub_hiddensize / FLOAT_VECTOR_SIZE;
                    for (uint32_t i = 0; i < count; ++i) {
                        add_v<ArchType::ASCEND_V220, float>(
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // dst
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // src0
                            ub_offset_f32[i * FLOAT_VECTOR_SIZE],                                // src1
                            seq_len_frag,                                                        // repeat
                            1,                                                                   // dstBlockStride
                            1,                                                                   // src0BlockStride
                            1,                                                                   // src1BlockStride
                            sub_hidden_d32 / 8,                                                  // dstRepeatStride
                            sub_hidden_d32 / 8,                                                  // src0RepeatStride
                            0                                                                    // src1RepeatStride
                        );
                    }
                    if (sub_hiddensize % FLOAT_VECTOR_SIZE > 0) {
                        __set_mask(sub_hiddensize % FLOAT_VECTOR_SIZE);
                        add_v<ArchType::ASCEND_V220, float>(
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // dst
                            ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // src0
                            ub_offset_f32[count * FLOAT_VECTOR_SIZE],                                // src1
                            seq_len_frag,                                                            // repeat
                            1,                                                                       // dstBlockStride
                            1,                                                                       // src0BlockStride
                            1,                                                                       // src1BlockStride
                            sub_hidden_d32 / 8,                                                      // dstRepeatStride
                            sub_hidden_d32 / 8,                                                      // src0RepeatStride
                            0                                                                        // src1RepeatStride
                        );
                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                    }
                }
                // src(float) <- src(float) * scale(float)
                PIPE_BARRIER(V);
                count = sub_hiddensize / FLOAT_VECTOR_SIZE;
                for (uint32_t i = 0; i < count; ++i) {
                    mul_v<ArchType::ASCEND_V220, float>(
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // dst
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * FLOAT_VECTOR_SIZE], // src0
                        ub_scale_[i * FLOAT_VECTOR_SIZE],                                    // src1
                        seq_len_frag,                                                        // repeat
                        1,                                                                   // dstBlockStride
                        1,                                                                   // src0BlockStride
                        1,                                                                   // src1BlockStride
                        sub_hidden_d32 / 8,                                                  // dstRepeatStride
                        sub_hidden_d32 / 8,                                                  // src0RepeatStride
                        0                                                                    // src1RepeatStride
                    );
                }
                // 非对齐场景处理：偏移的数据量按照对齐偏移sub_hidden_d32
                if (sub_hiddensize % FLOAT_VECTOR_SIZE > 0) {
                    __set_mask(sub_hiddensize % FLOAT_VECTOR_SIZE);
                    mul_v<ArchType::ASCEND_V220, float>(
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // dst
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * FLOAT_VECTOR_SIZE], // src0
                        ub_scale_[count * FLOAT_VECTOR_SIZE],                                    // src1
                        seq_len_frag,                                                            // repeat
                        1,                                                                       // dstBlockStride
                        1,                                                                       // src0BlockStride
                        1,                                                                       // src1BlockStride
                        sub_hidden_d32 / 8,                                                      // dstRepeatStride
                        sub_hidden_d32 / 8,                                                      // src0RepeatStride
                        0                                                                        // src1RepeatStride
                    );
                }
                // // cast src(float) -> src(half)
                count = numel_kv / MAX_NUMEL_INST_B32;
                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                PIPE_BARRIER(V);
                for (uint32_t i = 0; i < count; ++i) {
                    conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(
                        ub_kv_fp16_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32], // dst
                        ub_kv_fp32_[dequant_ping_pang * num_deq_kv + i * MAX_NUMEL_INST_B32], // src
                        255,                                                                  // repeat
                        1,                                                                    // dstBlockStride
                        1,                                                                    // srcBlockStride
                        4,                                                                    // dstRepeatStride
                        8                                                                     // srcRepeatStride
                    );
                }
                conv_v<ArchType::ASCEND_V220, float, OUT_DTYPE>(
                    ub_kv_fp16_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32], // dst
                    ub_kv_fp32_[dequant_ping_pang * num_deq_kv + count * MAX_NUMEL_INST_B32], // src
                    CeilDiv<uint32_t>((numel_kv - count * MAX_NUMEL_INST_B32), FLOAT_VECTOR_SIZE),// repeat
                    1,                                                                        // dstBlockStride
                    1,                                                                        // srcBlockStride
                    4,                                                                        // dstRepeatStride
                    8                                                                         // srcRepeatStride
                );
                SET_FLAG(V, MTE3, EVENT_ID0 + dequant_ping_pang);
                WAIT_FLAG(V, MTE3, EVENT_ID0 + dequant_ping_pang);

                // 非对齐场景处理：非对齐headsize需要依照情况手动偏移dummydata
                uint32_t align_size = (sub_m * embedding_size % 32);
                uint32_t padd_gap = (align_size > 0) ? ((UB_ALIGN_BYTE - align_size) >= CONST_16 ? 1 : 0) : 0;
                ub_to_gm_align<ArchType::ASCEND_V220, IN_DTYPE>(dst[(start_seq + si) * hidden_size],
                                                            ub_kv_fp16_[dequant_ping_pang * num_deq_kv],
                                                                0,                          // sid
                                                                seq_len_frag,               // nBurst
                                                                sub_m * embedding_size * 2, // lenBurst
                                                                0,                          // leftPaddingNum
                                                                0,                          // rightPaddingNum
                                                                padd_gap,                   // srcGap
                                                                (hidden_size - sub_hiddensize) * 2  // dstGap
                );
                SET_FLAG(MTE3, V, EVENT_ID5 + dequant_ping_pang);
                dequant_ping_pang = 1 - dequant_ping_pang;
            }
            start_seq += sub_qk_n;
        }
        WAIT_FLAG(MTE3, V, EVENT_ID5);
        WAIT_FLAG(V, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE3, V, EVENT_ID6);
        WAIT_FLAG(V, MTE2, EVENT_ID6);
    }
    
    __aicore__ __attribute__((always_inline)) inline void InnerRunVector(uint32_t cur_batch, uint32_t start_head, uint32_t cur_nIndx, uint32_t cur_kv_seqlen, uint32_t cur_head_num,
                                                                         uint32_t offset_tiling, uint32_t kv_seqlen, uint32_t embed_split_size_v, uint32_t embed_split_loop_v)
    {
        uint32_t kv_start_head = start_head / group_num; //30 ~ 32
        uint32_t kv_end_head = (start_head + cur_head_num + group_num - 1) / group_num;
        uint32_t cur_kvhead_num = kv_end_head - kv_start_head;
        uint32_t kv_head_idx = kv_start_head + sub_block_idx * cur_kvhead_num / 2;
        uint32_t head_idx = start_head + sub_block_idx * cur_head_num / 2;
        uint32_t addr_o_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 6 + offset_tiling));
        uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 7 + offset_tiling));
        uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
        uint32_t mask_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_gm + 10 + offset_tiling));
        uint32_t mask_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_gm + 14 + offset_tiling));
        uint64_t mask_scalar = (uint64_t)(((uint64_t)mask_high32) << 32 | mask_loww32);
        uint32_t addr_l_high32 = 0;
        uint32_t addr_l_loww32 = 0;
        uint64_t addr_l_scalar = 0;
        uint64_t o_offset = 0;
        uint32_t l_offset = 0;
        // o #((num_tokens, num_heads, kvsplit, head_size))
        // l  (numt_tokens, num_heads, kvsplit)
        if constexpr (SplitKV) {
            addr_l_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 11 + offset_tiling));
            addr_l_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 12 + offset_tiling));
            addr_l_scalar = (uint64_t)(((uint64_t)addr_l_high32) << 32 | addr_l_loww32);
            uint32_t addr_o_fd_high32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 15 + offset_tiling));
            uint32_t addr_o_fd_loww32 = (uint32_t)(*((__gm__ uint32_t *)tiling_gm + 16 + offset_tiling));
            uint64_t addr_o_fd_scalar = (uint64_t)(((uint64_t)addr_o_fd_high32) << 32 | addr_o_fd_loww32);
            o_offset = addr_o_fd_scalar * kv_split_core_num + head_idx * __k * kv_split_core_num + cur_nIndx * __k;
            l_offset = addr_l_scalar + head_idx * kv_split_core_num + cur_nIndx;
        } else {
            if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                o_offset = addr_o_scalar + head_idx * embedding_size_v;
            } else {
                o_offset = addr_o_scalar + head_idx * embedding_size;
            }
        }
        uint32_t pp_n_scalar = block_size_calc;
        uint32_t sub_n_loop = pp_n_scalar / block_size;
        uint32_t real_n_loop = (cur_kv_seqlen + block_size - 1) / block_size;

        uint32_t n_loop = (cur_kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;
        uint64_t mask_offset = cur_batch % modCoef / divCoef * batch_stride + head_idx * head_stride + (uint64_t)cur_nIndx * kv_split_per_core;
        mask_offset += mask_scalar;

        uint32_t qk_n = pp_n_scalar;
        uint32_t qk_round_n = RoundUp<BLOCK_SIZE>(qk_n);

        uint32_t qk_n_2 = pp_n_scalar;
        uint32_t qk_round_n_2 = RoundUp<BLOCK_SIZE>(qk_n);

        uint32_t sub_m = (sub_block_idx == 1) ? (cur_head_num - cur_head_num / 2) : cur_head_num / 2;
        uint32_t sub_m_d128 = (sub_m + 127) / 128;  // up aligned to 128
        uint32_t sub_m_d64 = (sub_m + 63) / 64;     // up aligned to 128
        uint32_t round_sub_m = (sub_m + 15) / 16 * 16;

        uint32_t start_kv = cur_nIndx * kv_split_per_core;


        uint32_t hiddenSizeOffset = kv_head_idx * embedding_size;
        uint32_t gm_scale_hidden_size = kv_head_idx * embedding_size;
        uint32_t hiddenSizeOffset1 = k_bias_flag ? hiddenSizeOffset : 0;
        uint32_t hiddenSizeOffset2 = v_bias_flag ? hiddenSizeOffset : 0;
        uint32_t sub_m_kv = (sub_block_idx == 1) ? (cur_kvhead_num - cur_kvhead_num / 2) : cur_kvhead_num / 2;
        
        if constexpr (compressType == CompressType::COMPRESS_TYPE_KVHEAD) {
            uint32_t razor_start_head = (cur_batch * q_heads + start_head) % q_head_original;
            uint32_t razor_head_idx = razor_start_head + sub_block_idx * cur_head_num / 2;
            gm_scale_hidden_size = razor_head_idx * embedding_size;
            hiddenSizeOffset1 = k_bias_flag ? gm_scale_hidden_size : 0;
            hiddenSizeOffset2 = v_bias_flag ? gm_scale_hidden_size : 0;
        }

        for (uint32_t n_idx = 0; n_idx < n_loop; n_idx+=1) {
            if (n_idx == (n_loop - 1)) {
                qk_n = (cur_kv_seqlen - n_idx * pp_n_scalar);
                qk_round_n = RoundUp<16>(qk_n);
            }
            if ((n_idx + 1) == (n_loop - 1)) {
                qk_n_2 = (cur_kv_seqlen - (n_idx + 1) * pp_n_scalar);
                qk_round_n_2 = RoundUp<16>(qk_n_2);
            }
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT ||
                          tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                // [qk_n, sub_m, head_size]
                if (sub_m_kv > 0) {
                    DequantKV(gm_k16_ping_[hiddenSizeOffset], // dst
                              gm_k8_,                                                           // src
                              gm_offset1_[hiddenSizeOffset1],                                    // deq_offset
                              gm_scale1_[hiddenSizeOffset],                                            // deq_scale
                              num_kv_heads * embedding_size,                                     // hidden_size
                              cur_batch,                                                         // batch_idx
                              n_idx,                                                             // n_idx
                              cur_kv_seqlen,                                                     // kv_seq_len
                              sub_m_kv,                                                             // sub_m
                              hiddenSizeOffset,                                                      // hiddenSizeOffset
                              real_n_loop,                                                       // real_n_loop
                              sub_n_loop,                                                        // sub_n_loop
                              start_kv,                                                          // start_kv
                              k_bias_flag);
                }
                FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_K0_READY);
                if ((n_idx + 1) < n_loop) {
                    if (sub_m_kv > 0) {
                        DequantKV(gm_k16_pong_[hiddenSizeOffset], // dst
                                  gm_k8_,                                                // src
                                  gm_offset1_[hiddenSizeOffset1],                                    // deq_offset
                                  gm_scale1_[hiddenSizeOffset],                                            // deq_scale
                                  num_kv_heads * embedding_size,                                     // hidden_size
                                  cur_batch,                                                         // batch_idx
                                  n_idx + 1,                                                         // seq_idx
                                  cur_kv_seqlen,                                                     // kv_seq_len
                                  sub_m_kv,                                                             // sub_m
                                  hiddenSizeOffset,                                                      // hiddenSizeOffset
                                  real_n_loop,                                                       // real_n_loop
                                  sub_n_loop,                                                        // sub_n_loop
                                  start_kv,                                                          // start_kv
                                  k_bias_flag);
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_K1_READY);
                }
            }
            if constexpr (tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANT ||
                          tilingKeyType == TilingKeyType::TILING_INT8_VEC_QUANTBF16) {
                if (sub_m_kv > 0) {
                    DequantKV(gm_v16_ping_[hiddenSizeOffset], // dst
                              gm_v8_,                                                // src
                              gm_offset2_[hiddenSizeOffset2],                                    // deq_offset
                              gm_scale2_[hiddenSizeOffset],                                            // deq_scale
                              num_kv_heads * embedding_size,                                     // hidden_size
                              cur_batch,                                                         // batch_idx
                              n_idx,                                                             // n_idx
                              cur_kv_seqlen,                                                     // kv_seq_len
                              sub_m_kv,                                                             // sub_m
                              hiddenSizeOffset,                                                      // hiddenSizeOffset
                              real_n_loop,                                                       // real_n_loop
                              sub_n_loop,                                                        // sub_n_loop
                              start_kv,                                                          // start_kv
                              v_bias_flag);
                    SET_FLAG(V, MTE2, EVENT_ID6);
                    WAIT_FLAG(V, MTE2, EVENT_ID6);
                }
                FftsCrossCoreSync<PIPE_MTE3, 2>(VEC_DEQ_V0_READY);
            }
            WaitFlagDev(QK_READY_DECODER);
            /* ************ softmax1 stage1  ************* */
            WAIT_FLAG(MTE3, MTE2, EVENT_ID3);
            if (sub_m > 0) {
                // input QK shape (sub_m, qk_round_n)
                SoftmaxStage1(
                    p_gm_tensor[(uint64_t)block_idx * TMP_SIZE * T_BLOCK_OFFSET +
                        (uint64_t)sub_block_idx * cur_head_num / 2 * qk_round_n * T_BLOCK_OFFSET],
                    s_gm_tensor[(int64_t)block_idx * TMP_SIZE_DECODER +
                        (int64_t)sub_block_idx * cur_head_num / 2 * qk_round_n],
                    mask_gm_tensor[mask_offset + (uint64_t)n_idx * pp_n_scalar],
                    dm32_ubuf_tensor, ll_ubuf_tensor, pm32_ubuf_tensor,
                    n_idx, qk_n, qk_round_n, sub_m, mask_offset, sub_n_loop, cur_batch, start_kv, real_n_loop, head_idx, pm_flag_scalar1
                );
               // input QK shape (sub_m, qk_round_n)
            }
            FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY_DECODER);
            SET_FLAG(MTE3, MTE2, EVENT_ID3);
            WAIT_FLAG(MTE3, MTE2, EVENT_ID3);
            
            SET_FLAG(MTE3, MTE2, EVENT_ID3);
            /* ************ softmax2 stage1  ************* */
            WaitFlagDev(UPDATE_READY_DECODER);
            uint32_t embed_split_size = embed_split_size_v;
            uint32_t round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
            if (sub_m > 0) {
                if constexpr (pagedAttnVariant == PagedAttnVariant::MULTI_LATENT) {
                    for (uint32_t embed_split_idx = 0; embed_split_idx < embed_split_loop_v; ++embed_split_idx) {              
                        uint32_t embed_split_offset_tight = embed_split_idx * embed_split_size;
                        uint32_t embed_split_offset = embed_split_idx * round_embed_split_size;
                        if (embed_split_idx == embed_split_loop_v - 1) {
                            embed_split_size = embedding_size_v - embed_split_offset_tight;
                            round_embed_split_size = RoundUp<T_BLOCK_SIZE>(embed_split_size);
                        }
                        /* ************ softmax2 stage1  ************* */
                        SoftmaxStage2MLA(
                            o_tmp_gm_tensor[(int64_t)(block_idx * TMP_SIZE * 2 +
                                sub_block_idx * cur_head_num / 2 * round_v + embed_split_offset)],
                            go_gm_tensor[(int64_t)(block_idx * TMP_SIZE +
                                sub_block_idx * cur_head_num / 2 * round_v + embed_split_offset)],
                            o_gm_tensor[(int64_t)(o_offset + embed_split_offset_tight)], 
                                dm32_ubuf_tensor, ll_ubuf_tensor, pm32_ubuf_tensor,
                            n_idx, n_loop, qk_n, RoundUp<T_BLOCK_SIZE>(qk_round_n), sub_m, l_offset, o_offset, head_idx,
                            embed_split_size, round_embed_split_size, embed_split_idx, embed_split_loop_v, pm_flag_scalar1);
                    }
                }
            }
        }
    }

private:

    __gm__ mm1CopyType *__restrict__ s_gm{nullptr};
    __gm__ IN_DTYPE *__restrict__ p_gm{nullptr};
    __gm__ mm2CopyType *__restrict__ o_tmp_gm{nullptr};
    __gm__ float *__restrict__ go_gm{nullptr};
    __gm__ float *__restrict__ o_core_tmp_gm{nullptr};
    __gm__ float *__restrict__ l_gm{nullptr};
    __gm__ int32_t* __restrict__ gm_block_tables_{nullptr};

    __gm__ OUT_DTYPE *__restrict__ o_gm{nullptr};
    __gm__ OUT_DTYPE *__restrict__ mask_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_gm{nullptr};
    __gm__ float *__restrict__ razor_offset_gm{nullptr};
    __gm__ float *__restrict__ logN_gm{nullptr};

    UbufAlloc<pagedAttnVariant> UbAllocator;
    const uint32_t ls32_ubuf_offset = UbAllocator.ls32_ubuf_offset;
    const uint32_t lp_ubuf_offset = UbAllocator.lp_ubuf_offset;
    const uint32_t lp32_ubuf_offset = UbAllocator.lp32_ubuf_offset;
    const uint32_t mask_ubuf_offset = UbAllocator.mask_ubuf_offset;
    const uint32_t lo_ubuf_offset = UbAllocator.lo_ubuf_offset;
    const uint32_t mask32_ubuf_offset = UbAllocator.mask32_ubuf_offset;
    const uint32_t ls16_ubuf_offset = UbAllocator.ls16_ubuf_offset;
    
    const uint32_t lm32_ubuf_offset = UbAllocator.lm32_ubuf_offset;
    const uint32_t hm32_ubuf_offset = UbAllocator.hm32_ubuf_offset;
    const uint32_t pm32_ubuf_offset = UbAllocator.pm32_ubuf_offset;
    const uint32_t pm32_ubuf_stage2_offset = UbAllocator.pm32_ubuf_stage2_offset;
    const uint32_t descale1_offset = UbAllocator.descale1_offset;
    const uint32_t descale2_offset = UbAllocator.descale2_offset;
    const uint32_t dm32_ubuf_offset = UbAllocator.dm32_ubuf_offset;
    const uint32_t dm32_ubuf_stage2_offset = UbAllocator.dm32_ubuf_stage2_offset;
    const uint32_t ll_ubuf_offset = UbAllocator.ll_ubuf_offset; 
    const uint32_t ll_ubuf_stage2_offset = UbAllocator.ll_ubuf_stage2_offset;      
    const uint32_t gm32_ubuf_offset = UbAllocator.gm32_ubuf_offset;  
    const uint32_t gl_ubuf_offset = UbAllocator.gl_ubuf_offset;          
    const uint32_t gl32_ubuf_offset = UbAllocator.gl32_ubuf_offset;        
    const uint32_t go_ubuf_offset = UbAllocator.go_ubuf_offset;            
    const uint32_t go32_ubuf_offset = UbAllocator.go32_ubuf_offset;          
    const uint32_t tv32_ubuf_offset = UbAllocator.tv32_ubuf_offset;       

    const uint32_t addr_kv8 = 0;
    const uint32_t addr_kv16 = addr_kv8 + 32 * 4 * 128 * sizeof(int8_t);
    const uint32_t addr_kv32 = addr_kv16 + 32 * 4 * 128 * sizeof(half);
    const uint32_t addr_offset = addr_kv32 + 32 * 4 * 128 * sizeof(float);
    const uint32_t addr_scale = addr_offset + 8 * 128 * sizeof(int32_t);
    const uint32_t addr_offset32 = addr_scale + 8 * 128 * sizeof(int32_t);

    


    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<float> ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_ubuf_offset);
    AscendC::LocalTensor<half> ls16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(ls32_ubuf_offset);
    AscendC::LocalTensor<int32_t> lsint32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(ls32_ubuf_offset);
    AscendC::LocalTensor<IN_DTYPE> lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DTYPE>(lp_ubuf_offset);
    AscendC::LocalTensor<float> lp32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lp32_ubuf_offset);
    AscendC::LocalTensor<OUT_DTYPE> mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(mask_ubuf_offset);
    AscendC::LocalTensor<float> lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
    AscendC::LocalTensor<int32_t> loint32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(lo_ubuf_offset);
    AscendC::LocalTensor<float> mask32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(mask32_ubuf_offset);
    AscendC::LocalTensor<float> lm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm32_ubuf_offset);
    AscendC::LocalTensor<float> hm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(hm32_ubuf_offset);
    AscendC::LocalTensor<float> pm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(pm32_ubuf_offset);
    AscendC::LocalTensor<float> pm32_ubuf_stage2_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(pm32_ubuf_stage2_offset);
    AscendC::LocalTensor<float> descale1_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(descale1_offset);
    AscendC::LocalTensor<float> descale2_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(descale2_offset);
    AscendC::LocalTensor<float> gm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gm32_ubuf_offset);
    AscendC::LocalTensor<float> dm32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm32_ubuf_offset);

    AscendC::LocalTensor<float> dm32_stage2_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm32_ubuf_stage2_offset);
    AscendC::LocalTensor<float> ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
    AscendC::LocalTensor<float> ll_stage2_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_stage2_offset);
    AscendC::LocalTensor<OUT_DTYPE> gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(gl_ubuf_offset);
    AscendC::LocalTensor<float> gl32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl32_ubuf_offset);
    AscendC::LocalTensor<float> tv32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv32_ubuf_offset);
    AscendC::LocalTensor<OUT_DTYPE> go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(go_ubuf_offset);
    AscendC::LocalTensor<float> go32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go32_ubuf_offset);
    AscendC::LocalTensor<int32_t> goint32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(go32_ubuf_offset);

    AscendC::GlobalTensor<mmScaleType> deq_scale1_gm_tensor;
    AscendC::GlobalTensor<mmScaleType> deq_scale2_gm_tensor;
    AscendC::GlobalTensor<mmScaleType> scale_gm_tensor;
    AscendC::GlobalTensor<mmBiasType> bias1_gm_tensor;
    AscendC::GlobalTensor<mmBiasType> bias2_gm_tensor;

    AscendC::GlobalTensor<OUT_DTYPE> mask_gm_tensor;
    AscendC::GlobalTensor<OUT_DTYPE> o_gm_tensor;
    AscendC::GlobalTensor<float> razor_offset_gm_tensor;
    AscendC::GlobalTensor<mm1CopyType> s_gm_tensor;
    AscendC::GlobalTensor<IN_DTYPE> p_gm_tensor;
    AscendC::GlobalTensor<mm2OutputType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<float> go_gm_tensor;
    AscendC::GlobalTensor<float> o_core_tmp_gm_tensor;
    AscendC::GlobalTensor<float> l_gm_tensor;

    GlobalT<int8_t> gm_k8_;
    GlobalT<int8_t> gm_v8_;
    GlobalT<OUT_DTYPE> gm_k16_ping_;
    GlobalT<OUT_DTYPE> gm_k16_pong_;
    GlobalT<OUT_DTYPE> gm_v16_ping_;
    GlobalT<OUT_DTYPE> gm_v16_pong_;
    GlobalT<int32_t> gm_offset1_;
    GlobalT<int32_t> gm_offset2_;
    GlobalT<float> gm_scale1_;
    GlobalT<float> gm_scale2_;
    LocalT<int8_t> ub_kv_int8_ = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(addr_kv8);
    LocalT<OUT_DTYPE> ub_kv_fp16_ = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(addr_kv16);
    LocalT<int32_t> ub_kv_int32_ = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(addr_kv32);
    LocalT<float> ub_kv_fp32_ = buf.GetBuffer<BufferType::ASCEND_UB, float>(addr_kv32);
    LocalT<int32_t> ub_offset_ = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(addr_offset);
    LocalT<float> ub_offset_f32 = buf.GetBuffer<BufferType::ASCEND_UB, float>(addr_offset32);
    LocalT<float> ub_scale_ = buf.GetBuffer<BufferType::ASCEND_UB, float>(addr_scale);

    uint32_t k_bias_flag{0};
    uint32_t v_bias_flag{0};
    uint32_t pQuantOnline{0};
    uint32_t pQuantType{0};

    uint32_t go_flag_scalar{1};
    uint32_t gl_flag_scalar{1};
    uint32_t pm_flag_scalar1{1};
    uint32_t pm_flag_scalar2{0};
    ScaleType scaleType = ScaleType::SCALE_TOR;
    float tor_logN{0};
    uint32_t num_tokens{0};
    uint32_t q_heads{0};
    uint32_t num_kv_heads{0};
    uint32_t embedding_size{0};
    uint32_t embedding_size_v{0};
    uint32_t block_size{0};
    uint32_t max_context_len{0};
    uint32_t start_head{0};
    uint32_t cur_head_num{0};
    uint32_t __k{0};
    uint32_t round_k{0};
    uint32_t __v{0};
    uint32_t round_v{0};
    uint32_t cur_batch{0};
    float tor{0};
    uint64_t sub_block_idx{0};
    uint32_t batch_stride{0};
    uint32_t head_stride{0};
    uint64_t former_batch{0};
    uint32_t former_head_split{0};
    uint32_t split_size{0};
    uint64_t tail_batch{0};
    uint32_t tail_head_split{0};
    uint32_t core_per_batch{0};
    uint32_t process_num{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t kv_split_per_core{0};
    uint32_t kv_split_core_num{0};
    uint32_t block_size_calc{0};
    uint32_t former_group_num_move{1};
    uint32_t tail_group_num_move{1};
    uint32_t embed_split_size_v_former{0};
    uint32_t embed_split_loop_v_former{1};
    uint32_t embed_split_size_v_tail{0};
    uint32_t embed_split_loop_v_tail{1};


    uint32_t modCoef{0xffffffff}; // 对batch_idx取模的参数，适用于多头自适应压缩场景
    uint32_t divCoef{1}; // 对batch_idx做除法的参数，适用于多头自适应压缩场景
    uint32_t q_head_original{0};
    uint32_t compressHead{0};

    uint32_t max_num_blocks_per_query{0};
    uint32_t group_num{0};

    uint32_t prefill_batch_size_;
    uint32_t decoder_batch_size_;
};
#endif