/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
#include "collectives.cce"

template<typename T>
__attribute__((always_inline)) inline __aicore__ void SumByPairsBigData(
    __ubuf__ int64_t *ctrlFlagsUB, __ubuf__ int64_t *ctrlFlagsUB1, __ubuf__ int64_t *ctrlFlagsUB2, __gm__ T* buff[8], int64_t x,
    int64_t blockNumPerGroup, int64_t corePerRank, int64_t coreSegmentedIdx, int64_t magic, int64_t deterministicOffNum,
    int64_t thisNPUProcessDataNum, int64_t thisNPUCoreGroupAvgDMADataNum, int64_t dataOffsetNum, int64_t dataSizeRemain, __ubuf__ T *inputUB[2],
    int op, int rank, int rankSize, int64_t allTimes, int64_t allDataSizeNeed2Add) {
    int64_t target = 0;
    __gm__ int64_t *ctrlFlagsGM;
    __gm__ int64_t *ctrlFlagsGMTemp;
    __gm__ int64_t *ctrlFlagsGMTemp1;
    int64_t buffOffsetNum;
    __gm__ T *processOutput;
    int64_t outputOffsetNum;

    if (x == 0) {
        return;
    }

    int64_t multiple = GetDeterministicRankOffset(x);
    if (x % 2 == 1) {
        target = x - multiple;
        ctrlFlagsGMTemp = (__gm__ int64_t*)buff[rank] + (blockNumPerGroup + target * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
        ctrlFlagsGMTemp1 = (__gm__ int64_t*)buff[rank] + (blockNumPerGroup + x * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
        buffOffsetNum = deterministicOffNum + x * thisNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
        processOutput = (__gm__ T*)((__gm__ int64_t *)buff[rank] + dataOffsetNum);
        outputOffsetNum = deterministicOffNum + target * thisNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
    } else {
        target = x - multiple;
        ctrlFlagsGMTemp = (__gm__ int64_t*)buff[rank] + (blockNumPerGroup * 2 + (target + multiple / 2) * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;

        int64_t multipleTemp = multiple;
        while (x + multipleTemp / 2 >= rankSize) {
            multipleTemp /= 2;
        }
        if (multipleTemp > 0) {
            if ((x + multipleTemp / 2) != x) {
                ctrlFlagsGMTemp1 = (__gm__ int64_t*)buff[rank] +
                    (blockNumPerGroup * 2 + (x + multipleTemp / 2) * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
            } else {
                ctrlFlagsGMTemp1 = (__gm__ int64_t*)buff[rank] +
                    (blockNumPerGroup + x * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
            }

        }

        buffOffsetNum = deterministicOffNum + x * thisNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
        processOutput = (__gm__ T*)((__gm__ int64_t *)buff[rank] + dataOffsetNum);
        outputOffsetNum = deterministicOffNum + target * thisNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
    }
    AscendC::PipeBarrier<PIPE_ALL>();

    while (true) {
        if (*ctrlFlagsUB >= allTimes) {
            break;
        }

        CpGM2UB(ctrlFlagsUB1, ctrlFlagsGMTemp, sizeof(int64_t));
        CpGM2UB(ctrlFlagsUB2, ctrlFlagsGMTemp1, sizeof(int64_t));
        AscendC::PipeBarrier<PIPE_ALL>();
        if ((*ctrlFlagsUB1 >> 10) != (magic >> 10) || (*ctrlFlagsUB2 >> 10) != (magic >> 10)) {
            continue;
        }

        *ctrlFlagsUB1 = ((*ctrlFlagsUB1 & 0x3FF) <= (*ctrlFlagsUB2 & 0x3FF)) ? *ctrlFlagsUB1 : *ctrlFlagsUB2;
        AscendC::PipeBarrier<PIPE_ALL>();

        int64_t preparedDataGroupCount = (*ctrlFlagsUB1 & 0x3FF);
        if (*ctrlFlagsUB >= preparedDataGroupCount) {
            continue;
        }

        dataSizeRemain = (preparedDataGroupCount - *ctrlFlagsUB) * DMA_SIZE_PER_FLAG;
        if (preparedDataGroupCount * DMA_SIZE_PER_FLAG > allDataSizeNeed2Add) {
            dataSizeRemain = allDataSizeNeed2Add - *ctrlFlagsUB * DMA_SIZE_PER_FLAG;
        }
        ProcessDataNew<T>(dataSizeRemain, inputUB, buff[rank], dataOffsetNum, buffOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T),
                          processOutput, outputOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T), op);
        AscendC::PipeBarrier<PIPE_ALL>();
        *ctrlFlagsUB = preparedDataGroupCount;
        CpUB2GM((__gm__ int64_t *) buff[rank] + GetBlockIdx() * MEM_DMA_UNIT_INT_NUM, ctrlFlagsUB1, sizeof(int64_t));
        AscendC::PipeBarrier<PIPE_ALL>();
    }
}

template<typename T>
__attribute__((always_inline)) inline __aicore__ void LcalAllReduceDeterministicBigDataOrigin(
    __gm__ T* buff[8], __gm__ T *input, __gm__ T *output, int64_t blockNumPerGroup, uint32_t rank, uint32_t rankSize,
    uint64_t len, int64_t magic, __ubuf__ int64_t* ctrlFlagsUB, __ubuf__ int64_t* ctrlFlagsUB1,
    __ubuf__ int64_t* ctrlFlagsUB2, __ubuf__ T* inputUB[2], int64_t dataOffsetNum, int64_t flagOffset1st,
    int64_t flagOffset2nd, int64_t x, int64_t corePerRank, int64_t coreSegmentedIdx, int op)
{
    const int64_t singleNPUProcessDataNum = len / rankSize;
    int64_t thisNPUProcessDataNum = singleNPUProcessDataNum;
    if (rank == rankSize - 1) {
        thisNPUProcessDataNum = len - rank * singleNPUProcessDataNum;
    }

    int64_t xNPUProcessDataNum = singleNPUProcessDataNum;
    if (x == rankSize - 1) {
        xNPUProcessDataNum = len - x * singleNPUProcessDataNum;
    }

    const int64_t xNPUCoreGroupAvgDMADataNum = xNPUProcessDataNum / corePerRank;
    const int64_t thisNPUCoreGroupAvgDMADataNum = thisNPUProcessDataNum / corePerRank;

    int64_t dataSizeRemain = xNPUCoreGroupAvgDMADataNum * sizeof(T);
    if (coreSegmentedIdx == corePerRank - 1) {
        dataSizeRemain = (xNPUProcessDataNum - coreSegmentedIdx * xNPUCoreGroupAvgDMADataNum) * sizeof(T);
    }

    int64_t buffOffsetNum = x * singleNPUProcessDataNum + coreSegmentedIdx * xNPUCoreGroupAvgDMADataNum;
    int64_t deterministicOffNum = len;

    if (GetBlockIdx() < blockNumPerGroup) {
        __gm__ T *receiveBuff = (__gm__ T*)((__gm__ int64_t*)buff[rank] + dataOffsetNum);
        __gm__ int64_t* ctrlFlagsGM = (__gm__ int64_t*)buff[rank] + flagOffset1st;
        input2BuffRankMagic(dataSizeRemain, inputUB[0], receiveBuff, buffOffsetNum, input, buffOffsetNum, ctrlFlagsUB, ctrlFlagsGM, magic);
        return;
    }

    *ctrlFlagsUB = 0;
    *ctrlFlagsUB1 = 0;
    *ctrlFlagsUB2 = 0;
    int64_t allDataSizeNeed2Add = thisNPUCoreGroupAvgDMADataNum * sizeof(T);
    if (coreSegmentedIdx == corePerRank - 1) {
        allDataSizeNeed2Add = (thisNPUProcessDataNum - coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum) * sizeof(T);
    }
    int64_t allTimes = CeilDiv(allDataSizeNeed2Add, DMA_SIZE_PER_FLAG);

    if (GetBlockIdx() < blockNumPerGroup * 2) {
        __gm__ int64_t* ctrlFlagsGMX = (__gm__ int64_t*)buff[x] + (coreSegmentedIdx + rank * corePerRank) * MEM_DMA_UNIT_INT_NUM;
        __gm__ int64_t* ctrlFlagsGM = (__gm__ int64_t*)buff[rank] + GetBlockIdx() * MEM_DMA_UNIT_INT_NUM;

        __gm__ T *receiveBuff = (__gm__ T*)((__gm__ int64_t *)buff[rank] + dataOffsetNum);
        __gm__ T *sendBuff = (__gm__ T *)((__gm__ int64_t *)buff[x] + dataOffsetNum);
        AscendC::PipeBarrier<PIPE_ALL>();
        while (true) {
            if (*ctrlFlagsUB >= allTimes) {
                break;
            }

            CpGM2UB(ctrlFlagsUB1, ctrlFlagsGMX, sizeof(int64_t));
            AscendC::PipeBarrier<PIPE_ALL>();
            if ((*ctrlFlagsUB1 >> 10) != (magic >> 10)) {
                continue;
            }

            int64_t preparedDataGroupCount = *ctrlFlagsUB1 & 0x3FF;
            if (*ctrlFlagsUB >= preparedDataGroupCount) {
                continue;
            }

            buffOffsetNum = rank * singleNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
            dataSizeRemain = (preparedDataGroupCount - *ctrlFlagsUB) * DMA_SIZE_PER_FLAG;
            if (preparedDataGroupCount * DMA_SIZE_PER_FLAG > allDataSizeNeed2Add) {
                dataSizeRemain = allDataSizeNeed2Add - *ctrlFlagsUB * DMA_SIZE_PER_FLAG;
            }
            int64_t revBuffOffsetNum = deterministicOffNum + x * thisNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;

            GM2GMPingPong<T>(dataSizeRemain, inputUB, receiveBuff, revBuffOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T),
                             sendBuff, buffOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T));
            AscendC::PipeBarrier<PIPE_ALL>();
            *ctrlFlagsUB = preparedDataGroupCount;
            if (x == 0) {
                CpUB2GM((__gm__ int64_t *) buff[rank] + (GetBlockIdx() + blockNumPerGroup) * MEM_DMA_UNIT_INT_NUM, ctrlFlagsUB1, sizeof(int64_t));
            }
            CpUB2GM(ctrlFlagsGM, ctrlFlagsUB1, sizeof(int64_t));
            AscendC::SetFlag<AscendC::HardEvent::MTE3_MTE2>(EVENT_ID0);
            AscendC::WaitFlag<AscendC::HardEvent::MTE3_MTE2>(EVENT_ID0);
        }
    }

    if (GetBlockIdx() >= blockNumPerGroup * 2) {
        if (x == 0) {
            return;
        }

        if (rankSize >= 4) {
            AscendC::PipeBarrier<PIPE_ALL>();
            SumByPairsBigData(ctrlFlagsUB, ctrlFlagsUB1, ctrlFlagsUB2, buff, x, blockNumPerGroup, corePerRank, coreSegmentedIdx, magic, deterministicOffNum, thisNPUProcessDataNum,
                       thisNPUCoreGroupAvgDMADataNum, dataOffsetNum, dataSizeRemain, inputUB, op, rank, rankSize, allTimes, allDataSizeNeed2Add);
        } else {
            __gm__ int64_t* ctrlFlagsGMPre = (__gm__ int64_t*)buff[rank] + (blockNumPerGroup * 2 + (x - 1) * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
            __gm__ int64_t* ctrlFlagsGM = (__gm__ int64_t*)buff[rank] + (blockNumPerGroup + x * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
            buffOffsetNum = deterministicOffNum + x * thisNPUProcessDataNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
            __gm__ T *processOutput = (__gm__ T*)((__gm__ int64_t *)buff[rank] + dataOffsetNum);
            int64_t outputOffsetNum = deterministicOffNum + coreSegmentedIdx * thisNPUCoreGroupAvgDMADataNum;
            AscendC::PipeBarrier<PIPE_ALL>();
            while (true) {
                if (*ctrlFlagsUB >= allTimes) {
                    break;
                }

                CpGM2UB(ctrlFlagsUB1, ctrlFlagsGMPre, sizeof(int64_t));
                CpGM2UB(ctrlFlagsUB2, ctrlFlagsGM, sizeof(int64_t));
                AscendC::PipeBarrier<PIPE_ALL>();
                if ((*ctrlFlagsUB1 >> 10) != (magic >> 10) || (*ctrlFlagsUB2 >> 10) != (magic >> 10)) {
                    continue;
                }

                *ctrlFlagsUB1 = ((*ctrlFlagsUB1 & 0x3FF) <= (*ctrlFlagsUB2 & 0x3FF)) ? *ctrlFlagsUB1 : *ctrlFlagsUB2;
                AscendC::PipeBarrier<PIPE_ALL>();
                int64_t preparedDataGroupCount = (*ctrlFlagsUB1 & 0x3FF);
                if (*ctrlFlagsUB >= preparedDataGroupCount) {
                    continue;
                }

                dataSizeRemain = (preparedDataGroupCount - *ctrlFlagsUB) * DMA_SIZE_PER_FLAG;
                if (preparedDataGroupCount * DMA_SIZE_PER_FLAG > allDataSizeNeed2Add) {
                    dataSizeRemain = allDataSizeNeed2Add - *ctrlFlagsUB * DMA_SIZE_PER_FLAG;
                }
                ProcessDataNew<T>(dataSizeRemain, inputUB, buff[rank], dataOffsetNum, buffOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T),
                                  processOutput, outputOffsetNum + (*ctrlFlagsUB) * DMA_SIZE_PER_FLAG / sizeof(T), op);
                AscendC::PipeBarrier<PIPE_ALL>();
                *ctrlFlagsUB = preparedDataGroupCount;
                CpUB2GM((__gm__ int64_t *) buff[rank] + GetBlockIdx() * MEM_DMA_UNIT_INT_NUM, ctrlFlagsUB1, sizeof(int64_t));
                AscendC::PipeBarrier<PIPE_ALL>();
            }
        }
        SetFlag(ctrlFlagsUB, (__gm__ int64_t *)buff[rank] + (GetLcalBlockNum() + GetBlockIdx()) * MEM_DMA_UNIT_INT_NUM, magic);
        return;
    }

    __gm__ int64_t* ctrlFlagsGMX;
    if (rankSize >= 4) {
        ctrlFlagsGMX = (__gm__ int64_t*)buff[x] +
            (GetLcalBlockNum() + 2 * blockNumPerGroup +
                (rankSize > 4 ? 4 : 2) * corePerRank + coreSegmentedIdx) *  MEM_DMA_UNIT_INT_NUM;
    } else {
        ctrlFlagsGMX = (__gm__ int64_t*)buff[x] +
            (GetLcalBlockNum() + 2 * blockNumPerGroup +
                (rankSize - 1) * corePerRank + coreSegmentedIdx) * MEM_DMA_UNIT_INT_NUM;
    }

    constexpr int32_t lastFlagPos = 8;
    constexpr int32_t sumPairGroup = 2;
    if (rankSize > lastFlagPos) {
        ctrlFlagsGMX = (__gm__ int64_t*)buff[x] + MEM_DMA_UNIT_INT_NUM *
            (GetLcalBlockNum() + sumPairGroup * blockNumPerGroup + lastFlagPos * corePerRank + coreSegmentedIdx);
    }

    dataSizeRemain = xNPUCoreGroupAvgDMADataNum * sizeof(T);
    if (coreSegmentedIdx == corePerRank - 1) {
        dataSizeRemain = (xNPUProcessDataNum - coreSegmentedIdx * xNPUCoreGroupAvgDMADataNum) * sizeof(T);
    }

    buffOffsetNum = coreSegmentedIdx * xNPUCoreGroupAvgDMADataNum;
    CheckFlag((__ubuf__ int64_t*)ctrlFlagsUB, ctrlFlagsGMX, (int64_t)magic);

    __gm__ T *sendBuff = (__gm__ T*)((__gm__ int64_t*)buff[x] + dataOffsetNum);
    int64_t revBuffOffsetNum = x * singleNPUProcessDataNum + buffOffsetNum;
    int64_t sendBuffOffsetNum = deterministicOffNum + buffOffsetNum;

    GM2GMPingPong<T>(dataSizeRemain, inputUB, output, revBuffOffsetNum, sendBuff, sendBuffOffsetNum);
    return;
}

template<typename T>
__attribute__((always_inline)) inline __aicore__ void LcalAllReduceDeterministicBigData(ALLREDUCE_ARGS_FUN_16P(T))
{
    DumpLcclLogInfo(dumpAddr, LogId::OVERALL, static_cast<Op>(op));
    DumpLcclLogInfo(dumpAddr, LogId::INIT, static_cast<Op>(op));
    magic <<= 10;
    const int64_t dataOffsetNum = GetLcalBlockNum() * 2 * MEM_DMA_UNIT_INT_NUM;
    int64_t flagOffset1st = MEM_DMA_UNIT_INT_NUM * GetBlockIdx();
    constexpr int32_t maxBuffSize = 16;
    __gm__ T* buff[maxBuffSize] = {
        buff0, buff1, buff2, buff3,
        buff4, buff5, buff6, buff7,
        buff8, buff9, buff10, buff11,
        buff12, buff13, buff14, buff15
    };
    __ubuf__ int64_t* ctrlFlagsUB = (__ubuf__ int64_t*)(0);
    __ubuf__ int64_t* ctrlFlagsUB1 = (__ubuf__ int64_t*)(32);
    __ubuf__ int64_t* ctrlFlagsUB2 = (__ubuf__ int64_t*)(64);
    __ubuf__ T* inputUB[2] = {(__ubuf__ T*)(96), (__ubuf__ T*)(97440)};

    int64_t blockNumPerGroup = GetLcalBlockNum() / 3;
    int64_t corePerRank = blockNumPerGroup / rankSize;
    int64_t coreSegmentedIdx = GetBlockIdx() % corePerRank;

    int64_t x = GetBlockIdx() / corePerRank;
    if (GetBlockIdx() >= blockNumPerGroup && GetBlockIdx() < 2 * blockNumPerGroup) {
        x = (GetBlockIdx() - blockNumPerGroup) / corePerRank;
        flagOffset1st = (GetBlockIdx() - blockNumPerGroup) * MEM_DMA_UNIT_INT_NUM;
    } else if (GetBlockIdx() >= 2 * blockNumPerGroup) {
        x = (GetBlockIdx() - blockNumPerGroup * 2) / corePerRank;
        flagOffset1st = (GetBlockIdx() - blockNumPerGroup * 2) * MEM_DMA_UNIT_INT_NUM;
    }
    int64_t flagOffset2nd = GetLcalBlockNum() * MEM_DMA_UNIT_INT_NUM + flagOffset1st;

    int64_t ipcBuffMaxNum = IPC_BUFF_MAX_SIZE / sizeof(T);
    DumpLcclLogInfo(dumpAddr, LogId::INIT, static_cast<Op>(op));

    DumpLcclLogInfo(dumpAddr, LogId::PROCESS, static_cast<Op>(op));
    int64_t ipcBuffDeterministicMaxNum = DETERMINISTIC_BUFF_SIZE / sizeof(T);
    int64_t loopTimes = CeilDiv(len, ipcBuffDeterministicMaxNum);
    for (int64_t i = 0; i < loopTimes; i++) {
        *ctrlFlagsUB = 0;
        *ctrlFlagsUB1 = 0;
        *ctrlFlagsUB2 = 0;
        AscendC::PipeBarrier<PIPE_ALL>();

        int64_t processedNum = i * ipcBuffDeterministicMaxNum;
        int64_t remainNum = (len - processedNum < ipcBuffDeterministicMaxNum) ? len - processedNum : ipcBuffDeterministicMaxNum;

        PostSyncBigData<T>(ctrlFlagsUB, buff, rank, rankSize, dataOffsetNum, ipcBuffMaxNum, magic, i);
        LcalAllReduceDeterministicBigDataOrigin<T>(
            buff, input + processedNum, output + processedNum, blockNumPerGroup, rank, rankSize, remainNum, (magic + i) << 10, ctrlFlagsUB, ctrlFlagsUB1,
            ctrlFlagsUB2, inputUB, dataOffsetNum, flagOffset1st, flagOffset2nd, x, corePerRank, coreSegmentedIdx, op);
    }
    DumpLcclLogInfo(dumpAddr, LogId::PROCESS, static_cast<Op>(op));
    DumpLcclLogInfo(dumpAddr, LogId::OVERALL, static_cast<Op>(op));
}