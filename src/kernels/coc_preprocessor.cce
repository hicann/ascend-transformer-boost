#ifndef __COC_PREPROCESSOR__
#define __COC_PREPROCESSOR__

#ifdef __DAV_C220_VEC__

#include <type_traits>
#include "coc_internal.cce"
#include "kernel_operator.h"
using namespace AscendC;

template <typename LhsDtype, typename RhsDtype, typename MmadDtype>
class BasePadder {
public:
    class LoopIter {
    public:
        inline __aicore__ LoopIter(int32_t batch_size, int32_t n_rows, int32_t n_cols, int32_t n_cols_aligned) :
                batch_size(batch_size), n_rows(n_rows), n_cols(n_cols), n_cols_aligned(n_cols_aligned)
        {
            int32_t align_core_num = get_block_num() * get_subblockdim();
            int32_t align_core_idx = get_block_idx() * get_subblockdim() + get_subblockid();
            int32_t n_rows_per_core_base = n_rows / align_core_num;
            int32_t n_rows_remainder = n_rows % align_core_num;
            int32_t row_offset_base = align_core_idx * n_rows_per_core_base;
            if (align_core_idx < n_rows_remainder) {
                n_rows_this_core = n_rows_per_core_base + 1;
                row_offset_this_core = row_offset_base + align_core_idx;
            } else {
                n_rows_this_core = n_rows_per_core_base;
                row_offset_this_core = row_offset_base + n_rows_remainder;
            }
            n_cols_this_core = n_cols;
            col_offset_this_core = 0;

            src_core_offset = 1LL * row_offset_this_core * n_cols;
            dst_core_offset = 1LL * row_offset_this_core * n_cols_aligned;
        }

        inline __aicore__ void InitBatchLoop()
        {
            batch_idx = 0;

            src_batch_offset = 0;
            dst_batch_offset = 0;
        }

        inline __aicore__ bool EndBatchLoop() const
        {
            return batch_idx == batch_size;
        }

        inline __aicore__ void NextBatchLoop()
        {
            ++batch_idx;
            if (EndBatchLoop()) {
                return;
            }

            src_batch_offset = batch_idx * n_rows * n_cols;
            dst_batch_offset = batch_idx * n_rows * n_cols_aligned;
        }

        inline __aicore__ void InitRowLoop(int32_t max_rows_per_loop)
        {
            this->max_rows_per_loop = max_rows_per_loop;
            n_rows_complete = 0;
            src_row_loop_offset = 0;
            dst_row_loop_offset = 0;

            n_rows_this_loop = (n_rows_this_core < max_rows_per_loop) ? n_rows_this_core : max_rows_per_loop;
        }

        inline __aicore__ bool EndRowLoop() const
        {
            return n_rows_complete == n_rows_this_core;
        }

        inline __aicore__ void NextRowLoop()
        {
            n_rows_complete += n_rows_this_loop;
            if (EndRowLoop()) {
                return;
            }

            if (n_rows_complete + n_rows_this_loop > n_rows_this_core) {
                n_rows_this_loop = n_rows_this_core - n_rows_complete;
            }
            src_row_loop_offset = n_rows_complete * n_cols;
            dst_row_loop_offset = n_rows_complete * n_cols_aligned;
        }

        inline __aicore__ void InitColLoop(int32_t max_cols_per_loop) 
        {
            this->max_cols_per_loop = max_cols_per_loop;
            n_cols_complete = 0;
            col_loop_offset = 0;

            n_cols_this_loop = (n_cols < max_cols_per_loop) ? n_cols : max_cols_per_loop;
        }

        inline __aicore__ bool EndColLoop() const
        {
            return n_cols_complete == n_cols_this_core;
        }

        inline __aicore__ void NextColLoop()
        {
            n_cols_complete += n_cols_this_loop;
            if (EndColLoop()) {
                return;
            }

            if (n_cols_complete + n_cols_this_loop > n_cols_this_core) {
                n_cols_this_loop = n_cols_this_core - n_cols_complete;
            }
            col_loop_offset = n_cols_complete;
        }

        inline __aicore__ int64_t src_offset() const
        {
            return src_core_offset + src_batch_offset + src_row_loop_offset + col_loop_offset;
        }

        inline __aicore__ int64_t dst_offset() const
        {
            return dst_core_offset + dst_batch_offset + dst_row_loop_offset + col_loop_offset;
        }

        int32_t batch_size;
        int32_t n_rows;
        int32_t n_cols;
        int32_t n_cols_aligned;

        int32_t n_rows_this_core;
        int32_t n_cols_this_core;
        int32_t row_offset_this_core;
        int32_t col_offset_this_core;

        int32_t max_rows_per_loop;
        int32_t max_cols_per_loop;

        int32_t batch_idx;
        int32_t n_rows_complete;
        int32_t n_cols_complete;

        int32_t n_rows_this_loop;
        int32_t n_cols_this_loop;

        int64_t src_core_offset;
        int64_t dst_core_offset;
        int64_t src_batch_offset;
        int64_t dst_batch_offset;
        int64_t src_row_loop_offset;
        int64_t dst_row_loop_offset;
        int64_t col_loop_offset;
    };

    __aicore__ explicit BasePadder() = default;

    inline __aicore__ void SetArgs(__gm__ uint8_t *gm_a, __gm__ uint8_t *gm_b, const LcalWorkspaceInfo &workspace_info,
            int32_t batch_size, int32_t m, int32_t k, int32_t n, int32_t m_align, int32_t k_align, int32_t n_align, bool aligned_a, bool aligned_b, bool trans_a, bool trans_b)
    {
        this->gm_a = reinterpret_cast<__gm__ LhsDtype *>(gm_a);
        this->gm_b = reinterpret_cast<__gm__ RhsDtype *>(gm_b);

        this->batch_size = batch_size;
        this->m = m;
        this->k = k;
        this->n = n;
        this->trans_a = trans_a;
        this->trans_b = trans_b;

        this->m_align = m_align;
        this->k_align = k_align;
        this->n_align = n_align;
        
        this->aligned_a = aligned_a;
        this->aligned_b = aligned_b;

        gm_a_align = reinterpret_cast<__gm__ MmadDtype *>(workspace_info.gm_a_align ? workspace_info.gm_a_align : gm_a);
        gm_b_align = reinterpret_cast<__gm__ MmadDtype *>(workspace_info.gm_b_align ? workspace_info.gm_b_align : gm_b);
    }

protected:
    inline __aicore__ void PadMatrix(__gm__ MmadDtype *gm_dst, __gm__ MmadDtype *gm_src,
            int32_t n_rows, int32_t n_cols, int32_t n_cols_aligned)
    {
        LoopIter it(batch_size, n_rows, n_cols, n_cols_aligned);

        const int32_t MAX_LEN = Block32B<MmadDtype>::AlignDown(MAX_UB_BUFF / sizeof(MmadDtype));
        int32_t n_cols_round = Block32B<MmadDtype>::AlignUp(n_cols);
        int32_t max_rows_per_loop = (n_cols_round <= MAX_LEN) ? (MAX_LEN / n_cols_round) : 1;
        int32_t max_cols_per_loop = (n_cols_round <= MAX_LEN) ? n_cols : MAX_LEN;

        auto ub_base = reinterpret_cast<__ubuf__ MmadDtype *>((uintptr_t)0);

        for (it.InitBatchLoop(); !it.EndBatchLoop(); it.NextBatchLoop()) {
            for (it.InitColLoop(max_cols_per_loop); !it.EndColLoop(); it.NextColLoop()) {
                int32_t src_gap = n_cols - it.n_cols_this_loop;
                int32_t dst_gap = n_cols_aligned - it.n_cols_this_loop;
                for (it.InitRowLoop(max_rows_per_loop); !it.EndRowLoop(); it.NextRowLoop()) {
                    auto src = gm_src + it.src_offset();
                    auto dst = gm_dst + it.dst_offset();

                    CopyGmToUbufAlign(ub_base, src, it.n_rows_this_loop, it.n_cols_this_loop, src_gap);

                    SetFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
                    WaitFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);

                    CopyUbufToGmAlign(dst, ub_base, it.n_rows_this_loop, it.n_cols_this_loop, dst_gap);

                    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
                    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
                }
            }
        }
    }

    inline __aicore__ void Barrier()
    {
        FFTSCrossCoreSync<PIPE_MTE3>(0, AIV_FINISH_ALIGN_FLAG_ID);
        WaitEvent(AIV_FINISH_ALIGN_FLAG_ID);

        FFTSCrossCoreSync<PIPE_MTE3>(2, AIC_WAIT_AIV_FINISH_ALIGN_FLAG_ID);
        PipeBarrier<PIPE_ALL>();
    }

    __gm__ LhsDtype *__restrict__ gm_a{ nullptr };
    __gm__ RhsDtype *__restrict__ gm_b{ nullptr };
    __gm__ MmadDtype *__restrict__ gm_a_align{ nullptr };
    __gm__ MmadDtype *__restrict__ gm_b_align{ nullptr };

    int32_t batch_size;

    int32_t m_align;
    int32_t n_align;
    int32_t k_align;

    int32_t m;
    int32_t n;
    int32_t k;

    bool trans_a;
    bool trans_b;

    int32_t aligned_a;
    int32_t aligned_b;

    LcalWorkspaceInfo workspace_info;
};

template <typename InputDtype>
class Padder : public BasePadder<InputDtype, InputDtype, InputDtype> {
public:
    __aicore__ explicit Padder() = default;

    inline __aicore__ void Run(int32_t expert_per_rank = 1)
    {
        if (this->aligned_a) {
            int n_rows = this->trans_a ? this->k : this->m;
            int n_cols = this->trans_a ? this->m : this->k;
            int n_cols_aligned = this->trans_a ? this->m_align : this->k_align;

            this->PadMatrix(this->gm_a_align, this->gm_a, n_rows, n_cols, n_cols_aligned);
        }

        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
        WaitFLag<HardEvent::MTE3_MTE2>(EVENT_ID1);

        if (this->aligned_b) {
            int n_rows = this->trans_b ? this->n : this->k;
            int n_cols = this->trans_b ? this->k : this->n;
            int n_cols_aligned = this->trans_b ? this->k_align : this->n_align;

            this->PadMatrix(this->gm_b_align, this->gm_b, n_rows * expert_per_rank, n_cols, n_cols_aligned);
        }

        this->Barrier();
    }
};

class FormatOffset {
public:
    static constexpr int32_t max_len = 49152;
    static inline __aicore__ void Loop(__gm__ int32_t *dst, int32_t offset, int32_t len)
    {
        static const auto ub_offset = reinterpret_cast<__ubuf__ int32_t *>((uintptr_t)0);

        int32_t repeat_num = Block256B<int32_t>::Count(len);
        int32_t loop_num = DivCeil(repeat_num, repeat_num);
        uint8_t repeat_this_loop = static_cast<uint8_t>(repeat);
        for (int32_t loop_idx = 0; loop_idx < loop_num; ++loop_idx) {
            if (loop_idx == loop_num - 1) {
                repeat_this_loop = repeat_num - loop_idx * repeat;
            }
            VectorDup(ub_offset + loop_idx * repeat * Block256B<int32_t>::size, offset, repeat_this_loop, 1, 8);
        }
        SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
        WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);

        CopyUbufToGmAlign(dst, ub_offset, 1, len, 0);
        SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
        WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
    }

private:
    static constexpr uint8_t repeat = 255;
};


template<>
class Padder<int8_t> : public BasePadder<int8_t, int8_t, int8_t> {
public:
    __aicore__ explicit Padder() = default;

    inline __aicore__ void SetArgs(__gm__ uint8_t *gm_a, __gm__ uint8_t *gm_b, const LcalWorkspaceInfo &workspace_info,
            int32_t batch_size, int32_t m, int32_t k, int32_t n, int32_t m_align, int32_t k_align, int32_t n_align, bool aligned_a, bool aligned_b, bool trans_a, bool trans_b,
            __gm__ uint8_t *gm_dequant_offset = nullptr,
            QuantGranularity dequant_granularity = QuantGranularity::QUANT_GRANULARITY_UNDEFINED)
    {
        this->BasePadder<int8_t, int8_t, int8_t>::SetArgs(gm_a, gm_b, workspace_info, batch_size, m, k, n,
                m_align, k_align, n_align, aligned_a, aligned_b, trans_a, trans_b);
        
        if (gm_dequant_offset != nullptr && dequant_granularity == QuantGranularity::PER_TENSOR) {
            offset = *reinterpret_cast<__gm__ int32_t *>(gm_dequant_offset);
            gm_format_dequant_offset = reinterpret_cast<__gm__ int32_t *>(workspace_info.gm_dequant_param);
            need_format_dequant_offset = true;
        }
    }

    inline __aicore__ void Run(int32_t expert_per_rank = 1)
    {
        if (this->aligned_a) {
            int n_rows = this->trans_a ? this->k : this->m;
            int n_cols = this->trans_a ? this->m : this->k;
            int n_cols_aligned = this->trans_a ? this->m_align : this->k_align;

            this->PadMatrix(this->gm_a_align, this->gm_a, n_rows, n_cols, n_cols_aligned);
        }

        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
        WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);

        if (this->aligned_b) {
            int n_rows = this->trans_b ? this->n : this->k;
            int n_cols = this->trans_b ? this->k : this->n;
            int n_cols_aligned = this->trans_b ? this->k_align : this->n_align;

            this->PadMatrix(this->gm_b_align, this->gm_b, n_rows * expert_per_rank, n_cols, n_cols_aligned);
        }

        if (need_format_dequant_offset) {
            SetFlag<HardEvent::MTE3_V>(EVENT_ID1);
            WaitFlag<HardEvent::MTE3_V>(EVENT_ID1);
            FormatOffset();
        }

        this->Barrier();
    }

private:
    inline __aicore__ void FormatOffset()
    {
        int32_t align_core_idx = get_block_idx() * get_subblockdim() + get_subblockid();
        int32_t align_core_num = get_block_num() * get_subblockdim();

        int32_t len = FormatOffset::max_len;
        int32_t loop_num = DivCeil(n, len);
        for (int32_t i = align_core_idx; i < loop_num; i += align_core_num) {
            int32_t n_complete = i * len;
            if (n_complete + len > n) {
                len = n - n_complete;
            }
            FormatOffset::Loop(gm_format_dequant_offset + n_complete, offset, len);
        }
    }

    __gm__ int32_t *gm_format_dequant_offset;
    int32_t offset;
    bool need_format_dequant_offset{ false };
};


template <typename InputDtype, QuantGranularity MODE>
class DequantPadder : public BasePadder<InputDtype, int8_t, InputDtype> {
public:
    __aicore__ explicit DequantPadder() = default;
    inline __aicore__ void SetArgs(__gm__ uint8_t *gm_a, __gm__ uint8_t *gm_b, const LcalWorkspaceInfo &workspace_info,
            int32_t batch_size, int32_t m, int32_t k, int32_t n, int32_t m_align, int32_t k_align, int32_t n_align, bool aligned_a, bool aligned_b, bool trans_a, bool trans_b,
            __gm__ uint8_t *gm_dequant_scale, __gm__ uint8_t *gm_dequant_offset)
    {}
    inline __aicore__ void Run() {}
};


template <>
class DequantPadder<half, QuantGranularity::PER_TENSOR> : public BasePadder<half, int8_t, half> {
public:
    __aicore__ explicit DequantPadder() = default;

    inline __aicore__ void SetArgs(__gm__ uint8_t *gm_a, __gm__ uint8_t *gm_b, const LcalWorkspaceInfo &workspace_info,
            int32_t batch_size, int32_t m, int32_t k, int32_t n, int32_t m_align, int32_t k_align, int32_t n_align, bool aligned_a, bool aligned_b, bool trans_a, bool trans_b,
            __gm__ uint8_t *gm_dequant_scale, __gm__ uint8_t *gm_dequant_offset)
    {
        this->BasePadder<half, int8_t, half>::SetArgs(gm_a, gm_b, workspace_info, batch_size, m, k, n,
                m_align, k_align, n_align, aligned_a, aligned_b, trans_a, trans_b);
        
        scale = *reinterpret_cast<__gm__ half *>(gm_dequant_scale);
        if (gm_dequant_offset) {
            offset = *reinterpret_cast<__gm__ half *>(gm_dequant_offset);
            has_offset = true;
        }
    }

    inline __aicore__ void Run()
    {
        if (this->aligned_a) {
            int n_rows = this->trans_a ? this->k : this->m;
            int n_cols = this->trans_a ? this->m : this->k;
            int n_cols_aligned = this->trans_a ? this->m_align : this->k_align;

            this->PadMatrix(this->gm_a_align, this->gm_a, n_rows, n_cols, n_cols_aligned);
        }

        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
        WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);

        int n_rows = this->trans_b ? this->n : this->k;
        int n_cols = this->trans_b ? this->k : this->n;
        int n_cols_aligned = this->trans_b ? this->k_align : this->n_align;

        DequantAndPadMatrix(this->gm_b_align, this->gm_b, n_rows, n_cols, n_cols_aligned);

        this->Barrier();
    }

private:
    inline __aicore__ void DequantAndPadMatrix(__gm__ half *gm_dst, __gm__ int8_t *gm_src,
            int32_t n_rows, int32_t n_cols, int32_t n_cols_aligned)
    {
        LoopIter it(this->batch_size, n_rows, n_cols, n_cols_aligned);

        const int32_t MAX_LEN = Block256B<half>::AlignDown(MAX_UB_BUFF / (sizeof(int8_t) + sizeof(half)));
        int32_t n_cols_round = Block32B<int8_t>::AlignUp(n_cols);
        int32_t max_rows_per_loop = (n_cols_round <= MAX_LEN) ? (MAX_LEN / n_cols_round) : 1;
        int32_t max_cols_per_loop = (n_cols_round <= MAX_LEN) ? n_cols : MAX_LEN;

        auto ub_vconv = reinterpret_cast<__ubuf__ int8_t *>((uintptr_t)0);
        auto ub_muls = reinterpret_cast<__ubuf__ half *>((uintptr_t)(MAX_LEN * sizeof(int8_t)));

        for (it.InitBatchLoop(); !it.EndBatchLoop(); it.NextBatchLoop()) {
            for (it.InitColLoop(max_cols_per_loop); !it.EndColLoop(); it.NextColLoop()) {
                int32_t src_gap = n_cols - it.n_cols_this_loop;
                int32_t dst_gap = n_cols_aligned - it.n_cols_this_loop;
                for (it.InitRowLoop(max_rows_per_loop); !it.EndRowLoop(); it.NextRowLoop()) {
                    auto src = gm_src + it.src_offset();
                    auto dst = gm_dst + it.dst_offset();

                    CopyGmToUbufAlign(ub_vconv, src, it.n_rows_this_loop, it.n_cols_this_loop, src_gap);

                    int32_t n_blocks_per_row = Block32B<int8_t>::Count(it.n_cols_this_loop) *
                            (sizeof(half) / sizeof(int8_t));
                    int32_t n_blocks = it.n_rows_this_loop * n_blocks_per_row;
                    int32_t repeat_times = DivCeil(n_blocks, VEC_BLOCK_PER_REPEAT);

                    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
                    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);

                    uint8_t repeat = REPEAT_PER_LOOP;
                    for (int32_t n_repeat_complete = 0; n_repeat_complete < repeat_times; n_repeat_complete += repeat) {
                        if (n_repeat_complete + repeat > repeat_times) {
                            repeat = repeat_times - n_repeat_complete;
                        }
                        Vconv(ub_muls + n_repeat_complete * Block256B<half>::size,
                                ub_vconv + n_repeat_complete * Block256B<half>::size, offset, 1, 1, 8, 4);
                    }

                    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
                    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);

                    if (has_offset) {
                        PipeBarrier<PIPE_V>();

                        repeat = REPEAT_PER_LOOP;
                        for (int32_t n_repeat_complete = 0; n_repeat_complete < repeat_times;
                                n_repeat_complete += repeat) {
                            if (n_repeat_complete + repeat > repeat_times) {
                                repeat = repeat_times - n_repeat_complete;
                            }
                            Vadds(ub_muls + n_repeat_complete * Block256B<half>::size,
                                    ub_muls + n_repeat_complete * Block256B<half>::size, offset, 1, 1, 8, 8);
                        }
                    }

                    PipeBarrier<PIPE_V>();

                    repeat = REPEAT_PER_LOOP;
                    for (int32_t n_repeat_complete = 0; n_repeat_complete < repeat_times; n_repeat_complete += repeat) {
                        if (n_repeat_complete + repeat > repeat_times) {
                            repeat = repeat_times - n_repeat_complete;
                        }
                        Vmuls(ub_muls + n_repeat_complete * Block256B<half>::size,
                                ub_muls + n_repeat_complete * Block256B<half>::size, scale, repeat, 1, 1, 8, 8);
                    }

                    int32_t ubuf_gap = n_blocks_per_row - Block32B<half>::Count(it.n_cols_this_loop);

                    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
                    WaitFLag<HardEvent::V_MTE3>(EVENT_ID0);

                    CopyUbufToGmAlign(dst, ub_muls, it.n_rows_this_loop, it.n_cols_this_loop, dst_gap, ubuf_gap);

                    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
                    WaitFLag<HardEvent::MTE3_V>(EVENT_ID0);
                }
            }
        }
    }

    half scale;
    half offset;
    bool has_offset{ false };
};

