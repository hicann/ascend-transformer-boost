/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
#ifdef __CCE_KT_TEST__
#include "stub_def.h"
#include "stub_fun.h"
#else
#define __aicore__ [aicore]
#endif
#include "kernels/utils/kernel/common.h"
#include "kernels/utils/kernel/mem.h"
#include "kernels/utils/kernel/iterator.h"
#include "kernels/utils/kernel/mma.h"
#include "kernels/utils/kernel/utils.h"
#include "kernels/utils/kernel/simd.h"

constexpr int32_t BLOCK_SIZE_16 = 16;
constexpr int32_t BLOCK_SIZE_32 = 32;
constexpr int32_t MAX_HW_SYNC_COUNTER = 15;

constexpr uint32_t AIC_FINISH_FLAG_ID = 1;
constexpr uint32_t AIV_FINISH_FLAG_ID = 2;
constexpr uint32_t AIV_FINISH_MOVE_BIAS_FLAG_ID = 3;
constexpr uint32_t SYNC_MODE = 2;
constexpr uint32_t CONST16 = 16;

__aicore__ __force_inline__ uint64_t RoundUp16(const uint64_t val) { return (val + CONST16 - 1) / CONST16 * CONST16; }

__force_inline__ __aicore__ void GetBlockIdx(int32_t loop_idx,
                                             int32_t m_loop,
                                             int32_t n_loop,
                                             int32_t swizzl_direction,
                                             int32_t swizzl_count,
                                             int32_t &m_idx,
                                             int32_t &n_idx)
{
    int32_t in_batch_idx = loop_idx % (m_loop * n_loop);
    if (swizzl_direction == 0) { // Zn
        int32_t tile_block_loop = (m_loop + swizzl_count - 1) / swizzl_count;
        int32_t tile_block_idx = in_batch_idx / (swizzl_count * n_loop);
        int32_t in_tile_block_idx = in_batch_idx % (swizzl_count * n_loop);

        int32_t n_row = swizzl_count;
        if (tile_block_idx == tile_block_loop - 1) {
            n_row = m_loop - swizzl_count * tile_block_idx;
        }
        m_idx = tile_block_idx * swizzl_count + in_tile_block_idx % n_row;
        n_idx = in_tile_block_idx / n_row;
    } else if (swizzl_direction == 1) { // Nz
        int32_t tile_block_loop = (n_loop + swizzl_count - 1) / swizzl_count;
        int32_t tile_block_idx = in_batch_idx / (swizzl_count * m_loop);
        int32_t in_tile_block_idx = in_batch_idx % (swizzl_count * m_loop);

        int32_t n_col = swizzl_count;
        if (tile_block_idx == tile_block_loop - 1) {
            n_col = n_loop - swizzl_count * tile_block_idx;
        }
        m_idx = in_tile_block_idx / n_col;
        n_idx = tile_block_idx * swizzl_count + in_tile_block_idx % n_col;
    }
}

#ifdef __DAV_C220_CUBE__

constexpr int32_t L0AB_PINGPONG_BUFFER_LEN_FP16 = 16384;    // 32 KB
constexpr int32_t L0AB_PINGPONG_BUFFER_LEN_INT8 = 32768;    // 32 KB
constexpr int32_t CUBE_MATRIX_SIZE_256 = 256;               // 16 * 16
constexpr int32_t CUBE_MATRIX_SIZE_512 = 16 * 32;           // 16 * 23
constexpr int64_t L1_PINGPONG_BUFFER_LEN_FP16 = 131072;     // 256 KB
constexpr int64_t L1_IDENTITY_MATRIX_BUFFER_SIZE = 1024;    // 1 KB
constexpr int64_t L1_PINGPONG_BUFFER_LEN_INT8 = 131072 * 2; // 256 KB
constexpr int64_t ND2NZ_STRIDE_LIMIT = 65536;

template <uint32_t SWIZZL_DIRECT,
          bool TA,
          bool TB,
          bool SPLIT_K = false,
          bool HAVE_BIAS = false,
          bool IS_INT8 = false,
          typename InDtype = half,
          typename OutDtype = half,
          typename TBIAS = half,
          typename TDESCALE = float>
class PpMatmul {
public:
    __aicore__ explicit PpMatmul(){};

    __aicore__ __force_inline__ void SetArgs(__gm__ uint8_t *__restrict__ a,
                                             __gm__ uint8_t *__restrict__ b,
                                             __gm__ uint8_t *__restrict__ c,
                                             __gm__ uint8_t *__restrict__ bias,
                                             int32_t batch_real,
                                             int32_t m_real,
                                             int32_t k_real,
                                             int32_t n_real,
                                             int32_t m0_real,
                                             int32_t k0_real,
                                             int32_t n0_real,
                                             int32_t m_loop_real,
                                             int32_t k_loop_real,
                                             int32_t n_loop_real,
                                             int32_t core_loop_real,
                                             int32_t direct_count)
    {
        gm_a.SetGlobalBuffer((__gm__ InDtype *)(a));
        gm_b.SetGlobalBuffer((__gm__ InDtype *)(b));
        gm_bias.SetGlobalBuffer((__gm__ TBIAS *)(bias));
        gm_c.SetGlobalBuffer((__gm__ OutDtype *)(c));

        batch_size = batch_real;
        m = m_real;
        k = k_real;
        n = n_real;

        m0 = m0_real;
        k0 = k0_real;
        n0 = n0_real;

        block_size = IS_INT8 ? BLOCK_SIZE_32 : BLOCK_SIZE_16;
        cube_matrix_size = IS_INT8 ? CUBE_MATRIX_SIZE_512 : CUBE_MATRIX_SIZE_256;
        L1_PINGPONG_BUFFER_LEN = (m0 * k0 + cube_matrix_size - 1) / cube_matrix_size * cube_matrix_size +
                                 (n0 * k0 + cube_matrix_size - 1) / cube_matrix_size * cube_matrix_size;
        L0AB_PINGPONG_BUFFER_LEN = IS_INT8 ? L0AB_PINGPONG_BUFFER_LEN_INT8 : L0AB_PINGPONG_BUFFER_LEN_FP16;

        int32_t a_l1_size = m0 * k0 * sizeof(InDtype);
        int32_t b_l1_size = n0 * k0 * sizeof(InDtype);
        l1_base_b = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(
            ((a_l1_size + cube_matrix_size - 1) / cube_matrix_size * cube_matrix_size));
        core_num = AscendC::GetBlockNum();
        core_idx = AscendC::GetBlockIdx();
        m_loop = m_loop_real;
        k_loop = k_loop_real;
        n_loop = n_loop_real;
        core_loop = core_loop_real;
        swizzl_count = direct_count;
        ping_flag = 1;
    }

    __aicore__ __force_inline__ void run()
    {
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(FIX, M, EVENT_ID0);
        for (int32_t loop_idx = 0; loop_idx < core_loop; loop_idx++) {
            if (loop_idx % core_num != core_idx) {
                continue;
            }
            int32_t batch_idx = loop_idx / (m_loop * n_loop);
            int32_t m_idx, n_idx;
            GetBlockIdx(loop_idx, m_loop, n_loop, SWIZZL_DIRECT, swizzl_count, m_idx, n_idx);

            int64_t offset_a, offset_b, offset_bias, offset_scalar, offset_a_next, offset_b_next;
            int64_t offset_c = batch_idx * m * n + m_idx * m0 * n + n_idx * n0;
            int32_t m_actual = (m_idx == (m_loop - 1)) ? (m - m_idx * m0) : m0;
            int32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
            int32_t m_round = 0;
            int32_t n_round = 0;
            if (IS_INT8) {
                // directive Restrictions
                if (TA) {
                    m_round = (m_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
                } else {
                    m_round = (m_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
                }
                if (TB) {
                    n_round = (n_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
                } else {
                    n_round = (n_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
                }
            } else {
                m_round = (m_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
                n_round = (n_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
            }

            int32_t mn_max = m_round > n_round ? m_round : n_round;
            int32_t k_part_len = 0;
            if (IS_INT8) {
                k_part_len = L0AB_PINGPONG_BUFFER_LEN_INT8 / mn_max / BLOCK_SIZE_32 * BLOCK_SIZE_32;
            } else {
                k_part_len = L0AB_PINGPONG_BUFFER_LEN / mn_max / BLOCK_SIZE_16 * BLOCK_SIZE_16; // 16 <= mn_max <= 1024
            }
            k_part_len = L0AB_PINGPONG_BUFFER_LEN / mn_max / BLOCK_SIZE_16 * BLOCK_SIZE_16; // 16 <= mn_max <= 1024

            if (TA) {
                offset_a = batch_idx * m * k + m_idx * m0;
            } else {
                offset_a = batch_idx * m * k + m_idx * m0 * k;
            }

            if (TB) {
                offset_b = batch_idx * k * n + n_idx * n0 * k;
            } else {
                offset_b = batch_idx * k * n + n_idx * n0;
            }
            offset_bias = batch_idx * n + n_idx * n0;
            offset_scalar = n_idx * n0 * 2;

            int32_t k_actual = (k_loop == 1) ? k : k0;
            int32_t k_round = (k_actual + block_size - 1) / block_size * block_size; // int8 ：32 fp16 ：16

            auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

            AscendC::LocalTensor<InDtype> l1_buf_a = ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l1_buf_b = ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l0a_buf = ping_flag ? l0a_base : l0a_base[L0AB_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l0b_buf = ping_flag ? l0b_base : l0b_base[L0AB_PINGPONG_BUFFER_LEN];

            WAIT_FLAG(MTE1, MTE2, event_id);
            // *** load matrix A to L1
            if ((m == 1) || (m_actual == 1 && !TA)) {
                gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(l1_buf_a, gm_a[offset_a], 1,
                                                                                         RoundUp16(1), 1,
                                                                                         k_round, // lenBurst
                                                                                         RoundUp16(k_round), k_round);
            } else {
                if (TA) {
                    gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_a, gm_a[offset_a], k_actual, k_round, k, m_actual, m_round, m);
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_a, gm_a[offset_a], m_actual, m_round, m, k_actual, k_round, k);
                }
            }
            SET_FLAG(MTE2, MTE1, event_id);

            // *** load matrix B to L1
            WAIT_FLAG(MTE1, MTE2, event_id + 2);
            if (TB) {
                gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                    l1_buf_b, gm_b[offset_b], n_actual, n_round, n, k_actual, k_round, k);
            } else {
                gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                    l1_buf_b, gm_b[offset_b], k_actual, k_round, k, n_actual, n_round, n);
            }
            SET_FLAG(MTE2, MTE1, event_id + 2);

            for (int32_t k_idx = 0; k_idx < k_loop; k_idx++) {
                if (TA) {
                    offset_a = batch_idx * m * k + k_idx * k0 * m + m_idx * m0;
                } else {
                    offset_a = batch_idx * m * k + m_idx * m0 * k + k_idx * k0;
                }

                if (TB) {
                    offset_b = batch_idx * k * n + n_idx * n0 * k + k_idx * k0;
                } else {
                    offset_b = batch_idx * k * n + k_idx * k0 * n + n_idx * n0;
                }

                int32_t k_actual = (k_idx == (k_loop - 1)) ? (k - k_idx * k0) : k0;
                int32_t k_round = (k_actual + block_size - 1) / block_size * block_size;
                int32_t k_part_loop = (k_actual + k_part_len - 1) / k_part_len;

                AscendC::LocalTensor<InDtype> l1_buf_a = ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                AscendC::LocalTensor<InDtype> l1_buf_b = ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

                if (k_idx < k_loop - 1) {
                    if (TA) {
                        offset_a_next = batch_idx * m * k + (k_idx + 1) * k0 * m + m_idx * m0;
                    } else {
                        offset_a_next = batch_idx * m * k + m_idx * m0 * k + (k_idx + 1) * k0;
                    }

                    if (TB) {
                        offset_b_next = batch_idx * k * n + n_idx * n0 * k + (k_idx + 1) * k0;
                    } else {
                        offset_b_next = batch_idx * k * n + (k_idx + 1) * k0 * n + n_idx * n0;
                    }

                    int32_t k_actual_next = ((k_idx + 1) == (k_loop - 1)) ? (k - (k_idx + 1) * k0) : k0;
                    int32_t k_round_next = (k_actual_next + block_size - 1) / block_size * block_size;

                    AscendC::LocalTensor<InDtype> l1_buf_a_next =
                        (1 - ping_flag) ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l1_buf_b_next =
                        (1 - ping_flag) ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];

                    auto event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                    WAIT_FLAG(MTE1, MTE2, event_id_next);
                    // *** load matrix A to L1
                    if ((m == 1) || (m_actual == 1 && !TA)) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(
                            l1_buf_a_next,       // dst
                            gm_a[offset_a_next], // src
                            1, RoundUp16(1), 1,
                            k_round_next, // lenBurst
                            RoundUp16(k_round_next), k_round_next);
                    } else {
                        if (TA) {
                            gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                                l1_buf_a_next, gm_a[offset_a_next], k_actual_next, k_round_next, k, m_actual, m_round,
                                m);
                        } else {
                            gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                                l1_buf_a_next, gm_a[offset_a_next], m_actual, m_round, n, k_actual_next, k_round_next,
                                k);
                        }
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next);

                    // *** load matrix B to L1
                    WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
                    if (TB) {
                        gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_b_next, gm_b[offset_b_next], n_actual, n_round, n, k_actual_next, k, k);
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, half, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_b_next, gm_b[offset_b_next], k_actual_next, k_round_next, k, n_actual, n_round, n);
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next + 2);
                }

                for (int k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
                    int32_t k0_round = (k_part_idx < k_part_loop - 1) ? k_part_len : k_round - k_part_idx * k_part_len;
                    int32_t k0_actual =
                        (k_part_idx < k_part_loop - 1) ? k_part_len : k_actual - k_part_idx * k_part_len;

                    auto mte1_mad_ping_flag = 1 - k_part_idx % 2;
                    auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
                    AscendC::LocalTensor<InDtype> l0a_buf = l0a_base[(k_part_idx % 2) * L0AB_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l0b_buf = l0b_base[(k_part_idx % 2) * L0AB_PINGPONG_BUFFER_LEN];
                    // *** load matrix A from L1 to L0A
                    if (k_part_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, event_id);
                    }
                    WAIT_FLAG(M, MTE1, mte1_mad_event_id);
                    if ((m == 1) || (m_actual == 1 && !TA)) {
                        l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf, l1_buf_a[k_part_idx * k_part_len], 0,
                            (k0_round + cube_matrix_size - 1) / cube_matrix_size, // repeat
                            0,
                            1, // srcStride
                            0,
                            0 // dstStride
                        );
                    } else {
                        if (TA) {
                            for (int i = 0; i < m_round / BLOCK_SIZE_16; i++) {
                                l1_to_l0_a<ArchType::ASCEND_V220, InDtype, true, DataFormat::VECTOR,
                                           DataFormat::VECTOR>(
                                    l0a_buf[i * k0_round * BLOCK_SIZE_16],
                                    l1_buf_a[k_part_idx * k_part_len * BLOCK_SIZE_16 + i * k_round * BLOCK_SIZE_16], 0,
                                    k0_round / BLOCK_SIZE_16, // repeat
                                    0,
                                    1, // srcStride
                                    0,
                                    0 // dstStride

                                );
                            }
                        } else {

                            for (int32_t i = 0; i < m_round / BLOCK_SIZE_16; i++) {
                                l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR,
                                           DataFormat::VECTOR>(
                                    l0a_buf[i * k0_round * BLOCK_SIZE_16],                                  // dst
                                    l1_buf_a[k_part_idx * k_part_len * m_round + i * CUBE_MATRIX_SIZE_256], // src
                                    0,
                                    k0_round / BLOCK_SIZE_16, // repeat
                                    0,
                                    m_round / BLOCK_SIZE_16, // srcStride
                                    0,
                                    0 // dstStride
                                );
                            }
                        }
                    }
                    if (k_part_idx == k_part_loop - 1) {
                        SET_FLAG(MTE1, MTE2, event_id);
                    }

                    // *** load matrix B from L1 to L0B
                    if (k_part_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, event_id + 2);
                    }
                    if (TB) {
                        l1_to_l0_b<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf,                                     // dst
                            l1_buf_b[k_part_idx * k_part_len * n_round], // src
                            0,
                            k0_round * n_round / CUBE_MATRIX_SIZE_256, // repeat
                            0,
                            1, // srcStride
                            0,
                            0 // dstStride
                        );
                    } else {
                        for (int32_t i = 0; i < k0_round / BLOCK_SIZE_16; i++) {
                            l1_to_l0_b<ArchType::ASCEND_V220, InDtype, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                                l0b_buf[i * n_round * BLOCK_SIZE_16],
                                l1_buf_b[(k_part_idx * k_part_len + i * BLOCK_SIZE_16) * BLOCK_SIZE_16], 0,
                                n_round / BLOCK_SIZE_16, // repeat
                                0,
                                k_round / BLOCK_SIZE_16, // srcStride
                                0,
                                0 // dstStride
                            );
                        }
                    }
                    if (k_part_idx == k_part_loop - 1) {
                        SET_FLAG(MTE1, MTE2, event_id + 2);
                    }

                    SET_FLAG(MTE1, M, mte1_mad_event_id);
                    WAIT_FLAG(MTE1, M, mte1_mad_event_id);

                    bool init_c = (k_idx == 0 && k_part_idx == 0);
                    if (init_c) {
                        WAIT_FLAG(FIX, M, EVENT_ID0);
                    }
                    AscendC::PipeBarrier<PIPE_M>();
                    if (m != 1 && m_actual == 1 && TA) {
                        mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>(l0c_buf, l0a_buf, l0b_buf,
                                                                                    BLOCK_SIZE_16, // m
                                                                                    n_actual,      // n
                                                                                    k0_actual,     // k
                                                                                    init_c         // cmatrixInitVal
                        );
                    } else {
                        mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>(l0c_buf, l0a_buf, l0b_buf,
                                                                                    m_actual,  // m
                                                                                    n_actual,  // n
                                                                                    k0_actual, // k
                                                                                    init_c     // cmatrixInitVal
                        );
                    }
                    AscendC::PipeBarrier<PIPE_M>();
                    SET_FLAG(M, MTE1, mte1_mad_event_id);
                }
                ping_flag = 1 - ping_flag;
            }

            SET_FLAG(M, FIX, EVENT_ID0);
            WAIT_FLAG(M, FIX, EVENT_ID0);

            if (gm_c.GetPhyAddr() == gm_bias.GetPhyAddr()) {
                WaitFlagDev(AIV_FINISH_MOVE_BIAS_FLAG_ID);
            }

            // copy from L0C to gm
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, OutDtype, float>(gm_c[offset_c], l0c_buf,
                                                                              m_actual, // MSize
                                                                              n_actual, // NSize
                                                                              m_round,  // srcStride
                                                                              n         // dstStride_dst_D
            );
            SET_FLAG(FIX, M, EVENT_ID0);

            if (!IS_INT8 && gm_bias.GetPhyAddr() != nullptr) {
                FftsCrossCoreSync<PIPE_FIX, SYNC_MODE>(AIC_FINISH_FLAG_ID);
                if ((loop_idx / core_num + 1) % MAX_HW_SYNC_COUNTER == 0) {
                    WaitFlagDev(AIV_FINISH_FLAG_ID);
                }
            }
        }

        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        AscendC::PipeBarrier<PIPE_ALL>();
    }

private:
    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::GlobalTensor<InDtype> gm_a;
    AscendC::GlobalTensor<InDtype> gm_b;
    AscendC::GlobalTensor<TBIAS> gm_bias;
    AscendC::GlobalTensor<OutDtype> gm_c;

    AscendC::LocalTensor<InDtype> l1_base_a = buf.template GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
    AscendC::LocalTensor<InDtype> l1_base_b = buf.template GetBuffer<BufferType::ASCEND_CB, InDtype>(128 * 1024);
    AscendC::LocalTensor<InDtype> l0a_base = buf.template GetBuffer<BufferType::ASCEND_L0A, InDtype>(0);
    AscendC::LocalTensor<InDtype> l0b_base = buf.template GetBuffer<BufferType::ASCEND_L0B, InDtype>(0);
    AscendC::LocalTensor<float> l0c_buf = buf.template GetBuffer<BufferType::ASCEND_L0C, float>(0);

    uint16_t bias_bt{0};

    int32_t core_num{0};

    int32_t batch_size{0};
    int32_t m{0};
    int32_t k{0};
    int32_t n{0};

    int32_t m0{0};
    int32_t k0{0};
    int32_t n0{0};

    int32_t m_loop{0};
    int32_t n_loop{0};
    int32_t k_loop{0};
    int32_t core_loop{0};
    int32_t core_idx{0};
    int32_t ping_flag{0};
    int32_t block_size{0};
    int32_t cube_matrix_size{0};

    int32_t swizzl_count{0};

    int64_t L1_PINGPONG_BUFFER_LEN{0};
    int32_t L0AB_PINGPONG_BUFFER_LEN{0};
};

extern "C" __global__ __aicore__ void pp_matmul_add(__gm__ uint8_t *__restrict__ ffts_addr,
                                                    __gm__ uint8_t *__restrict__ gm_a,
                                                    __gm__ uint8_t *__restrict__ gm_b,
                                                    __gm__ uint8_t *__restrict__ gm_bias,
                                                    __gm__ uint8_t *__restrict__ gm_c,
                                                    __gm__ uint8_t *__restrict__ tiling_para_gm)
{
    // fp16
    PpMatmul<0, false, false, false> matmul_z;
    PpMatmul<0, true, false, false> matmul_ta_z;
    PpMatmul<0, false, true, false> matmul_tb_z;
    PpMatmul<0, true, true, false> matmul_ta_tb_z;

    PpMatmul<1, false, false, false> matmul_n;
    PpMatmul<1, true, false, false> matmul_ta_n;
    PpMatmul<1, false, true, false> matmul_tb_n;
    PpMatmul<1, true, true, false> matmul_ta_tb_n;

    SetPadding<uint64_t>(0);
    SetAtomicnone();
    SetNdpara(1, 0, 0);
    SetFftsBaseAddr((uint64_t)ffts_addr);

    // get tiling args
    auto tiling_para = (__gm__ int32_t *)(tiling_para_gm);
    int32_t batch_size = tiling_para[0];
    int32_t M = tiling_para[1];
    int32_t K = tiling_para[2];
    int32_t N = tiling_para[3];

    int32_t M0 = tiling_para[4];
    int32_t K0 = tiling_para[5];
    int32_t N0 = tiling_para[6];

    int32_t M_Loop = tiling_para[7];
    int32_t K_Loop = tiling_para[8];
    int32_t N_Loop = tiling_para[9];

    int32_t core_loop = tiling_para[10];
    int32_t swizzl_count = tiling_para[11];
    int32_t tiling_key = tiling_para[12];

    switch (tiling_key) {
        case 0b000000: // swizzl = 0 transa = 0 transb = 0 splitk = 0 bias = 0 int8 = 0
        case 0b000010: // swizzl = 0 transa = 0 transb = 0 splitk = 0 bias = 1 int8 = 0
            matmul_z.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                             core_loop, swizzl_count);
            matmul_z.run();
            break;
        case 0b010000: // swizzl = 0 transa = 1 transb = 0 splitk = 0 bias = 0 int8 = 0
        case 0b010010: // swizzl = 0 transa = 1 transb = 0 splitk = 0 bias = 1 int8 = 0
            matmul_ta_z.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                                core_loop, swizzl_count);
            matmul_ta_z.run();
            break;
        case 0b001000: // swizzl = 0 transa = 0 transb = 1 splitk = 0 bias = 0 int8 = 0
        case 0b001010: // swizzl = 0 transa = 0 transb = 1 splitk = 0 bias = 1 int8 = 0
            matmul_tb_z.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                                core_loop, swizzl_count);
            matmul_tb_z.run();
            break;
        case 0b011000: // swizzl = 0 transa = 1 transb = 1 splitk = 0 bias = 0 int8 = 0
        case 0b011010: // swizzl = 0 transa = 1 transb = 1 splitk = 0 bias = 1 int8 = 0
            matmul_ta_tb_z.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                                   core_loop, swizzl_count);
            matmul_ta_tb_z.run();
            break;
        case 0b100000: // swizzl = 1 transa = 0 transb = 0 splitk = 0 bias = 0 int8 = 0
        case 0b100010: // swizzl = 1 transa = 0 transb = 0 splitk = 0 bias = 1 int8 = 0
            matmul_n.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                             core_loop, swizzl_count);
            matmul_n.run();
            break;
        case 0b110000: // swizzl = 1 transa = 1 transb = 0 splitk = 0 bias = 0 int8 = 0
        case 0b110010: // swizzl = 1 transa = 1 transb = 0 splitk = 0 bias = 1 int8 = 0
            matmul_ta_n.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                                core_loop, swizzl_count);
            matmul_ta_n.run();
            break;
        case 0b101000: // swizzl = 1 transa = 0 transb = 1 splitk = 0 bias = 0 int8 = 0
        case 0b101010: // swizzl = 1 transa = 0 transb = 1 splitk = 0 bias = 1 int8 = 0
            matmul_tb_n.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                                core_loop, swizzl_count);
            matmul_tb_n.run();
            break;
        case 0b111000: // swizzl = 1 transa = 1 transb = 1 splitk = 0 bias = 0 int8 = 0
        case 0b111010: // swizzl = 1 transa = 1 transb = 1 splitk = 0 bias = 1 int8 = 0
            matmul_ta_tb_n.SetArgs(gm_a, gm_b, gm_c, gm_bias, batch_size, M, K, N, M0, K0, N0, M_Loop, K_Loop, N_Loop,
                                   core_loop, swizzl_count);
            matmul_ta_tb_n.run();
            break;
        default: break;
    }
}
#endif

#ifdef __DAV_C220_VEC__
extern "C" __global__ __aicore__ void pp_matmul_add(__gm__ uint8_t *__restrict__ ffts_addr,
                                                    __gm__ uint8_t *__restrict__ gm_a_in,
                                                    __gm__ uint8_t *__restrict__ gm_b_in,
                                                    __gm__ uint8_t *__restrict__ gm_bias_in,
                                                    __gm__ uint8_t *__restrict__ gm_c_in,
                                                    __gm__ uint8_t *__restrict__ tiling_para_gm)
{
    SetAtomicnone();
    SetMasknorm();
    SetFftsBaseAddr((uint64_t)ffts_addr);

    if (gm_bias_in == nullptr) {
        return;
    }

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<half> ub_ab;
    AscendC::LocalTensor<half> ub_c;
    AscendC::LocalTensor<half> ub_result;

    AscendC::GlobalTensor<half> gm_bias;
    AscendC::GlobalTensor<half> gm_c;
    gm_c.SetGlobalBuffer((__gm__ half *)(gm_c_in));
    gm_bias.SetGlobalBuffer((__gm__ half *)(gm_bias_in));

    // get tiling args
    auto tiling_para = (__gm__ int32_t *)(tiling_para_gm);
    int32_t batch_size = tiling_para[0];
    int32_t m = tiling_para[1];
    int32_t n = tiling_para[3];

    int32_t m0 = tiling_para[4];
    int32_t n0 = tiling_para[6];

    int32_t m_loop = tiling_para[7];
    int32_t n_loop = tiling_para[9];

    int32_t core_loop = tiling_para[10];
    int32_t swizzl_count = tiling_para[11];
    int32_t tiling_key = tiling_para[12];
    int32_t SWIZZL_DIRECT = ((tiling_key & 0b100000) >> 5);

    int core_num = AscendC::GetBlockNum();
    int core_idx = AscendC::GetBlockIdx() / 2;

    SET_FLAG(V, MTE2, EVENT_ID0);
    SET_FLAG(MTE3, V, EVENT_ID0);
    for (int32_t loop_idx = 0; loop_idx < core_loop; loop_idx++) {
        if (loop_idx % core_num != core_idx) {
            continue;
        }

        int32_t batch_idx = loop_idx / (m_loop * n_loop);
        int32_t m_idx, n_idx;
        GetBlockIdx(loop_idx, m_loop, n_loop, SWIZZL_DIRECT, swizzl_count, m_idx, n_idx);

        int64_t offset_c = batch_idx * m * n + m_idx * m0 * n + n_idx * n0;
        int32_t m_actual = (m_idx == (m_loop - 1)) ? (m - m_idx * m0) : m0;
        int32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
        int32_t m_round = (m_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
        int32_t n_round = (n_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;

        int32_t m_actual_per_vec = m_actual / 2;
        if (AscendC::GetSubBlockIdx() != 0) {
            offset_c += m_actual_per_vec * n;
            m_actual_per_vec = m_actual - m_actual_per_vec;
        }

        if (m_actual_per_vec == 0) {
            continue;
        }

        int ub_buf_len = (m0 * n0 + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16; // <= 64KB

        ub_ab = buf.GetBuffer<BufferType::ASCEND_UB, half>(0);
        ub_c = buf.GetBuffer<BufferType::ASCEND_UB, half>(ub_buf_len * sizeof(half));
        ub_result = buf.GetBuffer<BufferType::ASCEND_UB, half>(2 * ub_buf_len * sizeof(half));

        bool aligned = (n % BLOCK_SIZE_16 == 0);

        WAIT_FLAG(V, MTE2, EVENT_ID0);

        if (aligned) {
            gm_to_ub<ArchType::ASCEND_V220, half>(ub_c, gm_bias[offset_c],
                                                  0,                             // sid
                                                  m_actual_per_vec,              // nBurst
                                                  n_round / BLOCK_SIZE_16,       // lenBurst
                                                  (n - n_round) / BLOCK_SIZE_16, // srcGap
                                                  0                              // dstGap
            );
        } else {
            gm_to_ub_align<ArchType::ASCEND_V220, half>(ub_c, gm_bias[offset_c],
                                                        0,                             // sid
                                                        m_actual_per_vec,              // nBurst
                                                        n_actual * sizeof(half),       // lenBurst
                                                        0,                             // leftPaddingNum
                                                        0,                             // rightPaddingNum
                                                        (n - n_actual) * sizeof(half), // srcGap
                                                        0                              // dstGap
            );
        }

        if (gm_c_in == gm_bias_in) {
            FftsCrossCoreSync<PIPE_MTE2, 1>(AIV_FINISH_MOVE_BIAS_FLAG_ID);
        }

        WaitFlagDev(AIC_FINISH_FLAG_ID);

        if (aligned) {
            gm_to_ub<ArchType::ASCEND_V220, half>(ub_ab, gm_c[offset_c],
                                                  0,                             // sid
                                                  m_actual_per_vec,              // nBurst
                                                  n_round / BLOCK_SIZE_16,       // lenBurst
                                                  (n - n_round) / BLOCK_SIZE_16, // srcGap
                                                  0                              // dstGap
            );
        } else {
            gm_to_ub_align<ArchType::ASCEND_V220, half>(ub_ab, gm_c[offset_c],
                                                        0,                             // sid
                                                        m_actual_per_vec,              // nBurst
                                                        n_actual * sizeof(half),       // lenBurst
                                                        0,                             // leftPaddingNum
                                                        0,                             // rightPaddingNum
                                                        (n - n_actual) * sizeof(half), // srcGap
                                                        0                              // dstGap
            );
        }

        SET_FLAG(MTE2, V, EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID0);

        WAIT_FLAG(MTE3, V, EVENT_ID0);

        constexpr int ADD_REPEAT = 128;
        constexpr int ADD_NUM_PER_REPEAT = ADD_REPEAT * 128;
        for (int i = 0; i < (m_actual_per_vec * n_actual + ADD_NUM_PER_REPEAT - 1) / ADD_NUM_PER_REPEAT; i++) {
            add_v<ArchType::ASCEND_V220, half>(ub_result[i * ADD_NUM_PER_REPEAT], ub_ab[i * ADD_NUM_PER_REPEAT],
                                               ub_c[i * ADD_NUM_PER_REPEAT], ADD_REPEAT, 1, 1, 1, 8, 8, 8);
        }

        SET_FLAG(V, MTE2, EVENT_ID0);

        SET_FLAG(V, MTE3, EVENT_ID0);
        WAIT_FLAG(V, MTE3, EVENT_ID0);

        if (aligned) {
            ub_to_gm<ArchType::ASCEND_V220, half>(gm_c[offset_c], ub_result,
                                                  0,                            // sid
                                                  m_actual_per_vec,             // nBurst
                                                  n_round / BLOCK_SIZE_16,      // lenBurst
                                                  0,                            // srcGap
                                                  (n - n_round) / BLOCK_SIZE_16 // dstGap
            );
        } else {
            ub_to_gm_align<ArchType::ASCEND_V220, half>(gm_c[offset_c], ub_result,
                                                        0,                            // sid
                                                        m_actual_per_vec,             // nBurst
                                                        n_actual * sizeof(half),      // lenBurst
                                                        0,                            // leftPaddingNum
                                                        0,                            // rightPaddingNum
                                                        0,                            // srcGap
                                                        (n - n_actual) * sizeof(half) // dstGap
            );
        }

        SET_FLAG(MTE3, V, EVENT_ID0);

        if ((loop_idx / core_num + 1) % MAX_HW_SYNC_COUNTER == 1) {
            FftsCrossCoreSync<PIPE_MTE3, 1>(AIV_FINISH_FLAG_ID);
        }
    }
    WAIT_FLAG(V, MTE2, EVENT_ID0);
    WAIT_FLAG(MTE3, V, EVENT_ID0);

    AscendC::PipeBarrier<PIPE_ALL>();
}
#endif
