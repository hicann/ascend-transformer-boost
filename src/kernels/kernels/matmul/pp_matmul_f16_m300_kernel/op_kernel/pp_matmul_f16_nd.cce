/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifdef __CCE_KT_TEST__
#include "stub_def.h"
#include "stub_fun.h"
#define __aicore__
#else
#define __aicore__ [aicore]
#endif

#include "kernels/matmul/tiling/tiling_data.h"
#include "kernels/utils/kernel/common.h"
#include "kernels/utils/kernel/hardware.h"
#include "kernels/utils/kernel/iterator.h"
#include "kernels/utils/kernel/mma.h"
#include "kernels/utils/kernel/utils.h"

#define __force_inline__ inline __attribute__((always_inline))
#define EN_SHUFFLE true

constexpr uint32_t L0_PINGPONG_BUFFER_LEN = 16384;
constexpr uint32_t L1_PINGPONG_BUFFER_LEN = 131072;
constexpr uint32_t CONST_16 = 16;
constexpr uint32_t CONST_32 = 32;
constexpr uint32_t CONST_256 = 256;
constexpr uint64_t ND2NZ_STRIDE_LIMIT = 65536;

enum class ShuffleType : uint32_t { NO_SHUFFLE = 0, CORE_IDX, MIDX, NIDX };

struct PpTilingDataNd {
    uint32_t batchSize{0};
    uint32_t m{0};     // 实际输入的 m
    uint32_t k{0};     // 实际输入的 k
    uint32_t n{0};     // 实际输入的 n
    uint32_t m0{0};
    uint32_t k0{0};
    uint32_t n0{0};
    uint32_t mLoop{1};
    uint32_t kLoop{1};
    uint32_t nLoop{1};
    uint32_t coreLoop{1};
    uint32_t tilingKey{0};
};

struct MatCoord {
    uint64_t m{0};
    uint64_t k{0};
    uint64_t n{0};
};

__aicore__ __force_inline__ uint64_t RoundUp16(const uint64_t val)
{
    return (val + CONST_16 - 1) / CONST_16 * CONST_16;
}

__aicore__ __force_inline__ uint64_t RoundUp256(const uint64_t val)
{
    return (val + CONST_256 - 1) / CONST_256 * CONST_256;
}

template <bool TA, bool TB, typename InDtype = half, typename OutDtype = half> class PpMatmul {
public:
    __aicore__ explicit PpMatmul(){};

    __aicore__ __force_inline__ void SetArgs(__gm__ uint8_t* __restrict__ a,
                                             __gm__ uint8_t* __restrict__ b,
                                             __gm__ uint8_t* __restrict__ c,
                                             __gm__ uint8_t* __restrict__ tiling_data)
    {
        gm_a.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype*>(a));
        gm_b.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype*>(b));
        gm_c.SetGlobalBuffer(reinterpret_cast<__gm__ OutDtype*>(c));
        auto gm_tiling_data = reinterpret_cast<__gm__ PpTilingDataNd*>(tiling_data);
        batch_size = gm_tiling_data->batchSize;
        m = gm_tiling_data->m;
        k = gm_tiling_data->k;
        n = gm_tiling_data->n;
        m0 = gm_tiling_data->m0;
        k0 = gm_tiling_data->k0;
        n0 = gm_tiling_data->n0;
        tdim.m = gm_tiling_data->mLoop;
        tdim.k = gm_tiling_data->kLoop;
        tdim.n = gm_tiling_data->nLoop;
        core_loop = gm_tiling_data->coreLoop;

        l1_base_a = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
        l1_base_b = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(RoundUp256((uint64_t)m0 * k0 * sizeof(InDtype)));
        num_core = AscendC::GetBlockNum();
        core_idx = AscendC::GetBlockIdx();
        ping_flag = 1;
    }

    __aicore__ __force_inline__ void GetBlockIdx(uint64_t index, MatCoord& tidx)
    {
        uint64_t in_batch_idx = index % (tdim.m * tdim.n);
        tidx.m = in_batch_idx / tdim.n;
        tidx.n = in_batch_idx % tdim.n;
    }

    __aicore__ __force_inline__ void run()
    {
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);

        for (uint64_t loop_idx = core_idx; loop_idx < core_loop; ++loop_idx) {
            uint64_t batch_idx = loop_idx / tdim.n / tdim.m;
            MatCoord tidx{0};
            GetBlockIdx(loop_idx, tidx);
            uint64_t offset_a = 0, offset_b = 0, offset_a_next = 0, offset_b_next = 0;
            uint64_t offset_c = batch_idx * m * n + tidx.m * m0 * n + tidx.n * n0;
            uint64_t m_actual = (tidx.m == (tdim.m - 1)) ? (m - tidx.m * m0) : m0;
            uint64_t n_actual = (tidx.n == (tdim.n - 1)) ? (n - tidx.n * n0) : n0;
            uint64_t m_round = RoundUp16(m_actual);
            uint64_t n_round = RoundUp16(n_actual);
            uint64_t mn_max = m_round > n_round ? m_round : n_round;
            uint64_t k_part_len = L0_PINGPONG_BUFFER_LEN / mn_max / CONST_16 * CONST_16;
            uint64_t shuffle_k = EN_SHUFFLE ? (core_idx % tdim.k) : 0;
            offset_a = batch_idx * m * k + tidx.m * m0 * k + shuffle_k * k0;

            if (TB) {
                offset_b = batch_idx * k * n + tidx.n * n0 * k + shuffle_k * k0;
            } else {
                offset_b = batch_idx * k * n + shuffle_k * k0 * n + tidx.n * n0;
            }

            uint64_t k_actual = (shuffle_k == tdim.k - 1) ? k - shuffle_k * k0 : k0;
            uint64_t k_round = (k_actual + 15) & ~15;

            AscendC::LocalTensor<InDtype> l1_buf_a = ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l1_buf_b = ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l0a_buf = ping_flag ? l0a_base : l0a_base[L0_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l0b_buf = ping_flag ? l0b_base : l0b_base[L0_PINGPONG_BUFFER_LEN];
            event_t event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

            if (loop_idx == core_idx) {
                WAIT_FLAG(MTE1, MTE2, event_id);
                // *** load matrix A to L1
                if ((m == 1) || (m_actual == 1 && !TA)) {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(
                        l1_buf_a, gm_a[offset_a], 1, RoundUp16(1), 1, k_round, RoundUp16(k_round), k_round);
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_a, gm_a[offset_a], m_actual, m_round, m, k_actual, k_round, k);
                }
                SET_FLAG(MTE2, MTE1, event_id);
                // *** load matrix B to L1
                WAIT_FLAG(MTE1, MTE2, event_id + 2);
                if constexpr (TB) {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_b, gm_b[offset_b], n_actual, n_round, n, k_actual, k_round, k);
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_b, gm_b[offset_b], k_actual, k_round, k, n_actual, n_round, n);
                }
                SET_FLAG(MTE2, MTE1, event_id + 2);
            }

            for (tidx.k = 0; tidx.k < tdim.k; ++tidx.k) {
                shuffle_k = EN_SHUFFLE ? (tidx.k + core_idx) % tdim.k : tidx.k;
                uint64_t k_actual = (shuffle_k == (tdim.k - 1)) ? (k - shuffle_k * k0) : k0;
                uint64_t k_round = (k_actual + 15) & ~15;
                fdim.k = (k_actual + k_part_len - 1) / k_part_len;

                AscendC::LocalTensor<InDtype> l1_buf_a = ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                AscendC::LocalTensor<InDtype> l1_buf_b = ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

                if (tidx.k < tdim.k - 1) {
                    uint64_t shuffle_k_next = EN_SHUFFLE ? (core_idx + tidx.k + 1) % tdim.k : (tidx.k + 1);
                    offset_a_next = batch_idx * m * k + tidx.m * m0 * k + shuffle_k_next * k0;

                    if (TB) {
                        offset_b_next = batch_idx * k * n + tidx.n * n0 * k + shuffle_k_next * k0;
                    } else {
                        offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + tidx.n * n0;
                    }

                    uint64_t k_actual_next = (shuffle_k_next == (tdim.k - 1)) ? (k - shuffle_k_next * k0) : k0;
                    uint64_t k_round_next = (k_actual_next + 15) & ~15;

                    AscendC::LocalTensor<InDtype> l1_buf_a_next =
                        (1 - ping_flag) ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l1_buf_b_next =
                        (1 - ping_flag) ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                    event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                    WAIT_FLAG(MTE1, MTE2, event_id_next);
                    // *** load matrix A to L1
                    if ((m == 1) || (m_actual == 1 && !TA)) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(
                            l1_buf_a_next, gm_a[offset_a_next], 1, RoundUp16(1), 1, k_round_next,
                            RoundUp16(k_round_next), k_round_next);
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_a_next, gm_a[offset_a_next], m_actual, m_round, m, k_actual_next, k_round, k);
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next);

                    // *** load matrix B to L1
                    WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
                    if (TB) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_b_next, gm_b[offset_b_next], n_actual, n_round, n, k_actual_next, k_round_next, k);
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_b_next, gm_b[offset_b_next], k_actual_next, k_round_next, k, n_actual, n_round, n);
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next + 2);
                }

                if (tidx.k == tdim.k - 1 && loop_idx + num_core < core_loop) {
                    uint64_t b_idx_next = (loop_idx + num_core) / tdim.n / tdim.m;
                    MatCoord tidx{0};
                    GetBlockIdx(loop_idx + num_core, tidx);
                    uint64_t shuffle_k_next = EN_SHUFFLE ? (core_idx % tdim.k) : 0;
                    uint64_t m_actual_next = (tidx.m == (tdim.m - 1)) ? (m - tidx.m * m0) : m0;
                    uint64_t n_actual_next = (tidx.n == (tdim.n - 1)) ? (n - tidx.n * n0) : n0;
                    uint64_t m_round_next = (m_actual_next + 15) & ~15;
                    uint64_t n_round_next = (n_actual_next + 15) & ~15;
                    uint64_t k_actual_next = (shuffle_k_next == (tdim.k - 1)) ? (k - shuffle_k_next * k0) : k0;
                    uint64_t k_round_next = (k_actual_next + 15) & ~15;
                    offset_a_next = b_idx_next * m * k + tidx.m * m0 * k + shuffle_k_next * k0;
                    if (TB) {
                        offset_b_next = b_idx_next * k * n + tidx.n * n0 * k + shuffle_k_next * k0;
                    } else {
                        offset_b_next = b_idx_next * k * n + shuffle_k_next * k0 * n + tidx.n * n0;
                    }

                    AscendC::LocalTensor<InDtype> l1_buf_a_next =
                        (1 - ping_flag) ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l1_buf_b_next =
                        (1 - ping_flag) ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                    event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                    WAIT_FLAG(MTE1, MTE2, event_id_next);
                    // *** load matrix A to L1
                    if ((m == 1) || (m_actual_next == 1 && !TA)) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(
                            l1_buf_a_next, gm_a[offset_a_next], 1, RoundUp16(1), 1, k_round_next,
                            RoundUp16(k_round_next), k_round_next);
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_a_next, gm_a[offset_a_next], m_actual_next, m_round_next, m, k_actual_next,
                            k_round_next, k);
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next);

                    // *** load matrix B to L1
                    WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
                    if (TB) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_b_next, gm_b[offset_b_next], n_actual_next, n_round_next, n, k_actual_next,
                            k_round_next, k);
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                            l1_buf_b_next, gm_b[offset_b_next], k_actual_next, k_round_next, k, n_actual_next,
                            n_round_next, n);
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next + 2);
                }

                MatCoord fidx{0};
                for (fidx.k = 0; fidx.k < fdim.k; ++fidx.k) {
                    uint32_t k0_round = (fidx.k < fdim.k - 1) ? k_part_len : k_round - fidx.k * k_part_len;
                    uint32_t k0_actual = (fidx.k < fdim.k - 1) ? k_part_len : k_actual - fidx.k * k_part_len;

                    auto mte1_mad_ping_flag = 1 - fidx.k % 2;
                    auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
                    AscendC::LocalTensor<InDtype> l0a_buf = l0a_base[(fidx.k % 2) * L0_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l0b_buf = l0b_base[(fidx.k % 2) * L0_PINGPONG_BUFFER_LEN];

                    // *** load matrix A from L1 to L0A
                    if (fidx.k == 0) {
                        WAIT_FLAG(MTE2, MTE1, event_id);
                    }
                    WAIT_FLAG(M, MTE1, mte1_mad_event_id);
                    if ((m == 1) || (m_actual == 1 && !TA)) {
                        l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf, l1_buf_a[fidx.k * k_part_len], 0,
                            (k0_round + CONST_256 - 1) / CONST_256, // repeat
                            0,
                            1, // srcStride
                            0,
                            0 // dstStride
                        );
                    } else {
                        l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::ZN, DataFormat::ZZ>(
                            l0a_buf, l1_buf_a[fidx.k * k_part_len * m_round], m_round,
                            k0_round, // repeat
                            1,
                            m_round / CONST_16, // srcStride
                            k0_round / CONST_16,
                            1 // dstStride
                        );
                    }
                    if (fidx.k == fdim.k - 1) {
                        SET_FLAG(MTE1, MTE2, event_id);
                    }

                    // *** load matrix B from L1 to L0B
                    if (fidx.k == 0) {
                        WAIT_FLAG(MTE2, MTE1, event_id + 2);
                    }
                    if (TB) {
                        l1_to_l0_b<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf, l1_buf_b[fidx.k * k_part_len * n_round], 0,
                            k0_round * n_round / CONST_256, // repeat
                            0,
                            1, // srcStride
                            0,
                            0 // dstStride
                        );
                    } else {
                        l1_to_l0_b<ArchType::ASCEND_V220, InDtype, false, DataFormat::ZN, DataFormat::NZ>(
                            l0b_buf, l1_buf_b[fidx.k * k_part_len * CONST_16], n_round,
                            k0_round, // repeat
                            k_round / CONST_16,
                            1, // srcStride
                            1,
                            n_round / CONST_16 // dstStride
                        );
                    }
                    if (fidx.k == fdim.k - 1) {
                        SET_FLAG(MTE1, MTE2, event_id + 2);
                    }

                    SET_FLAG(MTE1, M, mte1_mad_event_id);
                    WAIT_FLAG(MTE1, M, mte1_mad_event_id);

                    bool init_c = (tidx.k == 0 && fidx.k == 0);
                    if (init_c) {
                        WAIT_FLAG(FIX, M, EVENT_ID0);
                    }

                    if (m != 1 && m_actual == 1 && TA) {
                        mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>(l0c_buf, l0a_buf, l0b_buf,
                                                                                    16,        // m
                                                                                    n_actual,  // n
                                                                                    k0_actual, // k
                                                                                    init_c     // cmatrixInitVal
                        );
                    } else {
                        mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>(l0c_buf, l0a_buf, l0b_buf,
                                                                                    m_actual,  // m
                                                                                    n_actual,  // n
                                                                                    k0_actual, // k
                                                                                    init_c     // cmatrixInitVal
                        );
                    }

                    AscendC::PipeBarrier<PIPE_M>();
                    SET_FLAG(M, MTE1, mte1_mad_event_id);
                }

                ping_flag = 1 - ping_flag;
            }

            SET_FLAG(M, FIX, EVENT_ID0);
            WAIT_FLAG(M, FIX, EVENT_ID0);

            // copy from L0C to gm
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, OutDtype, float>(gm_c[offset_c], // dst
                                                                              l0c_buf,        // src
                                                                              m_actual,       // MSize
                                                                              n_actual,       // NSize
                                                                              m_round,        // srcStride
                                                                              n               // dstStride_dst_D
            );
            SET_FLAG(FIX, M, EVENT_ID0);
        }

        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        AscendC::PipeBarrier<PIPE_ALL>();
    }

private:
    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::GlobalTensor<InDtype> gm_a;
    AscendC::GlobalTensor<InDtype> gm_b;
    AscendC::GlobalTensor<OutDtype> gm_c;
    AscendC::LocalTensor<InDtype> l1_base_a = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
    AscendC::LocalTensor<InDtype> l1_base_b = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
    AscendC::LocalTensor<InDtype> l0a_base = buf.GetBuffer<BufferType::ASCEND_L0A, InDtype>(0);
    AscendC::LocalTensor<InDtype> l0b_base = buf.GetBuffer<BufferType::ASCEND_L0B, InDtype>(0);
    AscendC::LocalTensor<float> l0c_buf = buf.GetBuffer<BufferType::ASCEND_L0C, float>(0);

    uint32_t num_core{0};
    uint32_t batch_size{0};
    uint32_t m{0};
    uint32_t k{0};
    uint32_t n{0};
    uint32_t m0{0};
    uint32_t k0{0};
    uint32_t n0{0};
    MatCoord tdim{0};
    MatCoord fdim{0};
    uint32_t core_loop{0};
    uint32_t core_idx{0};
    uint32_t ping_flag{0};
};

extern "C" __global__ __aicore__ void pp_matmul_f16_nd(__gm__ uint8_t* __restrict__ gm_a,
                                                       __gm__ uint8_t* __restrict__ gm_b,
                                                       __gm__ uint8_t* __restrict__ gm_c,
                                                       __gm__ uint8_t* __restrict__ gm_tiling_data)
{
    __gm__ PpTilingDataNd *tiling_data = reinterpret_cast<__gm__ PpTilingDataNd *>(gm_tiling_data);
    PpMatmul<false, false> matmul_00;
    PpMatmul<false, true> matmul_01;

    SetPadding<uint64_t>((uint64_t)0);
    SetNdpara(1, 0, 0);
    SetAtomicnone();
    SetMasknorm();

    switch (tiling_data->tilingKey) {
        case 0b00:
            matmul_00.SetArgs(gm_a, gm_b, gm_c, gm_tiling_data);
            matmul_00.run();
            break;
        case 0b01:
            matmul_01.SetArgs(gm_a, gm_b, gm_c, gm_tiling_data);
            matmul_01.run();
            break;
        default: break;
    }
}
