/*
* Copyright (c) 2024 Huawei Technologies Co., Ltd.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#ifdef __CCE_KT_TEST__
#include "stub_def.h"
#include "stub_fun.h"
#else
#define __aicore__ [aicore]
#endif


#include "kernel_operator.h"
#include "mla_preprocess.h"
#include "mixops/include/common.h"
#include "mixops/include/iterator.h"
#include "mixops/include/mem.h"
#include "mixops/include/mma.h"
#include "mixops/include/utils.h"
#include "mixops/include/simd.h"

#include "mixops/norm/common/common_tiling_data.h"
#include "mixops/norm/common/op_kernel/common_quant.h"
#include "mixops/utils/common/kernel/kernel_utils.h"

//sync
constexpr int32_t RMSNORMQUANT1 = 1;
constexpr int32_t MM1 = 2;
constexpr int32_t ROPEPRESYNC = 3;
constexpr int32_t RMSNORMQUANT2 = 4;
constexpr int32_t MM2 = 5;
constexpr int32_t MM2QUANT = 6;
constexpr int32_t MMAIC = 7;
constexpr int32_t MMAIV = 8;
constexpr int32_t MM2OUT = 9;
constexpr int32_t MTE2WAIT = 10;

//ropeConcat
constexpr uint32_t ELE_NUM_FP16 = 16; // 一个block fp16元素个数
constexpr uint32_t ELE_NUM_FP32 = 8; // 一个block字节数 fp32元素个数
constexpr uint8_t DEFAULT_REPEAT_STRIDE = 8; // 默认stride, 8 * 32 = 256

//rmsNormQuant
constexpr int32_t NUM_PER_REP_FP32 = 64;  // ONE_REPEAT_BYTE_SIZE / sizeof(float);
constexpr float ZERO = 0;
constexpr uint32_t BUF_FACTOR = 3;        // 1(g) + 1(sqx) + 1(sum) = 3
constexpr uint32_t OFFSET_GAMMA = 0;      // the offset of gamma is 0
constexpr uint32_t OFFSET_SQX = 1;        // the offset of sqx is 1
constexpr uint32_t OFFSET_SUM = 2;        // the offset of sum is 2
constexpr uint32_t OFFSET_WORKSPACE = 3;  // the offset of workspace is 3
constexpr uint32_t REPEAT_TIME_256 = 256; // 128 default stride
constexpr uint32_t REPEAT_TIME_128 = 128; // 128 default stride
constexpr uint32_t REPEAT_TIME_64 = 64;   // 64 default stride


//pp matmul
namespace{
    constexpr uint32_t HIDDTEN_STATE = 7168;
    constexpr uint32_t FLOAT_BLOCK_SIZE = 64;
    constexpr uint32_t HALF_BLOCK_SIZE = 64;
    constexpr uint32_t HALF_VECTOR_SIZE = 64;
    constexpr uint32_t MM1_OUT_SIZE = 2112;
    constexpr uint32_t SPLIT_SIZE_ONE = 576;
    constexpr uint32_t SPLIT_SIZE_TWO = 1536;
    constexpr uint32_t SPLIT_RMSNRORM_SIZE_ONE = 512;
    constexpr uint32_t SPLIT_RMSNRORM_SIZE_TWO = 64;
    constexpr uint32_t ROPE_SPLIT_SIZE_ONE = 64;
    constexpr uint32_t ROPE_SPLIT_SIZE_TWO = 128;

    constexpr uint32_t MMSIZE1 = 128 * 192; // 24576
    constexpr uint32_t MMSIZE2 = 64 * 128; // 8192

    constexpr uint64_t L0_PINGPONG_BUFFER_LEN = 32768;  // 32 KB
    constexpr uint64_t L1_PINGPONG_BUFFER_LEN = 262144; // 256 KB
    constexpr uint64_t BLOCK_SIZE_16 = 16;
    constexpr uint64_t BLOCK_SIZE_32 = 32;
    constexpr uint64_t CUBE_MATRIX_SIZE_512 = 16 * 32; // 16 * 23
    constexpr uint64_t FB_BUFF_SIZE = 1024 * 7;
    constexpr uint64_t SCALE_L1_LEN = 4096;
    constexpr uint64_t BIAS_L1_LEN = 2048;
    constexpr uint64_t CONST_4 = 4;
    constexpr uint64_t CONST_64 = 64;
    constexpr uint64_t CONST_128 = 128;

    constexpr uint64_t BLOCK_SIZE_INT8 = 32;

    constexpr uint32_t L0AB_PINGPONG_BUFFER_LEN_INT8 = 32768; // 32 KB
    constexpr uint64_t L1_PINGPONG_BUFFER_LEN_FP16 = 131072; // 256 KB
    constexpr uint64_t L1_PINGPONG_BUFFER_LEN_INT8 = 262144; // 256 KB
    constexpr uint64_t CONST_8 = 8;
    constexpr uint64_t CONST_32 = 32;
    constexpr uint64_t ND2NZ_STRIDE_LIMIT = 65536;

    constexpr uint32_t AIC_FINISH_FLAG_ID = 1;
    constexpr uint32_t AIV_FINISH_FLAG_ID = 2;
    constexpr uint32_t MAX_HW_SYNC_COUNTER = 15;
    constexpr uint32_t SYNC_MODE = 2;
}

template <typename QkDtype, typename CosDtype>
class RopeFp16 {
public:
    __aicore__ inline RopeFp16()
        : blockIdx_(AscendC::GetBlockIdx())
    {
    }

    __aicore__ inline void RopeInit(AscendC::GlobalTensor<QkDtype> &qGm,
                                      AscendC::GlobalTensor<CosDtype> &cosGm,
                                      AscendC::GlobalTensor<CosDtype> &sinGm,
                                      AscendC::GlobalTensor<QkDtype> &outRopeConcatGm,
                                      MLATilingData &ropeConcatParams)
    {
        this->qGm_ = qGm;
        this->cosGm_ = cosGm;
        this->sinGm_ = sinGm;
        this->outRopeConcatGm_ = outRopeConcatGm;

        headDim = ropeConcatParams.headDim;
        headNumQ = ropeConcatParams.headNumQ;
        rotaryCoeff = ropeConcatParams.rotaryCoeff;
        ntokens = ropeConcatParams.ntokens;
        realCore = ropeConcatParams.realCore;
        nlCoreRun = ropeConcatParams.nlCoreRun;
        lCoreRun = ropeConcatParams.lCoreRun;
        maxNPerLoopForUb = ropeConcatParams.maxNPerLoopForUb;
        preCoreLoopTime = ropeConcatParams.preCoreLoopTime;
        preCoreLoopNLast = ropeConcatParams.preCoreLoopNLast;
        lastCoreLoopTime = ropeConcatParams.lastCoreLoopTime;
        lastCoreLoopNLast = ropeConcatParams.lastCoreLoopNLast;
        concatSize = ropeConcatParams.concatSize;
        blockIdx_ = (blockIdx_ / 2) * 2 + static_cast<uint64_t>(GetSubBlockidx());
        loopTime = (blockIdx_ == realCore - 1) ? lastCoreLoopTime : preCoreLoopTime;
        lastLoopN = (blockIdx_ == realCore - 1) ? lastCoreLoopNLast : preCoreLoopNLast;
        this->repeatSize_ = 64;  // 128 = 256B / sizeof(fp32)
        this->rotateStride_ = this->headDim / this->rotaryCoeff;
        headBlockLen = static_cast<uint16_t>(this->headDim / ELE_NUM_FP16);
        headBlockLenFP32 = static_cast<uint16_t>(this->headDim / ELE_NUM_FP32);
        rotaryLen = static_cast<uint16_t>(this->rotateStride_ / ELE_NUM_FP32);
        concatBlockLen = static_cast<uint16_t>(this->concatSize / ELE_NUM_FP16);
        outLineOffset = this->headDim + this->concatSize;
        uint32_t dataNum = this->headDim * this->maxNPerLoopForUb;
        dataSizeFp16 = dataNum * sizeof(QkDtype);
        dataSizeFp32 = dataNum * sizeof(float);
        uint32_t concatDataSize =
            this->concatSize * sizeof(QkDtype) * this->maxNPerLoopForUb;
    }

    __aicore__ inline void Process()
    {
        if (blockIdx_ >= realCore) {
            return;
        }
        uint64_t startCoreLineIndex = this->blockIdx_ * this->nlCoreRun; // 当前核处理head起始位置
        // 生成 [maxNPerLoopForUb,head_dim] 的 neg
        AscendC::LocalTensor<float> negLocal = buf.GetBuffer<BufferType::ASCEND_UB, float>(dataSizeFp32 * 4 + dataSizeFp16 * 3);
        ExpandNeg(negLocal, this->maxNPerLoopForUb);
        // 遍历处理每轮数据
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        for (uint32_t zz = 0; zz < this->loopTime; ++zz) {
            uint16_t loopN = (zz == this->loopTime - 1) ? this->lastLoopN : this->maxNPerLoopForUb;
            uint64_t startHead = startCoreLineIndex + zz * this->maxNPerLoopForUb;
            uint64_t endHead = startHead + loopN;

            // 搬入数据Q
            AscendC::LocalTensor<QkDtype> inputQ = buf.GetBuffer<BufferType::ASCEND_UB, QkDtype>(0);
            AscendC::LocalTensor<float> inputQCastFP32 = buf.GetBuffer<BufferType::ASCEND_UB, float>(dataSizeFp16);
            AscendC::LocalTensor<float> reverseQ = buf.GetBuffer<BufferType::ASCEND_UB, float>(dataSizeFp32 + dataSizeFp16);
            uint64_t qOffset = startHead * 192 + 128;
            CopyQGenReverseQ(inputQ, inputQCastFP32, reverseQ, qOffset, loopN);

            // 搬入数据cos/sin
            AscendC::LocalTensor<QkDtype> inputCos = buf.GetBuffer<BufferType::ASCEND_UB, QkDtype>(dataSizeFp32 * 2 + dataSizeFp16);
            AscendC::LocalTensor<QkDtype> inputSin = buf.GetBuffer<BufferType::ASCEND_UB, QkDtype>(dataSizeFp32 * 2 + dataSizeFp16 * 2);
            uint64_t startSinCosHeadIndex = startHead;
            uint64_t headRemain = startHead % this->headNumQ;
            uint64_t localStartAddr = 0;
            if (headRemain != 0) {  // 需要前处理
                uint64_t preProcessHeadNum = this->headNumQ - headRemain;
                uint64_t needToProcesHead = preProcessHeadNum > loopN ? loopN : preProcessHeadNum;
                CopyCosSin(inputCos,
                    inputSin,
                    localStartAddr,
                    (startSinCosHeadIndex / this->headNumQ) * this->headDim,
                    needToProcesHead);
                startSinCosHeadIndex += needToProcesHead;  
                localStartAddr += needToProcesHead * this->headDim;
            }
            // 循环迭代处理剩余数据
            if (startSinCosHeadIndex < endHead) {
                uint64_t startSinCosIndex = startSinCosHeadIndex / this->headNumQ;
                uint64_t endSinCosIndex = (endHead + this->headNumQ - 1) / this->headNumQ;
                for (uint32_t index = startSinCosIndex; index < endSinCosIndex; ++index) {
                    // 尾数处理
                    uint32_t repeatNum = index == endSinCosIndex - 1 ? endHead - index * this->headNumQ
                                                                     : this->headNumQ;
                    CopyCosSin(inputCos, inputSin, localStartAddr, index * this->headDim, repeatNum);
                    localStartAddr += this->headDim * this->headNumQ;
                }
            }
            AscendC::LocalTensor<float> inputCosCastFP32 = buf.GetBuffer<BufferType::ASCEND_UB, float>(dataSizeFp32 * 2 + dataSizeFp16 * 3);
            AscendC::LocalTensor<float> inputSinCastFP32 = buf.GetBuffer<BufferType::ASCEND_UB, float>(dataSizeFp32 * 3 + dataSizeFp16 * 3);
            AscendC::Cast(inputCosCastFP32, inputCos, AscendC::RoundMode::CAST_NONE, loopN * this->headDim);
            AscendC::Cast(inputSinCastFP32, inputSin, AscendC::RoundMode::CAST_NONE, loopN * this->headDim);
            AscendC::PipeBarrier<PIPE_V>();

            // 计算rope结果
            uint32_t repeatTime = this->headDim * loopN;
            AscendC::Mul(inputQCastFP32, inputCosCastFP32, inputQCastFP32, repeatTime);
            AscendC::Mul(reverseQ, negLocal, reverseQ, repeatTime);
            AscendC::PipeBarrier<PIPE_V>();

            AscendC::Mul(reverseQ, inputSinCastFP32, reverseQ, repeatTime);
            AscendC::PipeBarrier<PIPE_V>();

            AscendC::Add(inputQCastFP32, reverseQ, inputQCastFP32, repeatTime);
            AscendC::PipeBarrier<PIPE_V>();
            
            // // 搬出rope结果
            // // cast fp16/bf16
            AscendC::Cast(inputQ, inputQCastFP32, AscendC::RoundMode::CAST_RINT, loopN * this->headDim);
            AscendC::PipeBarrier<PIPE_V>();
            uint64_t outQOffset = startHead * outLineOffset + this->concatSize;
            SET_FLAG(V, MTE3, EVENT_ID1);
            WAIT_FLAG(V, MTE3, EVENT_ID1);
            AscendC::DataCopy(this->outRopeConcatGm_[outQOffset], inputQ, {loopN , headBlockLen, 0, concatBlockLen});
            SET_FLAG(MTE3, MTE2, EVENT_ID1);
        }
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
    }
    // 构建tensor -1 -1 -1 1 1 1
    template <typename BUF_TYPE>
    __aicore__ inline void ExpandNeg(const AscendC::LocalTensor<BUF_TYPE> &tempBuf, uint32_t headNumTemp)
    {
        for (uint32_t i = 0; i < this->rotateStride_; ++i) {
            tempBuf.SetValue(i, (BUF_TYPE)-1);
            tempBuf.SetValue(i + this->rotateStride_, (BUF_TYPE)1);
        }
        AscendC::SetFlag<HardEvent::S_V>(EVENT_ID1);
        AscendC::WaitFlag<HardEvent::S_V>(EVENT_ID1);
        AscendC::Copy(tempBuf[this->headDim], tempBuf, this->headDim, headNumTemp-1, {1, 1, headBlockLenFP32, 0});
        AscendC::PipeBarrier<PIPE_V>();
    }

    template <typename BUF_TYPE>
    __aicore__ inline void CopyQGenReverseQ(const AscendC::LocalTensor<BUF_TYPE> &tempBufQ,
        const AscendC::LocalTensor<float> &tempBufQCast, const AscendC::LocalTensor<float> &tempBufRverseQ,
        uint64_t qOffset, uint16_t loopN)
    {
        // 搬入数据Q
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        AscendC::DataCopy(tempBufQ, this->qGm_[qOffset], {loopN, headBlockLen, 128 / 16, 0});
        SET_FLAG(MTE2, V, EVENT_ID1);
        WAIT_FLAG(MTE2, V, EVENT_ID1);
        // cast fp32
        AscendC::Cast(tempBufQCast, tempBufQ, AscendC::RoundMode::CAST_NONE, loopN * this->headDim);
        AscendC::PipeBarrier<PIPE_V>();
        // 搬入数据reverseQ
        AscendC::DataCopy(tempBufRverseQ, tempBufQCast[this->rotateStride_], {loopN, rotaryLen, rotaryLen, rotaryLen});
        AscendC::DataCopy(tempBufRverseQ[this->rotateStride_], tempBufQCast, {loopN, rotaryLen, rotaryLen, rotaryLen});
        AscendC::PipeBarrier<PIPE_V>();
    }

    template <typename BUF_TYPE>
    __aicore__ inline void CopyCosSin(const AscendC::LocalTensor<BUF_TYPE> &tempBufCos,
        const AscendC::LocalTensor<BUF_TYPE> &tempBufSin, uint64_t localStartAddr, uint64_t gmStartAddr,
        uint64_t repeatNum)
    {
        AscendC::DataCopy(tempBufCos[localStartAddr], this->cosGm_[gmStartAddr], {1, headBlockLen, 0, 0});
        AscendC::DataCopy(tempBufSin[localStartAddr], this->sinGm_[gmStartAddr], {1, headBlockLen, 0, 0});
        SET_FLAG(MTE2, V, EVENT_ID1);
        WAIT_FLAG(MTE2, V, EVENT_ID1);
        AscendC::Copy(tempBufCos[localStartAddr + this->headDim],
            tempBufCos[localStartAddr],
            this->headDim,
            repeatNum - 1,
            {1, 1, headBlockLen, 0});
        AscendC::Copy(tempBufSin[localStartAddr + this->headDim],
            tempBufSin[localStartAddr],
            this->headDim,
            repeatNum - 1,
            {1, 1, headBlockLen, 0});
        AscendC::PipeBarrier<PIPE_V>();
    }

private:
    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::GlobalTensor<QkDtype> qGm_;
    AscendC::GlobalTensor<CosDtype> cosGm_;
    AscendC::GlobalTensor<CosDtype> sinGm_;
    AscendC::GlobalTensor<QkDtype> outRopeConcatGm_;

    uint32_t repeatSize_{0};         // 一拍做几个元素
    uint32_t rotateStride_{0};       // this->headDim / 旋转系数
    uint32_t headDim;
    uint32_t headNumQ;
    uint32_t rotaryCoeff;
    uint32_t ntokens;
    uint32_t realCore;
    uint32_t nlCoreRun;
    uint32_t lCoreRun;
    uint32_t maxNPerLoopForUb;
    uint32_t preCoreLoopTime;
    uint32_t preCoreLoopNLast;
    uint32_t lastCoreLoopTime;
    uint32_t lastCoreLoopNLast;
    uint32_t concatSize;
    uint32_t blockIdx_;
    uint32_t loopTime{0};   // 当前核批处理数据轮数
    uint32_t lastLoopN{0};  // 当前核尾处理行数

    uint32_t dataSizeFp32;
    uint32_t dataSizeFp16;
    uint16_t headBlockLen{0};
    uint16_t headBlockLenFP32{0};
    uint16_t rotaryLen{0};
    uint16_t concatBlockLen{0};
    uint64_t outLineOffset{0};

};


#if __CCE_AICORE__ != 300
__aicore__ inline void ReduceSumFP32(const AscendC::LocalTensor<float>& dst_local, const AscendC::LocalTensor<float>& src_local,
                                     const AscendC::LocalTensor<float>& work_local, int32_t count)
    {
    // count need smaller than 255 repeat
    if (g_coreType == AscendC::AIV) {
        uint64_t mask = NUM_PER_REP_FP32;
        int32_t repeatTimes = count / NUM_PER_REP_FP32;
        int32_t tailCount = count % NUM_PER_REP_FP32;
        int32_t bodyCount = repeatTimes * NUM_PER_REP_FP32;
        AscendC::BinaryRepeatParams repeatParams;
        repeatParams.src0RepStride = AscendC::ONE_REPEAT_BYTE_SIZE / AscendC::ONE_BLK_SIZE;
        repeatParams.src0BlkStride = 1;
        repeatParams.src1RepStride = 0;
        repeatParams.src1BlkStride = 1;
        repeatParams.dstRepStride = 0;
        repeatParams.dstBlkStride = 1;
        Duplicate(work_local, ZERO, NUM_PER_REP_FP32);
        pipe_barrier(PIPE_V);
        if (likely(repeatTimes > 0)) {
            Add(work_local, src_local, work_local, mask, repeatTimes, repeatParams);
            pipe_barrier(PIPE_V);
        }
        if (unlikely(tailCount != 0)) {
            Add(work_local, src_local[bodyCount], work_local, tailCount, 1, repeatParams);
            pipe_barrier(PIPE_V);
        }
        AscendC::AscendCUtils::SetMask<float>(NUM_PER_REP_FP32);
        vcadd((__ubuf__ float*)dst_local.GetPhyAddr(), (__ubuf__ float*)work_local.GetPhyAddr(), 1, 0, 1, 0, false);
        pipe_barrier(PIPE_V);
    }
}
#endif

__aicore__ inline void ReduceSumCustom(const AscendC::LocalTensor<float>& dst_local, const AscendC::LocalTensor<float>& src_local,
                                       const AscendC::LocalTensor<float>& work_local, int32_t count)
    {
    #if __CCE_AICORE__ == 220
        ReduceSumFP32(dst_local, src_local, work_local, count);
    #elif __CCE_AICORE__ == 100
        float sum = 0;
        int32_t elementNumPerRep = AscendC::ONE_REPEAT_BYTE_SIZE / sizeof(float);
        AscendC::LocalTensor<float> src = src_local;
        while (count > elementNumPerRep) {
            int32_t repeatTimes = count / elementNumPerRep;
            int32_t tailCount = count % elementNumPerRep;
            int32_t bodyCount = repeatTimes * elementNumPerRep;
            if (repeatTimes > 0) {
                AscendC::AscendCUtils::SetMask<float>(elementNumPerRep);
                vcadd((__ubuf__ float*)work_local.GetPhyAddr(),
                 (__ubuf__ float*)src.GetPhyAddr(), repeatTimes, 1, 1, 8);
                AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
                AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
            }

            if (tailCount != 0) {
                AscendC::AscendCUtils::SetMask<float>(tailCount);
                vcadd((__ubuf__ float*)work_local[bodyCount].GetPhyAddr(),
                 (__ubuf__ float*)src[bodyCount].GetPhyAddr(), 1, 1, 1, 8);
                AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
                AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
                sum += work_local.GetValue(bodyCount);
            }

            count = repeatTimes;
            src = work_local;
        }

        if (count > 1) {
            AscendC::AscendCUtils::SetMask<float>(count);
            vcadd((__ubuf__ float*)work_local.GetPhyAddr(), (__ubuf__ float*)work_local.GetPhyAddr(), 1, 1, 1, 8);
            AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
            AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
        }

        sum += work_local.GetValue(0);
        AscendC::SetFlag<HardEvent::S_V>(EVENT_ID0);
        AscendC::WaitFlag<HardEvent::S_V>(EVENT_ID0);
        Duplicate(dst_local, sum, count);
    #elif __CCE_AICORE__ == 300
        ReduceSum(dst_local, src_local, work_local, count);
    #else
        ReduceSum(dst_local, src_local, dst_local, count);
    #endif
}


template <typename T, bool WITH_BETA, bool FastComputeMode = false>
class RmsNormQuant {
public:
    __aicore__ inline RmsNormQuant() {}
    
    __aicore__ inline void Init(AscendC::GlobalTensor<T> gammaGmTensor,
                                AscendC::GlobalTensor<T> betaGmTensor,
                                AscendC::GlobalTensor<T> quantScaleGmTensor,
                                AscendC::GlobalTensor<int8_t> quantOffsetGmTensor,
                                AscendC::GlobalTensor<T> inputGmTensor,
                                AscendC::GlobalTensor<int8_t> outputGmTensor,
                                uint32_t stride,
                                uint32_t num_col,
                                float avg_factor,
                                uint64_t gm_offset,
                                uint64_t gm_out_offset,
                                uint32_t row_work_,
                                const MLATilingData &mlaParams_)
    {
        this->gammaGmTensor = gammaGmTensor;
        this->betaGmTensor = betaGmTensor;
        this->quantScaleGmTensor = quantScaleGmTensor;
        this->quantOffsetGmTensor = quantOffsetGmTensor;
        this->inputGmTensor = inputGmTensor;
        this->outputGmTensor = outputGmTensor;

        num_col_ = num_col;
        avg_factor_ = avg_factor;
        epsilon_ = 1e-6;
        quantMin_ = -128;
        uint32_t num_row = mlaParams_.n;
        this->row_work = row_work;
        this->row_work_ = row_work_;
        gm_offset_ = gm_offset;
        gm_out_offset_ = gm_out_offset;
        num_col_align_int8 = (num_col_ + REPEAT_TIME_256 - 1) / REPEAT_TIME_256 * REPEAT_TIME_256;
        num_col_align_f16 = (num_col_ + REPEAT_TIME_128 - 1) / REPEAT_TIME_128 * REPEAT_TIME_128;
        num_col_align_f32 = (num_col_ + REPEAT_TIME_64 - 1) / REPEAT_TIME_64 * REPEAT_TIME_64;
        input_stride_ = stride;

        num_col_align_withStride_int8 = (num_col_ - input_stride_ + REPEAT_TIME_256 - 1) / REPEAT_TIME_256 * REPEAT_TIME_256;
        num_col_align_withStride_fp16 = (num_col_ - input_stride_ + REPEAT_TIME_128 - 1) / REPEAT_TIME_128 * REPEAT_TIME_128;
        num_col_align_withStride_fp32 = (num_col_ - input_stride_ + REPEAT_TIME_64 - 1) / REPEAT_TIME_64 * REPEAT_TIME_64;
    }

    __aicore__ inline void Launch(const AscendC::LocalTensor<int8_t>& dstTensor, const AscendC::LocalTensor<T>& srcTensor, const AscendC::LocalTensor<T>& gammaTensor, const AscendC::LocalTensor<T>& betaTensor,
                                   const AscendC::LocalTensor<T>& quantScaleTensor, const AscendC::LocalTensor<int8_t>& quantOffsetTensor, const AscendC::LocalTensor<float>& res1Tensor,
                                    const AscendC::LocalTensor<float>& res3Tensor)
    {

        this->dstTensor = dstTensor;
        this->srcTensor = srcTensor;
        this->gammaTensor = gammaTensor;
        this->betaTensor = betaTensor;
        this->fp32_xy = res1Tensor;
        this->buf = res3Tensor;

        AscendC::LocalTensor<float> g = buf[OFFSET_GAMMA * num_col_align_withStride_fp32];       // 0
        AscendC::LocalTensor<float> sqx = buf[OFFSET_SQX * num_col_align_withStride_fp32];       // 1
        AscendC::LocalTensor<float> work = buf[OFFSET_SUM * num_col_align_withStride_fp32];      // 2
        AscendC::LocalTensor<float> sum = buf[OFFSET_WORKSPACE * num_col_align_withStride_fp32]; // 4

        AscendC::DataCopy(gammaTensor, gammaGmTensor, AscendC::DataCopyParams(1, (num_col_ - input_stride_) / BLOCK_SIZE_16, 0, 0));
        AscendC::DataCopy(betaTensor, betaGmTensor, AscendC::DataCopyParams(1, (num_col_ - input_stride_) / BLOCK_SIZE_16, 0, 0));
        SET_FLAG(MTE2, V, EVENT_ID1);
        AscendC::DataCopy(
            quantScaleTensor, quantScaleGmTensor, AscendC::DataCopyParams(1, 1, 0, 0));
        AscendC::DataCopy(
            quantOffsetTensor, quantOffsetGmTensor, AscendC::DataCopyParams(1, 1, 0, 0));
        if (std::is_same<T, __bf16>::value) {
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);
            Cast(g, quantScaleTensor, AscendC::RoundMode::CAST_NONE, 1);
            AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
            AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
            input_scale_ = 1 / (float)(g.GetValue(0));
            input_offset_ = (float)(quantOffsetTensor.GetValue(0));
        } else {
            SET_FLAG(MTE2, S, EVENT_ID0);
            WAIT_FLAG(MTE2, S, EVENT_ID0);
            input_scale_ = 1 / (float)(quantScaleTensor.GetValue(0));
            input_offset_ = (float)(quantOffsetTensor.GetValue(0));
        }
        AscendC::SetFlag<HardEvent::S_V>(EVENT_ID0);
        AscendC::WaitFlag<HardEvent::S_V>(EVENT_ID0);
        WAIT_FLAG(MTE2, V, EVENT_ID1);
        Cast(buf[OFFSET_GAMMA * num_col_align_withStride_fp32], gammaTensor, AscendC::RoundMode::CAST_NONE, REPEAT_TIME_64,
            num_col_align_withStride_fp32 / REPEAT_TIME_64,
            {1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE / OFFSET_SUM});
        AscendC::PipeBarrier<PIPE_V>();
        uint64_t pid = 0;
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        while (pid < row_work_) {
            uint64_t offset = pid * num_col_;
            uint64_t outOffset = pid * (num_col_ - input_stride_);
            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
            AscendC::DataCopy(srcTensor, inputGmTensor[gm_offset_ + offset], AscendC::DataCopyParams(1, num_col_ / BLOCK_SIZE_16, 0, 0));
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);
            if (pid == row_work_ - 1) {
                AscendC::CrossCoreSetFlag<0x2, PIPE_MTE2>(MTE2WAIT);
            }
            Cast(fp32_xy, srcTensor[input_stride_], AscendC::RoundMode::CAST_NONE, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
            {1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE / OFFSET_SUM});
            AscendC::PipeBarrier<PIPE_V>();
            Mul(sqx, fp32_xy, fp32_xy, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                {1, 1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE});
            AscendC::PipeBarrier<PIPE_V>();
            Muls(sqx, sqx, avg_factor_, num_col_ - input_stride_);
            AscendC::PipeBarrier<PIPE_V>();
            ReduceSumCustom(sum, sqx, work, num_col_ - input_stride_);
            AscendC::PipeBarrier<PIPE_V>();
            Adds(sum, sum, epsilon_, 1);
            AscendC::PipeBarrier<PIPE_V>();
            Sqrt(sum, sum, 1);
            SET_FLAG(V, S, EVENT_ID0);
            WAIT_FLAG(V, S, EVENT_ID0);
            float factor = 1 / sum.GetValue(0);
            SET_FLAG(S, V, EVENT_ID0);
            WAIT_FLAG(S, V, EVENT_ID0);
            Muls(fp32_xy, fp32_xy, factor, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                {1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE});
            AscendC::PipeBarrier<PIPE_V>();
            Mul(fp32_xy, fp32_xy, g, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                {1, 1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE});
            AscendC::PipeBarrier<PIPE_V>();
            if constexpr (WITH_BETA) {
                AscendC::LocalTensor<T> b = this->betaTensor;
                Cast(work, b, AscendC::RoundMode::CAST_NONE, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                    {1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE / OFFSET_SUM});
                AscendC::PipeBarrier<PIPE_V>();
                Add(fp32_xy, fp32_xy, work, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                    {1, 1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE,
                    AscendC::DEFAULT_REPEAT_STRIDE});
                AscendC::PipeBarrier<PIPE_V>();
            }
            Muls(fp32_xy, fp32_xy, input_scale_, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                {1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE});
            AscendC::PipeBarrier<PIPE_V>();
            Adds(fp32_xy, fp32_xy, input_offset_, REPEAT_TIME_64, num_col_align_withStride_fp32 / REPEAT_TIME_64,
                {1, 1, AscendC::DEFAULT_REPEAT_STRIDE, AscendC::DEFAULT_REPEAT_STRIDE});
            AscendC::PipeBarrier<PIPE_V>();

            AscendC::LocalTensor<half> tmpfp16 = buf.ReinterpretCast<half>()[OFFSET_SUM * num_col_align_withStride_fp32 * 2];
            CastFrom32To16(tmpfp16, fp32_xy, num_col_align_withStride_fp32);
            AscendC::PipeBarrier<PIPE_V>();
            CastFromF16ToI8(dstTensor, tmpfp16, quantMin_, num_col_align_withStride_fp16);
            AscendC::PipeBarrier<PIPE_V>();
            SET_FLAG(V, MTE3, EVENT_ID0);
            WAIT_FLAG(V, MTE3, EVENT_ID0);
            AscendC::DataCopy(outputGmTensor[gm_out_offset_ + outOffset],
                              dstTensor,
                              AscendC::DataCopyParams(1, (num_col_ - input_stride_) / 32, 0, 0));
            SET_FLAG(MTE3, MTE2, EVENT_ID0);
            ++pid;
        }
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
    }

private:
    
private:

    AscendC::LocalTensor<int8_t> dstTensor;
    AscendC::LocalTensor<T> srcTensor;
    AscendC::LocalTensor<T> gammaTensor;
    AscendC::LocalTensor<T> betaTensor;
    AscendC::LocalTensor<float> fp32_xy;
    AscendC::LocalTensor<float> buf;

    AscendC::GlobalTensor<T> gammaGmTensor;
    AscendC::GlobalTensor<T> betaGmTensor;
    AscendC::GlobalTensor<T> quantScaleGmTensor;
    AscendC::GlobalTensor<int8_t> quantOffsetGmTensor;
    AscendC::GlobalTensor<T> inputGmTensor;
    AscendC::GlobalTensor<int8_t> outputGmTensor;

    uint32_t num_col_{0};    // 输入的列数
    uint32_t row_work_{0};   // 需要计算多少行
    uint32_t row_work{0};       // 需要计算多少行
    uint32_t row_step_{0};   // 除最后一次，每次搬入多少行
    uint32_t row_tail_{0};   // 最后一次搬入多少行数据
    uint64_t gm_offset_{0};  // GM数据起始位置偏移量
    uint64_t gm_out_offset_{0}; // GM数据起始位置偏移量
    float avg_factor_{1.0};  // num_col_的倒数
    float input_scale_{1.0}; // 非对称量化系数
    float input_offset_{0};  // 非对称量化偏移适配高精度
    int32_t input_stride_{0};
    float epsilon_{1e-12f};  // norm平滑参数
    uint32_t num_col_align_int8{0};
    uint32_t num_col_align_f16{0};
    uint32_t num_col_align_f32{0};
    uint32_t num_col_align_f32_long{0};
    uint32_t num_col_align_withStride_int8{0};
    uint32_t num_col_align_withStride_fp16{0};
    uint32_t num_col_align_withStride_fp32{0};
    uint32_t num_col_temp;
    half quantMin_{-128};
    uint32_t num_slice_{0};
    uint32_t tail_size_{0};
    uint32_t tail_copy_{0};
};

struct PpMatmulTilingData {
    uint32_t bSize{0};
    uint32_t mSize{0};
    uint32_t kSize{0};
    uint32_t nSize{0};
    uint32_t m0{0};
    uint32_t k0{0};
    uint32_t n0{0};
    uint32_t mLoop{1};
    uint32_t kLoop{1};
    uint32_t nLoop{1};
    uint32_t coreLoop{1};
    uint32_t swizzleCount{1};
    uint32_t tilingKey{0};
    uint32_t blockDim{1};
    uint32_t swizzleDirect{0};
    uint32_t enSplitK{0};
    uint32_t enShuffleK{0};
};

__aicore__ __force_inline__ uint64_t Min(const uint64_t a, const uint64_t b) { return a < b ? a : b; }

__aicore__ __force_inline__ uint64_t Max(const uint64_t a, const uint64_t b) { return a > b ? a : b; }

template <uint64_t Base>
__aicore__ __force_inline__ uint64_t RoundUp(const uint64_t val)
{
    return (val + Base - 1) / Base * Base;
}

template <uint64_t Divisor>
__aicore__ __force_inline__ uint64_t CeilDiv(const uint64_t dividend)
{
    return (dividend + Divisor - 1) / Divisor;
}

#ifdef __DAV_C220_CUBE__

struct MatCoord {
    uint64_t m{0};
    uint64_t k{0};
    uint64_t n{0};
};

template <bool transB, uint32_t swizzleDirect> class PpMatmulEinSum {
    using InDtype = __bf16;
    using OutDtype = __bf16;
    using AccumDtype = float;

    static constexpr uint32_t L0_PINGPONG_BUFFER_LEN = 16384;
    static constexpr uint32_t L1_PINGPONG_BUFFER_LEN = 131072;
    static constexpr uint32_t CONST_16 = 16;
    static constexpr uint32_t CONST_256 = 256;

public:
    __aicore__ explicit PpMatmulEinSum(){};

    __aicore__ __force_inline__ void Init(AscendC::GlobalTensor<InDtype> gmA,
                                          AscendC::GlobalTensor<InDtype> gmB,
                                          AscendC::GlobalTensor<OutDtype> gmC,
                                          MLATilingData &mlaParams);

    __aicore__ __force_inline__ void Process();

private:
    __aicore__ __force_inline__ void GetBaseBlockIdx(uint64_t index, MatCoord &tidx);

private:
    AscendC::GlobalTensor<InDtype> gm_a;
    AscendC::GlobalTensor<InDtype> gm_b;
    AscendC::GlobalTensor<OutDtype> gm_c;
    AscendC::LocalTensor<InDtype> l1_base_a;
    AscendC::LocalTensor<InDtype> l1_base_b;
    AscendC::LocalTensor<InDtype> l0a_base;
    AscendC::LocalTensor<InDtype> l0b_base;
    AscendC::LocalTensor<float> l0c_buf;

    uint32_t num_core{0};
    uint32_t batch_size{0};
    uint32_t m{0};
    uint32_t k{0};
    uint32_t n{0};
    uint32_t m0{0};
    uint32_t k0{0};
    uint32_t n0{0};
    MatCoord tdim{0};
    MatCoord fdim{0};
    uint32_t core_loop{0};
    uint32_t swizzle_cnt{1};
    uint32_t core_idx{0};
    uint32_t en_shuffle_k = 0;
    uint32_t ping_flag{0};
    uint64_t split_gap{0};
};

template <bool transB, uint32_t swizzleDirect>
__aicore__ __force_inline__ void PpMatmulEinSum<transB, swizzleDirect>::Init(AscendC::GlobalTensor<InDtype> gmA,
                                                                             AscendC::GlobalTensor<InDtype> gmB,
                                                                             AscendC::GlobalTensor<OutDtype> gmC,
                                                                             MLATilingData &mlaParams)
{
#ifdef __DAV_C220_CUBE__
    batch_size = mlaParams.mm3batchSize;
    m = mlaParams.mm3m;
    k = mlaParams.mm3k;
    n = mlaParams.mm3n;
    m0 = mlaParams.mm3m0;
    k0 = mlaParams.mm3k0;
    n0 = mlaParams.mm3n0;
    tdim.m = mlaParams.mm3mLoop;
    tdim.k = mlaParams.mm3kLoop;
    tdim.n = mlaParams.mm3nLoop;
    core_loop = mlaParams.mm3coreLoop;
    swizzle_cnt = mlaParams.mm3swizzleCnt;
    split_gap = 64;
    num_core = mlaParams.mm3blockDim;
    core_idx = AscendC::GetBlockIdx();
    ping_flag = 1;

    gm_a = gmA;
    gm_b = gmB;
    gm_c = gmC;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    l1_base_a = buf.template GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
    l1_base_b = buf.template GetBuffer<BufferType::ASCEND_CB, InDtype>(RoundUp<CONST_256>(m0 * k0 * sizeof(InDtype)));
    l0a_base = buf.template GetBuffer<BufferType::ASCEND_L0A, InDtype>(0);
    l0b_base = buf.template GetBuffer<BufferType::ASCEND_L0B, InDtype>(0);
#endif
    return;
}

template <bool transB, uint32_t swizzleDirect>
__aicore__ __force_inline__ void PpMatmulEinSum<transB, swizzleDirect>::GetBaseBlockIdx(uint64_t index, MatCoord &tidx)
{
    uint64_t in_batch_idx = index % (tdim.m * tdim.n);
    if constexpr (swizzleDirect == 0) { // Zn
        uint64_t tile_block_loop = (tdim.m + swizzle_cnt - 1) / swizzle_cnt;
        uint64_t tile_block_idx = in_batch_idx / (swizzle_cnt * tdim.n);
        uint64_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * tdim.n);

        uint64_t n_row = swizzle_cnt;
        if (tile_block_idx == tile_block_loop - 1) {
            n_row = tdim.m - swizzle_cnt * tile_block_idx;
        }
        tidx.m = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
        tidx.n = in_tile_block_idx / n_row;
        if (tile_block_idx % 2 != 0) {
            tidx.n = tdim.n - tidx.n - 1;
        }
    } else if constexpr (swizzleDirect == 1) { // Nz
        uint64_t tile_block_loop = (tdim.n + swizzle_cnt - 1) / swizzle_cnt;
        uint64_t tile_block_idx = in_batch_idx / (swizzle_cnt * tdim.m);
        uint64_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * tdim.m);

        uint64_t n_col = swizzle_cnt;
        if (tile_block_idx == tile_block_loop - 1) {
            n_col = tdim.n - swizzle_cnt * tile_block_idx;
        }
        tidx.m = in_tile_block_idx / n_col;
        tidx.n = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
        if (tile_block_idx % 2 != 0) {
            tidx.m = tdim.m - tidx.m - 1;
        }
    }
    return;
}

template <bool transB, uint32_t swizzleDirect>
__aicore__ __force_inline__ void PpMatmulEinSum<transB, swizzleDirect>::Process()
{
#ifdef __DAV_C220_CUBE__
    if (block_idx >= num_core) {
        return;
    }
    using LocalTensor = AscendC::LocalTensor<InDtype>;
    using CopyGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>;
    using CopyGmToCbufNd2Nz = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>;
    using CopyGmToCbufNz2Nz = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>;
    using LoadCbufToCa = l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::ZN, DataFormat::ZZ>;
    using LoadCbufToCb = l1_to_l0_b<ArchType::ASCEND_V220, InDtype, transB, DataFormat::ZN, DataFormat::NZ>;
    using Mad = mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>;
    using CopyCcToGm = l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, OutDtype, float>;
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID2);
    set_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID3);
    set_flag(PIPE_FIX, PIPE_M, EVENT_ID0);
    set_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    set_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);

    for (uint64_t loop_idx = core_idx; loop_idx < core_loop; loop_idx += num_core) {
        uint64_t batch_idx = loop_idx / tdim.n / tdim.m;
        MatCoord tidx{0};
        GetBaseBlockIdx(loop_idx, tidx);
        uint64_t offset_a = 0, offset_b = 0, offset_a_next = 0, offset_b_next = 0;
        uint64_t offset_c = tidx.m * m0 * batch_size * (n + split_gap) + batch_idx * (n + split_gap) + tidx.n * n0;
        uint64_t m_actual = (tidx.m == (tdim.m - 1)) ? (m - tidx.m * m0) : m0;
        uint64_t n_actual = (tidx.n == (tdim.n - 1)) ? (n - tidx.n * n0) : n0;
        uint64_t m_round = RoundUp<CONST_16>(m_actual);
        uint64_t n_round = RoundUp<CONST_16>(n_actual);
        uint64_t mn_max = m_round > n_round ? m_round : n_round;
        uint64_t k_part_len = L0_PINGPONG_BUFFER_LEN / mn_max / CONST_16 * CONST_16;
        uint64_t shuffle_k = en_shuffle_k ? (core_idx % tdim.k) : 0;
        offset_a = tidx.m * m0 * batch_size * (k + split_gap) + batch_idx * (k + split_gap) + shuffle_k * k0;

        if constexpr (transB) {
            offset_b = batch_idx * k * n + tidx.n * n0 * k + shuffle_k * k0;
        } else {
            offset_b = batch_idx * k * n + shuffle_k * k0 * n + tidx.n * n0;
        }

        uint64_t k_actual = (shuffle_k == tdim.k - 1) ? k - shuffle_k * k0 : k0;
        uint64_t k_round = (k_actual + CONST_16 - 1) / CONST_16 * CONST_16;

        LocalTensor l1_buf_a = ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
        LocalTensor l1_buf_b = ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
        event_t event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

        if (loop_idx == core_idx) {
            wait_flag(PIPE_MTE1, PIPE_MTE2, event_id);
            // *** load matrix A to L1
            if ((m == 1) || (m_actual == 1)) {
                CopyGmToCbuf(l1_buf_a,       // dst
                             gm_a[offset_a], // src
                             1,              // nTileActual
                             16,             // nTileCeil
                             1,              // nVal
                             k_actual,       // kTileActual
                             k_round,        // kTileCeil
                             k);             // dVal
            } else {
                CopyGmToCbufNd2Nz(l1_buf_a,                      // dst
                                  gm_a[offset_a],                // src
                                  m_actual,                      // nTileActual
                                  m_round,                       // nTileCeil
                                  m,                             // nVal
                                  k_actual,                      // dTileActual
                                  k_round,                       // dTileCeil
                                  (k + split_gap) * batch_size); // dVal
            }
            set_flag(PIPE_MTE2, PIPE_MTE1, event_id);
            // *** load matrix B to L1
            wait_flag(PIPE_MTE1, PIPE_MTE2, event_id + 2);
            if (transB) {
                CopyGmToCbufNd2Nz(l1_buf_b,       // dst
                                  gm_b[offset_b], // src
                                  n_actual,       // nTileActual
                                  n_round,        // nTileCeil
                                  n,              // nVal
                                  k_actual,       // dTileActual
                                  k_round,        // dTileCeil
                                  k);             // dVal
            } else {
                CopyGmToCbufNd2Nz(l1_buf_b,       // dst
                                  gm_b[offset_b], // src
                                  k_actual,       // nTileActual
                                  k_round,        // nTileCeil
                                  k,              // nVal
                                  n_actual,       // dTileActual
                                  n_round,        // dTileCeil
                                  n);             // dVal
            }
            set_flag(PIPE_MTE2, PIPE_MTE1, event_id + 2);
        }

        for (tidx.k = 0; tidx.k < tdim.k; ++tidx.k) {
            shuffle_k = en_shuffle_k ? (tidx.k + core_idx) % tdim.k : tidx.k;
            uint64_t k_actual = (shuffle_k == (tdim.k - 1)) ? (k - shuffle_k * k0) : k0;
            uint64_t k_round = (k_actual + CONST_16 - 1) / CONST_16 * CONST_16;
            fdim.k = (k_actual + k_part_len - 1) / k_part_len;

            LocalTensor l1_buf_a = ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
            LocalTensor l1_buf_b = ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
            auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

            if (tidx.k < tdim.k - 1) {
                uint64_t shuffle_k_next = en_shuffle_k ? (core_idx + tidx.k + 1) % tdim.k : (tidx.k + 1);
                offset_a_next =
                    tidx.m * m0 * batch_size * (k + split_gap) + batch_idx * (k + split_gap) + shuffle_k_next * k0;

                if constexpr (transB) {
                    offset_b_next = batch_idx * k * n + tidx.n * n0 * k + shuffle_k_next * k0;
                } else {
                    offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + tidx.n * n0;
                }

                uint64_t k_actual_next = (shuffle_k_next == (tdim.k - 1)) ? (k - shuffle_k_next * k0) : k0;
                uint64_t k_round_next = (k_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;

                LocalTensor l1_buf_a_next = (1 - ping_flag) ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                wait_flag(PIPE_MTE1, PIPE_MTE2, event_id_next);
                // *** load matrix A to L1
                if ((m == 1) || (m_actual == 1)) {
                    CopyGmToCbuf(l1_buf_a_next,       // dst
                                 gm_a[offset_a_next], // src
                                 m_actual,            // nTileActual
                                 m_round,             // nTileCeil
                                 m,                   // nVal
                                 k_actual_next,       // kTileActual
                                 k_round_next,        // kTileCeil
                                 k);                  // dVal
                } else {
                    CopyGmToCbufNd2Nz(l1_buf_a_next,                 // dst
                                      gm_a[offset_a_next],           // src
                                      m_actual,                      // nTileActual
                                      m_round,                       // nTileCeil
                                      m,                             // nVal
                                      k_actual_next,                 // dTileActual
                                      k_round_next,                  // dTileCeil
                                      (k + split_gap) * batch_size); // dVal
                }
                set_flag(PIPE_MTE2, PIPE_MTE1, event_id_next);

                // *** load matrix B to L1
                wait_flag(PIPE_MTE1, PIPE_MTE2, event_id_next + 2);
                if constexpr (transB) {
                    CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
                                      gm_b[offset_b_next], // src
                                      n_actual,            // nTileActual
                                      n_round,             // nTileCeil
                                      n,                   // nVal
                                      k_actual_next,       // dTileActual
                                      k_round_next,        // dTileCeil
                                      k);                  // dVal
                } else {
                    CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
                                      gm_b[offset_b_next], // src
                                      k_actual_next,       // nTileActual
                                      k_round_next,        // nTileCeil
                                      k,                   // nVal
                                      n_actual,            // dTileActual
                                      n_round,             // dTileCeil
                                      n);                  // dVal
                }
                set_flag(PIPE_MTE2, PIPE_MTE1, event_id_next + 2);
            }

            if (tidx.k == tdim.k - 1 && loop_idx + num_core < core_loop) {
                uint64_t b_idx_next = (loop_idx + num_core) / tdim.n / tdim.m;
                MatCoord tidx{0};
                GetBaseBlockIdx(loop_idx + num_core, tidx);
                uint64_t shuffle_k_next = en_shuffle_k ? (core_idx % tdim.k) : 0;
                uint64_t m_actual_next = (tidx.m == (tdim.m - 1)) ? (m - tidx.m * m0) : m0;
                uint64_t n_actual_next = (tidx.n == (tdim.n - 1)) ? (n - tidx.n * n0) : n0;
                uint64_t m_round_next = (m_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
                uint64_t n_round_next = (n_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
                uint64_t k_actual_next = (shuffle_k_next == (tdim.k - 1)) ? (k - shuffle_k_next * k0) : k0;
                uint64_t k_round_next = (k_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
                offset_a_next =
                    tidx.m * m0 * batch_size * (k + split_gap) + b_idx_next * (k + split_gap) + shuffle_k_next * k0;
                if constexpr (transB) {
                    offset_b_next = b_idx_next * k * n + tidx.n * n0 * k + shuffle_k_next * k0;
                } else {
                    offset_b_next = b_idx_next * k * n + shuffle_k_next * k0 * n + tidx.n * n0;
                }

                LocalTensor l1_buf_a_next = (1 - ping_flag) ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                wait_flag(PIPE_MTE1, PIPE_MTE2, event_id_next);
                // *** load matrix A to L1
                if (m == 1 || m_actual_next == 1) {
                    CopyGmToCbuf(l1_buf_a_next,       // dst
                                 gm_a[offset_a_next], // src
                                 m_actual_next,       // nTileActual
                                 m_round_next,        // nTileCeil
                                 m,                   // nVal
                                 k_actual_next,       // kTileActual
                                 k_round_next,        // kTileCeil
                                 k);                  // dVal
                } else {
                    CopyGmToCbufNd2Nz(l1_buf_a_next,                 // dst
                                      gm_a[offset_a_next],           // src
                                      m_actual_next,                 // nTileActual
                                      m_round_next,                  // nTileCeil
                                      m,                             // nVal
                                      k_actual_next,                 // dTileActual
                                      k_round_next,                  // dTileCeil
                                      (k + split_gap) * batch_size); // dVal
                }
                set_flag(PIPE_MTE2, PIPE_MTE1, event_id_next);

                // *** load matrix B to L1
                wait_flag(PIPE_MTE1, PIPE_MTE2, event_id_next + 2);
                if constexpr (transB) {
                    CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
                                      gm_b[offset_b_next], // src
                                      n_actual_next,       // nTileActual
                                      n_round_next,        // nTileCeil
                                      n,                   // nVal
                                      k_actual_next,       // dTileActual
                                      k_round_next,        // dTileCeil
                                      k);                  // dVal
                } else {
                    CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
                                      gm_b[offset_b_next], // src
                                      k_actual_next,       // nTileActual
                                      k_round_next,        // nTileCeil
                                      k,                   // nVal
                                      n_actual_next,       // dTileActual
                                      n_round_next,        // dTileCeil
                                      n);                  // dVal
                }
                set_flag(PIPE_MTE2, PIPE_MTE1, event_id_next + 2);
            }

            MatCoord fidx{0};
            for (fidx.k = 0; fidx.k < fdim.k; ++fidx.k) {
                uint32_t k0_round = (fidx.k < fdim.k - 1) ? k_part_len : k_round - fidx.k * k_part_len;
                uint32_t k0_actual = (fidx.k < fdim.k - 1) ? k_part_len : k_actual - fidx.k * k_part_len;

                auto mte1_mad_ping_flag = 1 - fidx.k % 2;
                auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
                LocalTensor l0a_buf = l0a_base[(fidx.k & 0b1) * L0_PINGPONG_BUFFER_LEN];
                LocalTensor l0b_buf = l0b_base[(fidx.k & 0b1) * L0_PINGPONG_BUFFER_LEN];

                // *** load matrix A from L1 to L0A
                if (fidx.k == 0) {
                    wait_flag(PIPE_MTE2, PIPE_MTE1, event_id);
                }
                wait_flag(PIPE_M, PIPE_MTE1, mte1_mad_event_id);
                if ((m == 1) || (m_actual == 1)) {
                    load_cbuf_to_ca((__ca__ InDtype *)l0a_buf.GetPhyAddr(),
                                    (__cbuf__ InDtype *)l1_buf_a[fidx.k * k_part_len].GetPhyAddr(),
                                    0,                                      // baseIdx
                                    (k0_round + CONST_256 - 1) / CONST_256, // repeat
                                    1,                                      // srcStride
                                    0,                                      // dstStride
                                    0,                                      // sid
                                    false,                                  // transpose
                                    inc                                     // addr_cal_mode_t
                    );
                } else {
                    LoadCbufToCa(l0a_buf,                                 // l0Tensor
                                 l1_buf_a[fidx.k * k_part_len * m_round], // l1Tensor
                                 m_round,                                 // mTileCeil
                                 k0_round,                                // kPartCeil
                                 1,                                       // mSrcStride
                                 m_round / CONST_16,                      // kSrcStride
                                 k0_round / CONST_16,                     // mDstStride
                                 1);                                      // kDstStride
                }
                if (fidx.k == fdim.k - 1) {
                    set_flag(PIPE_MTE1, PIPE_MTE2, event_id);
                }

                // *** load matrix B from L1 to L0B
                if (fidx.k == 0) {
                    wait_flag(PIPE_MTE2, PIPE_MTE1, event_id + 2);
                }
                if constexpr (transB) {
                    LoadCbufToCb(l0b_buf,                                 // l0Tensor
                                 l1_buf_b[fidx.k * k_part_len * n_round], // l1Tensor
                                 n_round,                                 // nTileCeil
                                 k0_round,                                // kPartCeil
                                 1,                                       // nSrcStride
                                 n_round / CONST_16,                      // kSrcStride
                                 1,                                       // nDstStride
                                 k0_round / CONST_16);                    // kDstStride
                } else {
                    LoadCbufToCb(l0b_buf,                                  // l0Tensor
                                 l1_buf_b[fidx.k * k_part_len * CONST_16], // l1Tensor
                                 n_round,                                  // nTileCeil
                                 k0_round,                                 // kPartCeil
                                 k_round / CONST_16,                       // nSrcStride
                                 1,                                        // kSrcStride
                                 1,                                        // nDstStride
                                 n_round / CONST_16);                      // kDstStride
                }
                if (fidx.k == fdim.k - 1) {
                    set_flag(PIPE_MTE1, PIPE_MTE2, event_id + 2);
                }

                set_flag(PIPE_MTE1, PIPE_M, mte1_mad_event_id);
                wait_flag(PIPE_MTE1, PIPE_M, mte1_mad_event_id);

                bool init_c = (tidx.k == 0 && fidx.k == 0);
                if (init_c) {
                    wait_flag(PIPE_FIX, PIPE_M, EVENT_ID0);
                }

                if (m != 1 && m_actual == 1) {
                    Mad(l0c_buf,   // c
                        l0a_buf,   // a
                        l0b_buf,   // b
                        CONST_16,  // mTileActual
                        n_actual,  // nTileActual
                        k0_actual, // kTileActual
                        init_c);   // initC
                } else {
                    Mad(l0c_buf,   // c
                        l0a_buf,   // a
                        l0b_buf,   // b
                        m_actual,  // mTileActual
                        n_actual,  // nTileActual
                        k0_actual, // kTileActual
                        init_c);   // initC
                }

                pipe_barrier(PIPE_M);
                set_flag(PIPE_M, PIPE_MTE1, mte1_mad_event_id);
            }

            ping_flag = 1 - ping_flag;
        }

        set_flag(PIPE_M, PIPE_FIX, EVENT_ID0);
        wait_flag(PIPE_M, PIPE_FIX, EVENT_ID0);

        // copy from L0C to gm
        CopyCcToGm(gm_c[offset_c],                // dst
                   l0c_buf,                       // src
                   m_actual,                      // mTileActual
                   n_actual,                      // nTileActual
                   m_round,                       // mTileCeil
                //    (n) * batch_size); // nActual
                   (n + split_gap) * batch_size); // nActual
        set_flag(PIPE_FIX, PIPE_M, EVENT_ID0);
    }

    wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID0);
    wait_flag(PIPE_M, PIPE_MTE1, EVENT_ID1);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID0);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID1);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID2);
    wait_flag(PIPE_MTE1, PIPE_MTE2, EVENT_ID3);
    wait_flag(PIPE_FIX, PIPE_M, EVENT_ID0);
    pipe_barrier(PIPE_ALL);
#endif
}

#endif

__aicore__ __force_inline__ uint32_t CeilDiv64(const uint32_t val) { return (val + CONST_64 - 1) / CONST_64; }

__aicore__ __force_inline__ uint32_t RoundUp512(const uint32_t val)
{
    return (val + CUBE_MATRIX_SIZE_512 - 1) / CUBE_MATRIX_SIZE_512 * CUBE_MATRIX_SIZE_512;
}

__aicore__ __force_inline__ uint32_t RoundUp32(const uint32_t val)
{
    return (val + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
}

__aicore__ __force_inline__ uint32_t RoundUp16(const uint32_t val)
{
    return (val + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
}

__aicore__ __force_inline__ uint32_t RoundUp8(const uint32_t val) { return (val + CONST_8 - 1) / CONST_8 * CONST_8; }

#if defined(__DAV_C220_CUBE__)
template <bool TA, bool TB, bool SPLIT_K = false, bool HAVE_BIAS = false, bool IS_INT8 = false, typename InDtype = half,
          typename OutDtype = half, typename BiasDtype = float, typename ScaleDtype = float>
class PpMatmulInt {
public:
    __aicore__ PpMatmulInt() {};

    __aicore__ __force_inline__ void Init(AscendC::GlobalTensor<InDtype> &gm_a,
                                                         AscendC::GlobalTensor<InDtype> &gm_b,
                                                         AscendC::GlobalTensor<BiasDtype> &gm_bias,
                                                         AscendC::GlobalTensor<OutDtype> &gm_c,
                                                         MLATilingData &mlaParams, uint32_t mode)
    {
        this->gm_a = gm_a;
        this->gm_b = gm_b;
        this->gm_c = gm_c;
        this->gm_bias = gm_bias;

        if (mode == 0) {
            batch_size = mlaParams.mm1batchSize;
            m = mlaParams.mm1m;
            k = mlaParams.mm1k;
            n = mlaParams.mm1n;
            m0 = mlaParams.mm1m0;
            k0 = mlaParams.mm1k0;
            n0 = mlaParams.mm1n0;
            m_loop = mlaParams.mm1mLoop;
            k_loop = mlaParams.mm1kLoop;
            n_loop = mlaParams.mm1nLoop;
            core_loop = mlaParams.mm1coreLoop;
            swizzle_cnt = mlaParams.mm1swizzleCnt;
            swizzlDirect = mlaParams.mm1swizzlDirect;
            en_shuffle_k = mlaParams.mm1enShuffleK;
        } else {
            batch_size = mlaParams.mm2batchSize;
            m = mlaParams.mm2m;
            k = mlaParams.mm2k;
            n = mlaParams.mm2n;
            m0 = mlaParams.mm2m0;
            k0 = mlaParams.mm2k0;
            n0 = mlaParams.mm2n0;
            m_loop = mlaParams.mm2mLoop;
            k_loop = mlaParams.mm2kLoop;
            n_loop = mlaParams.mm2nLoop;
            core_loop = mlaParams.mm2coreLoop;
            swizzle_cnt = mlaParams.mm2swizzleCnt;
            swizzlDirect = mlaParams.mm2swizzlDirect;
            en_shuffle_k = mlaParams.mm2enShuffleK;
        }

        block_size = BLOCK_SIZE_32;
        cube_matrix_size = CUBE_MATRIX_SIZE_512;
        uint32_t a_l1_size = RoundUp512(m0 * k0);
        if constexpr (TA || !TB) {
            a_l1_size = RoundUp512(RoundUp32(m0) * k0);
        }
        L1_PINGPONG_BUFFER_LEN = L1_PINGPONG_BUFFER_LEN_INT8;
        L0AB_PINGPONG_BUFFER_LEN = L0AB_PINGPONG_BUFFER_LEN_INT8;
        l1_base_b = l1_base_a[a_l1_size];
        core_num = AscendC::GetBlockNum();
        core_idx = AscendC::GetBlockIdx();
        ping_flag = 1;
    }

    __aicore__ __force_inline__ void GetBlockIdx(uint32_t index, uint32_t &m_idx, uint32_t &n_idx)
    {
        uint32_t in_batch_idx = index % (m_loop * n_loop);
        if (swizzlDirect == 0) { // Zn
            uint32_t tile_block_loop = (m_loop + swizzle_cnt - 1) / swizzle_cnt;
            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * n_loop);
            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * n_loop);

            uint32_t n_row = swizzle_cnt;
            if (tile_block_idx == tile_block_loop - 1) {
                n_row = m_loop - swizzle_cnt * tile_block_idx;
            }
            m_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
            n_idx = in_tile_block_idx / n_row;
            if (tile_block_idx % 2 != 0) {
                n_idx = n_loop - n_idx - 1;
            }
        } else { // Nz
            uint32_t tile_block_loop = (n_loop + swizzle_cnt - 1) / swizzle_cnt;
            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * m_loop);
            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * m_loop);

            uint32_t n_col = swizzle_cnt;
            if (tile_block_idx == tile_block_loop - 1) {
                n_col = n_loop - swizzle_cnt * tile_block_idx;
            }
            m_idx = in_tile_block_idx / n_col;
            n_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
            if (tile_block_idx % 2 != 0) {
                m_idx = m_loop - m_idx - 1;
            }
        }
    }

    __aicore__ __force_inline__ void PreloadB()
    {
        uint64_t offset_b;
        uint32_t m_idx = 0;
        uint32_t n_idx = 0;
        uint64_t batch_idx = core_idx / n_loop / m_loop;
        GetBlockIdx(core_idx, m_idx, n_idx);
        uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
        uint32_t n_round = 0;
        if (TB) {
            n_round = RoundUp16(n_actual);
        } else {
            n_round = RoundUp32(n_actual);
        }
        uint64_t shuffle_k = en_shuffle_k ? core_idx % k_loop : 0;
        uint32_t k_actual = (shuffle_k == k_loop - 1) ? k - shuffle_k * k0 : k0;
        uint32_t k_round = (k_actual + block_size - 1) / block_size * block_size; // int8 ：32 fp16 ：16
        if (TB) {
            offset_b =
                batch_idx * RoundUp16(n) * RoundUp32(k) + shuffle_k * k0 * RoundUp16(n) + n_idx * n0 * CONST_32;
        } else {
            offset_b =
                batch_idx * RoundUp16(k) * RoundUp32(n) + n_idx * n0 * RoundUp16(k) + shuffle_k * k0 * CONST_32;
        }
        AscendC::LocalTensor<InDtype> l1_buf_b =
                ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
        if (TB) {
            gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>(
                l1_buf_b, gm_b[offset_b], n_actual, n_round, RoundUp16(n), k_actual, k_round, RoundUp32(k));
        } else {
            gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>(
                l1_buf_b, gm_b[offset_b], k_actual, k_round, RoundUp16(k), n_actual, n_round, RoundUp32(n));
        }
    }

    __aicore__ __force_inline__ void run()
    {
        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        uint32_t m_idx = 0;
        uint32_t n_idx = 0;
        for (uint32_t loop_idx = core_idx; loop_idx < core_loop; loop_idx += core_num) {
            GetBlockIdx(loop_idx, m_idx, n_idx);
            uint64_t batch_idx = loop_idx / n_loop / m_loop;
            uint64_t offset_a;
            uint64_t offset_b;
            uint64_t offset_bias;
            uint64_t offset_scalar;
            uint64_t offset_a_next;
            uint64_t offset_b_next;
            uint64_t offset_c = batch_idx * m * n + m_idx * m0 * n + n_idx * n0;
            uint32_t m_actual = (m_idx == (m_loop - 1)) ? (m - m_idx * m0) : m0;
            uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
            uint32_t m_round = 0;
            uint32_t n_round = 0;
            uint64_t shuffle_k = en_shuffle_k ? core_idx % k_loop : 0;
            uint32_t m_round_16 = RoundUp16(m_actual);
            uint32_t m_round_32 = RoundUp32(m_actual);
            if (TA) {
                m_round = m_round_32;
            } else {
                m_round = m_round_16;
            }
            if (TB) {
                n_round = RoundUp16(n_actual);
            } else {
                n_round = RoundUp32(n_actual);
            }

            uint32_t mn_max = m_round > n_round ? m_round : n_round;
            uint32_t k_part_len = 0;
            k_part_len = L0AB_PINGPONG_BUFFER_LEN_INT8 / mn_max / BLOCK_SIZE_32 * BLOCK_SIZE_32;

            if (TA) {
                offset_a = batch_idx * m * k + m_idx * m0 + shuffle_k * k0;
            } else {
                offset_a = batch_idx * m * k + m_idx * m0 * k + shuffle_k * k0;
            }
            if (TB) {
                offset_b =
                    batch_idx * RoundUp16(n) * RoundUp32(k) + shuffle_k * k0 * RoundUp16(n) + n_idx * n0 * CONST_32;
            } else {
                offset_b =
                    batch_idx * RoundUp16(k) * RoundUp32(n) + n_idx * n0 * RoundUp16(k) + shuffle_k * k0 * CONST_32;
            }
            offset_bias = batch_idx * n + n_idx * n0;
            offset_scalar = batch_idx * n + n_idx * n0;

            uint32_t k_actual = (shuffle_k == k_loop - 1) ? k - shuffle_k * k0 : k0;
            uint32_t k_round = (k_actual + block_size - 1) / block_size * block_size; // int8 ：32 fp16 ：16

            AscendC::LocalTensor<InDtype> l1_buf_a =
                ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l1_buf_b =
                ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l0a_buf =
                ping_flag ? l0a_base : l0a_base[L0AB_PINGPONG_BUFFER_LEN];
            AscendC::LocalTensor<InDtype> l0b_buf =
                ping_flag ? l0b_base : l0b_base[L0AB_PINGPONG_BUFFER_LEN];

            auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
            if (HAVE_BIAS) {
                WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
                gm_to_l1<ArchType::ASCEND_V220, BiasDtype, DataFormat::ND, DataFormat::ND>(bias_l1,              // dst
                                                                                           gm_bias[offset_bias], // src
                                                                                           1, RoundUp16(1), 1, n_round,
                                                                                           RoundUp16(n_round), n_round);
                SET_FLAG(MTE2, MTE1, EVENT_ID6);
            }

            WAIT_FLAG(MTE1, MTE2, event_id);
            // *** load matrix A to L1
            if ((m == 1) || (m_actual == 1 && !TA)) {
                gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(l1_buf_a,       // dst
                                                                                         gm_a[offset_a], // src
                                                                                         1, RoundUp16(1), 1, k_round,
                                                                                         RoundUp16(k_round), k_round);
            } else {
                if (TA) {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_a, gm_a[offset_a], k_actual, k_round, k, m_actual, m_round, m);
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                        l1_buf_a, gm_a[offset_a], m_actual, m_round, m, k_actual, k_round, k);
                }
            }
            SET_FLAG(MTE2, MTE1, event_id);

            // *** load matrix B to L1
            WAIT_FLAG(MTE1, MTE2, event_id + CONST_2);
            if (loop_idx != core_idx) {
                if (TB) {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>(
                        l1_buf_b, gm_b[offset_b], n_actual, n_round, RoundUp16(n), k_actual, k_round, RoundUp32(k));
                } else {
                    gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>(
                        l1_buf_b, gm_b[offset_b], k_actual, k_round, RoundUp16(k), n_actual, n_round, RoundUp32(n));
                }
            }
            SET_FLAG(MTE2, MTE1, event_id + CONST_2);

            for (uint64_t k_idx = 0; k_idx < k_loop; k_idx++) {
                shuffle_k = en_shuffle_k ? (k_idx + core_idx) % k_loop : k_idx;
                uint32_t k_actual = (shuffle_k == (k_loop - 1)) ? (k - shuffle_k * k0) : k0;
                uint32_t k_round = (k_actual + block_size - 1) / block_size * block_size;
                uint32_t k_part_loop = (k_actual + k_part_len - 1) / k_part_len;

                AscendC::LocalTensor<InDtype> l1_buf_a =
                    ping_flag ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                AscendC::LocalTensor<InDtype> l1_buf_b =
                    ping_flag ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;

                if (k_idx < k_loop - 1) {
                    uint64_t shuffle_k_next = en_shuffle_k ? (core_idx + k_idx + 1) % k_loop : k_idx + 1;
                    if (TA) {
                        offset_a_next = batch_idx * m * k + m_idx * m0 + shuffle_k_next * k0 * m;
                    } else {
                        offset_a_next = batch_idx * m * k + m_idx * m0 * k + shuffle_k_next * k0;
                    }

                    if (TB) {
                        offset_b_next = batch_idx * RoundUp16(n) * RoundUp32(k) + shuffle_k_next * k0 * RoundUp16(n) +
                                        n_idx * n0 * CONST_32;
                    } else {
                        offset_b_next = batch_idx * RoundUp16(k) * RoundUp32(n) + n_idx * n0 * RoundUp16(k) +
                                        shuffle_k_next * k0 * CONST_32;
                    }

                    uint32_t k_actual_next = (shuffle_k_next == (k_loop - 1)) ? (k - shuffle_k_next * k0) : k0;
                    uint32_t k_round_next = (k_actual_next + block_size - 1) / block_size * block_size;

                    AscendC::LocalTensor<InDtype> l1_buf_a_next =
                        (1 - ping_flag) ? l1_base_a : l1_base_a[L1_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l1_buf_b_next =
                        (1 - ping_flag) ? l1_base_b : l1_base_b[L1_PINGPONG_BUFFER_LEN];
                    auto event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;

                    WAIT_FLAG(MTE1, MTE2, event_id_next);
                    // *** load matrix A to L1
                    if ((m == 1) || (m_actual == 1 && !TA)) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>(
                            l1_buf_a_next, gm_a[offset_a_next], 1, RoundUp16(1), 1, k_round_next,
                            RoundUp16(k_round_next), k_round_next);
                    } else {
                        if (TA) {
                            gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                                l1_buf_a_next, gm_a[offset_a_next], k_actual_next, k_round_next, k, m_actual, m_round,
                                m);
                        } else {
                            gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>(
                                l1_buf_a_next, gm_a[offset_a_next], m_actual, m_round, m, k_actual_next, k_round_next,
                                k);
                        }
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next);

                    // *** load matrix B to L1
                    WAIT_FLAG(MTE1, MTE2, event_id_next + CONST_2);
                    if (TB) {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>(l1_buf_b_next,
                                                                                                 gm_b[offset_b_next],
                                                                                                 n_actual,
                                                                                                 n_round,
                                                                                                 RoundUp16(n),
                                                                                                 k_actual_next,
                                                                                                 k_round_next,
                                                                                                 RoundUp32(k));
                    } else {
                        gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::NZ, DataFormat::NZ>(l1_buf_b_next,
                                                                                                 gm_b[offset_b_next],
                                                                                                 k_actual_next,
                                                                                                 k_round_next,
                                                                                                 RoundUp16(k),
                                                                                                 n_actual,
                                                                                                 n_round,
                                                                                                 RoundUp32(n));
                    }
                    SET_FLAG(MTE2, MTE1, event_id_next + CONST_2);
                }

                for (int k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
                    uint32_t k0_round = (k_part_idx < k_part_loop - 1) ? k_part_len : k_round - k_part_idx * k_part_len;
                    uint32_t k0_actual =
                        (k_part_idx < k_part_loop - 1) ? k_part_len : k_actual - k_part_idx * k_part_len;

                    auto mte1_mad_ping_flag = 1 - k_part_idx % 2;
                    auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
                    AscendC::LocalTensor<InDtype> l0a_buf =
                        l0a_base[(k_part_idx % 2) * L0AB_PINGPONG_BUFFER_LEN];
                    AscendC::LocalTensor<InDtype> l0b_buf =
                        l0b_base[(k_part_idx % 2) * L0AB_PINGPONG_BUFFER_LEN];

                    // *** load matrix A from L1 to L0A
                    if (k_part_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, event_id);
                    }
                    WAIT_FLAG(M, MTE1, mte1_mad_event_id);
                    if ((m == 1) || (m_actual == 1 && !TA)) {
                        l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0a_buf, l1_buf_a[k_part_idx * k_part_len], 0,
                            (k0_round + cube_matrix_size - 1) / cube_matrix_size, // repeat
                            0,
                            1, // srcStride
                            0,
                            0 // dstStride
                        );    // addr_cal_mode_t
                    } else {
                        if (TA) {
                            for (uint64_t i = 0; i < m_round / BLOCK_SIZE_32; i++) {
                                AscendC::LoadDataWithTranspose(
                                    l0a_buf[i * k0_round * BLOCK_SIZE_32],
                                    l1_buf_a[k_part_idx * k_part_len * BLOCK_SIZE_32 +
                                                    i * k_round * BLOCK_SIZE_32],
                                    AscendC::LoadData2dTransposeParams(0,                            // startIndexIn
                                                                       k0_round / BLOCK_SIZE_32,     // repeatTimesIn
                                                                       1,                            // srcStrideIn
                                                                       0,                            // dstGapIn
                                                                       k0_round / BLOCK_SIZE_32 - 1, // dstfracGapIn
                                                                       0)                            // addrModeIn
                                );
                            }
                        } else {
                            for (uint64_t i = 0; i < m_round / BLOCK_SIZE_16; i++) {
                                l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR,
                                           DataFormat::VECTOR>(
                                    l0a_buf[i * k0_round * BLOCK_SIZE_16],
                                    l1_buf_a[k_part_idx * k_part_len * m_round + i * cube_matrix_size], 0,
                                    k0_round / block_size, // repeat
                                    0,
                                    m_round / BLOCK_SIZE_16, // srcStride
                                    0,
                                    0 // dstStride
                                );    // addr_cal_mode_t
                            }
                        }
                    }
                    if (k_part_idx == k_part_loop - 1) {
                        SET_FLAG(MTE1, MTE2, event_id);
                    }

                    // *** load matrix B from L1 to L0B
                    if (k_part_idx == 0) {
                        WAIT_FLAG(MTE2, MTE1, event_id + CONST_2);
                    }
                    if (TB) {
                        l1_to_l0_b<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                            l0b_buf, l1_buf_b[k_part_idx * k_part_len * n_round], 0,
                            k0_round * n_round / cube_matrix_size, // repeat
                            0,
                            1, // srcStride
                            0,
                            0 // dstStride
                        );    // addr_cal_mode_t
                    } else {
                        for (uint64_t i = 0; i < k0_round / BLOCK_SIZE_32; i++) {
                            AscendC::LoadDataWithTranspose(
                                l0b_buf[i * RoundUp16(n_actual) * BLOCK_SIZE_32],
                                l1_buf_b[(k_part_idx * k_part_len + i * BLOCK_SIZE_32) * BLOCK_SIZE_32],
                                AscendC::LoadData2dTransposeParams(0,                       // startIndexIn
                                                                   n_round / BLOCK_SIZE_32, // repeatTimesIn
                                                                   k_round / BLOCK_SIZE_32, // srcStrideIn
                                                                   1,                       // dstGapIn
                                                                   0,                       // dstfracGapIn
                                                                   0)                       // addrModeIn
                            );
                        }
                    }
                    if (k_part_idx == k_part_loop - 1) {
                        SET_FLAG(MTE1, MTE2, event_id + CONST_2);
                    }

                    SET_FLAG(MTE1, M, mte1_mad_event_id);
                    WAIT_FLAG(MTE1, M, mte1_mad_event_id);

                    bool init_c = (k_idx == 0 && k_part_idx == 0);
                    bool sp_flag = (m != 1 && m_actual == 1 && TA);
                    if (init_c) {
                        WAIT_FLAG(FIX, M, EVENT_ID0);
                    }
                    if (init_c) {
                        if (HAVE_BIAS) {
                            WAIT_FLAG(MTE2, MTE1, EVENT_ID6);
                            CopyCbufToBt<BiasDtype>(((uint64_t)bias_bt), bias_l1, (uint16_t)0ULL, 1,
                                                    CeilDiv64(n_actual * CONST_4), 0, 0);
                            SET_FLAG(MTE1, MTE2, EVENT_ID7); // bias ready, mte2 can begin move A/B or scale
                            SET_FLAG(MTE1, M, EVENT_ID7);    // bias ready, mmad can begin
                            WAIT_FLAG(MTE1, M, EVENT_ID7);   // wait move bias fron L1 to BT
                            mmad<ArchType::ASCEND_V220, InDtype, InDtype, BiasDtype, false>(
                                l0c_buf, l0a_buf, l0b_buf, ((uint64_t)bias_bt),
                                sp_flag ? m_round_16 : m_actual, // m
                                n_actual,                        // n
                                k0_actual,                       // k
                                0                                // cmatrixInitVal
                            );
                        } else {
                            mmad<ArchType::ASCEND_V220, InDtype, InDtype, BiasDtype, false>(
                                l0c_buf, l0a_buf, l0b_buf,
                                sp_flag ? m_round_16 : m_actual, // m
                                n_actual,                        // n
                                k0_actual,                       // k
                                1);                              // cmatrixInitVal
                        }
                    } else {
                        mmad<ArchType::ASCEND_V220, InDtype, InDtype, BiasDtype, false>(
                            l0c_buf, l0a_buf, l0b_buf,
                            sp_flag ? m_round_16 : m_actual, // m
                            n_actual,                        // n
                            k0_actual,                       // k
                            0);                              // cmatrixInitVal
                    }
                    AscendC::PipeBarrier<PIPE_M>();
                    SET_FLAG(M, MTE1, mte1_mad_event_id);
                }

                ping_flag = 1 - ping_flag;
            }
            SET_FLAG(M, FIX, EVENT_ID0);
            WAIT_FLAG(M, FIX, EVENT_ID0);
            AscendC::PipeBarrier<PIPE_FIX>();
            // copy from L0C to gm
            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, int32_t, int32_t>(gm_c[offset_c], // dst
                                                                               l0c_buf,      // src
                                                                               m_actual,              // MSize
                                                                               n_actual,              // NSize
                                                                               m_round_16,            // srcStride
                                                                               n                      // dstStride_dst_D
            );
            SET_FLAG(FIX, M, EVENT_ID0);
            FftsCrossCoreSync<PIPE_FIX, SYNC_MODE>(MMAIC);

            if ((loop_idx / core_num + 1) % MAX_HW_SYNC_COUNTER == 0) {
                WaitFlagDev(MMAIV);
            }
        }
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        AscendC::PipeBarrier<PIPE_ALL>();
    }

private:
    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::GlobalTensor<InDtype> gm_a;
    AscendC::GlobalTensor<InDtype> gm_b;
    AscendC::GlobalTensor<BiasDtype> gm_bias;
    AscendC::GlobalTensor<ScaleDtype> gm_descale;
    AscendC::GlobalTensor<OutDtype> gm_c;

    AscendC::LocalTensor<InDtype> l1_base_a =
        buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(SCALE_L1_LEN + BIAS_L1_LEN);
    AscendC::LocalTensor<InDtype> l1_base_b =
        buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(L1_PINGPONG_BUFFER_LEN_FP16);
    AscendC::LocalTensor<InDtype> l0a_base = buf.GetBuffer<BufferType::ASCEND_L0A, InDtype>(0);
    AscendC::LocalTensor<InDtype> l0b_base = buf.GetBuffer<BufferType::ASCEND_L0B, InDtype>(0);
    AscendC::LocalTensor<BiasDtype> l0c_buf = buf.GetBuffer<BufferType::ASCEND_L0C, BiasDtype>(0);
    AscendC::LocalTensor<ScaleDtype> scale_l1 = buf.GetBuffer<BufferType::ASCEND_CB, ScaleDtype>(BIAS_L1_LEN);
    AscendC::LocalTensor<BiasDtype> bias_l1 = buf.GetBuffer<BufferType::ASCEND_CB, BiasDtype>(0);

    uint64_t bias_bt{0};

    uint32_t core_num{0};

    uint32_t batch_size{0};
    uint32_t m{0};
    uint32_t k{0};
    uint32_t n{0};

    uint32_t m0{0};
    uint32_t k0{0};
    uint32_t n0{0};

    uint32_t m_loop{0};
    uint32_t n_loop{0};
    uint32_t k_loop{0};
    uint32_t core_loop{0};
    uint32_t core_idx{0};
    uint32_t ping_flag{0};
    uint32_t block_size{0};
    uint32_t cube_matrix_size{0};
    uint32_t swizzle_cnt{1};
    uint32_t en_shuffle_k{0};
    uint32_t swizzlDirect{0};

    uint64_t L1_PINGPONG_BUFFER_LEN{0};
    uint32_t L0AB_PINGPONG_BUFFER_LEN{0};
};

#elif defined(__DAV_C220_VEC__)

template <bool TA, bool TB, bool SPLIT_K = false, bool HAVE_BIAS = false, bool IS_INT8 = false, typename InDtype = half,
          typename OutDtype = half, typename BiasDtype = float, typename ScaleDtype = float>
class PpMatmulIntVec {
public:
    __aicore__ PpMatmulIntVec() {};

    __aicore__ __force_inline__ void Init(AscendC::GlobalTensor<int32_t> &gm_c,
                                          AscendC::GlobalTensor<OutDtype> &gm_c_bf16,
                                          AscendC::GlobalTensor<ScaleDtype> &gm_descale,
                                          MLATilingData &mlaParams, uint32_t mode)
    {
        this->gm_c = gm_c;
        this->gm_c_bf16 = gm_c_bf16;
        this->gm_descale = gm_descale;
        if (mode == 0) {
            batch_size = mlaParams.mm1batchSize;
            m = mlaParams.mm1m;
            k = mlaParams.mm1k;
            n = mlaParams.mm1n;
            m0 = mlaParams.mm1m0;
            k0 = mlaParams.mm1k0;
            n0 = mlaParams.mm1n0;
            m_loop = mlaParams.mm1mLoop;
            k_loop = mlaParams.mm1kLoop;
            n_loop = mlaParams.mm1nLoop;
            core_loop = mlaParams.mm1coreLoop;
            swizzle_cnt = mlaParams.mm1swizzleCnt;
            swizzlDirect = mlaParams.mm1swizzlDirect;
            en_shuffle_k = mlaParams.mm1enShuffleK;
        } else {
            batch_size = mlaParams.mm2batchSize;
            m = mlaParams.mm2m;
            k = mlaParams.mm2k;
            n = mlaParams.mm2n;
            m0 = mlaParams.mm2m0;
            k0 = mlaParams.mm2k0;
            n0 = mlaParams.mm2n0;
            m_loop = mlaParams.mm2mLoop;
            k_loop = mlaParams.mm2kLoop;
            n_loop = mlaParams.mm2nLoop;
            core_loop = mlaParams.mm2coreLoop;
            swizzle_cnt = mlaParams.mm2swizzleCnt;
            swizzlDirect = mlaParams.mm2swizzlDirect;
            en_shuffle_k = mlaParams.mm2enShuffleK;
        }
        ub_c = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(0);
        ub_c_tmp = buf.GetBuffer<BufferType::ASCEND_UB, float>(95 * 1024);
        ub_c_bf16 = buf.GetBuffer<BufferType::ASCEND_UB, __bf16>(0);
        ub_scale = buf.GetBuffer<BufferType::ASCEND_UB, float>(190 * 1024);
        block_size = BLOCK_SIZE_32;
        core_num = AscendC::GetBlockNum();
        core_idx = AscendC::GetBlockIdx() / 2;
        ping_flag = 1;
    }

    __aicore__ __force_inline__ void GetBlockIdx(uint32_t index, uint32_t &m_idx, uint32_t &n_idx)
    {
        uint32_t in_batch_idx = index % (m_loop * n_loop);
        if (swizzlDirect == 0) { // Zn
            uint32_t tile_block_loop = (m_loop + swizzle_cnt - 1) / swizzle_cnt;
            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * n_loop);
            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * n_loop);

            uint32_t n_row = swizzle_cnt;
            if (tile_block_idx == tile_block_loop - 1) {
                n_row = m_loop - swizzle_cnt * tile_block_idx;
            }
            m_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
            n_idx = in_tile_block_idx / n_row;
            if (tile_block_idx % 2 != 0) {
                n_idx = n_loop - n_idx - 1;
            }
        } else { // Nz
            uint32_t tile_block_loop = (n_loop + swizzle_cnt - 1) / swizzle_cnt;
            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * m_loop);
            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * m_loop);

            uint32_t n_col = swizzle_cnt;
            if (tile_block_idx == tile_block_loop - 1) {
                n_col = n_loop - swizzle_cnt * tile_block_idx;
            }
            m_idx = in_tile_block_idx / n_col;
            n_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
            if (tile_block_idx % 2 != 0) {
                m_idx = m_loop - m_idx - 1;
            }
        }
    }

    __aicore__ __force_inline__ void run()
    {
        uint32_t m_idx = 0;
        uint32_t n_idx = 0;
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, V, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        for (uint32_t loop_idx = core_idx; loop_idx < core_loop; loop_idx += core_num) {
            GetBlockIdx(loop_idx, m_idx, n_idx);
            uint64_t batch_idx = loop_idx / n_loop / m_loop;
            uint64_t offset_a;

            uint64_t offset_b;
            uint64_t offset_bias;
            uint64_t offset_scalar;
            uint64_t offset_a_next;
            uint64_t offset_b_next;
            uint64_t offset_c = batch_idx * m * n + m_idx * m0 * n + n_idx * n0;
            uint32_t m_actual = (m_idx == (m_loop - 1)) ? (m - m_idx * m0) : m0;
            uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
            uint32_t m_round = 0;
            uint32_t n_round = 0;
            m_round = RoundUp8(m_actual);
            n_round = RoundUp8(n_actual);
            uint32_t n_round16 = RoundUp16(n_actual);
            uint32_t m_actual_per_vec = m_actual / 2;
            if (GetSubBlockidx() != 0) {
                offset_c += m_actual_per_vec * n;
                m_actual_per_vec = m_actual - m_actual_per_vec;
            }

            if (m_actual_per_vec == 0) {
                WaitFlagDev(MMAIC);
                if ((loop_idx / core_num + 1) % MAX_HW_SYNC_COUNTER == 1) {
                    FftsCrossCoreSync<PIPE_MTE3, SYNC_MODE>(MMAIV);
                }
                continue;
            }
            offset_scalar = batch_idx * n + n_idx * n0;
            uint32_t ub_buf_len = RoundUp32(m0 * n0); // <= 64KB
            bool aligned_s32 = (n % 8 == 0);          // 32B aligned
            bool aligned_f16 = (n % 16 == 0);         // 32B aligned
            WAIT_FLAG(V, MTE2, EVENT_ID0);
            if (aligned_s32) {
                gm_to_ub<ArchType::ASCEND_V220, float>(ub_scale, gm_descale[offset_scalar],
                                                       0,                           // sid
                                                       1,                           // nBurst
                                                       n_round * 4 / BLOCK_SIZE_32, // lenBurst
                                                       0,                           // srcStride
                                                       0                            // dstStride
                );
            } else {
                gm_to_ub_align<ArchType::ASCEND_V220, float>(ub_scale, gm_descale[offset_scalar],
                                                             0,                        // sid
                                                             1,                        // nBurst
                                                             n_actual * sizeof(float), // lenBurst
                                                             0,                        // leftPaddingNum
                                                             0,                        // rightPaddingNum
                                                             0,                        // srcGap
                                                             0                         // dstGap
                );
            }
            WaitFlagDev(MMAIC);
            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
            if (aligned_s32) {
                gm_to_ub<ArchType::ASCEND_V220, int32_t>(ub_c, gm_c[offset_c],
                                                         0,                 // sid
                                                         m_actual_per_vec,  // nBurst
                                                         n_round / 8,       // lenBurst
                                                         (n - n_round) / 8, // srcStride
                                                         0                  // dstStride
                );
            } else {
                gm_to_ub_align<ArchType::ASCEND_V220, int32_t>(ub_c, gm_c[offset_c],
                                                               0,                                // sid
                                                               m_actual_per_vec,                 // nBurst
                                                               n_actual * sizeof(int32_t),       // lenBurst
                                                               0,                                // leftPaddingNum
                                                               0,                                // rightPaddingNum
                                                               (n - n_actual) * sizeof(int32_t), // srcGap
                                                               0                                 // dstGap
                );
            }
            SET_FLAG(MTE2, V, EVENT_ID0);
            WAIT_FLAG(MTE2, V, EVENT_ID0);

            WAIT_FLAG(MTE3, V, EVENT_ID0);
            // //  CASTF32 * f32 tf16
            constexpr uint32_t maxRepeat = 255;
            constexpr uint32_t perRepeatNum = maxRepeat * 64;
            uint32_t loopCnt = (m_actual_per_vec * n_actual + perRepeatNum - 1) / perRepeatNum;
            for (uint32_t i = 0; i < loopCnt; i++) {
                conv_v<ArchType::ASCEND_V220, int32_t, float>(ub_c.ReinterpretCast<float>()[perRepeatNum * i],
                                                              ub_c[perRepeatNum * i],
                                                              (uint8_t)maxRepeat, // repeat
                                                              (uint16_t)1,        // dstBlockStride
                                                              (uint16_t)1,        // srcBlockStride
                                                              (uint16_t)8,        // dstRepeatStride
                                                              (uint16_t)8         // srcRepeatStride
                );
            }
            AscendC::PipeBarrier<PIPE_V>();

            uint32_t nRepeatCnt = (n_actual + 64 - 1) / 64;
            for (uint32_t i = 0; i < m_actual_per_vec; ++i) {
                mul_v<ArchType::ASCEND_V220, float>(ub_c_tmp[i * n_round],
                                                    ub_c.ReinterpretCast<float>()[i * n_round],
                                                    ub_scale.ReinterpretCast<float>(),
                                                    (uint8_t)(nRepeatCnt), // repeat
                                                    (uint8_t)1,            // dstBlockStride
                                                    (uint8_t)1,            // src0BlockStride
                                                    (uint8_t)1,            // src1BlockStride
                                                    (uint8_t)8,            // dstRepeatStride
                                                    (uint8_t)8,            // src0RepeatStride
                                                    (uint8_t)8             // src1RepeatStride
                );
            }
            SET_FLAG(V, MTE2, EVENT_ID0);
            AscendC::PipeBarrier<PIPE_V>();
            if (n_actual % 16 > 8) {
                for (uint32_t i = 0; i < loopCnt; i++) {
                    convr_v<ArchType::ASCEND_V220, float, __bf16>(ub_c_bf16[perRepeatNum * i],
                                                                  ub_c_tmp[perRepeatNum * i],
                                                                  (uint8_t)maxRepeat, // repeat
                                                                  (uint16_t)1,        // dstBlockStride
                                                                  (uint16_t)1,        // srcBlockStride
                                                                  (uint16_t)4,        // dstRepeatStride
                                                                  (uint16_t)8         // srcRepeatStride
                    );
                }
            } else {
                for (uint32_t i = 0; i < m_actual_per_vec; i++) {
                    convr_v<ArchType::ASCEND_V220, float, __bf16>(ub_c_bf16[n_round16 * i],
                                                                  ub_c_tmp[n_round * i],
                                                                  (uint8_t)nRepeatCnt, // repeat
                                                                  (uint16_t)1,         // dstBlockStride
                                                                  (uint16_t)1,         // srcBlockStride
                                                                  (uint16_t)4,         // dstRepeatStride
                                                                  (uint16_t)8          // srcRepeatStride
                    );
                }
            }
            SET_FLAG(V, MTE3, EVENT_ID0);
            WAIT_FLAG(V, MTE3, EVENT_ID0);
            if (aligned_f16) {
                ub_to_gm<ArchType::ASCEND_V220, __bf16>(gm_c_bf16[offset_c], ub_c_bf16, 0,
                                                        m_actual_per_vec,  // nBurst
                                                        n_round / 16,      // lenBurst
                                                        0,                 // srcStride
                                                        (n - n_round) / 16 // dstStride
                );
            } else {
                ub_to_gm_align<ArchType::ASCEND_V220, __bf16>(gm_c_bf16[offset_c], ub_c_bf16, 0,
                                                              m_actual_per_vec,             // nBurst
                                                              n_actual * sizeof(half),      // lenBurst
                                                              0,                            // leftPaddingNum
                                                              0,                            // rightPaddingNum
                                                              0,                            // srcGap
                                                              (n - n_actual) * sizeof(half) // dstGap
                );
            }
            SET_FLAG(MTE3, V, EVENT_ID0);
            SET_FLAG(MTE3, MTE2, EVENT_ID0);
            if ((loop_idx / core_num + 1) % MAX_HW_SYNC_COUNTER == 1) {
                FftsCrossCoreSync<PIPE_MTE3, SYNC_MODE>(MMAIV);
            }
        }
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        AscendC::PipeBarrier<PIPE_ALL>();
    }

private:
    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::GlobalTensor<InDtype> gm_a;
    AscendC::GlobalTensor<InDtype> gm_b;
    AscendC::GlobalTensor<BiasDtype> gm_bias;
    AscendC::GlobalTensor<ScaleDtype> gm_descale;
    AscendC::GlobalTensor<int32_t> gm_c;
    AscendC::GlobalTensor<OutDtype> gm_c_bf16;

    AscendC::LocalTensor<int32_t> ub_c = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(0);
    AscendC::LocalTensor<float> ub_c_tmp = buf.GetBuffer<BufferType::ASCEND_UB, float>(0);
    AscendC::LocalTensor<__bf16> ub_c_bf16 = buf.GetBuffer<BufferType::ASCEND_UB, __bf16>(0);
    AscendC::LocalTensor<float> ub_scale = buf.GetBuffer<BufferType::ASCEND_UB, float>(0);

    uint32_t core_num{0};

    uint32_t batch_size{0};
    uint32_t m{0};
    uint32_t k{0};
    uint32_t n{0};

    uint32_t m0{0};
    uint32_t k0{0};
    uint32_t n0{0};

    uint32_t m_loop{0};
    uint32_t n_loop{0};
    uint32_t k_loop{0};
    uint32_t core_loop{0};
    uint32_t core_idx{0};
    uint32_t ping_flag{0};
    uint32_t block_size{0};
    uint32_t cube_matrix_size{0};
    uint32_t swizzle_cnt{1};
    uint32_t en_shuffle_k{0};
    uint32_t swizzlDirect{0};

    uint64_t L1_PINGPONG_BUFFER_LEN{0};
    uint32_t L0AB_PINGPONG_BUFFER_LEN{0};
};
#endif

template<typename IN_DATA_TYPE>
class MLAOperation {
public:
    __aicore__ inline MLAOperation(const MLATilingData &mlaParams_, GM_ADDR tilingGm)
    {
        blockIdx = AscendC::GetBlockIdx();
#ifdef __DAV_C220_VEC__
        sub_block_idx = static_cast<uint64_t>(GetSubBlockidx());
#endif
        vectorBlockIdx = (blockIdx / 2) * 2 + sub_block_idx;
        this->n = mlaParams_.n;
        this->num_core_ = mlaParams_.rmsNumCore1;
        this->num_col_1 = mlaParams_.rmsNumCol1;
        this->num_col_2 = mlaParams_.rmsNumCol2;
        this->num_row = mlaParams_.n;
        this->epsilon_ = 1e-6;
        this->mlaParams = mlaParams_;
    }

    __aicore__ inline void Init(GM_ADDR hiddenStateGm, GM_ADDR gamma1Gm, GM_ADDR beta1Gm, GM_ADDR quantScale1Gm, GM_ADDR quantOffset1Gm, GM_ADDR wdqkvGm, GM_ADDR bias1Gm, GM_ADDR gamma2Gm, GM_ADDR beta2Gm, GM_ADDR quantScale2Gm, GM_ADDR quantOffset2Gm,
                                GM_ADDR gamma3Gm, GM_ADDR sin1Gm, GM_ADDR cos1Gm, GM_ADDR sin2Gm, GM_ADDR cos2Gm, GM_ADDR keycacheGm, GM_ADDR slotMappingGm,
                                GM_ADDR wuqGm, GM_ADDR bias2Gm, GM_ADDR wukGm, GM_ADDR descale1Gm, GM_ADDR descale2Gm, GM_ADDR qGm, GM_ADDR keycacheOutGm, GM_ADDR s1Gm, GM_ADDR s2Gm, GM_ADDR s3Gm, GM_ADDR s4Gm)
    {
        hiddenStateGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(hiddenStateGm));
        gamma1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(gamma1Gm));
        quantScale1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(quantScale1Gm));
        quantOffset1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(quantOffset1Gm));

        wdqkvGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(wdqkvGm));
        gamma2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(gamma2Gm));
        quantScale2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(quantScale2Gm));
        quantOffset2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(quantOffset2Gm));
        gamma3GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(gamma3Gm));
        sin1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(sin1Gm));
        cos1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(cos1Gm));
        sin2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(sin2Gm));
        cos2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(cos2Gm));
        keycacheGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(keycacheOutGm));
        slotMappingGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(slotMappingGm));
        wuqGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(wuqGm));
        wukGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(wukGm));
        descale1gmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(descale1Gm));
        descale2gmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(descale2Gm));
        s1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t *>(s1Gm));
        s2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(s2Gm));
        s3GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(s3Gm));
        s4GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(s4Gm));
        qGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(qGm));
        bias1gmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(bias1Gm));
        bias2gmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(bias2Gm));

        beta1GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(beta1Gm));
        beta2GmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(beta2Gm));
        #ifdef __DAV_C220_CUBE__
            pp_matmul_w8a8_0010.Init(s1GmTensor, wdqkvGmTensor, bias1gmTensor, s4GmTensor, mlaParams, 0);
            pp_matmul_w8a8_00102.Init(s1GmTensor, wuqGmTensor, bias2gmTensor, s4GmTensor, mlaParams, 1);
            mm_ein_sum.Init(s2GmTensor, wukGmTensor, qGmTensor, mlaParams);
        #endif

        #ifdef __DAV_C220_VEC__
            pp_matmul_w8a8_0010.Init(s4GmTensor, s3GmTensor, descale1gmTensor, mlaParams, 0);
            pp_matmul_w8a8_00102.Init(s4GmTensor, s2GmTensor, descale2gmTensor, mlaParams, 1);
            row_work = (num_row + num_core_ - 1) / num_core_;
            row_work_ = 0;
            uint32_t need_core = (num_row + row_work - 1) / row_work;
            if (vectorBlockIdx < need_core - 1) {
                row_work_ = row_work;
            } else if (vectorBlockIdx == need_core - 1) {
                row_work_ = num_row - (need_core - 1) * row_work;
            } else {
                row_work_ = 0;
            }
            this->splitN = mlaParams.perTaskNum;
            rmsNormQuant1.Init(gamma1GmTensor,
                           beta1GmTensor,
                           quantScale1GmTensor,
                           quantOffset1GmTensor,
                           hiddenStateGmTensor,
                           s1GmTensor,
                           0,
                           num_col_1,
                           0.0001395089285,
                           vectorBlockIdx * static_cast<uint64_t>(row_work) * num_col_1,
                           vectorBlockIdx * static_cast<uint64_t>(row_work) * num_col_1,
                           row_work_,
                           mlaParams);
            rmsNormQuant2.Init(gamma2GmTensor,
                           beta2GmTensor,
                           quantScale2GmTensor,
                           quantOffset2GmTensor,
                           s3GmTensor,
                           s1GmTensor,
                           SPLIT_SIZE_ONE,
                           num_col_2,
                           0.000651041666,
                           vectorBlockIdx * static_cast<uint64_t>(row_work) * num_col_2,
                           vectorBlockIdx * static_cast<uint64_t>(row_work) * SPLIT_SIZE_TWO,
                           row_work_,
                           mlaParams);
            ropeFp16.RopeInit(s2GmTensor, cos2GmTensor, sin2GmTensor, qGmTensor, mlaParams);
            ubTensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(0);
            ub8Tensor = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(0);
            ub32Tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(0);
        #endif
    }
    
    __aicore__ inline void ProcessCube();

    __aicore__ inline void ProcessVector();

private:
    template <class T1>
    __aicore__ inline void RmsNormAndRopeConvergence1(const AscendC::LocalTensor<T1>& srcTensor, const AscendC::LocalTensor<T1>& gammaTensor,
                                                      const AscendC::LocalTensor<T1>& sinTensor, const AscendC::LocalTensor<T1>& cosTensor,
                                                      const AscendC::LocalTensor<int32_t>& slotMappingTensor, const uint32_t sN,
                                                      const AscendC::LocalTensor<float>& rmsNormTensor, const AscendC::LocalTensor<float>& gammaFp32,
                                                      const AscendC::LocalTensor<float>& ropeKTensor, const AscendC::LocalTensor<float>& ropeKRevertTensor,
                                                      const AscendC::LocalTensor<float>& calTensor,
                                                      const AscendC::LocalTensor<T1>& outTmpTensor)
    {
        AscendC::DataCopy(
            gammaTensor, gamma3GmTensor, AscendC::DataCopyParams(1, SPLIT_RMSNRORM_SIZE_ONE / BLOCK_SIZE_16, 0, 0));
        SET_FLAG(MTE2, V, EVENT_ID1);
        WAIT_FLAG(MTE2, V, EVENT_ID1);
        Cast(gammaFp32, gammaTensor, AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_ONE);
        AscendC::DataCopyPad(slotMappingTensor, slotMappingGmTensor, AscendC::DataCopyExtParams(1, n * 4, 0, 0, 0),
                             AscendC::DataCopyPadExtParams<int32_t>(false, 0, 8 - n % 8, 0));
        SET_FLAG(MTE2, V, EVENT_ID2);
        WAIT_FLAG(MTE2, V, EVENT_ID2);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        for(uint64_t loop = 0; loop < sN; ++loop){
            uint64_t offset = vectorBlockIdx * static_cast<uint64_t>(row_work) * num_col_2 + loop * MM1_OUT_SIZE;
            WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
            AscendC::DataCopy(
                srcTensor, s3GmTensor[offset], AscendC::DataCopyParams(1, MM1_OUT_SIZE / BLOCK_SIZE_16, 0, 0));
            AscendC::DataCopy(sinTensor,
                              sin1GmTensor[(row_work * vectorBlockIdx + loop) * SPLIT_RMSNRORM_SIZE_TWO],
                              AscendC::DataCopyParams(1, SPLIT_RMSNRORM_SIZE_TWO / BLOCK_SIZE_16, 0, 0));
            AscendC::DataCopy(cosTensor,
                              cos1GmTensor[(row_work * vectorBlockIdx + loop) * SPLIT_RMSNRORM_SIZE_TWO],
                              AscendC::DataCopyParams(1, SPLIT_RMSNRORM_SIZE_TWO / BLOCK_SIZE_16, 0, 0));
            SET_FLAG(MTE2, V, EVENT_ID0);
            SET_FLAG(V, S, EVENT_ID0);
            WAIT_FLAG(V, S, EVENT_ID0);
            int64_t slotValue = static_cast<int64_t>(slotMappingTensor.GetValue(loop + vectorBlockIdx * row_work));
            uint64_t cacheStart = static_cast<uint64_t>(slotValue) * static_cast<uint64_t>(SPLIT_SIZE_ONE);
            SET_FLAG(S, MTE3, EVENT_ID0);
            /* RmsNorm start */
            WAIT_FLAG(MTE2, V, EVENT_ID0);
            Cast(rmsNormTensor, srcTensor, AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_ONE);
            AscendC::PipeBarrier<PIPE_V>();
            Mul(calTensor, rmsNormTensor, rmsNormTensor, SPLIT_RMSNRORM_SIZE_ONE);
            AscendC::PipeBarrier<PIPE_V>();
            ReduceSumCustom(calTensor[SPLIT_RMSNRORM_SIZE_ONE], calTensor, calTensor[SPLIT_RMSNRORM_SIZE_ONE * 2], SPLIT_RMSNRORM_SIZE_ONE);
            SET_FLAG(V, S, EVENT_ID1);
            WAIT_FLAG(V, S, EVENT_ID1);
            float rms = sqrt(calTensor.GetValue(SPLIT_RMSNRORM_SIZE_ONE) / SPLIT_RMSNRORM_SIZE_ONE + epsilon_);
            SET_FLAG(S, V, EVENT_ID1);
            WAIT_FLAG(S, V, EVENT_ID1);
            AscendC::PipeBarrier<PIPE_V>();
            Duplicate(calTensor, rms, SPLIT_RMSNRORM_SIZE_ONE);
            AscendC::PipeBarrier<PIPE_V>();
            Div(calTensor, rmsNormTensor, calTensor, SPLIT_RMSNRORM_SIZE_ONE);
            AscendC::PipeBarrier<PIPE_V>();
            Mul(rmsNormTensor, gammaFp32, calTensor, SPLIT_RMSNRORM_SIZE_ONE);
            AscendC::PipeBarrier<PIPE_V>();
            if (std::is_same<T1, __bf16>::value) {
                Cast(outTmpTensor, rmsNormTensor, AscendC::RoundMode::CAST_RINT, SPLIT_RMSNRORM_SIZE_ONE);
            } else {
                Cast(outTmpTensor, rmsNormTensor, AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_ONE);
            }
            AscendC::PipeBarrier<PIPE_V>();
             /* RmsNorm end */
            // /* Rope K start */
            uint64_t revertOffset = SPLIT_RMSNRORM_SIZE_TWO / 2;
            Cast(ropeKTensor, srcTensor[SPLIT_RMSNRORM_SIZE_ONE], AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_TWO);
            Cast(ropeKRevertTensor[revertOffset], srcTensor[SPLIT_RMSNRORM_SIZE_ONE], AscendC::RoundMode::CAST_NONE, revertOffset);
            Cast(ropeKRevertTensor, srcTensor[SPLIT_RMSNRORM_SIZE_ONE + revertOffset], AscendC::RoundMode::CAST_NONE, revertOffset);
            Duplicate(calTensor, static_cast<float>(-1), revertOffset);
            Duplicate(calTensor[revertOffset], static_cast<float>(1), revertOffset);
            AscendC::PipeBarrier<PIPE_V>();
            Cast(calTensor[SPLIT_RMSNRORM_SIZE_TWO], cosTensor, AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_TWO);
            Cast(calTensor[SPLIT_RMSNRORM_SIZE_TWO * 2], sinTensor, AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_TWO);
            AscendC::PipeBarrier<PIPE_V>();
            Mul(ropeKTensor, calTensor[SPLIT_RMSNRORM_SIZE_TWO], ropeKTensor, SPLIT_RMSNRORM_SIZE_TWO);
            Mul(ropeKRevertTensor, calTensor[SPLIT_RMSNRORM_SIZE_TWO * 2], ropeKRevertTensor, SPLIT_RMSNRORM_SIZE_TWO);
            AscendC::PipeBarrier<PIPE_V>();
            Mul(ropeKRevertTensor, calTensor, ropeKRevertTensor, SPLIT_RMSNRORM_SIZE_TWO);
            AscendC::PipeBarrier<PIPE_V>();
            Add(ropeKRevertTensor, ropeKTensor, ropeKRevertTensor, SPLIT_RMSNRORM_SIZE_TWO);
            AscendC::PipeBarrier<PIPE_V>();
            if (std::is_same<T1, __bf16>::value) {
                Cast(outTmpTensor[SPLIT_RMSNRORM_SIZE_ONE], ropeKRevertTensor, AscendC::RoundMode::CAST_RINT, SPLIT_RMSNRORM_SIZE_TWO);
            } else {
                Cast(outTmpTensor[SPLIT_RMSNRORM_SIZE_ONE], ropeKRevertTensor, AscendC::RoundMode::CAST_NONE, SPLIT_RMSNRORM_SIZE_TWO);
            }
            AscendC::PipeBarrier<PIPE_V>();
            /* Rope K end */
            SET_FLAG(V, MTE3, EVENT_ID0);
            WAIT_FLAG(V, MTE3, EVENT_ID0);
            WAIT_FLAG(S, MTE3, EVENT_ID0);
            DataCopy(keycacheGmTensor[cacheStart], outTmpTensor, SPLIT_SIZE_ONE);
            SET_FLAG(MTE3, MTE2, EVENT_ID1);
        }
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
    }

private:
    uint32_t n;
    uint32_t splitN;
    uint32_t rotaryCoeff;
    uint32_t blockIdx;
    uint32_t sub_block_idx;
    uint32_t vectorBlockIdx;
    uint32_t blockOffset;
    uint32_t perTaskNum;
    uint32_t resTaskNum;
    MLATilingData mlaParams;

    uint32_t num_core_;
    uint32_t num_col_1;
    uint32_t num_col_2;
    float epsilon_;
    uint32_t num_row;
    uint32_t quantMin_;
    uint32_t row_work;
    uint32_t row_work_;
    
    AsdopsBuffer<ArchType::ASCEND_V220> buf;
    AscendC::LocalTensor<IN_DATA_TYPE> ubTensor;
    AscendC::LocalTensor<int8_t> ub8Tensor;
    AscendC::LocalTensor<float> ub32Tensor;

    AscendC::GlobalTensor<IN_DATA_TYPE> hiddenStateGmTensor;

    AscendC::GlobalTensor<IN_DATA_TYPE> gamma1GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> quantScale1GmTensor;
    AscendC::GlobalTensor<int8_t> quantOffset1GmTensor;

    AscendC::GlobalTensor<int8_t> wdqkvGmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> gamma2GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> quantScale2GmTensor;
    AscendC::GlobalTensor<int8_t> quantOffset2GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> gamma3GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> sin1GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> cos1GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> sin2GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> cos2GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> keycacheGmTensor;
    AscendC::GlobalTensor<int32_t> slotMappingGmTensor;
    AscendC::GlobalTensor<int8_t> wuqGmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> wukGmTensor;

    AscendC::GlobalTensor<IN_DATA_TYPE> qGmTensor;
    AscendC::GlobalTensor<int8_t> s1GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> s2GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> s3GmTensor;
    AscendC::GlobalTensor<int32_t> s4GmTensor;
    AscendC::GlobalTensor<float> descale1gmTensor;
    AscendC::GlobalTensor<float> descale2gmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> beta1GmTensor;
    AscendC::GlobalTensor<IN_DATA_TYPE> beta2GmTensor;

    AscendC::GlobalTensor<int32_t> bias1gmTensor;
    AscendC::GlobalTensor<int32_t> bias2gmTensor;

#ifdef __DAV_C220_CUBE__
    PpMatmulInt<false, true, false, true, true, int8_t, int32_t, int32_t> pp_matmul_w8a8_0010;
    PpMatmulInt<false, true, false, true, true, int8_t, int32_t, int32_t> pp_matmul_w8a8_00102;
    PpMatmulEinSum<false, 0> mm_ein_sum;
#endif

#ifdef __DAV_C220_VEC__
    PpMatmulIntVec<false, true, false, true, true, int8_t, IN_DATA_TYPE, int32_t> pp_matmul_w8a8_0010;
    PpMatmulIntVec<false, true, false, true, true, int8_t, IN_DATA_TYPE, int32_t> pp_matmul_w8a8_00102;
    RmsNormQuant<IN_DATA_TYPE, true, false> rmsNormQuant1;
    RmsNormQuant<IN_DATA_TYPE, true, false> rmsNormQuant2;
    RopeFp16<IN_DATA_TYPE, IN_DATA_TYPE> ropeFp16;
#endif
};

template<typename IN_DATA_TYPE>
__aicore__ inline void MLAOperation<IN_DATA_TYPE>::ProcessCube()
{
#ifdef __DAV_C220_CUBE__
    PIPE_BARRIER(ALL);
    WaitFlagDev(MTE2WAIT);
    pp_matmul_w8a8_0010.PreloadB();
    WaitFlagDev(MM1);
    pp_matmul_w8a8_0010.run();
    
    WaitFlagDev(MTE2WAIT);
    pp_matmul_w8a8_00102.PreloadB();
    WaitFlagDev(MM2QUANT);
    pp_matmul_w8a8_00102.run();
    WaitFlagDev(ROPEPRESYNC);
    mm_ein_sum.Process();
#endif
}

template<typename IN_DATA_TYPE>
__aicore__ inline void MLAOperation<IN_DATA_TYPE>::ProcessVector()
{
    #ifdef __DAV_C220_VEC__
        PIPE_BARRIER(ALL);
        if(row_work_ != 0){
            uint32_t gm_offset_ = static_cast<uint64_t>(row_work) * num_col_1;
            uint32_t num_col_align_int8 = (num_col_1 + REPEAT_TIME_256 - 1) / REPEAT_TIME_256 * REPEAT_TIME_256;
            uint32_t num_col_align_f16 = (num_col_1 + REPEAT_TIME_128 - 1) / REPEAT_TIME_128 * REPEAT_TIME_128;
            uint32_t num_col_align_f32 = (num_col_1 + REPEAT_TIME_64 - 1) / REPEAT_TIME_64 * REPEAT_TIME_64;
            AscendC::LocalTensor<IN_DATA_TYPE> input_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(0);
            AscendC::LocalTensor<IN_DATA_TYPE> gamma_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(HIDDTEN_STATE * 2);
            AscendC::LocalTensor<IN_DATA_TYPE> beta_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2);
            AscendC::LocalTensor<IN_DATA_TYPE> scale_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2);
            AscendC::LocalTensor<int8_t> offset_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + 32);
            AscendC::LocalTensor<float> res1_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + 64);
            AscendC::LocalTensor<float> res3_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + 64 + num_col_align_f32 * 4);
            AscendC::LocalTensor<int8_t> output_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + HIDDTEN_STATE * 2 + 64 + num_col_align_f32 * 4
                                                                                                  + BUF_FACTOR * num_col_align_f32 * 4 + 32);
            rmsNormQuant1.Launch(output_tensor,
                                    input_tensor,
                                    gamma_tensor,
                                    beta_tensor,
                                    scale_tensor,
                                    offset_tensor,
                                    res1_tensor,
                                    res3_tensor);
        } else {
            AscendC::CrossCoreSetFlag<0x2, PIPE_MTE2>(MTE2WAIT);
        }
        PIPE_BARRIER(ALL);
        FftsCrossCoreSync<PIPE_MTE3, 0>(RMSNORMQUANT1);
        WaitFlagDev(RMSNORMQUANT1);
        AscendC::CrossCoreSetFlag<0x2, PIPE_MTE3>(MM1);
        pp_matmul_w8a8_0010.run();
        FftsCrossCoreSync<PIPE_MTE3, 0>(RMSNORMQUANT2);
        WaitFlagDev(RMSNORMQUANT2);
        if (row_work_ != 0) {
            uint32_t num_col_align_int8 = (num_col_2 + REPEAT_TIME_256 - 1) / REPEAT_TIME_256 * REPEAT_TIME_256;
            uint32_t num_col_align_f16 = (num_col_2 + REPEAT_TIME_128 - 1) / REPEAT_TIME_128 * REPEAT_TIME_128;
            uint32_t num_col_align_f32 = (num_col_2 + REPEAT_TIME_64 - 1) / REPEAT_TIME_64 * REPEAT_TIME_64;
            AscendC::LocalTensor<IN_DATA_TYPE> input_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(0);
            AscendC::LocalTensor<IN_DATA_TYPE> gamma_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2);
            AscendC::LocalTensor<IN_DATA_TYPE> beta_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2 + SPLIT_SIZE_TWO * 2);
            AscendC::LocalTensor<IN_DATA_TYPE> scale_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2 + SPLIT_SIZE_TWO * 2 + SPLIT_SIZE_TWO * 2);
            AscendC::LocalTensor<int8_t> offset_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(MM1_OUT_SIZE * 2 + SPLIT_SIZE_TWO * 2 + SPLIT_SIZE_TWO * 2 + 32);
            AscendC::LocalTensor<float> res1_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(MM1_OUT_SIZE * 2 + SPLIT_SIZE_TWO * 2 + SPLIT_SIZE_TWO * 2 + 64);
            AscendC::LocalTensor<float> res3_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(MM1_OUT_SIZE * 2 + SPLIT_SIZE_TWO * 2 + SPLIT_SIZE_TWO * 2 + 64 + num_col_align_f32 * 4);
            AscendC::LocalTensor<int8_t> output_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(MM1_OUT_SIZE * 2 + SPLIT_SIZE_TWO * 2 + SPLIT_SIZE_TWO * 2 + 64 + num_col_align_f32 * 4
                                                                                                    + BUF_FACTOR * num_col_align_f32 * 4 + 32);
            rmsNormQuant2.Launch(output_tensor,
                                    input_tensor,
                                    gamma_tensor,
                                    beta_tensor,
                                    scale_tensor,
                                    offset_tensor,
                                    res1_tensor,
                                    res3_tensor);
        } else {
            AscendC::CrossCoreSetFlag<0x2, PIPE_MTE2>(MTE2WAIT);
        }
        PIPE_BARRIER(ALL);
        FftsCrossCoreSync<PIPE_MTE3, 0>(MM2);
        WaitFlagDev(MM2);
        AscendC::CrossCoreSetFlag<0x2, PIPE_MTE3>(MM2QUANT);
        
        if(row_work_ != 0) {
            AscendC::LocalTensor<IN_DATA_TYPE> input_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(0);
            AscendC::LocalTensor<IN_DATA_TYPE> gamma_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2);
            AscendC::LocalTensor<IN_DATA_TYPE> sin_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2 + SPLIT_RMSNRORM_SIZE_ONE * 2);
            AscendC::LocalTensor<IN_DATA_TYPE> cos_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2 + SPLIT_RMSNRORM_SIZE_ONE * 2 + row_work_ * SPLIT_RMSNRORM_SIZE_TWO * 2);
            AscendC::LocalTensor<int32_t> slotMapping_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(MM1_OUT_SIZE * 2 + SPLIT_RMSNRORM_SIZE_ONE * 2 + row_work_ * SPLIT_RMSNRORM_SIZE_TWO * 4);
            AscendC::LocalTensor<float> tmp32_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(MM1_OUT_SIZE * 2 + SPLIT_RMSNRORM_SIZE_ONE * 2 + row_work_ * SPLIT_RMSNRORM_SIZE_TWO * 4 + 4096 * 32);
            AscendC::LocalTensor<IN_DATA_TYPE> temp_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(MM1_OUT_SIZE * 2 + SPLIT_RMSNRORM_SIZE_ONE * 2 + row_work_ * SPLIT_RMSNRORM_SIZE_TWO * 4 + 4096 * 32 +
                                                                                                SPLIT_RMSNRORM_SIZE_ONE * 3 * 4 + SPLIT_RMSNRORM_SIZE_TWO * 2 * 4);
            RmsNormAndRopeConvergence1<IN_DATA_TYPE>(input_tensor,// n * 576
                                            gamma_tensor,// gamma
                                            sin_tensor,// sin
                                            cos_tensor,// cons
                                            slotMapping_tensor,// slotMapping
                                            row_work_, tmp32_tensor, tmp32_tensor[SPLIT_RMSNRORM_SIZE_ONE], tmp32_tensor[SPLIT_RMSNRORM_SIZE_ONE + SPLIT_RMSNRORM_SIZE_ONE],
                                            tmp32_tensor[SPLIT_RMSNRORM_SIZE_ONE + SPLIT_RMSNRORM_SIZE_ONE + SPLIT_RMSNRORM_SIZE_TWO],
                                            tmp32_tensor[SPLIT_RMSNRORM_SIZE_ONE + SPLIT_RMSNRORM_SIZE_ONE + SPLIT_RMSNRORM_SIZE_TWO + SPLIT_RMSNRORM_SIZE_TWO],
                                            temp_tensor);
        }
        PIPE_BARRIER(ALL);
        pp_matmul_w8a8_00102.run();
        FftsCrossCoreSync<PIPE_MTE3, 0>(MM2OUT);
        WaitFlagDev(MM2OUT);
        AscendC::CrossCoreSetFlag<0x2, PIPE_MTE3>(ROPEPRESYNC);
        ropeFp16.Process();
        PIPE_BARRIER(ALL);
    #endif
}
template <class T> __aicore__ inline void InitPrivateTilingData(const __gm__ uint8_t *tiling, T *const_data)
{
    const __gm__ uint32_t *src = (const __gm__ uint32_t *)tiling;
    uint32_t *dst = (uint32_t *)const_data;
    for (auto i = 0; i < sizeof(T) / 4; i++) {
        *(dst + i) = *(src + i);
    }
}

extern "C" __global__ __aicore__ void MLA_preprocess_attention_bf16(GM_ADDR sync, GM_ADDR hiddenStateGm, GM_ADDR gamma1Gm, GM_ADDR beta1Gm, GM_ADDR quantScale1Gm, GM_ADDR quantOffset1Gm,
                                                    GM_ADDR wdqkvGm, GM_ADDR bias1Gm, GM_ADDR gamma2Gm, GM_ADDR beta2Gm, GM_ADDR quantScale2Gm, GM_ADDR quantOffset2Gm,
                                                    GM_ADDR gamma3Gm, GM_ADDR sin1Gm, GM_ADDR cos1Gm, GM_ADDR sin2Gm, GM_ADDR cos2Gm, GM_ADDR keycacheGm, GM_ADDR slotMappingGm,
                                                    GM_ADDR wuqGm, GM_ADDR bias2Gm, GM_ADDR wukGm, GM_ADDR descale1Gm, GM_ADDR descale2Gm,
                                                    GM_ADDR qGm, GM_ADDR keycacheOutGm, GM_ADDR s1Gm, GM_ADDR s2Gm, GM_ADDR s3Gm, GM_ADDR s4Gm,
                                                    GM_ADDR tilingGm)
{
    SetFftsBaseAddr((unsigned long)sync);
    SetAtomicnone();
    SetMasknorm();
    #ifdef __DAV_C220_CUBE__
        SetPadding<uint64_t>((uint64_t)0);
        SetNdpara(1, 0, 0);
    #endif
    MLATilingData mlaTilingData;

    mlaTilingData.perTaskNum = (uint32_t)(*((__gm__ int32_t *)tilingGm + 43));
    mlaTilingData.resTaskNum = (uint32_t)(*((__gm__ int32_t *)tilingGm + 44));
    mlaTilingData.numCore = (uint32_t)(*((__gm__ int32_t *)tilingGm + 45));
    mlaTilingData.n = (uint32_t)(*((__gm__ int32_t *)tilingGm + 0));
    
    mlaTilingData.mm1batchSize = (uint32_t)(*((__gm__ int32_t *)tilingGm + 1));
    mlaTilingData.mm1m = (uint32_t)(*((__gm__ int32_t *)tilingGm + 2));
    mlaTilingData.mm1k = (uint32_t)(*((__gm__ int32_t *)tilingGm + 3));
    mlaTilingData.mm1n = (uint32_t)(*((__gm__ int32_t *)tilingGm + 4));
    mlaTilingData.mm1m0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 5));
    mlaTilingData.mm1k0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 6));
    mlaTilingData.mm1n0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 7));
    mlaTilingData.mm1mLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 8));
    mlaTilingData.mm1kLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 9));
    mlaTilingData.mm1nLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 10));
    mlaTilingData.mm1coreLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 11));
    mlaTilingData.mm1swizzleCnt = (uint32_t)(*((__gm__ int32_t *)tilingGm + 12));
    mlaTilingData.mm1enShuffleK = (uint32_t)(*((__gm__ int32_t *)tilingGm + 12));
    mlaTilingData.mm1blockDim = (uint32_t)(*((__gm__ int32_t *)tilingGm + 14));

    mlaTilingData.mm2batchSize = (uint32_t)(*((__gm__ int32_t *)tilingGm + 15));
    mlaTilingData.mm2m = (uint32_t)(*((__gm__ int32_t *)tilingGm + 16));
    mlaTilingData.mm2k = (uint32_t)(*((__gm__ int32_t *)tilingGm + 17));
    mlaTilingData.mm2n = (uint32_t)(*((__gm__ int32_t *)tilingGm + 18));
    mlaTilingData.mm2m0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 19));
    mlaTilingData.mm2k0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 20));
    mlaTilingData.mm2n0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 21));
    mlaTilingData.mm2mLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 22));
    mlaTilingData.mm2kLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 23));
    mlaTilingData.mm2nLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 24));
    mlaTilingData.mm2coreLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 25));
    mlaTilingData.mm2swizzleCnt = (uint32_t)(*((__gm__ int32_t *)tilingGm + 26));
    mlaTilingData.mm2enShuffleK = (uint32_t)(*((__gm__ int32_t *)tilingGm + 27));
    mlaTilingData.mm2blockDim = (uint32_t)(*((__gm__ int32_t *)tilingGm + 28));

    mlaTilingData.mm3batchSize = (uint32_t)(*((__gm__ int32_t *)tilingGm + 29));
    mlaTilingData.mm3m = (uint32_t)(*((__gm__ int32_t *)tilingGm + 30));
    mlaTilingData.mm3k = (uint32_t)(*((__gm__ int32_t *)tilingGm + 31));
    mlaTilingData.mm3n = (uint32_t)(*((__gm__ int32_t *)tilingGm + 32));
    mlaTilingData.mm3m0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 33));
    mlaTilingData.mm3k0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 34));
    mlaTilingData.mm3n0 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 35));
    mlaTilingData.mm3mLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 36));
    mlaTilingData.mm3kLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 37));
    mlaTilingData.mm3nLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 38));
    mlaTilingData.mm3coreLoop = (uint32_t)(*((__gm__ int32_t *)tilingGm + 39));
    mlaTilingData.mm3swizzleCnt = (uint32_t)(*((__gm__ int32_t *)tilingGm + 40));
    mlaTilingData.mm3enShuffleK = (uint32_t)(*((__gm__ int32_t *)tilingGm + 41));
    mlaTilingData.mm3blockDim = (uint32_t)(*((__gm__ int32_t *)tilingGm + 42));

    mlaTilingData.rmsNumCore1 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 46));
    mlaTilingData.rmsNumCol1 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 47));
    mlaTilingData.rmsNumCore2 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 48));
    mlaTilingData.rmsNumCol2 = (uint32_t)(*((__gm__ int32_t *)tilingGm + 49));

    mlaTilingData.hiddenSizeQ = (uint32_t)(*((__gm__ int32_t *)tilingGm + 50));
    mlaTilingData.headNumQ = (uint32_t)(*((__gm__ int32_t *)tilingGm + 51));
    mlaTilingData.headDim = (uint32_t)(*((__gm__ int32_t *)tilingGm + 52));
    mlaTilingData.concatSize = (uint32_t)(*((__gm__ int32_t *)tilingGm + 53));
    mlaTilingData.rotaryCoeff = (uint32_t)(*((__gm__ int32_t *)tilingGm + 54));
    mlaTilingData.ntokens = (uint32_t)(*((__gm__ int32_t *)tilingGm + 55));
    mlaTilingData.realCore = (uint32_t)(*((__gm__ int32_t *)tilingGm + 56));
    mlaTilingData.nlCoreRun = (uint32_t)(*((__gm__ int32_t *)tilingGm + 57));
    mlaTilingData.lCoreRun = (uint32_t)(*((__gm__ int32_t *)tilingGm + 58));
    mlaTilingData.maxNPerLoopForUb = (uint32_t)(*((__gm__ int32_t *)tilingGm + 59));
    mlaTilingData.preCoreLoopTime = (uint32_t)(*((__gm__ int32_t *)tilingGm + 60));
    mlaTilingData.preCoreLoopNLast = (uint32_t)(*((__gm__ int32_t *)tilingGm + 61));
    mlaTilingData.lastCoreLoopTime = (uint32_t)(*((__gm__ int32_t *)tilingGm + 62));
    mlaTilingData.lastCoreLoopNLast = (uint32_t)(*((__gm__ int32_t *)tilingGm + 63));

    MLAOperation<__bf16> op(mlaTilingData, tilingGm);
    op.Init(hiddenStateGm, gamma1Gm, beta1Gm, quantScale1Gm, quantOffset1Gm, wdqkvGm, bias1Gm, gamma2Gm, beta2Gm, quantScale2Gm, quantOffset2Gm, 
            gamma3Gm, sin1Gm, cos1Gm, sin2Gm, cos2Gm, keycacheGm, slotMappingGm,
            wuqGm, bias2Gm, wukGm, descale1Gm, descale2Gm, qGm, keycacheOutGm, s1Gm, s2Gm, s3Gm, s4Gm);
#ifdef __DAV_C220_CUBE__
    op.ProcessCube();
#elif __DAV_C220_VEC__
    op.ProcessVector();
#endif
}