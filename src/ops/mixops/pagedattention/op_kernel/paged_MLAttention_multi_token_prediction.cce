/*
* Copyright (c) 2024 Huawei Technologies Co., Ltd.
* This file is a part of the CANN Open Software.
* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#include "mixops/include/common.h"
#include "mixops/include/common_func.h"
#include "mixops/include/simd.h"
#include "mixops/include/iterator.h"
#include "mixops/include/mma.h"
#include "mixops/include/utils.h"
#include "kernel_operator.h"

#ifdef __CCE_KT_TEST__
#define __aicore__
#else
#define __aicore__ [aicore]
#endif

// FFTS Flag
constexpr int32_t QK_READY = 1;
constexpr int32_t SOFTMAX_READY = 2;
constexpr int32_t UPDATE_READY = 3;
constexpr int32_t BIT_SHIFT = 8;
constexpr int32_t SOFTMAX_MAX_LENGTH = 256;
constexpr int32_t LOCAL_SIZE = 6;

const int32_t TILING_BATCH = 0;
const int32_t TILING_NUMHEADS = 1;
const int32_t TILING_HEADDIM = 2;
const int32_t TILING_NUMBLOKS = 3;
const int32_t TILING_BLOCKSIZE = 4;
const int32_t TILING_MAXBLOCKS = 5;
const int32_t TILING_TOR = 6;
const int32_t TILING_KVHEADS = 7;
const int32_t TILING_FORMER_BATCH = 8;
const int32_t TILING_FORMER_HEAD = 9;
const int32_t TILING_TAIL_BATCH = 10;
const int32_t TILING_TAIL_HEAD = 11;
const int32_t TILING_HEADNUM_MOVE = 12;
const int32_t TILING_MASK_MAX_LEN = 13;
const int32_t TILING_BATCH_STRIDE = 14;
const int32_t TILING_HEAD_STRIDE = 15;
const int32_t TILING_KEY = 16;
const int32_t TILING_HEADSIZE = 17;
const int32_t TILING_PARASIZE = 18;
const int32_t TILING_GROUPNUM = 19;
const int32_t TILING_FORMER_GROUP_MOVE = 20;
const int32_t TILING_TAIL_GROUP_MOVE = 21;
const int32_t TILING_MAX_KVSEQLEN = 22;
const int32_t TILING_KVSPLIT = 23;
const int32_t TILING_KVCORENUM = 24;
const int32_t TILING_BLOCKSIZE_CALC = 25;
const int32_t TILING_TOTAL_BLOCK_NUM = 26;
const int32_t TILING_PREFILL_BS = 27;
const int32_t TILING_DECODER_BS = 28;
const int32_t TILING_HEADDIM_V = 29;
const int32_t TILING_MODCOEF = 30;
const int32_t TILING_DIVCOEF = 31;
const int32_t TILING_QHEADORIGINAL = 32;
const int32_t TILING_COMPRESSHEAD = 33;
const int32_t TILING_QUANTYPE = 34;
const int32_t TILING_DATA_SHAPE_TYPE = 35;
const int32_t TILING_SCALETYPE = 36;
const int32_t TILING_MASK_TYPE_ND = 37;
const int32_t TILING_MTP_HEAD_SPLIT_SIZE = 42;
const int32_t TILING_MTP_HEAD_SPLIT_NUM = 43;

enum class CalcMode {
    CALC_MODE_256 = 0,
    CALC_MODE_576 = 1,
};

enum class FuncType {
    MULTI_TOKEN_PREDICTION = 0,
    UNEQUAL_QK_LEN_PREFILL = 1,
};

#ifdef __DAV_C220_CUBE__
constexpr int32_t L0AB_HALF_BUF_SIZE = 16384;  // 128 * 128
constexpr int32_t BLOCK_SIZE = 16;
constexpr int32_t CUBE_MATRIX_SIZE = 256;         // 16 * 16
constexpr int32_t L0AB_UINT8_BLOCK_SIZE = 32768;  // 128 * 128 * 2B
constexpr int32_t TMP_SIZE = 32768 * 4;
constexpr int32_t TMP_SIZEW = 32768;
constexpr int32_t BLOCK_EMBED = 256;
constexpr int32_t BLOCK_QK = 128;

template<typename mm1InputType, typename mm2InputType, typename mmOutputType, CalcMode CUBE_MODE, typename HighPrecisionMode, FuncType WorkMode>
class PagedMLAMultiTokenPredictionAic {
public:
    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint32_t BlockSize()
    {
        return 32 / sizeof(T);
    }

   template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint32_t MatrixSize()
    {
        return 512 / sizeof(T);
    } 

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t BlockSizeRoundUp(uint64_t num)
    {
        return (num + BlockSize<T>() - 1) / BlockSize<T>() * BlockSize<T>();
    }

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t NumBlocksRoundUp(uint64_t num)
    {
        return (num + BlockSize<T>() - 1) / BlockSize<T>();
    }

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t MatrixSizeRoundUp(uint64_t num)
    {
        return (num + MatrixSize<T>() - 1) / MatrixSize<T>() * MatrixSize<T>();
    }

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t NumMatrixsRoundUp(uint64_t num)
    {
        return (num + MatrixSize<T>() - 1) / MatrixSize<T>();
    }
    
    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t L0HalfSize()
    {
        return 32 * 1024 / sizeof(T);
    }

    __aicore__ __attribute__((always_inline)) inline PagedMLAMultiTokenPredictionAic(
        __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ q_gm, __gm__ uint8_t *__restrict__ kv_gm,
        __gm__ uint8_t *__restrict__ mask_gm, __gm__ uint8_t *__restrict__ block_tables_gm, 
        __gm__ uint8_t *__restrict__ o_gm, __gm__ uint8_t *__restrict__ s_gm,
        __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
        __gm__ uint8_t *__restrict__ upo_tmp_gm, __gm__ uint8_t *__restrict__ tiling_para_gm) :
        q_gm(q_gm), kv_gm(kv_gm), block_tables_gm(block_tables_gm),
        s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm), upo_tmp_gm(upo_tmp_gm), 
        tiling_para_gm(tiling_para_gm)
    {
        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_NUMHEADS));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM));
        this->embdv = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM_V));
        this->kv_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_KVHEADS));
        this->max_num_blocks_per_query = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MAXBLOCKS));
        this->block_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_TOTAL_BLOCK_NUM));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        this->max_kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MAX_KVSEQLEN));
        this->mask_type = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MASK_TYPE_ND));
        this->cur_qN_blk_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_SIZE));
        this->cur_qN_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_NUM));
	    this->embed_round = RoundUp<uint64_t>(embd, BlockSize<mm1InputType>());
        this->group_num = q_heads / kv_heads;
        this->stride_qo = q_heads * embd;
        this->stride_k = kv_heads * embd;
        this->batch_stride_k = batch_size * max_kv_seqlen * stride_k * 2;

        if constexpr (CUBE_MODE == CalcMode::CALC_MODE_256) {
            const uint32_t l1p_buf_addr_offset = 0;
	        const uint32_t l1k_buf_addr_offset = L0AB_HALF_BUF_SIZE * 2 * 2;
            const uint32_t l1q_buf_addr_offset = 128 * 256 * 2 * 2 * 2 + L0AB_HALF_BUF_SIZE * 2 * 2;
            l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mm1InputType>(l1q_buf_addr_offset);
            l1k_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mm1InputType>(l1k_buf_addr_offset);
            l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mm2InputType>(l1p_buf_addr_offset);
        } else {
            const uint32_t l1p_buf_addr_offset = 0;
	        const uint32_t l1q_buf_addr_offset = 128 * embed_round * 2 * 2 + L0AB_HALF_BUF_SIZE * 2;
            const uint32_t l1k_buf_addr_offset = L0AB_HALF_BUF_SIZE * 2;
            l1q_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mm1InputType>(l1q_buf_addr_offset);
            l1k_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mm1InputType>(l1k_buf_addr_offset);
            l1p_buf_addr_tensor = buf.GetBuffer<BufferType::ASCEND_CB, mm2InputType>(l1p_buf_addr_offset);
        }

        q_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1InputType *>(this->q_gm));
        kv_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm1InputType *>(this->kv_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ HighPrecisionMode *>(this->s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2InputType *>(this->p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmOutputType *>(this->o_tmp_gm));
        upo_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmOutputType *>(this->upo_tmp_gm));

        SET_FLAG(MTE1, MTE2, EVENT_ID0);
        SET_FLAG(MTE1, MTE2, EVENT_ID1);
        SET_FLAG(MTE1, MTE2, EVENT_ID2);
        SET_FLAG(MTE1, MTE2, EVENT_ID3);
        SET_FLAG(MTE1, MTE2, EVENT_ID4);
        SET_FLAG(MTE1, MTE2, EVENT_ID5);
        SET_FLAG(MTE1, MTE2, EVENT_ID6);
        SET_FLAG(MTE1, MTE2, EVENT_ID7);
        SET_FLAG(M, MTE1, EVENT_ID0);
        SET_FLAG(M, MTE1, EVENT_ID1);
        SET_FLAG(M, MTE1, EVENT_ID2);
        SET_FLAG(M, MTE1, EVENT_ID3);
        SET_FLAG(FIX, M, EVENT_ID0);
        SET_FLAG(FIX, M, EVENT_ID1);
    }

    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num;
        uint32_t next_process = 0;
        uint32_t process_idx = 0;
        for (uint32_t process = block_idx; process < process_num; process = next_process) {
            if constexpr (WorkMode == FuncType::UNEQUAL_QK_LEN_PREFILL) {
                while (process >= cur_total_q_blk_num) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 9 + offset_tiling));
                }
                process_idx = process - pre_total_q_blk_num;
            } else {
                cur_batch = process / cur_qN_blk_num;
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                process_idx = process % cur_qN_blk_num;
            }
            next_process = process + block_num;
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling)); // = blocksize
            uint32_t addr_q_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 4 + offset_tiling));
            uint32_t addr_q_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 5 + offset_tiling));
            uint64_t addr_q_scalar = (uint64_t)(((uint64_t)addr_q_high32) << 32 | addr_q_loww32);

            uint32_t qS_blk_idx = process_idx / cur_qN_blk_num;
            uint32_t qN_blk_idx = process_idx % cur_qN_blk_num;
            
            uint32_t head_start_idx = qN_blk_idx * cur_qN_blk_size;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qS_blk_size_actual = (qS_blk_idx == (m_loop - 1)) ? (q_seqlen - qS_blk_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qN_blk_size_actual = (qN_blk_idx == (cur_qN_blk_num - 1)) ? (q_heads - qN_blk_idx * cur_qN_blk_size) : cur_qN_blk_size;

            uint32_t qk_m = qS_blk_size_actual * qN_blk_size_actual;
            uint32_t qk_round_m = (qk_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint32_t qk_round_split = qk_round_m / BLOCK_SIZE;

            /**************** pre_load *****************/
            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * L0AB_HALF_BUF_SIZE;

            uint64_t q_offset = addr_q_scalar + head_start_idx * embd + qS_blk_idx * pp_m_scalar * stride_qo;

            uint64_t k_offset = 0;
            uint64_t n_align = RoundUp<uint64_t>(pp_n_scalar, 32 / sizeof(mm1InputType));

            uint32_t sv_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint32_t n_end = n_loop;
            if (mask_type == 3) {
                uint32_t no_mask_kv_seq = kv_seqlen - q_seqlen;
                uint32_t no_skip_kv_seq = (qS_blk_idx + 1) * pp_m_scalar + no_mask_kv_seq;
                no_skip_kv_seq = (no_skip_kv_seq > kv_seqlen) ? kv_seqlen : no_skip_kv_seq;
                n_end = (no_skip_kv_seq + pp_n_scalar - 1) / pp_n_scalar;
            }
            uint32_t sv_n_triu = n_end * pp_n_scalar;
            uint32_t s_block_stack = 1;
            uint32_t after = 1;
            uint32_t qk_split_k = BLOCK_EMBED;
            qk_split_k *= (2 / sizeof(mm1InputType));
            uint32_t pv_split_k = BLOCK_EMBED;
            if constexpr (CUBE_MODE == CalcMode::CALC_MODE_256) {
                s_block_stack = 2;
                after = 2;
                qk_split_k = BLOCK_QK;
                pv_split_k = BLOCK_EMBED;
            }
            uint32_t n_pingpong_flag = 0;
            uint32_t loopQK = (embd + qk_split_k - 1) / qk_split_k;
            uint32_t loopK = (embd + pv_split_k - 1) / pv_split_k;
            uint32_t loopV = (embdv + pv_split_k - 1) / pv_split_k;
            if constexpr (CUBE_MODE == CalcMode::CALC_MODE_256) {
                uint64_t l1_stride = qk_round_m * qk_split_k;
                for (uint32_t n_idx = 0; n_idx < n_end + after; n_idx += s_block_stack) {
                    if (n_idx < n_end) {
                        n_pingpong_flag = (n_idx / 2) % 2;
                        uint64_t l1qoffset = 0;
                        for (uint32_t k_idx = 0; k_idx < loopQK; k_idx++) {
                            uint32_t initc = (k_idx == 0) ? 1 : 0;
                            uint32_t qk_k = (k_idx == (loopQK - 1)) ? embd - k_idx * qk_split_k : qk_split_k;
                            uint32_t qk_round_k = (qk_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                            for (uint32_t split_idx = 0; split_idx < s_block_stack && n_idx + split_idx < n_end; split_idx++) {
                                pingpong_flag = (n_idx + split_idx) % 2;
                                offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                                uint32_t block_table_id = (uint32_t)(*((__gm__ int32_t *)block_tables_gm +
                                    cur_batch * max_num_blocks_per_query + n_idx + split_idx));
                                k_offset = block_table_id * block_size * stride_k + (head_start_idx / group_num) * embd;
                                uint64_t k_offsetk = k_offset + qk_split_k * k_idx;
                                if (n_idx + split_idx == (n_loop - 1)) {
                                    qk_n = (kv_seqlen - (n_idx + split_idx) * pp_n_scalar);
                                } else {
                                    qk_n = pp_n_scalar;
                                }
                                qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                                uint64_t l1koffset = RoundUp<uint32_t>(qk_round_n, 32 /sizeof(mm1InputType)) * qk_split_k * k_idx +
                                                     n_align * embed_round * ((n_idx + split_idx) % 4);
                                if (k_idx % 2 == 0 && split_idx == 0) {
                                    WAIT_FLAG(MTE1, MTE2, n_pingpong_flag * 3 + k_idx / 2);
                                }
                                gm_to_l1<ArchType::ASCEND_V220, mm1InputType, DataFormat::ND, DataFormat::NZ>(
                                    l1k_buf_addr_tensor[l1koffset],
                                    kv_gm_tensor[k_offsetk],
                                    qk_n,        // nValue
                                    RoundUp<uint64_t>(qk_round_n, 32 / sizeof(mm1InputType)),  // dstNzC0Stride
                                    0,            // dstNzMatrixStride, unused
                                    qk_k,         // dValue
                                    0,            // dstNzMatrixStride, unused
                                    stride_k   // srcDValue
                                );
                                SET_FLAG(MTE2, MTE1, pingpong_flag);
                                WAIT_FLAG(MTE2, MTE1, pingpong_flag);
                                WAIT_FLAG(M, MTE1, pingpong_flag + 2);
                                l1_to_l0_b<ArchType::ASCEND_V220, mm1InputType, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                    l0b_buf_tensor[offset],
                                    l1k_buf_addr_tensor[l1koffset],
                                    0,
                                    NumMatrixsRoundUp<mm1InputType>(
                                    RoundUp<uint32_t>(qk_round_k, 32 / sizeof(mm1InputType)) *
                                    RoundUp<uint64_t>(qk_round_n, 32 / sizeof(mm1InputType))), // repeat
                                    0,
                                    1,                                        // srcStride
                                    0,
                                    0                                        // dstStride
                                );
                                SET_FLAG(MTE1, M, pingpong_flag + 2);
                                if (n_idx == 0 && split_idx == 0 && k_idx == 0) {
                                    WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
                                    if (qk_m == 1) {
                                        gm_to_l1<ArchType::ASCEND_V220, mm1InputType, DataFormat::ND, DataFormat::ND>(
                                            l1q_buf_addr_tensor,
                                            q_gm_tensor[q_offset],
                                            1,
                                            0,
                                            0,
                                            RoundUp<uint32_t>(embd, 32 / sizeof(mm1InputType)),
                                            0,
                                            0
                                        );
                                    } else {
                                        for (uint32_t qS_idx = 0; qS_idx < qS_blk_size_actual; qS_idx++) {
                                            AscendC::DataCopy(l1q_buf_addr_tensor[qS_idx * 16],
                                                              q_gm_tensor[q_offset + qS_idx * stride_qo],
                                                              AscendC::Nd2NzParams(1,
                                                                                   qN_blk_size_actual,
                                                                                   embd,
                                                                                   0,
                                                                                   embd,
                                                                                   qk_round_m,
                                                                                   qS_blk_size_actual,
                                                                                   16));
                                        }
                                    }
                                    SET_FLAG(MTE2, MTE1, EVENT_ID4);
                                    WAIT_FLAG(MTE2, MTE1, EVENT_ID4);
                                }
                                WAIT_FLAG(M, MTE1, pingpong_flag);
                                uint32_t round_row_1 = RoundUp<uint32_t>(qk_round_k, 32 / sizeof(mm1InputType));
                                if (qk_m == 1) {
                                    l1_to_l0_a<ArchType::ASCEND_V220, mm1InputType, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                        l0a_buf_tensor[offset],
                                        l1q_buf_addr_tensor[k_idx * qk_split_k],
                                        0,
                                        NumMatrixsRoundUp<mm1InputType>(round_row_1),  // repeat
                                        0,
                                        1,                                                    // srcStride
                                        0,
                                        0                                                    // dstStride
                                    );
                                } else {
                                    l1_to_l0_a<ArchType::ASCEND_V220, mm1InputType, false, DataFormat::NZ, DataFormat::ZZ>(
                                        l0a_buf_tensor[offset], l1q_buf_addr_tensor[l1qoffset], qk_round_m, round_row_1, 0, 0,
                                        0, 0);
                                }
                                if (n_idx + split_idx == n_end - 1 && k_idx == loopQK - 1) {
                                    SET_FLAG(MTE1, MTE2, EVENT_ID7);
                                }
                                SET_FLAG(MTE1, M, pingpong_flag);
                                WAIT_FLAG(MTE1, M, pingpong_flag);
                                WAIT_FLAG(MTE1, M, pingpong_flag + 2);
                                if (initc && split_idx == 0) {
                                    WAIT_FLAG(FIX, M, EVENT_ID0);
                                    WAIT_FLAG(FIX, M, EVENT_ID1);
                                }
                                mmad<ArchType::ASCEND_V220, mm1InputType, mm1InputType, mmOutputType, false>(
                                    l0c_buf_tensor[split_idx * qk_round_m * pp_n_scalar],
                                    l0a_buf_tensor[offset],
                                    l0b_buf_tensor[offset],
                                    qk_m,  // m
                                    qk_n,  // n
                                    qk_k,   // k
                                    initc   // cmatrixInitVal
                                );
                                SET_FLAG(M, MTE1, pingpong_flag);
                                SET_FLAG(M, MTE1, pingpong_flag + 2);
                            }
                            l1qoffset += l1_stride;
                        }
                        SET_FLAG(M, FIX, EVENT_ID0);
                        WAIT_FLAG(M, FIX, EVENT_ID0);
                        uint32_t sv_n_triu = n_end * pp_n_scalar;
                        if (n_idx + s_block_stack > n_end - 1) {
                            sv_n = sv_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : sv_n_triu - n_idx * pp_n_scalar;
                        } else {
                            sv_n = pp_n_scalar * s_block_stack;
                        }
                        sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
	                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, HighPrecisionMode, mmOutputType>(
	                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / 2) % 2 * TMP_SIZEW],
	                        l0c_buf_tensor,
	                        qk_m,       // MSize
	                        sv_round_n, // NSize
	                        qk_round_m, // srcStrideD
	                        sv_round_n  // dstStride_dst_D
	                    );
                        SET_FLAG(FIX, M, EVENT_ID0);  
                        SET_FLAG(FIX, M, EVENT_ID1); 
                        FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
                    }
                    if (n_idx >= after) {
                        uint32_t now_id = n_idx - after;
                        n_pingpong_flag = (now_id / 2) % 2;
                        uint64_t gm_p_offset = ((uint64_t)block_idx * TMP_SIZE + n_pingpong_flag * TMP_SIZEW) * 2 / sizeof(mm2InputType);
                        uint64_t gm_upo_offset = (uint64_t)block_idx * TMP_SIZE * LOCAL_SIZE + n_pingpong_flag * TMP_SIZEW * loopV;
                        uint32_t sv_n_triu = n_end * pp_n_scalar;
                        if (now_id + s_block_stack > n_end - 1) {
                            sv_n = sv_n_triu > kv_seqlen ? kv_seqlen - now_id * pp_n_scalar : sv_n_triu - now_id * pp_n_scalar;
                        } else {
                            sv_n = pp_n_scalar * s_block_stack;
                        }
                        sv_round_n = (sv_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        for (uint32_t k_idx = 0; k_idx < loopV; k_idx++) {
                            uint32_t qk_k = (k_idx == (loopV - 1)) ? embdv - k_idx * pv_split_k : pv_split_k;
                            uint32_t qk_round_k = (qk_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                            for (uint32_t split_idx = 0; split_idx < s_block_stack && now_id + split_idx < n_end; split_idx++) {
                                pingpong_flag = split_idx;
                                uint32_t initc = 1 - split_idx;
                                uint32_t l0_offset = pingpong_flag * L0AB_HALF_BUF_SIZE;
                                if ((now_id + split_idx) == (n_loop - 1)) {
                                    qk_n = (kv_seqlen - (now_id + split_idx) * pp_n_scalar);
                                } else {
                                    qk_n = pp_n_scalar;
                                }
                                qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                                uint64_t l1voffset = RoundUp<uint64_t>(qk_round_n, 32 / sizeof(mm1InputType)) * pv_split_k * k_idx +
                                                     n_align * embed_round * ((now_id + split_idx) % 4);
                                WAIT_FLAG(M, MTE1, EVENT_ID2);
                                WAIT_FLAG(M, MTE1, EVENT_ID3);
                                for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                                    l1_to_l0_b<ArchType::ASCEND_V220, mm2InputType, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                                        l0b_buf_tensor[l0b_load_idx * qk_round_k * BLOCK_SIZE],
                                        l1k_buf_addr_tensor[l1voffset + l0b_load_idx * CUBE_MATRIX_SIZE],
                                        0,
                                        qk_round_k / BLOCK_SIZE,     // repeat
                                        0,
                                        qk_round_n / BLOCK_SIZE,  // srcStride
                                        0,
                                        0                        // dstStride
                                    );
                                }
                                if (split_idx == s_block_stack - 1 || now_id + split_idx == n_end - 1) {
                                    SET_FLAG(MTE1, MTE2, n_pingpong_flag * 3 + k_idx);
                                }
                                SET_FLAG(MTE1, M, EVENT_ID4);
                                if (k_idx == 0) {
                                    if (split_idx == 0) {
                                        WaitFlagDev(SOFTMAX_READY);
                                        WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
                                        if (qk_m == 1) {
                                            gm_to_l1<ArchType::ASCEND_V220, mm2InputType, DataFormat::ND, DataFormat::ND>(
                                                l1p_buf_addr_tensor,
                                                p_gm_tensor[gm_p_offset],
                                                1,
                                                0,
                                                0,
                                                RoundUp<uint64_t>(sv_round_n, BlockSize<mm2InputType>()),               // lenBurst
                                                0,
                                                0
                                            );
                                        } else {
                                            gm_to_l1<ArchType::ASCEND_V220, mm2InputType, DataFormat::ND, DataFormat::NZ>(
                                                l1p_buf_addr_tensor,
                                                p_gm_tensor[gm_p_offset],
                                                qk_m,        // nValue
                                                qk_round_m,  // dstNzC0Stride
                                                0,            // dstNzMatrixStride, unused
                                                sv_n,        // dValue
                                                0,            // dstNzMatrixStride, unused
                                                sv_round_n * 2 / sizeof(mm2InputType)  // srcDValue
                                            );
                                        }
                                        SET_FLAG(MTE2, MTE1, pingpong_flag + 2);
                                        WAIT_FLAG(MTE2, MTE1, pingpong_flag + 2);
                                    }
                                    WAIT_FLAG(M, MTE1, pingpong_flag);
				                    uint32_t round_row = RoundUp<uint32_t>(RoundUp<uint64_t>(qk_round_n, BlockSize<mm2InputType>()), 32 / sizeof(mm2InputType));
                                    if (qk_m == 1) {
                                        l1_to_l0_a<ArchType::ASCEND_V220, mm2InputType, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                            l0a_buf_tensor[l0_offset],
                                            l1p_buf_addr_tensor[pp_n_scalar * split_idx],
                                            0,
                                            NumMatrixsRoundUp<mm2InputType>(round_row),  // repeat
                                            0,
                                            1,                                                       // srcStride
                                            0,
                                            0                                                       // dstStride
                                        );
                                    } else {
					                    l1_to_l0_a<ArchType::ASCEND_V220, mm2InputType, false, DataFormat::NZ, DataFormat::ZZ>(
                                            l0a_buf_tensor[l0_offset], l1p_buf_addr_tensor[pp_n_scalar * qk_round_m * split_idx], qk_round_m,
                                            round_row, // repeat
                                            0,
                                            0, // srcStride
                                            0,
                                            0 // dstStride
                                        );
                                    }
                                    SET_FLAG(MTE1, M, pingpong_flag + 6);
                                    WAIT_FLAG(MTE1, M, pingpong_flag + 6);
                                }
                                if (k_idx == loopV - 1 && (split_idx == s_block_stack - 1 || now_id + split_idx == n_end - 1)) {
                                    SET_FLAG(MTE1, MTE2, EVENT_ID6);
                                }
                                if (split_idx == 0) {
                                    WAIT_FLAG(FIX, M, EVENT_ID0);
                                    WAIT_FLAG(FIX, M, EVENT_ID1);
                                }
                                WAIT_FLAG(MTE1, M, EVENT_ID4);
                                mmad<ArchType::ASCEND_V220, mm2InputType, mm2InputType, mmOutputType, false>(
                                    l0c_buf_tensor,
                                    l0a_buf_tensor[l0_offset],
                                    l0b_buf_tensor,
                                    qk_m,  // m
                                    qk_k,   // n
                                    qk_n,  // k
                                    initc   // cmatrixInitVal
                                );
                                PIPE_BARRIER(M);
                                if (k_idx == loopV - 1) {
                                    SET_FLAG(M, MTE1, pingpong_flag);
                                }
                                SET_FLAG(M, MTE1, EVENT_ID2);
                                SET_FLAG(M, MTE1, EVENT_ID3);
                            }
                            SET_FLAG(M, FIX, EVENT_ID0);
                            WAIT_FLAG(M, FIX, EVENT_ID0);
                            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mmOutputType, mmOutputType>(
                                o_tmp_gm_tensor[gm_upo_offset + k_idx * TMP_SIZEW],
                                l0c_buf_tensor,
                                qk_m,        // MSize
                                qk_round_k,     // NSize
                                qk_round_m,  // srcStride
                                qk_round_k     // dstStride_dst_D
                            );   
                            SET_FLAG(FIX, M, EVENT_ID0);
                            SET_FLAG(FIX, M, EVENT_ID1);
                        }
                        FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY);
                        for (uint32_t k_idx = loopV; k_idx < loopK; k_idx++) {
                            SET_FLAG(MTE1, MTE2, n_pingpong_flag * 3 + k_idx);
                        }
                    }
                }
            } else {
                for (uint32_t n_idx = 0; n_idx < n_end + after; n_idx += s_block_stack) {
                    if (n_idx < n_end) {
                        n_pingpong_flag = n_idx % 2;
                        uint32_t block_table_id = (uint32_t)(*((__gm__ int32_t *)block_tables_gm +
                            cur_batch * max_num_blocks_per_query + n_idx));
                        k_offset = block_table_id * block_size * stride_k + (head_start_idx / group_num) * embd;
                        uint64_t l1koffset = n_align * embed_round * n_pingpong_flag;
                        uint64_t l1qoffset = 0;
                        uint64_t k_offsetk = k_offset;
                        if (n_idx == (n_loop - 1)) {
                            qk_n = (kv_seqlen - (n_idx) * pp_n_scalar);
                        } else {
                            qk_n = pp_n_scalar;
                        }
                        qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        for (uint32_t k_idx = 0; k_idx < loopQK; k_idx++) {
                            uint32_t initc = (k_idx == 0) ? 1 : 0;
                            uint32_t qk_k = (k_idx == (loopQK - 1)) ? embd - k_idx * qk_split_k : qk_split_k;
                            uint32_t qk_round_k = (qk_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                            
                            WAIT_FLAG(MTE1, MTE2, n_pingpong_flag * 3 + k_idx);
                            gm_to_l1<ArchType::ASCEND_V220, mm1InputType, DataFormat::ND, DataFormat::NZ>(
                                l1k_buf_addr_tensor[l1koffset],
                                kv_gm_tensor[k_offsetk],
                                qk_n,        // nValue
                                RoundUp<uint64_t>(qk_round_n, 32 / sizeof(mm1InputType)),  // dstNzC0Stride
                                0,            // dstNzMatrixStride, unused
                                qk_k,         // dValue
                                0,            // dstNzMatrixStride, unused
                                stride_k   // srcDValue
                            );
                            SET_FLAG(MTE2, MTE1, EVENT_ID0);
                            WAIT_FLAG(MTE2, MTE1, EVENT_ID0);
                            WAIT_FLAG(M, MTE1, EVENT_ID2);
	                        l1_to_l0_b<ArchType::ASCEND_V220, mm1InputType, false, DataFormat::VECTOR,
	                                       DataFormat::VECTOR>(
	                            l0b_buf_tensor, l1k_buf_addr_tensor[l1koffset], 0,
	                            NumMatrixsRoundUp<mm1InputType>(
	                                RoundUp<uint32_t>(qk_round_k, 32 / sizeof(mm1InputType)) *
	                                RoundUp<uint64_t>(qk_round_n, 32 / sizeof(mm1InputType))), // repeat
	                            0,
	                            1, // srcStride
	                            0,
	                            0 // dstStride
	                        );
                            SET_FLAG(MTE1, M, EVENT_ID2);
                            if (n_idx == 0 && k_idx == 0) {
                                WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
                                if (qk_m == 1) {
                                    gm_to_l1<ArchType::ASCEND_V220, mm1InputType, DataFormat::ND, DataFormat::ND>(
                                        l1q_buf_addr_tensor,
                                        q_gm_tensor[q_offset],
                                        1,
                                        0,
                                        0,
                                        RoundUp<uint32_t>(embd, 32 / sizeof(mm1InputType)),               // lenBurst
                                        0,
                                        0
                                    );
                                } else {
                                    for (uint32_t qS_idx = 0; qS_idx < qS_blk_size_actual; qS_idx++) {
                                        AscendC::DataCopy(l1q_buf_addr_tensor[qS_idx * 16],
                                                          q_gm_tensor[q_offset + qS_idx * stride_qo],
                                                          AscendC::Nd2NzParams(1,
                                                                               qN_blk_size_actual,
                                                                               embd,
                                                                               0,
                                                                               embd,
                                                                               qk_round_m,
                                                                               qS_blk_size_actual,
                                                                               16));
                                    }
                                    
                                }
                                SET_FLAG(MTE2, MTE1, EVENT_ID4);
                                WAIT_FLAG(MTE2, MTE1, EVENT_ID4);
                            }
                            WAIT_FLAG(M, MTE1, EVENT_ID0);
			                uint32_t round_row_1 = RoundUp<uint32_t>(qk_round_k, 32 / sizeof(mm1InputType));
                            if (qk_m == 1) {
                                l1_to_l0_a<ArchType::ASCEND_V220, mm1InputType, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                    l0a_buf_tensor,
                                    l1q_buf_addr_tensor[k_idx * qk_split_k],
                                    0,
                                    NumMatrixsRoundUp<mm1InputType>(round_row_1), // repeat
                                    0,
                                    1,                                                    // srcStride
                                    0,
                                    0                                                    // dstStride
                                );
                            } else {
                                l1_to_l0_a<ArchType::ASCEND_V220, mm1InputType, false, DataFormat::NZ, DataFormat::ZZ>(
                                    l0a_buf_tensor, l1q_buf_addr_tensor[l1qoffset], qk_round_m, round_row_1, 0, 0,
                                    0, 0);
                            }
                            if (n_idx == n_end - 1 && k_idx == loopQK - 1) {
                                SET_FLAG(MTE1, MTE2, EVENT_ID7);
                            }
                            SET_FLAG(MTE1, M, EVENT_ID0);
                            WAIT_FLAG(MTE1, M, EVENT_ID0);
                            WAIT_FLAG(MTE1, M, EVENT_ID2);
                            if (initc) {
                                WAIT_FLAG(FIX, M, n_pingpong_flag);
                            }
                            mmad<ArchType::ASCEND_V220, mm1InputType, mm1InputType, mmOutputType, false>(
                                l0c_buf_tensor[n_pingpong_flag * L0AB_HALF_BUF_SIZE],
                                l0a_buf_tensor,
                                l0b_buf_tensor,
                                qk_m,  // m
                                qk_n,  // n
                                qk_k,   // k
                                initc      // cmatrixInitVal
                            );
                            SET_FLAG(M, MTE1, EVENT_ID0);
                            SET_FLAG(M, MTE1, EVENT_ID2);
                            k_offsetk += qk_split_k;
                            l1koffset += n_align * qk_split_k;
                            l1qoffset += qk_round_m * qk_split_k;
                        }
                        SET_FLAG(M, FIX, n_pingpong_flag);
                        WAIT_FLAG(M, FIX, n_pingpong_flag);
	                    l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, HighPrecisionMode, mmOutputType>(
	                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx) % 2 * TMP_SIZEW],
	                        l0c_buf_tensor[n_pingpong_flag * L0AB_HALF_BUF_SIZE],
	                        qk_m,        // MSize
	                        qk_round_n,  // NSize
	                        qk_round_m,  // srcStrideD
	                        qk_round_n  // dstStride_dst_D
	                    );
                        SET_FLAG(FIX, M, n_pingpong_flag);  
                        FftsCrossCoreSync<PIPE_FIX, 2>(QK_READY);
                    }
                    if (n_idx >= after) {
                        uint32_t now_id = n_idx - after;
                        n_pingpong_flag = now_id % 2;
                        uint64_t gm_p_offset = ((uint64_t)block_idx * TMP_SIZE + n_pingpong_flag * TMP_SIZEW) * 2 / sizeof(mm2InputType);
                        uint64_t gm_upo_offset = (uint64_t)block_idx * TMP_SIZE * LOCAL_SIZE + n_pingpong_flag * TMP_SIZEW * loopV;
                        if (now_id == (n_loop - 1)) {
                            qk_n = (kv_seqlen - (now_id) * pp_n_scalar);
                        } else {
                            qk_n = pp_n_scalar;
                        }
                        qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        for (uint32_t k_idx = 0; k_idx < loopV; k_idx++) {
                            uint32_t qk_k = (k_idx == (loopV - 1)) ? embdv - k_idx * pv_split_k : pv_split_k;
                            uint32_t qk_round_k = (qk_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                            uint64_t l1voffset = n_align * embed_round * n_pingpong_flag;
                            l1voffset += n_align * pv_split_k * k_idx;
                            WAIT_FLAG(M, MTE1, EVENT_ID2);
                            for (uint32_t l0b_load_idx = 0; l0b_load_idx < qk_round_n / BLOCK_SIZE; ++l0b_load_idx) {
                                l1_to_l0_b<ArchType::ASCEND_V220, mm2InputType, true, DataFormat::VECTOR, DataFormat::VECTOR>(
                                    l0b_buf_tensor[l0b_load_idx * qk_round_k * BLOCK_SIZE],
                                    l1k_buf_addr_tensor[l1voffset + l0b_load_idx * CUBE_MATRIX_SIZE],
                                    0,
                                    qk_round_k / BLOCK_SIZE,     // repeat
                                    0,
                                    qk_round_n / BLOCK_SIZE,  // srcStride
                                    0,
                                    0                        // dstStride
                                );
                            }
                            SET_FLAG(MTE1, M, EVENT_ID4);
                            SET_FLAG(MTE1, MTE2, n_pingpong_flag * 3 + k_idx);
                            if (k_idx == 0) {
                                WaitFlagDev(SOFTMAX_READY);
                                WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
                                if (qk_m == 1) {
                                    gm_to_l1<ArchType::ASCEND_V220, mm2InputType, DataFormat::ND, DataFormat::ND>(
                                        l1p_buf_addr_tensor,
                                        p_gm_tensor[gm_p_offset],
                                        1,
                                        0,
                                        0,
                                        RoundUp<uint64_t>(qk_round_n, BlockSize<mm2InputType>()),               // lenBurst
                                        0,
                                        0
                                    );
                                } else {
                                    gm_to_l1<ArchType::ASCEND_V220, mm2InputType, DataFormat::ND, DataFormat::NZ>(
                                        l1p_buf_addr_tensor,
                                        p_gm_tensor[gm_p_offset],
                                        qk_m,        // nValue
                                        qk_round_m,  // dstNzC0Stride
                                        0,            // dstNzMatrixStride, unused
                                        qk_n,        // dValue
                                        0,            // dstNzMatrixStride, unused
                                        qk_round_n * 2 / sizeof(mm2InputType)  // srcDValue
                                    );
                                }
                                SET_FLAG(MTE2, MTE1, EVENT_ID2);
                                WAIT_FLAG(MTE2, MTE1, EVENT_ID2);
                                WAIT_FLAG(M, MTE1, EVENT_ID0);
				                uint32_t round_row = RoundUp<uint32_t>(RoundUp<uint64_t>(qk_round_n, BlockSize<mm2InputType>()), 32 / sizeof(mm2InputType));
                                if (qk_m == 1) {
                                    l1_to_l0_a<ArchType::ASCEND_V220, mm2InputType, false, DataFormat::VECTOR, DataFormat::VECTOR>(
                                        l0a_buf_tensor,
                                        l1p_buf_addr_tensor,
                                        0,
                                        NumMatrixsRoundUp<mm2InputType>(round_row),  // repeat
                                        0,
                                        1,                                                       // srcStride
                                        0,
                                        0                                                       // dstStride
                                    );
                                } else {
                                    l1_to_l0_a<ArchType::ASCEND_V220, mm2InputType, false, DataFormat::NZ, DataFormat::ZZ>(
	                                    l0a_buf_tensor, l1p_buf_addr_tensor, qk_round_m,
	                                    round_row, // repeat
	                                    0,
	                                    0, // srcStride
	                                    0,
	                                    0 // dstStride
	                                );
                                }
                                SET_FLAG(MTE1, MTE2, EVENT_ID6);
                                SET_FLAG(MTE1, M, EVENT_ID6);
                                WAIT_FLAG(MTE1, M, EVENT_ID6);
                            }
                            WAIT_FLAG(MTE1, M, EVENT_ID4);
                            WAIT_FLAG(FIX, M, EVENT_ID0);
                            WAIT_FLAG(FIX, M, EVENT_ID1);
                            mmad<ArchType::ASCEND_V220, mm2InputType, mm2InputType, mmOutputType, false>(
                                l0c_buf_tensor,
                                l0a_buf_tensor,
                                l0b_buf_tensor,
                                qk_m,  // m
                                qk_k,   // n
                                qk_n,  // k
                                1      // cmatrixInitVal
                            );
                            if (k_idx == loopV - 1) {
                                SET_FLAG(M, MTE1, EVENT_ID0);
                            }
                            SET_FLAG(M, MTE1, EVENT_ID2);
                            SET_FLAG(M, FIX, EVENT_ID0);
                            WAIT_FLAG(M, FIX, EVENT_ID0);
                            l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, mmOutputType, mmOutputType>(
                                o_tmp_gm_tensor[gm_upo_offset + k_idx * TMP_SIZEW],
                                l0c_buf_tensor,
                                qk_m,        // MSize
                                qk_round_k,     // NSize
                                qk_round_m,  // srcStride
                                qk_round_k     // dstStride_dst_D
                            );   
                            SET_FLAG(FIX, M, EVENT_ID0);
                            SET_FLAG(FIX, M, EVENT_ID1);
                        }
                        for (uint32_t k_idx = loopV; k_idx < loopQK; k_idx++) {
                            SET_FLAG(MTE1, MTE2, n_pingpong_flag * 3 + k_idx);
                        }
                        FftsCrossCoreSync<PIPE_FIX, 2>(UPDATE_READY);
                    }
                }
            }
        }
    }
    __aicore__ __attribute__((always_inline)) inline ~PagedMLAMultiTokenPredictionAic()
    {
        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
        WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
        WAIT_FLAG(M, MTE1, EVENT_ID0);
        WAIT_FLAG(M, MTE1, EVENT_ID1);
        WAIT_FLAG(M, MTE1, EVENT_ID2);
        WAIT_FLAG(M, MTE1, EVENT_ID3);
        WAIT_FLAG(FIX, M, EVENT_ID0);
        WAIT_FLAG(FIX, M, EVENT_ID1);
        PIPE_BARRIER(ALL);
    }
private:
    __gm__ uint8_t *__restrict__ q_gm{nullptr};
    __gm__ uint8_t *__restrict__ kv_gm{nullptr}; 
    __gm__ uint8_t *__restrict__ block_tables_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ upo_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};

    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::LocalTensor<mm1InputType> l1q_buf_addr_tensor;
    AscendC::LocalTensor<mm1InputType> l1k_buf_addr_tensor;
    AscendC::LocalTensor<mm2InputType> l1p_buf_addr_tensor;

    AscendC::GlobalTensor<mm1InputType> q_gm_tensor;
    AscendC::GlobalTensor<mm1InputType> kv_gm_tensor;
    AscendC::GlobalTensor<HighPrecisionMode> s_gm_tensor;
    AscendC::GlobalTensor<mm2InputType> p_gm_tensor;
    AscendC::GlobalTensor<mmOutputType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<mmOutputType> upo_gm_tensor;

    AscendC::LocalTensor<mm1InputType> l0a_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0A, mm1InputType>(0);
    AscendC::LocalTensor<mm1InputType> l0b_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0B, mm1InputType>(0);
    AscendC::LocalTensor<mmOutputType> l0c_buf_tensor = buf.GetBuffer<BufferType::ASCEND_L0C, mmOutputType>(0);

    uint32_t batch_size{0};
    uint32_t max_kv_seqlen{0}; 
    uint32_t max_num_blocks_per_query{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    uint32_t embdv{0};
    uint32_t embed_round{0};
    uint32_t kv_heads{0};
    uint32_t block_size{0};
    uint32_t total_q_blk_num{0};
    uint32_t group_num{0};
    uint64_t stride_qo{0};
    uint64_t stride_k{0};
    uint64_t batch_stride_k{0};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t mask_type{0};
    uint32_t cur_qN_blk_size{0};
    uint32_t cur_qN_blk_num{0};
};

#elif __DAV_C220_VEC__
constexpr int32_t BLOCK_SIZE = 16;
constexpr int32_t FLOAT_BLOCK_SIZE = 8;
constexpr int32_t VECTOR_SIZE = 128;
constexpr int32_t FLOAT_VECTOR_SIZE = 64;
constexpr int32_t UB_UINT8_BLOCK_SIZE = 16384;  // 64 * 128 * 2B
constexpr int32_t UB_HALF_BUF_SIZE = 8192;
constexpr int32_t UB_UINT8_LINE_SIZE = 512;     // 128 * 4B
constexpr int32_t UB_FLOAT_LINE_SIZE = 64;     // 128
constexpr int32_t UB_HALF_LINE_SIZE = 128;       // UB_FLOAT_LINE_SIZE * 2
constexpr int32_t TMP_SIZE = 32768 * 4;             // 128 * 256
constexpr int32_t TMP_SIZET = 16384;
constexpr int32_t BLOCK_QK_C = 256;
constexpr int32_t BLOCK_QK_V = 128;
constexpr int32_t ROWMAX_TEMP_BUF_OFFSET = 1024;

template<typename IN_DATA_TYPE, CalcMode CUBE_MODE, typename QKV_DT = IN_DATA_TYPE, typename O_DT = IN_DATA_TYPE, FuncType WorkMode = FuncType::MULTI_TOKEN_PREDICTION>
class PagedMLAMultiTokenPredictionHpAiv {
public:
    __aicore__ __attribute__((always_inline)) inline PagedMLAMultiTokenPredictionHpAiv(
        __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ mask_gm,
        __gm__ uint8_t *__restrict__ o_gm, __gm__ uint8_t *__restrict__ s_gm,
        __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
        __gm__ uint8_t *__restrict__ upo_tmp_gm,
        __gm__ uint8_t *__restrict__ tiling_para_gm) 
        : mask_gm(mask_gm), o_gm(o_gm), s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm), upo_tmp_gm(upo_tmp_gm), tiling_para_gm(tiling_para_gm)
    {
    	mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DATA_TYPE *>(mask_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ O_DT *>(o_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ QKV_DT *>(p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(o_tmp_gm));
        upo_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(upo_tmp_gm));

        this->sub_block_idx = GetSubBlockidx();
        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->max_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MASK_MAX_LEN));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_NUMHEADS));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM));
        this->embdv = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM_V));
        this->tor = (half)(*((__gm__ float *)tiling_para_gm + TILING_TOR));
        this->block_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        this->head_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEAD_STRIDE));
        this->mask_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BATCH_STRIDE));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_TOTAL_BLOCK_NUM));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        this->long_seq = 0;
        this->mask_type = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MASK_TYPE_ND));
        this->cur_qN_blk_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_SIZE));
        this->cur_qN_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_NUM));
        this->stride_qo = q_heads * embdv;
        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        SET_FLAG(MTE3, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID3);
	    SET_FLAG(V, MTE2, EVENT_ID4);
        SET_FLAG(MTE3, V, EVENT_ID0);
    }
    
    __aicore__ __attribute__((always_inline)) inline void __set_mask(int32_t len)
    {
        uint64_t mask = 0;
        uint64_t one = 1;
        uint64_t temp = len % FLOAT_VECTOR_SIZE;
        for (int64_t i = 0; i < temp; i++) {
            mask |= one << i;
        }

        if (len == VECTOR_SIZE || len == 0) {
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        } else if (len >= FLOAT_VECTOR_SIZE) {
            SetVectorMask<int8_t>(mask, (uint64_t)-1);
        } else {
            SetVectorMask<int8_t>(0x0, mask);
        }
    }

    __aicore__ __attribute__((always_inline)) inline void __set_vcg_mask(int32_t len)
    {
        if (len > 16 || len < 1) {
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            return;
        }
        uint64_t subMask = ((uint64_t) 1 << len) - 1;
        uint64_t maskValue = (subMask << 48) + (subMask << 32) + (subMask << 16) + subMask +
                             (subMask << 56) + (subMask << 40) + (subMask << 24) + (subMask << 8);
        SetVectorMask<int8_t>(maskValue, maskValue);
    }

    template<typename Dtype>
    __aicore__ __attribute__((always_inline)) inline uint32_t VectorSize()
    {
        return 256 / sizeof(Dtype);
    }

    template<typename Dtype>
    __aicore__ __attribute__((always_inline)) inline uint64_t NumVectorsRoundUp(uint64_t num)
    {
        return (num + VectorSize<Dtype>() - 1) / VectorSize<Dtype>();
    }

    __aicore__ __attribute__((always_inline)) inline void DeqPerHeadS322F32(AscendC::LocalTensor<float> &s,
                                                                            __gm__ uint8_t *deq_qk_gm,
                                                                            __gm__ uint8_t *off_qk_gm,
                                                                            uint32_t head_idx, uint32_t len)
    {
        // dequant QK
        // int32_t转成float类型
        conv_v<ArchType::ASCEND_V220, int32_t, float>(s, s.ReinterpretCast<int32_t>(),
                                                      NumVectorsRoundUp<int32_t>(len), // repeat
                                                      1,                        // dstBlockStride
                                                      1,                        // srcBlockStride
                                                      8,                        // dstRepeatStride
                                                      8                         // srcRepeatStride
        );
        PIPE_BARRIER(V);
        float s_quant_scale = *((__gm__ float *)deq_qk_gm + head_idx);
        muls_v<ArchType::ASCEND_V220, float>(s, s, s_quant_scale,
                                             NumVectorsRoundUp<float>(len), // repeat 
                                             1,                             // dstBlockStride
                                             1,                             // srcBlockStride
                                             8,                             // dstRepeatStride
                                             8                              // srcRepeatStride
        );
        PIPE_BARRIER(V);
    }

    template <typename T>
    __aicore__ inline void DivRepeatM(const AscendC::LocalTensor<T> &dst, const AscendC::LocalTensor<T> &src0,
                                      const AscendC::LocalTensor<T> &src1, const uint32_t sub_m, const uint32_t qk_n,
                                      const uint32_t qk_round_n)
    {
        uint32_t T_BLOCK_SIZE = BlockSize<T>();
        uint32_t T_VECTOR_SIZE = VectorSize<T>();
        for (uint32_t row_idx = 0; row_idx < qk_n / T_VECTOR_SIZE; ++row_idx) {
            div_v<ArchType::ASCEND_V220, T>(dst[row_idx * T_VECTOR_SIZE], src0[row_idx * T_VECTOR_SIZE], src1,
                                            sub_m,                     // repeat
                                            1,                         // dstBlockStride
                                            1,                         // src0BlockStride
                                            0,                         // src1BlockStride
                                            qk_round_n / T_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / T_BLOCK_SIZE, // src0RepeatStride
                                            1                          // src1RepeatStride
            );
        }
        if (qk_n % T_VECTOR_SIZE > 0) {
            __set_mask(qk_n % T_VECTOR_SIZE);
            div_v<ArchType::ASCEND_V220, T>(dst[qk_n / T_VECTOR_SIZE * T_VECTOR_SIZE],
                                            src0[qk_n / T_VECTOR_SIZE * T_VECTOR_SIZE], src1,
                                            sub_m,                     // repeat
                                            1,                         // dstBlockStride
                                            1,                         // src0BlockStride
                                            0,                         // src1BlockStride
                                            qk_round_n / T_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / T_BLOCK_SIZE, // src0RepeatStride
                                            1                          // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    template<bool IS_BF16 = true>
    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        
        uint32_t s_block_stack = 1;
        uint32_t after = 1;
        if constexpr (CUBE_MODE == CalcMode::CALC_MODE_256){
            s_block_stack = 2;
            after = 2; 
        }
	    uint32_t loopV = (embdv + BLOCK_QK_V - 1) / BLOCK_QK_V;
        uint32_t loopVt = (embdv + BLOCK_QK_C - 1) / BLOCK_QK_C;
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num;
        uint32_t next_process = 0;
        uint32_t process_idx = 0;
        for (uint32_t process = block_idx; process < process_num; process = next_process) {
            if constexpr (WorkMode == FuncType::UNEQUAL_QK_LEN_PREFILL) {
                while (process >= cur_total_q_blk_num) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 9 + offset_tiling));
                }
                process_idx = process - pre_total_q_blk_num;
            } else {
                cur_batch = process / cur_qN_blk_num;
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                process_idx = process % cur_qN_blk_num;
            }
            next_process = process + block_num;
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
            uint32_t mask_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10 + offset_tiling));
            uint32_t mask_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 14 + offset_tiling));
            uint64_t mask_scalar = (uint64_t)(((uint64_t)mask_high32) << 32 | mask_loww32);

            uint32_t qS_blk_idx = process_idx / cur_qN_blk_num;
            uint32_t qN_blk_idx = process_idx % cur_qN_blk_num;
            
            uint32_t head_start_idx = qN_blk_idx * cur_qN_blk_size;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qS_blk_size_actual = (qS_blk_idx == (m_loop - 1)) ? (q_seqlen - qS_blk_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qN_blk_size_actual = (qN_blk_idx == (cur_qN_blk_num - 1)) ? (q_heads - qN_blk_idx * cur_qN_blk_size) : cur_qN_blk_size;

            uint32_t qk_m = qS_blk_size_actual * qN_blk_size_actual;
            // when qS > 16, qN tile is 1, qS_blk is splited between 2 vector cores
            // when qS < 16, qN_blk is splited between 2 vector cores, each vector core pocesses multiple qS_blk's
            uint32_t sub_qN = (qN_blk_size_actual == 1) ?
                              0 : ((sub_block_idx == 1) ?
                                  (qN_blk_size_actual - qN_blk_size_actual / 2) :
                                  (qN_blk_size_actual / 2));

            uint32_t row_offset = (qN_blk_size_actual == 1) ?
                                  (qk_m / 2) :
                                  (qS_blk_size_actual * (qN_blk_size_actual / 2));
            uint32_t sub_m = (sub_block_idx == 1) ? (qk_m - row_offset) : row_offset;

            uint32_t round_sub_m = (sub_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
            uint32_t sub_m_d64 = (sub_m + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE; // up aligned to 64

            /******** pre_load *******/
            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint64_t mask_batch_offset = cur_batch * mask_stride * max_seqlen;
            uint64_t mask_head_offset = head_start_idx * ((uint64_t)head_stride) * max_seqlen;
            uint64_t mask_offset = mask_batch_offset + mask_head_offset + mask_scalar;
            mask_offset += qS_blk_idx * pp_m_scalar * max_seqlen;

            uint64_t o_offset = addr_o_scalar + head_start_idx * embdv + qS_blk_idx * pp_m_scalar * stride_qo;
            uint32_t n_end = n_loop;
            if (mask_type == 3) {
                uint32_t no_mask_kv_seq = kv_seqlen - q_seqlen;
                uint32_t no_skip_kv_seq = (qS_blk_idx + 1) * pp_m_scalar + no_mask_kv_seq;
                no_skip_kv_seq = (no_skip_kv_seq > kv_seqlen) ? kv_seqlen : no_skip_kv_seq;
                n_end = (no_skip_kv_seq + pp_n_scalar - 1) / pp_n_scalar;
            }
            uint32_t qk_n_triu = n_end * pp_n_scalar; 
            uint32_t m_slice = 0;
            uint32_t sub_qN_slice = 0;
            if (qN_blk_size_actual == 1) {
                m_slice = sub_m > 32 ? 32 : 0;
            } else {
                sub_qN_slice = sub_m > 32 ? sub_qN / 2 : 0;
                m_slice = sub_qN_slice * qS_blk_size_actual;
            }
            uint32_t m_end = sub_m > 32 ? 2 : 1;
            for (uint32_t n_idx = 0; n_idx < n_end + after; n_idx += s_block_stack) {
                if (n_idx < n_end) {
                    if (n_idx + s_block_stack > n_end - 1) {
                        qk_n = qk_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : qk_n_triu - n_idx * pp_n_scalar;
                    } else {
                        qk_n = pp_n_scalar * s_block_stack;
                    }
                    qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    if (qk_n <= VECTOR_SIZE) {
                        if (sub_m > 0 && mask_type != 0) {
                            WAIT_FLAG(V, MTE2, EVENT_ID1);
                            if (qN_blk_size_actual == 1) {
                                gm_to_ub_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                    mask16_ubuf_tensor,
                                    mask_gm_tensor[mask_offset + (uint64_t)sub_block_idx * qk_m / 2 * max_seqlen],
                                    0,                       // sid
                                    sub_m,                   // nBurst
                                    qk_n * 2,                // lenBurst
                                    0,                       // leftPaddingNum
                                    0,                       // rightPaddingNum
                                    (max_seqlen - qk_n) * 2, // srcGap
                                    0                        // dstGap
                                );
                            } else {
                                for (uint32_t qN_idx = 0; qN_idx < sub_qN; qN_idx++) {
                                    gm_to_ub_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        mask16_ubuf_tensor[qN_idx * qS_blk_size_actual * qk_round_n],
                                        mask_gm_tensor[mask_offset],
                                        0,                       // sid
                                        qS_blk_size_actual,      // nBurst
                                        qk_n * 2,                // lenBurst
                                        0,                       // leftPaddingNum
                                        0,                       // rightPaddingNum
                                        (max_seqlen - qk_n) * 2, // srcGap
                                        0                        // dstGap
                                    );
                                }
                            }
                            SET_FLAG(MTE2, V, EVENT_ID1);
                            mask_offset += qk_n;
                            WAIT_FLAG(MTE2, V, EVENT_ID1);
                            conv_v<ArchType::ASCEND_V220, IN_DATA_TYPE, float>(
                                mask_ubuf_tensor, mask16_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                4                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            if (head_stride == 0 && mask_type != 2) {
                                muls_v<ArchType::ASCEND_V220, float>(
                                    mask_ubuf_tensor,
                                    mask_ubuf_tensor,
                                    (float)-3e38,
                                    (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1, // dstBlockStride
                                    1, // srcBlockStride
                                    8, // dstRepeatStride
                                    8  // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                        }
                        WaitFlagDev(QK_READY);
                        if (sub_m > 0) {
                            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                            // input QK
                            gm_to_ub<ArchType::ASCEND_V220, float>(
                                ls_ubuf_tensor,
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                    (uint64_t)sub_block_idx * row_offset * qk_round_n],
                                0,                                // sid
                                sub_m,                                // nBurst
                                qk_round_n / FLOAT_BLOCK_SIZE,  // lenBurst
                                0,                                // srcGap
                                0                                 // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID0);
                            WAIT_FLAG(MTE2, V, EVENT_ID0);
                            // *** ls = tor * ls
                            muls_v<ArchType::ASCEND_V220, float>(ls_ubuf_tensor, ls_ubuf_tensor, tor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);

                            // *** ls = ls + mask
                            if (mask_type != 0) {
                                add_v<ArchType::ASCEND_V220, float>(
                                    ls_ubuf_tensor, 
                                    ls_ubuf_tensor,
                                    mask_ubuf_tensor,
                                    (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1,                                                                // dstBlockStride
                                    1,                                                                // src0BlockStride
                                    1,                                                                // src1BlockStride
                                    8,                                                                // dstRepeatStride
                                    8,                                                                // src0RepeatStride
                                    8                                                                 // src1RepeatStride
                                );
                                SET_FLAG(V, MTE2, EVENT_ID1);
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            // *** lm = rowmax(ls)
                            if (qk_n <= FLOAT_VECTOR_SIZE) {
                                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                cgmax_v<ArchType::ASCEND_V220, float>(
                                    tv_ubuf_tensor,
                                    ls_ubuf_tensor,
                                    sub_m,
                                    1,
                                    1,
                                    qk_round_n / FLOAT_BLOCK_SIZE
                                );
                                PIPE_BARRIER(V);
                                __set_vcg_mask((qk_n + FLOAT_BLOCK_SIZE - 1)/ FLOAT_BLOCK_SIZE);
                                cgmax_v<ArchType::ASCEND_V220, float>(
                                    lm_ubuf_tensor,
                                    tv_ubuf_tensor,
                                    (sub_m * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                    1,
                                    1,
                                    8
                                );
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, ls_ubuf_tensor,
                                                sub_m,
                                                1,
                                                1,
                                                qk_round_n / 8
                                );
                                PIPE_BARRIER(V);
                                cgmax_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, tv_ubuf_tensor,
                                                round_sub_m * 8 / 64,
                                                1,
                                                1,
                                                8
                                );
                                PIPE_BARRIER(V);
                                for (uint32_t rowMaxIdx = 1; rowMaxIdx < qk_n / FLOAT_VECTOR_SIZE; ++rowMaxIdx) {
                                    cgmax_v<ArchType::ASCEND_V220, float>(
                                        tv_ubuf_tensor,
                                        ls_ubuf_tensor[rowMaxIdx * FLOAT_VECTOR_SIZE],
                                        sub_m,
                                        1,
                                        1,
                                        qk_round_n / 8
                                    );
                                    PIPE_BARRIER(V);
                                    cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                            round_sub_m * 8 / 64,
                                            1,
                                            1,
                                            8
                                    );
                                    PIPE_BARRIER(V);
                                    __set_mask(sub_m);
                                    max_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, lm_ubuf_tensor,
                                        tv_ubuf_tensor,
                                        1,                        // repeat
                                        1,                            // dstBlockStride
                                        1,                            // src0BlockStride
                                        1,                            // src1BlockStride
                                        8,                            // dstRepeatStride
                                        8,                            // src0RepeatStride
                                        8 // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    cgmax_v<ArchType::ASCEND_V220, float>(
                                        tv_ubuf_tensor,
                                        ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        sub_m,
                                        1,
                                        1,
                                        qk_round_n / 8
                                    );
                                    PIPE_BARRIER(V);
                                    __set_vcg_mask((qk_n % FLOAT_VECTOR_SIZE + FLOAT_BLOCK_SIZE - 1)/ FLOAT_BLOCK_SIZE);
                                    cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                round_sub_m * 8 / 64,
                                                1,
                                                1,
                                                8
                                    );
                                    PIPE_BARRIER(V);
                                    __set_mask(sub_m);
                                    max_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, lm_ubuf_tensor,
                                        tv_ubuf_tensor,
                                        1,                        // repeat
                                        1,                            // dstBlockStride
                                        1,                            // src0BlockStride
                                        1,                            // src1BlockStride
                                        8,                            // dstRepeatStride
                                        8,                            // src0RepeatStride
                                        8 // src1RepeatStride
                                    );
                                }
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == 0) {
                                // *** hm = lm
                                ub_to_ub<ArchType::ASCEND_V220, float>(
                                    hm_ubuf_tensor,
                                    lm_ubuf_tensor,
                                    0,                              // sid
                                    1,                              // nBurst
                                    round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                              // srcGap
                                    0                               // dstGap
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** hm = vmax(lm, gm)
                                max_v<ArchType::ASCEND_V220, float>(
                                    hm_ubuf_tensor,
                                    lm_ubuf_tensor,
                                    gm_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = gm - hm
                                sub_v<ArchType::ASCEND_V220, float>(
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    gm_ubuf_tensor,
                                    hm_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = exp(dm)
                                exp_v<ArchType::ASCEND_V220, float>(
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // srcBlockStride
                                    8,         // dstRepeatStride
                                    8          // srcRepeatStride
                                );
                            }
                            // *** gm = hm
                            ub_to_ub<ArchType::ASCEND_V220, float>(
                                gm_ubuf_tensor,
                                hm_ubuf_tensor,
                                0,                              // sid
                                1,                              // nBurst
                                round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                0,                              // srcGap
                                0                               // dstGap
                            );
                            PIPE_BARRIER(V);
                            // *** hm_block = expand_to_block(hm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                hm_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                1,                             // dstBlockStride
                                8,                             // dstRepeatStride
                                round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** ls = ls - hm_block
                            for (uint32_t vsub_idx = 0; vsub_idx < qk_n / FLOAT_VECTOR_SIZE; ++vsub_idx) {
                                sub_v<ArchType::ASCEND_V220, float>(
                                    ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                    ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstBlockStride
                                    1,                             // src0BlockStride
                                    0,                             // src1BlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                              // src1RepeatStride
                                );
                            }
                            if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                sub_v<ArchType::ASCEND_V220, float>(
                                    ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                    tv_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstBlockStride
                                    1,                             // src0BlockStride
                                    0,                             // src1BlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                    qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                    1                              // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** ls = exp(ls)
                            exp_v<ArchType::ASCEND_V220, float>(
                                ls32_ubuf_tensor,
                                ls_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** lp = castfp32to16(ls)
                            if (IS_BF16) {
                                convr_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                    lp_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                    ls32_ubuf_tensor,
                                    (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1, // dstBlockStride
                                    1, // srcBlockStride
                                    4, // dstRepeatStride
                                    8  // srcRepeatStride
                                );
                            } else {
                                conv_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                    lp_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                    ls32_ubuf_tensor,
                                    (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1, // dstBlockStride
                                    1, // srcBlockStride
                                    4, // dstRepeatStride
                                    8  // srcRepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            
                            
                            SET_FLAG(V, MTE3, EVENT_ID0);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm<ArchType::ASCEND_V220, QKV_DT>(
                                p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                            (uint64_t)sub_block_idx * row_offset * qk_round_n],
                                lp_ubuf_tensor.ReinterpretCast<QKV_DT>(),
                                0,                               // sid
                                1,                               // nBurst
                                sub_m * qk_round_n / BLOCK_SIZE, // lenBurst
                                0,                               // srcGap
                                0                                // dstGap
                            );
                            
                            SET_FLAG(MTE3, MTE2, EVENT_ID0);
                            // *** ll = rowsum(ls32)
                            if (qk_n <= FLOAT_VECTOR_SIZE) {
                                __set_mask(qk_n);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor,
                                    ls32_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstRepeatStride
                                    1,                             // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                for (uint32_t rowSumIdx = 1; rowSumIdx < qk_n / FLOAT_VECTOR_SIZE; ++rowSumIdx) {
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[rowSumIdx * FLOAT_VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                }
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                PIPE_BARRIER(V);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor,
                                    ls32_ubuf_tensor,
                                    sub_m,                         // repeat
                                    1,                             // dstRepeatStride
                                    1,                             // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == 0) {
                                // *** gl = ll
                                ub_to_ub<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    ll_ubuf_tensor,
                                    0,                              // sid
                                    1,                              // nBurst
                                    round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                              // srcGap
                                    0                               // dstGap
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** gl = dm * gl
                                mul_v<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    gl_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** gl = ll + gl
                                add_v<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    gl_ubuf_tensor,
                                    ll_ubuf_tensor,
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                        }
                    } else {
                        bool last_n_loop = n_idx + s_block_stack > n_end - 1;
                        for (uint32_t split_idx = 0; split_idx < m_end; split_idx++) {
                            bool last_m_loop = split_idx == m_end - 1;
                            uint32_t m_split =  last_m_loop ? sub_m - split_idx * m_slice : m_slice;
                            uint32_t sub_qN_split = last_m_loop ? sub_qN - split_idx * sub_qN_slice : sub_qN_slice;
                            uint32_t round_m_split = (m_split + FLOAT_BLOCK_SIZE - 1) / FLOAT_BLOCK_SIZE * FLOAT_BLOCK_SIZE;
                            if (sub_m > 0 && mask_type != 0) {
                                WAIT_FLAG(V, MTE2, EVENT_ID1);
                                uint64_t mask_offset_tail = 0;
                                if (qN_blk_size_actual == 1) {
                                    gm_to_ub_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        mask16_ubuf_tensor,
                                        mask_gm_tensor[mask_offset + (sub_block_idx * qk_m / 2 + split_idx * m_slice) * max_seqlen],
                                        0,                       // sid
                                        m_split,                   // nBurst
                                        qk_n * 2,                // lenBurst
                                        0,                       // leftPaddingNum
                                        0,                       // rightPaddingNum
                                        (max_seqlen - qk_n) * 2, // srcGap
                                        0                        // dstGap
                                    );
                                } else {
                                    for (uint32_t qN_idx = 0; qN_idx < sub_qN_split; qN_idx++) {
                                        gm_to_ub_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                            mask16_ubuf_tensor[qN_idx * qS_blk_size_actual * qk_round_n],
                                            mask_gm_tensor[mask_offset],
                                            0,                       // sid
                                            qS_blk_size_actual,      // nBurst
                                            qk_n * 2,                // lenBurst
                                            0,                       // leftPaddingNum
                                            0,                       // rightPaddingNum
                                            (max_seqlen - qk_n) * 2, // srcGap
                                            0                        // dstGap
                                        );
                                    }
                                }
                                SET_FLAG(MTE2, V, EVENT_ID1);
                            }
                            if (split_idx == 0) {
                                WaitFlagDev(QK_READY);
                            }
                            if (sub_m > 0) {
                                if (m_split > 0) {
                                    if (mask_type != 0) {
                                        WAIT_FLAG(MTE2, V, EVENT_ID1);
                                        conv_v<ArchType::ASCEND_V220, IN_DATA_TYPE, float>(
                                            mask_ubuf_tensor, 
                                            mask16_ubuf_tensor,
                                            (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                                                // dstBlockStride
                                            1,                                                                // srcBlockStride
                                            8,                                                                // dstRepeatStride
                                            4                                                                 // srcRepeatStride
                                        );
                                        SET_FLAG(V, MTE2, EVENT_ID1);
                                        PIPE_BARRIER(V);
                                        if (head_stride == 0 && mask_type != 2) {
                                            muls_v<ArchType::ASCEND_V220, float>(
                                                mask_ubuf_tensor,
                                                mask_ubuf_tensor,
                                                (float)-3e38,
                                                (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                                1, // dstBlockStride
                                                1, // srcBlockStride
                                                8, // dstRepeatStride
                                                8  // srcRepeatStride
                                            );
                                            PIPE_BARRIER(V);
                                        }
                                    }
                                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                                    // input QK
                                    gm_to_ub<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor,
                                        s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                            (uint64_t)(sub_block_idx * row_offset + split_idx * m_slice) * qk_round_n],
                                        0,                                // sid
                                        m_split,                                // nBurst
                                        qk_round_n / FLOAT_BLOCK_SIZE,  // lenBurst
                                        0,                                // srcGap
                                        0                                 // dstGap
                                    );
                                    SET_FLAG(MTE2, V, EVENT_ID0);
                                    WAIT_FLAG(MTE2, V, EVENT_ID0);
                                    // *** ls = tor * ls
                                    muls_v<ArchType::ASCEND_V220, float>(
                                        ls_ubuf_tensor,
                                        ls_ubuf_tensor,
                                        tor,
                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // srcBlockStride
                                        8,                                                                // dstRepeatStride
                                        8                                                                 // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    // *** ls = ls + mask
                                    if (mask_type != 0) {
                                        add_v<ArchType::ASCEND_V220, float>(
                                            ls_ubuf_tensor, 
                                            ls_ubuf_tensor,
                                            mask_ubuf_tensor,
                                            (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1,                                                                // dstBlockStride
                                            1,                                                                // src0BlockStride
                                            1,                                                                // src1BlockStride
                                            8,                                                                // dstRepeatStride
                                            8,                                                                // src0RepeatStride
                                            8                                                                 // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                    if (qk_n == SOFTMAX_MAX_LENGTH) {
                                        cgmax_v<ArchType::ASCEND_V220, float>(
                                            tv_ubuf_tensor,
                                            ls_ubuf_tensor,
                                            m_split * qk_n / FLOAT_VECTOR_SIZE,
                                            1,
                                            1,
                                            8
                                        );
                                        PIPE_BARRIER(V);
                                        __set_mask(32);
                                        cgmax_v<ArchType::ASCEND_V220, float>(
                                            tv_ubuf_tensor,
                                            tv_ubuf_tensor,
                                            m_split,
                                            1,
                                            1,
                                            4
                                        );
                                        PIPE_BARRIER(V);
                                        __set_vcg_mask(4);
                                        cgmax_v<ArchType::ASCEND_V220, float>(
                                            lm_ubuf_tensor,
                                            tv_ubuf_tensor,
                                            (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                            1,
                                            1,
                                            8
                                        );
                                        PIPE_BARRIER(V);
                                        __set_mask(m_split);
                                    } else {
                                        cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, ls_ubuf_tensor,
                                                        m_split,
                                                        1,
                                                        1,
                                                        qk_round_n / FLOAT_BLOCK_SIZE
                                        );
                                        PIPE_BARRIER(V);
                                        cgmax_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, tv_ubuf_tensor,
                                                        (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                                        1,
                                                        1,
                                                        8
                                        );
                                        PIPE_BARRIER(V);
                                        for (uint64_t rowmax_idx = 1; rowmax_idx < (uint64_t)qk_n / FLOAT_VECTOR_SIZE; ++rowmax_idx) {
                                            cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, ls_ubuf_tensor[rowmax_idx * FLOAT_VECTOR_SIZE],
                                                        m_split,
                                                        1,
                                                        1,
                                                        qk_round_n / FLOAT_BLOCK_SIZE
                                            );
                                            PIPE_BARRIER(V);
                                            cgmax_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                        (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                                        1,
                                                        1,
                                                        8
                                            );
                                            PIPE_BARRIER(V);
                                            __set_mask(m_split);
                                            max_v<ArchType::ASCEND_V220, float>(lm_ubuf_tensor, lm_ubuf_tensor,
                                                tv_ubuf_tensor,
                                                1,                        // repeat
                                                1,                            // dstBlockStride
                                                1,                            // src0BlockStride
                                                1,                            // src1BlockStride
                                                8,                            // dstRepeatStride
                                                8,                            // src0RepeatStride
                                                8 // src1RepeatStride
                                            );
                                            PIPE_BARRIER(V);
                                        }
                                        if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                            __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                            cgmax_v<ArchType::ASCEND_V220, float>(
                                                tv_ubuf_tensor,
                                                ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                                m_split,
                                                1,
                                                1,
                                                qk_round_n / FLOAT_BLOCK_SIZE
                                            );
                                            PIPE_BARRIER(V);
                                            __set_vcg_mask((qk_n % FLOAT_VECTOR_SIZE + FLOAT_BLOCK_SIZE - 1)/ FLOAT_BLOCK_SIZE);
                                            cgmax_v<ArchType::ASCEND_V220, float>(
                                                tv_ubuf_tensor,
                                                tv_ubuf_tensor,
                                                (m_split * FLOAT_BLOCK_SIZE + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,
                                                1,
                                                1,
                                                8
                                            );
                                            PIPE_BARRIER(V);
                                            __set_mask(m_split);
                                            max_v<ArchType::ASCEND_V220, float>(
                                                lm_ubuf_tensor,
                                                lm_ubuf_tensor,
                                                tv_ubuf_tensor,
                                                1,                        // repeat
                                                1,                            // dstBlockStride
                                                1,                            // src0BlockStride
                                                1,                            // src1BlockStride
                                                8,                            // dstRepeatStride
                                                8,                            // src0RepeatStride
                                                8 // src1RepeatStride
                                            );
                                        }
                                    }
                                    PIPE_BARRIER(V);
                                    if (n_idx == 0) {
                                        // *** hm = lm
                                        ub_to_ub<ArchType::ASCEND_V220, float>(
                                            hm_ubuf_tensor[split_idx * m_slice],
                                            lm_ubuf_tensor,
                                            0,                              // sid
                                            1,                              // nBurst
                                            round_m_split / FLOAT_BLOCK_SIZE, // lenBurst
                                            0,                              // srcGap
                                            0                               // dstGap
                                        );
                                        PIPE_BARRIER(V);
                                    } else {
                                        // *** hm = vmax(lm, gm)
                                        max_v<ArchType::ASCEND_V220, float>(
                                            hm_ubuf_tensor[split_idx * m_slice],
                                            lm_ubuf_tensor,
                                            gm_ubuf_tensor[split_idx * m_slice],
                                            1,         // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        // *** dm = gm - hm
                                        sub_v<ArchType::ASCEND_V220, float>(
                                            dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            gm_ubuf_tensor[split_idx * m_slice],
                                            hm_ubuf_tensor[split_idx * m_slice],
                                            1,         // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        // *** dm = exp(dm)
                                        exp_v<ArchType::ASCEND_V220, float>(
                                            dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            1,         // repeat
                                            1,         // dstBlockStride
                                            1,         // srcBlockStride
                                            8,         // dstRepeatStride
                                            8          // srcRepeatStride
                                        );
                                    }
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    PIPE_BARRIER(V);
                                    // *** gm = hm
                                    ub_to_ub<ArchType::ASCEND_V220, float>(
                                        gm_ubuf_tensor[split_idx * m_slice],
                                        hm_ubuf_tensor[split_idx * m_slice],
                                        0,                              // sid
                                        1,                              // nBurst
                                        round_m_split / FLOAT_BLOCK_SIZE, // lenBurst
                                        0,                              // srcGap
                                        0                               // dstGap
                                    );
                                    PIPE_BARRIER(V);
                                    // *** hm_block = expand_to_block(hm), 存放于 tv
                                    brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                        tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                        hm_ubuf_tensor.ReinterpretCast<uint32_t>()[split_idx * m_slice],
                                        1,                             // dstBlockStride
                                        8,                             // dstRepeatStride
                                        round_m_split / FLOAT_BLOCK_SIZE // repeat
                                    );
                                    PIPE_BARRIER(V);
                                    // *** ls = ls - hm_block
                                    for (uint32_t vsub_idx = 0; vsub_idx < qk_n / FLOAT_VECTOR_SIZE; ++vsub_idx) {
                                        sub_v<ArchType::ASCEND_V220, float>(
                                            ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                            ls_ubuf_tensor[vsub_idx * FLOAT_VECTOR_SIZE],
                                            tv_ubuf_tensor,
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            0,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            1                              // src1RepeatStride
                                        );
                                    }
                                    if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                        __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                        sub_v<ArchType::ASCEND_V220, float>(
                                            ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                            ls_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                            tv_ubuf_tensor,
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            0,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            1                              // src1RepeatStride
                                        );
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                    PIPE_BARRIER(V);
                                    // *** ls = exp(ls)
                                    exp_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor,
                                        ls_ubuf_tensor,
                                        (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1,                                                                // dstBlockStride
                                        1,                                                                // srcBlockStride
                                        8,                                                                // dstRepeatStride
                                        8                                                                 // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);

                                    // *** lp = castfp32to16(ls)
                                    if (IS_BF16) {
                                        convr_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                            lp_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                            ls32_ubuf_tensor,
                                            (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1, // dstBlockStride
                                            1, // srcBlockStride
                                            4, // dstRepeatStride
                                            8  // srcRepeatStride
                                        );
                                    } else {
                                        conv_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                            lp_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                            ls32_ubuf_tensor,
                                            (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                            1, // dstBlockStride
                                            1, // srcBlockStride
                                            4, // dstRepeatStride
                                            8  // srcRepeatStride
                                        );
                                    }
                                    PIPE_BARRIER(V);

                                    
                                    SET_FLAG(V, MTE3, EVENT_ID0);
                                    WAIT_FLAG(V, MTE3, EVENT_ID0);
                                    ub_to_gm<ArchType::ASCEND_V220, QKV_DT>(
                                        p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                                ((uint64_t)sub_block_idx * row_offset + split_idx * m_slice) * qk_round_n],
                                        lp_ubuf_tensor.ReinterpretCast<QKV_DT>(),
                                        0,                                // sid
                                        m_split,                                // nBurst
                                        qk_round_n / BLOCK_SIZE,  // lenBurst
                                        0,                                // srcGap
                                        0                                 // dstGap
                                    );
                                     
                                    SET_FLAG(MTE3, MTE2, EVENT_ID0);
                                    // *** ll = rowsum(ls32)
                                    for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                                        add_v<ArchType::ASCEND_V220, float>(
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            1,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                    }
                                    if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                        __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                        add_v<ArchType::ASCEND_V220, float>(
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor,
                                            ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                            m_split,                         // repeat
                                            1,                             // dstBlockStride
                                            1,                             // src0BlockStride
                                            1,                             // src1BlockStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                            qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                        );
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                    PIPE_BARRIER(V);
                                    cadd_v<ArchType::ASCEND_V220, float>(
                                        ll_ubuf_tensor,
                                        ls32_ubuf_tensor,
                                        m_split,                         // repeat
                                        1,                             // dstRepeatStride
                                        1,                             // srcBlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // srcRepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                    if (n_idx == 0) {
                                        // *** gl = ll
                                        ub_to_ub<ArchType::ASCEND_V220, float>(
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            ll_ubuf_tensor,
                                            0,                              // sid
                                            1,                              // nBurst
                                            round_m_split / FLOAT_BLOCK_SIZE, // lenBurst
                                            0,                              // srcGap
                                            0                               // dstGap
                                        );
                                        PIPE_BARRIER(V);
                                    } else {
                                        __set_mask(m_split);
                                        // *** gl = dm * gl
                                        mul_v<ArchType::ASCEND_V220, float>(
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            1, // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        // *** gl = ll + gl
                                        add_v<ArchType::ASCEND_V220, float>(
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            gl_ubuf_tensor[split_idx * m_slice],
                                            ll_ubuf_tensor,
                                            1, // repeat
                                            1,         // dstBlockStride
                                            1,         // src0BlockStride
                                            1,         // src1BlockStride
                                            8,         // dstRepeatStride
                                            8,         // src0RepeatStride
                                            8          // src1RepeatStride
                                        );
                                        PIPE_BARRIER(V);
                                        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                    }
                                }
                            }
                        }
                        mask_offset += qk_n;
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);
                }
                if (n_idx >= after) {
                    WaitFlagDev(UPDATE_READY);
                    uint64_t cube_k = BLOCK_QK_C;
                    uint64_t cube_round_k = BLOCK_QK_C;
                    for (uint32_t k_idx = 0; k_idx < loopV; k_idx++) {
                        if (k_idx % 2 == 0 && embdv - k_idx * BLOCK_QK_V < BLOCK_QK_C) {
                            cube_k = embdv - k_idx * BLOCK_QK_V;
                            cube_round_k = (cube_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        uint64_t o_offsetk = o_offset + k_idx * BLOCK_QK_V;
                        uint32_t qk_k = (k_idx == (loopV - 1)) ? embdv - k_idx * BLOCK_QK_V : BLOCK_QK_V;
                        uint32_t qk_round_k = (qk_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        uint64_t ori_offset = (uint64_t)block_idx * TMP_SIZE * LOCAL_SIZE + k_idx / 2 * TMP_SIZET * 2  + ((n_idx - after) / s_block_stack) % 2 * TMP_SIZE / 4 * loopVt +
                                        (uint64_t)sub_block_idx * row_offset * cube_round_k;
                        if (k_idx % 2 == 1) {
                            ori_offset += BLOCK_QK_V;
                        }
                        if (sub_m > 0) {
                            if (n_idx == after) {
                                WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
                                gm_to_ub<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor,
                                    o_tmp_gm_tensor[ori_offset],
                                    0,                                   // sid
                                    sub_m,                                   // nBurst
                                    qk_round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                                    (cube_round_k - qk_round_k)/ FLOAT_BLOCK_SIZE,                                   // srcGap
                                    0                                    // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID3);
                                WAIT_FLAG(MTE2, V, EVENT_ID3);
                            } else {
                                WAIT_FLAG(V, MTE2, EVENT_ID2);
                                gm_to_ub<ArchType::ASCEND_V220, float>(
                                    lo_ubuf_tensor,
                                    o_tmp_gm_tensor[ori_offset],
                                    0,                                   // sid
                                    sub_m,                                   // nBurst
                                    qk_round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                                    (cube_round_k - qk_round_k)/ FLOAT_BLOCK_SIZE,                                   // srcGap
                                    0                                      // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID2);

                                WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
                                gm_to_ub<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor,
                                    upo_gm_tensor[(uint64_t)block_idx * TMP_SIZE + k_idx * TMP_SIZET +
                                            (uint64_t)sub_block_idx * row_offset * qk_round_k],
                                    0,                                   // sid
                                    1,                                   // nBurst
                                    sub_m * qk_round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                                    0,                                   // srcGap
                                    0                                    // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID3);

                                PIPE_BARRIER(V);
                                // *** dm_block = expand_to_block(dm), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                    dm_ubuf_tensor[((n_idx - after) / s_block_stack % 4) * UB_FLOAT_LINE_SIZE].ReinterpretCast<uint32_t>(),
                                    1,                             // dstBlockStride
                                    8,                             // dstRepeatStride
                                    round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                PIPE_BARRIER(V);
                                WAIT_FLAG(MTE2, V, EVENT_ID3);
                                // *** go = go * dm_block
                                for (uint32_t vmul_idx = 0; vmul_idx < qk_k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                                    mul_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                        go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                        tv_ubuf_tensor,
                                        sub_m,                       // repeat
                                        1,                           // dstBlockStride
                                        1,                           // src0BlockStride
                                        0,                           // src1BlockStride
                                        qk_round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                        qk_round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                        1                            // src1RepeatStride
                                    );
                                }
                                if (qk_k % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_k % FLOAT_VECTOR_SIZE);
                                    mul_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[qk_k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        go_ubuf_tensor[qk_k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        tv_ubuf_tensor,
                                        sub_m,                       // repeat
                                        1,                           // dstBlockStride
                                        1,                           // src0BlockStride
                                        0,                           // src1BlockStride
                                        qk_round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                        qk_round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                        1                            // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);
                                
                                WAIT_FLAG(MTE2, V, EVENT_ID2);
                                // *** go = lo + go
                                add_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor,
                                    go_ubuf_tensor,
                                    lo_ubuf_tensor,
                                    (sub_m * qk_round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE,  // repeat
                                    1,                                                              // dstBlockStride
                                    1,                                                              // src0BlockStride
                                    1,                                                              // src1BlockStride
                                    8,                                                              // dstRepeatStride
                                    8,                                                              // src0RepeatStride
                                    8                                                               // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                SET_FLAG(V, MTE2, EVENT_ID2);
                            }
                            // *** gl = castfp32to16(gl)
                            if (n_idx + s_block_stack > n_end + after - 1) {
                                PIPE_BARRIER(V);
                                // *** gl_block = expand_to_block(gl), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint32_t>(tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                    gl_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                    1,                              // dstBlockStride
                                    8,                              // dstRepeatStride
                                    round_sub_m / FLOAT_BLOCK_SIZE  // repeat
                                );
                                PIPE_BARRIER(V);
                                // *** go = go / gl_block
                                for (uint32_t vdiv_idx = 0; vdiv_idx < qk_k / FLOAT_VECTOR_SIZE; ++vdiv_idx) {
                                    div_v<ArchType::ASCEND_V220, float>(go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                        go_ubuf_tensor[vdiv_idx * FLOAT_VECTOR_SIZE],
                                        tv_ubuf_tensor,
                                        sub_m,                 // repeat
                                        1,                     // dstBlockStride
                                        1,                     // src0BlockStride
                                        0,                     // src1BlockStride
                                        qk_round_k / FLOAT_BLOCK_SIZE,  // dstRepeatStride
                                        qk_round_k / FLOAT_BLOCK_SIZE,  // src0RepeatStride
                                        1                      // src1RepeatStride
                                    );
                                }
                                if (qk_k % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_k % FLOAT_VECTOR_SIZE);
                                    div_v<ArchType::ASCEND_V220, float>(
                                        go_ubuf_tensor[qk_k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        go_ubuf_tensor[qk_k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        tv_ubuf_tensor,
                                        sub_m,                      // repeat
                                        1,                          // dstBlockStride
                                        1,                          // src0BlockStride
                                        0,                          // src1BlockStride
                                        qk_round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        1                           // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);
                                if (IS_BF16) {
                                    convr_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                        go_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                        go_ubuf_tensor,
                                        (sub_m * qk_round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1, // dstBlockStride
                                        1, // srcBlockStride
                                        4, // dstRepeatStride
                                        8  // srcRepeatStride
                                    );
                                } else {
                                    conv_v<ArchType::ASCEND_V220, float, IN_DATA_TYPE>(
                                        go_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                        go_ubuf_tensor,
                                        (sub_m * qk_round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                        1, // dstBlockStride
                                        1, // srcBlockStride
                                        4, // dstRepeatStride
                                        8  // srcRepeatStride
                                    );
                                }
                                PIPE_BARRIER(V);
                                // ********************* move O to GM ************************
                                SET_FLAG(V, MTE3, EVENT_ID1);
                                WAIT_FLAG(V, MTE3, EVENT_ID1);
                                if (qN_blk_size_actual == 1) {
                                    ub_to_gm_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                        o_gm_tensor[o_offsetk + (uint64_t)sub_block_idx * row_offset * stride_qo],
                                        go_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>(),
                                        0,                     // sid
                                        sub_m,                 // nBurst
                                        qk_k * 2,              // lenBurst
                                        0,                     // leftPaddingNum
                                        0,                     // rightPaddingNum
                                        0,                     // srcGap
                                        (stride_qo - qk_k) * 2 // dstGap
                                    );
                                } else {
                                    uint32_t sub_core_qN_offset = (uint64_t)sub_block_idx * (qN_blk_size_actual / 2) * embdv;
                                    for (uint32_t qN_idx = 0; qN_idx < sub_qN; qN_idx++) {
                                        ub_to_gm_align<ArchType::ASCEND_V220, IN_DATA_TYPE>(
                                            o_gm_tensor[o_offsetk + sub_core_qN_offset + qN_idx * embdv],
                                            go_ubuf_tensor.ReinterpretCast<IN_DATA_TYPE>()[qN_idx * qS_blk_size_actual * qk_round_k],
                                            0,
                                            qS_blk_size_actual,
                                            qk_k * 2,
                                            0,
                                            0,
                                            0,
                                            (stride_qo - qk_k) * 2
                                        );
                                    }
                                }
                                SET_FLAG(MTE3, MTE2, EVENT_ID2);
                            } else {
                                PIPE_BARRIER(V);
                                SET_FLAG(V, MTE3, EVENT_ID2);
                                WAIT_FLAG(V, MTE3, EVENT_ID2);
                                ub_to_gm<ArchType::ASCEND_V220, float>(
                                    upo_gm_tensor[(uint64_t)block_idx * TMP_SIZE +  k_idx * TMP_SIZET +
                                            (uint64_t)sub_block_idx * row_offset * qk_round_k],
                                    go_ubuf_tensor,
                                    0,                                // sid
                                    1,                                // nBurst
                                    sub_m * qk_round_k / FLOAT_BLOCK_SIZE,  // lenBurst
                                    0,                                // srcGap
                                    0                                 // dstGap
                                );
                                SET_FLAG(MTE3, MTE2, EVENT_ID2);
                            }
                        }
                    }
                }
            }
        }
    }


    __aicore__ __attribute__((always_inline)) inline ~PagedMLAMultiTokenPredictionHpAiv()
    {
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID3);
	    WAIT_FLAG(V, MTE2, EVENT_ID4);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }
private:
    __gm__ uint8_t *__restrict__ mask_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ upo_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
    __gm__ uint8_t *__restrict__ deq_qk_gm{nullptr};
    __gm__ uint8_t *__restrict__ off_qk_gm{nullptr};
    __gm__ uint8_t *__restrict__ quant_p_gm{nullptr};
    __gm__ uint8_t *__restrict__ deq_pv_gm{nullptr};
    __gm__ uint8_t *__restrict__ off_pv_gm{nullptr};

    const uint32_t ls_ubuf_offset = 0;
    const uint32_t lp_ubuf_offset = 0;
    const uint32_t ls32_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lo_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE;
    const uint32_t hm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE;
    const uint32_t gm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE;
    const uint32_t dm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 4 * UB_UINT8_LINE_SIZE;
    const uint32_t ll_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 8 * UB_UINT8_LINE_SIZE;
    const uint32_t gl_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 12 * UB_UINT8_LINE_SIZE;
    const uint32_t tv_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 13 * UB_UINT8_LINE_SIZE;
    const uint32_t p_scale_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 21 * UB_UINT8_LINE_SIZE;
    const uint32_t go_ubuf_offset = 9 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask16_ubuf_offset = 11 * UB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::LocalTensor<float> ls_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls_ubuf_offset);
    AscendC::LocalTensor<float> lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lp_ubuf_offset);
    AscendC::LocalTensor<float> ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_ubuf_offset);
    AscendC::LocalTensor<float> mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(mask_ubuf_offset);
    AscendC::LocalTensor<float> lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
    AscendC::LocalTensor<float> lm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lm_ubuf_offset);
    AscendC::LocalTensor<float> hm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(hm_ubuf_offset);
    AscendC::LocalTensor<float> gm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gm_ubuf_offset);
    AscendC::LocalTensor<float> dm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(dm_ubuf_offset);
    AscendC::LocalTensor<float> ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
    AscendC::LocalTensor<float> gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_offset);
    AscendC::LocalTensor<float> tv_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv_ubuf_offset);
    AscendC::LocalTensor<float> p_scale_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(p_scale_ubuf_offset);
    AscendC::LocalTensor<float> go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_offset);
    AscendC::LocalTensor<IN_DATA_TYPE> mask16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, IN_DATA_TYPE>(mask16_ubuf_offset);

    AscendC::GlobalTensor<IN_DATA_TYPE> mask_gm_tensor;
    AscendC::GlobalTensor<O_DT> o_gm_tensor;
    AscendC::GlobalTensor<float> s_gm_tensor;
    AscendC::GlobalTensor<QKV_DT> p_gm_tensor;
    AscendC::GlobalTensor<float> o_tmp_gm_tensor;
    AscendC::GlobalTensor<float> upo_gm_tensor;

    uint32_t batch_size{0};
    uint32_t max_seqlen{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    uint32_t embdv{0};
    float tor{0};
    uint32_t head_stride{0};
    uint32_t mask_stride{0};
    uint32_t total_q_blk_num{0};
    uint64_t stride_qo{0};

    int32_t sub_block_idx{-1};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t long_seq{0};
    uint32_t mask_type{0};
    uint32_t block_size{0};
    uint32_t cur_qN_blk_size{0};
    uint32_t cur_qN_blk_num{0};
};

template <typename mm1InputType, typename mm2InputType, typename mmOutputType, CalcMode CUBE_MODE, FuncType WorkMode>
class PagedMLAMultiTokenPredictionAiv {
public:
    __aicore__ __attribute__((always_inline)) inline PagedMLAMultiTokenPredictionAiv(
            __gm__ uint8_t *__restrict__ sync, __gm__ uint8_t *__restrict__ mask_gm,
            __gm__ uint8_t *__restrict__ o_gm, __gm__ uint8_t *__restrict__ s_gm,
            __gm__ uint8_t *__restrict__ p_gm, __gm__ uint8_t *__restrict__ o_tmp_gm,
            __gm__ uint8_t *__restrict__ upo_tmp_gm,
            __gm__ uint8_t *__restrict__ tiling_para_gm) 
            : mask_gm(mask_gm), o_gm(o_gm), s_gm(s_gm), p_gm(p_gm), o_tmp_gm(o_tmp_gm), upo_tmp_gm(upo_tmp_gm),
            tiling_para_gm(tiling_para_gm)
    {
        mask_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(mask_gm));
        o_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(o_gm));
        s_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ half *>(s_gm));
        p_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mm2InputType *>(p_gm));
        o_tmp_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmOutputType *>(o_tmp_gm));
        upo_gm_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ mmOutputType *>(upo_tmp_gm));

	    this->sub_block_idx = GetSubBlockidx();
        this->batch_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm));
        this->max_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_MASK_MAX_LEN));
        this->q_heads = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_NUMHEADS));
        this->embd = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM));
        this->embdv = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEADDIM_V));
        this->tor = (half)(*((__gm__ float *)tiling_para_gm + TILING_TOR));
        this->block_size = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BLOCKSIZE));
        this->head_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_HEAD_STRIDE));
        this->mask_stride = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_BATCH_STRIDE));
        this->total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + TILING_TOTAL_BLOCK_NUM));
        this->tiling_head_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_HEADSIZE));
        this->tiling_para_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_PARASIZE));
        this->long_seq = 0;
        this->mask_type = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MASK_TYPE_ND));
        this->cur_qN_blk_size = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_SIZE));
        this->cur_qN_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + TILING_MTP_HEAD_SPLIT_NUM));
        this->stride_qo = q_heads * embdv;

        SET_FLAG(MTE3, MTE2, EVENT_ID0);
        SET_FLAG(MTE3, MTE2, EVENT_ID1);
        SET_FLAG(MTE3, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID0);
        SET_FLAG(V, MTE2, EVENT_ID1);
        SET_FLAG(V, MTE2, EVENT_ID2);
        SET_FLAG(V, MTE2, EVENT_ID3);
	    SET_FLAG(V, MTE2, EVENT_ID4);
        SET_FLAG(V, MTE2, EVENT_ID6);
        SET_FLAG(V, MTE2, EVENT_ID7);
        SET_FLAG(MTE3, V, EVENT_ID0);
    }
    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint32_t BlockSize()
    {
        return 32 / sizeof(T);
    }

   template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint32_t MatrixSize()
    {
        return 512 / sizeof(T);
    } 

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t BlockSizeRoundUp(uint64_t num)
    {
        return (num + BlockSize<T>() - 1) / BlockSize<T>() * BlockSize<T>();
    }

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t NumBlocksRoundUp(uint64_t num)
    {
        return (num + BlockSize<T>() - 1) / BlockSize<T>();
    }
    __aicore__ __attribute__((always_inline)) inline void __set_mask(int32_t len)
    {
        uint64_t mask = 0;
        uint64_t one = 1;
        uint64_t temp = len % FLOAT_VECTOR_SIZE;
        for (int64_t i = 0; i < temp; i++) {
            mask |= one << i;
        }

        if (len == VECTOR_SIZE || len == 0) {
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        } else if (len >= FLOAT_VECTOR_SIZE) {
            SetVectorMask<int8_t>(mask, (uint64_t)-1);
        } else {
            SetVectorMask<int8_t>(0x0, mask);
        }
    }

    __aicore__ __attribute__((always_inline)) inline void __set_vcg_mask(int32_t len)
    {
        if (len > 16 || len < 1) {
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            return;
        }
        uint64_t subMask = ((uint64_t) 1 << len) - 1;
        uint64_t maskValue = (subMask << 48) + (subMask << 32) + (subMask << 16) + subMask;
        SetVectorMask<int8_t>(maskValue, maskValue);
    }

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint32_t VectorSize()
    {
        return 256 / sizeof(T);
    }

    template<typename T>
    __aicore__ __attribute__((always_inline)) inline uint64_t NumVectorsRoundUp(uint64_t num)
    {
        return (num + VectorSize<T>() - 1) / VectorSize<T>();
    }

    template <typename T>
    __aicore__ inline void RowMaxRepeatM(
        const AscendC::LocalTensor<T> &dst,
        const AscendC::LocalTensor<T> &src,
        const AscendC::LocalTensor<T> &tempTensor,
        const uint32_t& sub_m,
        const uint32_t& qk_n,
        const uint32_t& qk_round_n
    )
    {
        uint32_t T_BLOCK_SIZE = 32 / sizeof(T);
        uint32_t T_VECTOR_SIZE = 256 / sizeof(T);
        if (qk_n <= T_VECTOR_SIZE) {
            __set_mask(qk_n);
            cmax_v<ArchType::ASCEND_V220, T, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(
                dst,
                src,
                sub_m,                      // repeat
                1,                          // dstRepeatStride
                1,                          // srcBlockStride
                qk_round_n / T_BLOCK_SIZE   // srcRepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        else {
            ub_to_ub<ArchType::ASCEND_V220, T>(
                tempTensor,
                src,
                0,                                                      // sid
                sub_m,                                                  // nBurst
                8,                                                      // lenBurst
                (qk_round_n - T_VECTOR_SIZE) / T_BLOCK_SIZE,            // srcGap
                0                                                       // dstGap
            );
            PIPE_BARRIER(V);
            for (uint32_t rowmax_idx = 1; rowmax_idx < qk_n / T_VECTOR_SIZE; ++rowmax_idx) {
                max_v<ArchType::ASCEND_V220, T>(
                    tempTensor,
                    tempTensor,
                    src[rowmax_idx * T_VECTOR_SIZE],
                    sub_m,                         // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    8,                             // dstRepeatStride
                    8,                             // src0RepeatStride
                    qk_round_n / T_BLOCK_SIZE      // src1RepeatStride
                );
                PIPE_BARRIER(V);
            }
            if (qk_n % T_VECTOR_SIZE > 0) {
                __set_mask(qk_n % T_VECTOR_SIZE);
                max_v<ArchType::ASCEND_V220, T>(
                    tempTensor,
                    tempTensor,
                    src[qk_n / T_VECTOR_SIZE * T_VECTOR_SIZE],
                    sub_m,                         // repeat
                    1,                             // dstBlockStride
                    1,                             // src0BlockStride
                    1,                             // src1BlockStride
                    8,                             // dstRepeatStride
                    8,                             // src0RepeatStride
                    qk_round_n / T_BLOCK_SIZE      // src1RepeatStride
                );
            }
            PIPE_BARRIER(V);
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
            cmax_v<ArchType::ASCEND_V220, T, AscendC::ReduceOrder::ORDER_ONLY_VALUE>(
                dst,
                tempTensor,
                sub_m,      // repeat
                1,          // dstRepeatStride
                1,          // srcBlockStride
                8           // srcRepeatStride
            );
        }
        SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        PIPE_BARRIER(V);
    }

    template <typename T>
    __aicore__ inline void MulRepeatM(
        const AscendC::LocalTensor<T> &dst,
        const AscendC::LocalTensor<T> &src0,
        const AscendC::LocalTensor<T> &src1,
        const uint32_t& sub_m,
        const uint32_t& sub_n,
        const uint32_t& sub_round_n
    )
    {
        uint32_t T_BLOCK_SIZE = 32 / sizeof(T);
        uint32_t T_VECTOR_SIZE = 256 / sizeof(T);

        for (uint32_t vmuls_idx = 0; vmuls_idx < sub_n / T_VECTOR_SIZE; ++vmuls_idx) {
            mul_v<ArchType::ASCEND_V220, T>(
                dst[vmuls_idx * T_VECTOR_SIZE],
                src0[vmuls_idx * T_VECTOR_SIZE],
                src1,
                sub_m,                      // repeat
                1,                          // dstBlockStride
                1,                          // src0BlockStride
                0,                          // src1BlockStride
                sub_round_n / T_BLOCK_SIZE,  // dstRepeatStride
                sub_round_n / T_BLOCK_SIZE,  // src0RepeatStride
                1                           // src1RepeatStride
            );
        }
        if (sub_n % FLOAT_VECTOR_SIZE > 0) {
            __set_mask(sub_n % FLOAT_VECTOR_SIZE);
            mul_v<ArchType::ASCEND_V220, T>(
                dst[sub_n / T_VECTOR_SIZE * T_VECTOR_SIZE],
                src0[sub_n / T_VECTOR_SIZE * T_VECTOR_SIZE],
                src1,
                sub_m,                      // repeat
                1,                          // dstBlockStride
                1,                          // src0BlockStride
                0,                          // src1BlockStride
                sub_round_n / T_BLOCK_SIZE,  // dstRepeatStride
                sub_round_n / T_BLOCK_SIZE,  // src0RepeatStride
                1                           // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    template <typename T>
    __aicore__ inline void DivRepeatM(
        const AscendC::LocalTensor<T> &dst,
        const AscendC::LocalTensor<T> &src0,
        const AscendC::LocalTensor<T> &src1,
        const uint32_t& sub_m,
        const uint32_t& sub_n,
        const uint32_t& sub_round_n
    )
    {
        uint32_t temp = sizeof(T);
        uint32_t T_BLOCK_SIZE = 32 / sizeof(T);
        uint32_t T_VECTOR_SIZE = 256 / sizeof(T);

        for (uint32_t vdiv_idx = 0; vdiv_idx < sub_n / T_VECTOR_SIZE; ++vdiv_idx) {
            div_v<ArchType::ASCEND_V220, T>(
                dst[vdiv_idx * T_VECTOR_SIZE],
                src0[vdiv_idx * T_VECTOR_SIZE],
                src1,
                sub_m,                      // repeat
                1,                          // dstBlockStride
                1,                          // src0BlockStride
                0,                          // src1BlockStride
                sub_round_n / T_BLOCK_SIZE,  // dstRepeatStride
                sub_round_n / T_BLOCK_SIZE,  // src0RepeatStride
                1                           // src1RepeatStride
            );
        }
        if (sub_n % T_VECTOR_SIZE > 0) {
            __set_mask(sub_n % T_VECTOR_SIZE);
            div_v<ArchType::ASCEND_V220, T>(
                dst[sub_n / T_VECTOR_SIZE * T_VECTOR_SIZE],
                src0[sub_n / T_VECTOR_SIZE * T_VECTOR_SIZE],
                src1,
                sub_m,                      // repeat
                1,                          // dstBlockStride
                1,                          // src0BlockStride
                0,                          // src1BlockStride
                sub_round_n / T_BLOCK_SIZE,  // dstRepeatStride
                sub_round_n / T_BLOCK_SIZE,  // src0RepeatStride
                1                           // src1RepeatStride
            );
            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
        }
        PIPE_BARRIER(V);
    }

    __aicore__ __attribute__((always_inline)) inline void Run()
    {
        uint32_t s_block_stack = 1;
        uint32_t after = 1; 
        if constexpr (CUBE_MODE == CalcMode::CALC_MODE_256) {
            s_block_stack = 2;
            after = 2; 
        }
        uint32_t loopV = (embdv + BLOCK_QK_V - 1) / BLOCK_QK_V;
        uint32_t loopVt = (embdv + BLOCK_QK_C - 1) / BLOCK_QK_C;
        uint64_t cur_batch = 0;
        uint32_t pre_total_q_blk_num = 0;
        uint32_t offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
        uint32_t cur_total_q_blk_num = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 9 + offset_tiling));
        uint32_t process_num = total_q_blk_num;
        uint32_t next_process = 0;
        uint32_t process_idx = 0;
        for (uint32_t process = block_idx; process < process_num; process = next_process) {
            if constexpr (WorkMode == FuncType::UNEQUAL_QK_LEN_PREFILL) {
                while (process >= cur_total_q_blk_num) {
                    cur_batch++;
                    pre_total_q_blk_num = cur_total_q_blk_num;
                    offset_tiling += tiling_para_size;
                    cur_total_q_blk_num = (uint32_t)(*((__gm__ uint32_t *)tiling_para_gm + 9 + offset_tiling));
                }
                process_idx = process - pre_total_q_blk_num;
            } else {
                cur_batch = process / cur_qN_blk_num;
                offset_tiling = tiling_head_size + tiling_para_size * cur_batch;
                process_idx = process % cur_qN_blk_num;
            }
            next_process = process + block_num;
            // get tiling args
            uint32_t q_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + offset_tiling));
            uint32_t kv_seqlen = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 1 + offset_tiling));
            if (q_seqlen == 0 || kv_seqlen == 0) {
                continue;
            }
            uint32_t pp_m_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 2 + offset_tiling));
            uint32_t pp_n_scalar = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 3 + offset_tiling));
            uint32_t addr_o_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 6 + offset_tiling));
            uint32_t addr_o_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 7 + offset_tiling));
            uint64_t addr_o_scalar = (uint64_t)(((uint64_t)addr_o_high32) << 32 | addr_o_loww32);
            uint32_t mask_high32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 10 + offset_tiling));
            uint32_t mask_loww32 = (uint32_t)(*((__gm__ int32_t *)tiling_para_gm + 14 + offset_tiling));
            uint64_t mask_scalar = (uint64_t)(((uint64_t)mask_high32) << 32 | mask_loww32);

            uint32_t qS_blk_idx = process_idx / cur_qN_blk_num;
            uint32_t qN_blk_idx = process_idx % cur_qN_blk_num;
            
            uint32_t head_start_idx = qN_blk_idx * cur_qN_blk_size;

            uint32_t m_loop = (q_seqlen + pp_m_scalar - 1) / pp_m_scalar;
            uint32_t n_loop = (kv_seqlen + pp_n_scalar - 1) / pp_n_scalar;

            uint32_t qS_blk_size_actual = (qS_blk_idx == (m_loop - 1)) ? (q_seqlen - qS_blk_idx * pp_m_scalar) : pp_m_scalar;
            uint32_t qN_blk_size_actual = (qN_blk_idx == (cur_qN_blk_num - 1)) ? (q_heads - qN_blk_idx * cur_qN_blk_size) : cur_qN_blk_size;

            uint32_t qk_m = qS_blk_size_actual * qN_blk_size_actual;
            // when qS > 16, qN tile is 1, qS_blk is splited between 2 vector cores
            // when qS < 16, qN_blk is splited between 2 vector cores, each vector core pocesses multiple qS_blk's
            uint32_t sub_qN = (qN_blk_size_actual == 1) ?
                              0 : ((sub_block_idx == 1) ?
                                  (qN_blk_size_actual - qN_blk_size_actual / 2) :
                                  (qN_blk_size_actual / 2));

            uint32_t row_offset = (qN_blk_size_actual == 1) ?
                                  (qk_m / 2) :
                                  (qS_blk_size_actual * (qN_blk_size_actual / 2));
            uint32_t sub_m = (sub_block_idx == 1) ? (qk_m - row_offset) : row_offset;

            uint32_t sub_m_d128 = (sub_m + VECTOR_SIZE - 1) / VECTOR_SIZE;             // up aligned to 128
            uint32_t sub_m_d64 = (sub_m + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE;  // up aligned to 64
            uint32_t round_sub_m = (sub_m + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            /******** pre_load *******/
            uint32_t qk_n = n_loop == 1 ? kv_seqlen : pp_n_scalar;
            uint32_t qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

            uint32_t pingpong_flag = 0;
            uint32_t offset = pingpong_flag * UB_HALF_BUF_SIZE;
            uint64_t mask_batch_offset = cur_batch * mask_stride * max_seqlen;
            uint64_t mask_head_offset = head_start_idx * ((uint64_t)head_stride) * max_seqlen;
            uint64_t mask_offset = mask_batch_offset + mask_head_offset + mask_scalar;
            mask_offset += qS_blk_idx * pp_m_scalar * max_seqlen;

            uint64_t o_offset = addr_o_scalar + head_start_idx * embdv + qS_blk_idx * pp_m_scalar * stride_qo;
            uint32_t n_end = n_loop;
            if (mask_type == 3) {
                uint32_t no_mask_kv_seq = kv_seqlen - q_seqlen;
                uint32_t no_skip_kv_seq = (qS_blk_idx + 1) * pp_m_scalar + no_mask_kv_seq;
                no_skip_kv_seq = (no_skip_kv_seq > kv_seqlen) ? kv_seqlen : no_skip_kv_seq;
                n_end = (no_skip_kv_seq + pp_n_scalar - 1) / pp_n_scalar;
            }
            uint32_t qk_n_triu = n_end * pp_n_scalar;
            uint32_t m_slice = sub_m > 32 ? 32 : 0;
            uint32_t m_end = sub_m > 32 ? 2 : 1;

            for (uint32_t n_idx = 0; n_idx < n_end + after; n_idx += s_block_stack) {
                if (n_idx < n_end) {
                    if (n_idx + s_block_stack > n_end - 1) {
                        qk_n =
                            qk_n_triu > kv_seqlen ? kv_seqlen - n_idx * pp_n_scalar : qk_n_triu - n_idx * pp_n_scalar;
                    } else {
                        qk_n = pp_n_scalar * s_block_stack;
                    }
                    qk_round_n = (qk_n + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                    if (sub_m > 0 && mask_type != 0) {
                        if (qk_n <= pp_n_scalar) {
                            pingpong_flag = n_idx % 2;
                            offset = pingpong_flag * UB_HALF_BUF_SIZE;
                            WAIT_FLAG(V, MTE2, pingpong_flag + 2);
                            // when qS > 16, qN tile is 1, qS_blk is splited between 2 vector cores
                            // when qS < 16, qN_blk is splited between 2 vector cores, each vector core pocesses multiple qS_blk's
                            if (qN_blk_size_actual == 1) {
                                gm_to_ub_align<ArchType::ASCEND_V220, half>(
                                    mask_ubuf_tensor[offset],
                                    mask_gm_tensor[mask_offset + (uint64_t)sub_block_idx * qk_m / 2 * max_seqlen],
                                    0,                       // sid
                                    sub_m,                   // nBurst
                                    qk_n * 2,                // lenBurst
                                    0,                       // leftPaddingNum
                                    0,                       // rightPaddingNum
                                    (max_seqlen - qk_n) * 2, // srcGap
                                    0                        // dstGap
                                );
                            } else {
                                for (uint32_t qN_idx = 0; qN_idx < sub_qN; qN_idx++) {
                                    gm_to_ub_align<ArchType::ASCEND_V220, half>(
                                        mask_ubuf_tensor[offset + qN_idx * qS_blk_size_actual * qk_round_n],
                                        mask_gm_tensor[mask_offset],
                                        0,                       // sid
                                        qS_blk_size_actual,      // nBurst
                                        qk_n * 2,                // lenBurst
                                        0,                       // leftPaddingNum
                                        0,                       // rightPaddingNum
                                        (max_seqlen - qk_n) * 2, // srcGap
                                        0                        // dstGap
                                    );
                                }
                            }
                            SET_FLAG(MTE2, V, pingpong_flag + 2);
                        } else {
                            WAIT_FLAG(V, MTE2, EVENT_ID2);
                            if (qN_blk_size_actual == 1) {
                                gm_to_ub_align<ArchType::ASCEND_V220, half>(
                                    mask_ubuf_tensor[offset],
                                    mask_gm_tensor[mask_offset + (uint64_t)sub_block_idx * qk_m / 2 * max_seqlen],
                                    0,                       // sid
                                    sub_m,                   // nBurst
                                    qk_n * 2,                // lenBurst
                                    0,                       // leftPaddingNum
                                    0,                       // rightPaddingNum
                                    (max_seqlen - qk_n) * 2, // srcGap
                                    0                        // dstGap
                                );
                            } else {
                                for (uint32_t qN_idx = 0; qN_idx < sub_qN; qN_idx++) {
                                    gm_to_ub_align<ArchType::ASCEND_V220, half>(
                                        mask_ubuf_tensor[offset + qN_idx * qS_blk_size_actual * qk_round_n],
                                        mask_gm_tensor[mask_offset],
                                        0,                       // sid
                                        qS_blk_size_actual,      // nBurst
                                        qk_n * 2,                // lenBurst
                                        0,                       // leftPaddingNum
                                        0,                       // rightPaddingNum
                                        (max_seqlen - qk_n) * 2, // srcGap
                                        0                        // dstGap
                                    );
                                }
                            }
                            SET_FLAG(MTE2, V, EVENT_ID2);
                        }
                        mask_offset += qk_n;
                    }
                    WaitFlagDev(QK_READY);
                    uint32_t qk_n_reduce_sum = qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE;
                    if (qk_n <= VECTOR_SIZE) {
                        pingpong_flag = n_idx % 2;
                        offset = pingpong_flag * UB_HALF_BUF_SIZE;
                        if (sub_m > 0) {
                            // int32_t
                            WAIT_FLAG(MTE3, MTE2, pingpong_flag);
                            if (s_block_stack == 2) {
                                WAIT_FLAG(MTE3, MTE2, 1 - pingpong_flag);
                            }
                            // input QK
                            gm_to_ub<ArchType::ASCEND_V220, half>(
                                ls_ubuf_tensor[offset],
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                            (uint64_t)sub_block_idx * row_offset * qk_round_n],
                                0,                       // sid
                                sub_m,                   // nBurst
                                qk_round_n / BLOCK_SIZE, // lenBurst
                                0,                       // srcGap
                                0                        // dstGap
                            );
                            SET_FLAG(MTE2, V, EVENT_ID0);
                            WAIT_FLAG(MTE2, V, EVENT_ID0);
                            // *** ls = tor * ls
                            muls_v<ArchType::ASCEND_V220, half>(
                                ls_ubuf_tensor[offset], ls_ubuf_tensor[offset], tor,
                                (sub_m * qk_round_n + VECTOR_SIZE - 1) / VECTOR_SIZE, // repeat
                                1,                                                    // dstBlockStride
                                1,                                                    // srcBlockStride
                                8,                                                    // dstRepeatStride
                                8                                                     // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** ls = ls + mask
                            if (mask_type != 0) {
                                WAIT_FLAG(MTE2, V, pingpong_flag + 2);
                                add_v<ArchType::ASCEND_V220, half>(
                                    ls_ubuf_tensor[offset], ls_ubuf_tensor[offset], mask_ubuf_tensor[offset],
                                    (sub_m * qk_round_n + VECTOR_SIZE - 1) / VECTOR_SIZE, // repeat
                                    1,                                                    // dstBlockStride
                                    1,                                                    // src0BlockStride
                                    1,                                                    // src1BlockStride
                                    8,                                                    // dstRepeatStride
                                    8,                                                    // src0RepeatStride
                                    8                                                     // src1RepeatStride
                                );
                                SET_FLAG(V, MTE2, pingpong_flag + 2);
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            // *** lm = rowmax(ls)
                            __set_mask(qk_n);
                            cgmax_v<ArchType::ASCEND_V220, half>(tv_ubuf_tensor.ReinterpretCast<half>(),
                                                                ls_ubuf_tensor[offset], sub_m, 2, 1,
                                                                qk_round_n / BLOCK_SIZE);
                            PIPE_BARRIER(V);
                            __set_vcg_mask(qk_round_n / BLOCK_SIZE);
                            cgmax_v<ArchType::ASCEND_V220, half>(
                                lm_ubuf_tensor, tv_ubuf_tensor.ReinterpretCast<half>(),
                                (sub_m * BLOCK_SIZE + VECTOR_SIZE - 1) / VECTOR_SIZE, 1, 1, 8);
                            SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            PIPE_BARRIER(V);
                            if (n_idx == 0) {
                                // *** hm = lm
                                ub_to_ub<ArchType::ASCEND_V220, half>(hm_ubuf_tensor, lm_ubuf_tensor,
                                                                    0,                        // sid
                                                                    1,                        // nBurst
                                                                    round_sub_m / BLOCK_SIZE, // lenBurst
                                                                    0,                        // srcGap
                                                                    0                         // dstGap
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** hm = vmax(lm, gm)
                                max_v<ArchType::ASCEND_V220, half>(hm_ubuf_tensor, lm_ubuf_tensor, gm_ubuf_tensor,
                                                                sub_m_d128, // repeat
                                                                1,          // dstBlockStride
                                                                1,          // src0BlockStride
                                                                1,          // src1BlockStride
                                                                8,          // dstRepeatStride
                                                                8,          // src0RepeatStride
                                                                8           // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = gm - hm
                                sub_v<ArchType::ASCEND_V220, half>(
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_HALF_LINE_SIZE], gm_ubuf_tensor,
                                    hm_ubuf_tensor,
                                    sub_m_d128, // repeat
                                    1,          // dstBlockStride
                                    1,          // src0BlockStride
                                    1,          // src1BlockStride
                                    8,          // dstRepeatStride
                                    8,          // src0RepeatStride
                                    8           // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                            // *** gm = hm
                            ub_to_ub<ArchType::ASCEND_V220, half>(gm_ubuf_tensor, hm_ubuf_tensor,
                                                                0,                        // sid
                                                                1,                        // nBurst
                                                                round_sub_m / BLOCK_SIZE, // lenBurst
                                                                0,                        // srcGap
                                                                0                         // dstGap
                            );
                            PIPE_BARRIER(V);
                            // *** hm_block = expand_to_block(hm), 存放于 tv
                            brcb_v<ArchType::ASCEND_V220, uint16_t>(tv_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                    hm_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                    1,                             // dstBlockStride
                                                                    8,                             // dstRepeatStride
                                                                    round_sub_m / FLOAT_BLOCK_SIZE // repeat
                            );
                            PIPE_BARRIER(V);
                            // *** ls = ls - hm_block
                            for (uint32_t vsub_idx = 0; vsub_idx < qk_n / VECTOR_SIZE; ++vsub_idx) {
                                sub_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[offset + vsub_idx * VECTOR_SIZE],
                                                                ls_ubuf_tensor[offset + vsub_idx * VECTOR_SIZE],
                                                                tv_ubuf_tensor.ReinterpretCast<half>(),
                                                                sub_m,                   // repeat
                                                                1,                       // dstBlockStride
                                                                1,                       // src0BlockStride
                                                                0,                       // src1BlockStride
                                                                qk_round_n / BLOCK_SIZE, // dstRepeatStride
                                                                qk_round_n / BLOCK_SIZE, // src0RepeatStride
                                                                1                        // src1RepeatStride
                                );
                            }
                            if (qk_n % VECTOR_SIZE > 0) {
                                __set_mask(qk_n % VECTOR_SIZE);
                                sub_v<ArchType::ASCEND_V220, half>(
                                    ls_ubuf_tensor[offset + qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                    ls_ubuf_tensor[offset + qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<half>(),
                                    sub_m,                   // repeat
                                    1,                       // dstBlockStride
                                    1,                       // src0BlockStride
                                    0,                       // src1BlockStride
                                    qk_round_n / BLOCK_SIZE, // dstRepeatStride
                                    qk_round_n / BLOCK_SIZE, // src0RepeatStride
                                    1                        // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            // *** ls = castfp16to32(ls)
                            conv_v<ArchType::ASCEND_V220, half, float>(
                                ls32_ubuf_tensor, ls_ubuf_tensor[offset],
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                4                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** ls = exp(ls)
                            exp_v<ArchType::ASCEND_V220, float>(
                                ls32_ubuf_tensor, ls32_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                8,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            // *** lp = castfp32to16(ls)
                            conv_v<ArchType::ASCEND_V220, float, mm2InputType>(
                                lp_ubuf_tensor[offset], ls32_ubuf_tensor,
                                (sub_m * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                1,                                                                // dstBlockStride
                                1,                                                                // srcBlockStride
                                4,                                                                // dstRepeatStride
                                8                                                                 // srcRepeatStride
                            );
                            PIPE_BARRIER(V);

                            SET_FLAG(V, MTE3, EVENT_ID0);
                            // *** ll = rowsum(ls32)
                            if (qk_n <= FLOAT_VECTOR_SIZE) {
                                __set_mask(qk_n);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE], ls32_ubuf_tensor,
                                    sub_m,                        // repeat
                                    1,                            // dstRepeatStride
                                    1,                            // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE // srcRepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor, ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor, ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[qk_n / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE], ls32_ubuf_tensor,
                                    sub_m,                        // repeat
                                    1,                            // dstRepeatStride
                                    1,                            // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE // srcRepeatStride
                                );
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == 0) {
                                ub_to_ub<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor, ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    0,                              // sid
                                    1,                              // nBurst
                                    round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                              // srcGap
                                    0                               // dstGap
                                );
                            } else {
                                conv_v<ArchType::ASCEND_V220, half, float>(
                                    tv_ubuf_tensor, dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_HALF_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // srcBlockStride
                                    8,         // dstRepeatStride
                                    4          // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = exp(dm)
                                exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                                    sub_m_d64, // repeat
                                                                    1,         // dstBlockStride
                                                                    1,         // srcBlockStride
                                                                    8,         // dstRepeatStride
                                                                    8          // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm_block = expand_to_block(dm), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>()[VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                    1,                             // dstBlockStride
                                    8,                             // dstRepeatStride
                                    round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                PIPE_BARRIER(V);
                                // *** gl = dm * gl
                                mul_v<ArchType::ASCEND_V220, float>(gl_ubuf_tensor, tv_ubuf_tensor, gl_ubuf_tensor,
                                                                    sub_m_d64, // repeat
                                                                    1,         // dstBlockStride
                                                                    1,         // src0BlockStride
                                                                    1,         // src1BlockStride
                                                                    8,         // dstRepeatStride
                                                                    8,         // src0RepeatStride
                                                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** gl = ll + gl
                                add_v<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor, gl_ubuf_tensor,
                                    ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                            PIPE_BARRIER(V);
                            WAIT_FLAG(V, MTE3, EVENT_ID0);
                            ub_to_gm<ArchType::ASCEND_V220, mm2InputType>(
                                p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                            (uint64_t)sub_block_idx * row_offset * qk_round_n],
                                lp_ubuf_tensor[offset],
                                0,                               // sid
                                1,                               // nBurst
                                sub_m * qk_round_n / BLOCK_SIZE, // lenBurst
                                0,                               // srcGap
                                0                                // dstGap
                            );
                            SET_FLAG(MTE3, MTE2, pingpong_flag);
                            if (s_block_stack == 2) {
                                SET_FLAG(MTE3, MTE2, 1 - pingpong_flag);
                            }
                        }
                    } else {
                        bool last_n_loop = n_idx + s_block_stack > n_end - 1;
                        if (sub_m > 0) {
                            WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
                            // input QK
                            gm_to_ub<ArchType::ASCEND_V220, half>(
                                ls_ubuf_tensor,
                                s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                            (uint64_t)sub_block_idx * row_offset * qk_round_n],
                                0,                       // sid
                                m_slice,                 // nBurst
                                qk_round_n / BLOCK_SIZE, // lenBurst
                                0,                       // srcGap
                                0                        // dstGap
                            );
                            if (sub_m > m_slice) {
                                if (m_end > 1) {
                                    WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
                                }
                                gm_to_ub<ArchType::ASCEND_V220, half>(
                                    ls_ubuf_tensor[m_slice * qk_round_n],
                                    s_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                                (uint64_t)sub_block_idx * row_offset * qk_round_n + m_slice * qk_round_n],
                                    0,                       // sid
                                    sub_m - m_slice,         // nBurst
                                    qk_round_n / BLOCK_SIZE, // lenBurst
                                    0,                       // srcGap
                                    0                        // dstGap
                                );
                            }
                            SET_FLAG(MTE2, V, EVENT_ID0);
                            WAIT_FLAG(MTE2, V, EVENT_ID0);
                            // *** ls = tor * ls
                            muls_v<ArchType::ASCEND_V220, half>(
                                ls_ubuf_tensor, ls_ubuf_tensor, tor,
                                (sub_m * qk_round_n + VECTOR_SIZE - 1) / VECTOR_SIZE, // repeat
                                1,                                                    // dstBlockStride
                                1,                                                    // srcBlockStride
                                8,                                                    // dstRepeatStride
                                8                                                     // srcRepeatStride
                            );
                            PIPE_BARRIER(V);
                            if (mask_type != 0) {
                                WAIT_FLAG(MTE2, V, EVENT_ID2);
                                add_v<ArchType::ASCEND_V220, half>(
                                    ls_ubuf_tensor, ls_ubuf_tensor, mask_ubuf_tensor,
                                    (sub_m * qk_round_n + VECTOR_SIZE - 1) / VECTOR_SIZE, // repeat
                                    1,                                                    // dstBlockStride
                                    1,                                                    // src0BlockStride
                                    1,                                                    // src1BlockStride
                                    8,                                                    // dstRepeatStride
                                    8,                                                    // src0RepeatStride
                                    8                                                     // src1RepeatStride
                                );
                                SET_FLAG(V, MTE2, EVENT_ID2);
                                PIPE_BARRIER(V);
                            }
                            if (qk_n != SOFTMAX_MAX_LENGTH) {
                                ub_to_ub<ArchType::ASCEND_V220, half>(ls32_ubuf_tensor.ReinterpretCast<half>(),
                                                                    ls_ubuf_tensor,
                                                                    0,                        // sid
                                                                    sub_m,                    // nBurst
                                                                    VECTOR_SIZE / BLOCK_SIZE, // lenBurst
                                                                    (qk_round_n - VECTOR_SIZE) / BLOCK_SIZE, // srcGap
                                                                    0                                        // dstGap
                                );
                                PIPE_BARRIER(V);
                                __set_mask(qk_n - VECTOR_SIZE);
                                max_v<ArchType::ASCEND_V220, half>(ls32_ubuf_tensor.ReinterpretCast<half>(),
                                                                ls32_ubuf_tensor.ReinterpretCast<half>(),
                                                                ls_ubuf_tensor[VECTOR_SIZE],
                                                                sub_m,                  // repeat
                                                                1,                      // dstBlockStride
                                                                1,                      // src0BlockStride
                                                                1,                      // src1BlockStride
                                                                8,                      // dstRepeatStride
                                                                8,                      // src0RepeatStride
                                                                qk_round_n / BLOCK_SIZE // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                cgmax_v<ArchType::ASCEND_V220, half>(tv_ubuf_tensor.ReinterpretCast<half>(),
                                                                    ls32_ubuf_tensor.ReinterpretCast<half>(), sub_m, 2,
                                                                    1, 8);
                                PIPE_BARRIER(V);
                                __set_vcg_mask(VECTOR_SIZE / BLOCK_SIZE);
                                cgmax_v<ArchType::ASCEND_V220, half>(
                                    lm_ubuf_tensor, tv_ubuf_tensor.ReinterpretCast<half>(),
                                    (sub_m * BLOCK_SIZE + VECTOR_SIZE - 1) / VECTOR_SIZE, 1, 1, 8);
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            } else {
                                cgmax_v<ArchType::ASCEND_V220, half>(ls32_ubuf_tensor.ReinterpretCast<half>(),
                                                                    ls_ubuf_tensor, sub_m * qk_round_n / VECTOR_SIZE,
                                                                    1, 1, 8);
                                PIPE_BARRIER(V);
                                cgmax_v<ArchType::ASCEND_V220, half>(
                                    lm_ubuf_tensor, ls32_ubuf_tensor.ReinterpretCast<half>(),
                                    (sub_m * BLOCK_SIZE + VECTOR_SIZE - 1) / VECTOR_SIZE, 1, 1, 8);
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == 0) {
                                // *** hm = lm
                                ub_to_ub<ArchType::ASCEND_V220, half>(gm_ubuf_tensor, lm_ubuf_tensor,
                                                                    0,                        // sid
                                                                    1,                        // nBurst
                                                                    round_sub_m / BLOCK_SIZE, // lenBurst
                                                                    0,                        // srcGap
                                                                    0                         // dstGap
                                );
                                brcb_v<ArchType::ASCEND_V220, uint16_t>(tv_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                        lm_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                        1, // dstBlockStride
                                                                        8, // dstRepeatStride
                                                                        round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                PIPE_BARRIER(V);
                            } else {
                                // *** hm = vmax(lm, gm)
                                max_v<ArchType::ASCEND_V220, half>(hm_ubuf_tensor, lm_ubuf_tensor, gm_ubuf_tensor,
                                                                1, // repeat
                                                                1, // dstBlockStride
                                                                1, // src0BlockStride
                                                                1, // src1BlockStride
                                                                8, // dstRepeatStride
                                                                8, // src0RepeatStride
                                                                8  // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** hm_block = expand_to_block(hm), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint16_t>(tv_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                        hm_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                        1, // dstBlockStride
                                                                        8, // dstRepeatStride
                                                                        round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                // *** dm = gm - hm
                                sub_v<ArchType::ASCEND_V220, half>(
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_HALF_LINE_SIZE],
                                    gm_ubuf_tensor, hm_ubuf_tensor,
                                    1, // repeat
                                    1, // dstBlockStride
                                    1, // src0BlockStride
                                    1, // src1BlockStride
                                    8, // dstRepeatStride
                                    8, // src0RepeatStride
                                    8  // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** gm = hm
                                ub_to_ub<ArchType::ASCEND_V220, half>(gm_ubuf_tensor, hm_ubuf_tensor,
                                                                    0,                        // sid
                                                                    1,                        // nBurst
                                                                    round_sub_m / BLOCK_SIZE, // lenBurst
                                                                    0,                        // srcGap
                                                                    0                         // dstGap
                                );
                                PIPE_BARRIER(V);
                            }
                            // *** ls = ls - hm_block
                            for (uint32_t vsub_idx = 0; vsub_idx < qk_n / VECTOR_SIZE; ++vsub_idx) {
                                sub_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[vsub_idx * VECTOR_SIZE],
                                                                ls_ubuf_tensor[vsub_idx * VECTOR_SIZE],
                                                                tv_ubuf_tensor.ReinterpretCast<half>(),
                                                                sub_m,                   // repeat
                                                                1,                       // dstBlockStride
                                                                1,                       // src0BlockStride
                                                                0,                       // src1BlockStride
                                                                qk_round_n / BLOCK_SIZE, // dstRepeatStride
                                                                qk_round_n / BLOCK_SIZE, // src0RepeatStride
                                                                1                        // src1RepeatStride
                                );
                            }
                            if (qk_n % VECTOR_SIZE > 0) {
                                __set_mask(qk_n % VECTOR_SIZE);
                                sub_v<ArchType::ASCEND_V220, half>(ls_ubuf_tensor[qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                                                ls_ubuf_tensor[qk_n / VECTOR_SIZE * VECTOR_SIZE],
                                                                tv_ubuf_tensor.ReinterpretCast<half>(),
                                                                sub_m,                   // repeat
                                                                1,                       // dstBlockStride
                                                                1,                       // src0BlockStride
                                                                0,                       // src1BlockStride
                                                                qk_round_n / BLOCK_SIZE, // dstRepeatStride
                                                                qk_round_n / BLOCK_SIZE, // src0RepeatStride
                                                                1                        // src1RepeatStride
                                );
                                SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                            }
                            PIPE_BARRIER(V);
                            
                            for (uint32_t split_idx = 0; split_idx < m_end; split_idx++) {
                                bool last_m_loop = split_idx == m_end - 1;
                                uint32_t m_split = last_m_loop ? sub_m - split_idx * m_slice : m_slice;
                                uint32_t round_m_split = (m_split + FLOAT_BLOCK_SIZE - 1) / FLOAT_BLOCK_SIZE * FLOAT_BLOCK_SIZE;
                                // *** ls = castfp16to32(ls)
                                conv_v<ArchType::ASCEND_V220, half, float>(
                                    ls32_ubuf_tensor, ls_ubuf_tensor[split_idx * m_slice * qk_round_n],
                                    (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1, // dstBlockStride
                                    1, // srcBlockStride
                                    8, // dstRepeatStride
                                    4  // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** ls = exp(ls)
                                exp_v<ArchType::ASCEND_V220, float>(
                                    ls32_ubuf_tensor, ls32_ubuf_tensor,
                                    (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1, // dstBlockStride
                                    1, // srcBlockStride
                                    8, // dstRepeatStride
                                    8  // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** lp = castfp32to16(ls)
                                conv_v<ArchType::ASCEND_V220, float, mm2InputType>(
                                    lp_ubuf_tensor[split_idx * m_slice * qk_round_n], ls32_ubuf_tensor,
                                    (m_split * qk_round_n + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1, // dstBlockStride
                                    1, // srcBlockStride
                                    4, // dstRepeatStride
                                    8  // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                SET_FLAG(V, MTE3, EVENT_ID0);
                                // *** ll = rowsum(ls32)
                                for (uint32_t rowsum_idx = 1; rowsum_idx < qk_n / FLOAT_VECTOR_SIZE; ++rowsum_idx) {
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor, ls32_ubuf_tensor,
                                        ls32_ubuf_tensor[rowsum_idx * FLOAT_VECTOR_SIZE],
                                        m_split,                       // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                    PIPE_BARRIER(V);
                                }
                                if (qk_n % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_n % FLOAT_VECTOR_SIZE);
                                    add_v<ArchType::ASCEND_V220, float>(
                                        ls32_ubuf_tensor, ls32_ubuf_tensor, ls32_ubuf_tensor[qk_n_reduce_sum],
                                        m_split,                       // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        1,                             // src1BlockStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        qk_round_n / FLOAT_BLOCK_SIZE  // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);
                                cadd_v<ArchType::ASCEND_V220, float>(
                                    ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE + split_idx * m_slice],
                                    ls32_ubuf_tensor,
                                    m_split,                      // repeat
                                    1,                            // dstRepeatStride
                                    1,                            // srcBlockStride
                                    qk_round_n / FLOAT_BLOCK_SIZE // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                WAIT_FLAG(V, MTE3, EVENT_ID0);
                                ub_to_gm<ArchType::ASCEND_V220, mm2InputType>(
                                    p_gm_tensor[(uint64_t)block_idx * TMP_SIZE + (n_idx / s_block_stack) % 2 * TMP_SIZE / 4 +
                                                ((uint64_t)sub_block_idx * row_offset + split_idx * m_slice) * qk_round_n],
                                    lp_ubuf_tensor[split_idx * m_slice * qk_round_n],
                                    0,                       // sid
                                    m_split,                 // nBurst
                                    qk_round_n / BLOCK_SIZE, // lenBurst
                                    0,                       // srcGap
                                    0                        // dstGap
                                );
                                SET_FLAG(MTE3, MTE2, split_idx);
                            }
                            PIPE_BARRIER(V);
                            if (n_idx == 0) {
                                ub_to_ub<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor,
                                    ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    0,                              // sid
                                    1,                              // nBurst
                                    round_sub_m / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                              // srcGap
                                    0                               // dstGap
                                );
                            } else {
                                conv_v<ArchType::ASCEND_V220, half, float>(
                                    tv_ubuf_tensor,
                                    dm_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_HALF_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // srcBlockStride
                                    8,         // dstRepeatStride
                                    4          // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = exp(dm)
                                exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                                    sub_m_d64, // repeat
                                                                    1,         // dstBlockStride
                                                                    1,         // srcBlockStride
                                                                    8,         // dstRepeatStride
                                                                    8          // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm_block = expand_to_block(dm), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>()[VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                    1,                             // dstBlockStride
                                    8,                             // dstRepeatStride
                                    round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                PIPE_BARRIER(V);
                                // *** gl = dm * gl
                                mul_v<ArchType::ASCEND_V220, float>(gl_ubuf_tensor, tv_ubuf_tensor, gl_ubuf_tensor,
                                                                    sub_m_d64, // repeat
                                                                    1,         // dstBlockStride
                                                                    1,         // src0BlockStride
                                                                    1,         // src1BlockStride
                                                                    8,         // dstRepeatStride
                                                                    8,         // src0RepeatStride
                                                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** gl = ll + gl
                                add_v<ArchType::ASCEND_V220, float>(
                                    gl_ubuf_tensor, gl_ubuf_tensor,
                                    ll_ubuf_tensor[(n_idx / s_block_stack) % 4 * UB_FLOAT_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // src0BlockStride
                                    1,         // src1BlockStride
                                    8,         // dstRepeatStride
                                    8,         // src0RepeatStride
                                    8          // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                            }
                        }
                    }
                    FftsCrossCoreSync<PIPE_MTE3, 2>(SOFTMAX_READY);
                }
                if (n_idx >= after) {
                    WaitFlagDev(UPDATE_READY);
                    uint64_t cube_k = BLOCK_QK_C;
                    uint64_t cube_round_k = BLOCK_QK_C;
                    for (uint32_t k_idx = 0; k_idx < loopV; k_idx++) {
                        if (k_idx % 2 == 0 && embdv - k_idx * BLOCK_QK_V < BLOCK_QK_C) {
                            cube_k = embdv - k_idx * BLOCK_QK_V;
                            cube_round_k = (cube_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;
                        }
                        uint64_t o_offsetk = o_offset + k_idx * BLOCK_QK_V;
                        if (sub_m > 0) {
                            uint32_t qk_k = (k_idx == (loopV - 1)) ? embdv - k_idx * BLOCK_QK_V : BLOCK_QK_V;
                            uint32_t qk_round_k = (qk_k + BLOCK_SIZE - 1) / BLOCK_SIZE * BLOCK_SIZE;

                            uint64_t ori_offset = (uint64_t)block_idx * TMP_SIZE * LOCAL_SIZE +
                                                k_idx / 2 * TMP_SIZET * 2 +
                                                ((n_idx - after) / s_block_stack) % 2 * TMP_SIZE / 4 * loopVt +
                                                (uint64_t)sub_block_idx * row_offset * cube_round_k;
                            if (k_idx % 2 == 1) {
                                ori_offset += BLOCK_QK_V;
                            }
                            if (n_idx == after) {
                                WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
                                gm_to_ub<ArchType::ASCEND_V220, mmOutputType>(
                                    go_ubuf_tensor.template ReinterpretCast<mmOutputType>(),
                                    o_tmp_gm_tensor[ori_offset],
                                    0,                                              // sid
                                    sub_m,                                          // nBurst
                                    qk_round_k / FLOAT_BLOCK_SIZE,                  // lenBurst
                                    (cube_round_k - qk_round_k) / FLOAT_BLOCK_SIZE, // srcGap
                                    0                                               // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID3);
                                WAIT_FLAG(MTE2, V, EVENT_ID3);
                            } else {
                                WAIT_FLAG(V, MTE2, EVENT_ID4);
                                gm_to_ub<ArchType::ASCEND_V220, mmOutputType>(
                                    lo_ubuf_tensor.template ReinterpretCast<mmOutputType>(),
                                    o_tmp_gm_tensor[ori_offset],
                                    0,                                              // sid
                                    sub_m,                                          // nBurst
                                    qk_round_k / FLOAT_BLOCK_SIZE,                  // lenBurst
                                    (cube_round_k - qk_round_k) / FLOAT_BLOCK_SIZE, // srcGap
                                    0                                               // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID4);

                                WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
                                gm_to_ub<ArchType::ASCEND_V220, mmOutputType>(
                                    go_ubuf_tensor.template ReinterpretCast<mmOutputType>(),
                                    upo_gm_tensor[(uint64_t)block_idx * TMP_SIZE + k_idx * TMP_SIZET +
                                                    (uint64_t)sub_block_idx * row_offset * qk_round_k],
                                    0,                                     // sid
                                    1,                                     // nBurst
                                    sub_m * qk_round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                                     // srcGap
                                    0                                      // dstGap
                                );
                                SET_FLAG(MTE2, V, EVENT_ID3);

                                PIPE_BARRIER(V);
                                conv_v<ArchType::ASCEND_V220, half, float>(
                                    tv_ubuf_tensor,
                                    dm_ubuf_tensor[((n_idx - after) / s_block_stack % 4)  * UB_HALF_LINE_SIZE],
                                    sub_m_d64, // repeat
                                    1,         // dstBlockStride
                                    1,         // srcBlockStride
                                    8,         // dstRepeatStride
                                    4          // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm = exp(dm)
                                exp_v<ArchType::ASCEND_V220, float>(tv_ubuf_tensor, tv_ubuf_tensor,
                                                                    sub_m_d64, // repeat
                                                                    1,         // dstBlockStride
                                                                    1,         // srcBlockStride
                                                                    8,         // dstRepeatStride
                                                                    8          // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** dm_block = expand_to_block(dm), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint32_t>(
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>()[VECTOR_SIZE],
                                    tv_ubuf_tensor.ReinterpretCast<uint32_t>(),
                                    1,                             // dstBlockStride
                                    8,                             // dstRepeatStride
                                    round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                PIPE_BARRIER(V);
                                WAIT_FLAG(MTE2, V, EVENT_ID3);
                                // *** go = go * dm_block
                                for (uint32_t vmul_idx = 0; vmul_idx < qk_k / FLOAT_VECTOR_SIZE; ++vmul_idx) {
                                    mul_v<ArchType::ASCEND_V220, float>(
                                        go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE],
                                        go_ubuf_tensor[vmul_idx * FLOAT_VECTOR_SIZE], tv_ubuf_tensor[VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        0,                             // src1BlockStride
                                        qk_round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        1                              // src1RepeatStride
                                    );
                                }
                                if (qk_k % FLOAT_VECTOR_SIZE > 0) {
                                    __set_mask(qk_k % FLOAT_VECTOR_SIZE);
                                    mul_v<ArchType::ASCEND_V220, float>(
                                        go_ubuf_tensor[qk_k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        go_ubuf_tensor[qk_k / FLOAT_VECTOR_SIZE * FLOAT_VECTOR_SIZE],
                                        tv_ubuf_tensor[VECTOR_SIZE],
                                        sub_m,                         // repeat
                                        1,                             // dstBlockStride
                                        1,                             // src0BlockStride
                                        0,                             // src1BlockStride
                                        qk_round_k / FLOAT_BLOCK_SIZE, // dstRepeatStride
                                        qk_round_k / FLOAT_BLOCK_SIZE, // src0RepeatStride
                                        1                              // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);

                                WAIT_FLAG(MTE2, V, EVENT_ID4);
                                // *** go = lo + go
                                add_v<ArchType::ASCEND_V220, float>(
                                    go_ubuf_tensor, go_ubuf_tensor, lo_ubuf_tensor,
                                    (sub_m * qk_round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1,                                                                // dstBlockStride
                                    1,                                                                // src0BlockStride
                                    1,                                                                // src1BlockStride
                                    8,                                                                // dstRepeatStride
                                    8, // src0RepeatStride
                                    8  // src1RepeatStride
                                );
                                PIPE_BARRIER(V);
                                SET_FLAG(V, MTE2, EVENT_ID4);
                            }
                            // *** gl = castfp32to16(gl)
                            if (n_idx + s_block_stack > n_end + after - 1){
                                PIPE_BARRIER(V);
                                if (k_idx == 0) {
                                    conv_v<ArchType::ASCEND_V220, float, half>(gl_ubuf_tensor.ReinterpretCast<half>(),
                                                                            gl_ubuf_tensor,
                                                                            sub_m_d64, // repeat
                                                                            1,         // dstBlockStride
                                                                            1,         // srcBlockStride
                                                                            4,         // dstRepeatStride
                                                                            8          // srcRepeatStride
                                    );
                                }
                                PIPE_BARRIER(V);
                                // *** go = castfp32to16(go)
                                conv_v<ArchType::ASCEND_V220, float, half>(
                                    go_ubuf_tensor.ReinterpretCast<half>(), go_ubuf_tensor,
                                    (sub_m * qk_round_k + FLOAT_VECTOR_SIZE - 1) / FLOAT_VECTOR_SIZE, // repeat
                                    1,                                                                // dstBlockStride
                                    1,                                                                // srcBlockStride
                                    4,                                                                // dstRepeatStride
                                    8                                                                 // srcRepeatStride
                                );
                                PIPE_BARRIER(V);
                                // *** gl_block = expand_to_block(gl), 存放于 tv
                                brcb_v<ArchType::ASCEND_V220, uint16_t>(tv_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                        gl_ubuf_tensor.ReinterpretCast<uint16_t>(),
                                                                        1, // dstBlockStride
                                                                        8, // dstRepeatStride
                                                                        round_sub_m / FLOAT_BLOCK_SIZE // repeat
                                );
                                PIPE_BARRIER(V);
                                // *** go = go / gl_block
                                for (uint32_t vdiv_idx = 0; vdiv_idx < qk_k / VECTOR_SIZE; ++vdiv_idx) {
                                    div_v<ArchType::ASCEND_V220, half>(
                                        go_ubuf_tensor.ReinterpretCast<half>()[vdiv_idx * VECTOR_SIZE],
                                        go_ubuf_tensor.ReinterpretCast<half>()[vdiv_idx * VECTOR_SIZE],
                                        tv_ubuf_tensor.ReinterpretCast<half>(),
                                        sub_m,                   // repeat
                                        1,                       // dstBlockStride
                                        1,                       // src0BlockStride
                                        0,                       // src1BlockStride
                                        qk_round_k / BLOCK_SIZE, // dstRepeatStride
                                        qk_round_k / BLOCK_SIZE, // src0RepeatStride
                                        1                        // src1RepeatStride
                                    );
                                }
                                if (qk_k % VECTOR_SIZE > 0) {
                                    __set_mask(qk_k % VECTOR_SIZE);
                                    div_v<ArchType::ASCEND_V220, half>(
                                        go_ubuf_tensor.ReinterpretCast<half>()[qk_k / VECTOR_SIZE * VECTOR_SIZE],
                                        go_ubuf_tensor.ReinterpretCast<half>()[qk_k / VECTOR_SIZE * VECTOR_SIZE],
                                        tv_ubuf_tensor.ReinterpretCast<half>(),
                                        sub_m,                   // repeat
                                        1,                       // dstBlockStride
                                        1,                       // src0BlockStride
                                        0,                       // src1BlockStride
                                        qk_round_k / BLOCK_SIZE, // dstRepeatStride
                                        qk_round_k / BLOCK_SIZE, // src0RepeatStride
                                        1                        // src1RepeatStride
                                    );
                                    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
                                }
                                PIPE_BARRIER(V);
                                // ********************* move O to GM ************************
                                SET_FLAG(V, MTE3, EVENT_ID1);
                                WAIT_FLAG(V, MTE3, EVENT_ID1);
                                if (qN_blk_size_actual == 1) {
                                    ub_to_gm_align<ArchType::ASCEND_V220, half>(
                                        o_gm_tensor[o_offsetk + (uint64_t)sub_block_idx * row_offset * stride_qo],
                                        go_ubuf_tensor.ReinterpretCast<half>(),
                                        0,                     // sid
                                        sub_m,                 // nBurst
                                        qk_k * 2,              // lenBurst
                                        0,                     // leftPaddingNum
                                        0,                     // rightPaddingNum
                                        0,                     // srcGap
                                        (stride_qo - qk_k) * 2 // dstGap
                                    );
                                } else {
                                    uint32_t sub_core_qN_offset = (uint64_t)sub_block_idx * (qN_blk_size_actual / 2) * embdv;
                                    for (uint32_t qN_idx = 0; qN_idx < sub_qN; qN_idx++) {
                                        ub_to_gm_align<ArchType::ASCEND_V220, half>(
                                            o_gm_tensor[o_offsetk + sub_core_qN_offset + qN_idx * embdv],
                                            go_ubuf_tensor.ReinterpretCast<half>()[qN_idx * qS_blk_size_actual * qk_round_k],
                                            0,
                                            qS_blk_size_actual,
                                            qk_k * 2,
                                            0,
                                            0,
                                            0,
                                            (stride_qo - qk_k) * 2
                                        );
                                    }
                                }
                                
                                SET_FLAG(MTE3, MTE2, EVENT_ID2);
                            } else {
                                SET_FLAG(V, MTE3, EVENT_ID2);
                                WAIT_FLAG(V, MTE3, EVENT_ID2);
                                ub_to_gm<ArchType::ASCEND_V220, mmOutputType>(
                                    upo_gm_tensor[(uint64_t)block_idx * TMP_SIZE + k_idx * TMP_SIZET +
                                                (uint64_t)sub_block_idx * row_offset * qk_round_k],
                                    go_ubuf_tensor.template ReinterpretCast<mmOutputType>(),
                                    0,                                     // sid
                                    1,                                     // nBurst
                                    sub_m * qk_round_k / FLOAT_BLOCK_SIZE, // lenBurst
                                    0,                                     // srcGap
                                    0                                      // dstGap
                                );
                                SET_FLAG(MTE3, MTE2, EVENT_ID2);
                            }
                        }
                    }
                }
            }
        }
    }
    __aicore__ __attribute__((always_inline)) inline ~PagedMLAMultiTokenPredictionAiv()
    {
        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
        WAIT_FLAG(MTE3, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID0);
        WAIT_FLAG(V, MTE2, EVENT_ID1);
        WAIT_FLAG(V, MTE2, EVENT_ID2);
        WAIT_FLAG(V, MTE2, EVENT_ID3);
	    WAIT_FLAG(V, MTE2, EVENT_ID4);
        WAIT_FLAG(V, MTE2, EVENT_ID6);
	    WAIT_FLAG(V, MTE2, EVENT_ID7);
        WAIT_FLAG(MTE3, V, EVENT_ID0);
        PIPE_BARRIER(ALL);
    }
private:
    __gm__ uint8_t *__restrict__ mask_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_gm{nullptr};
    __gm__ uint8_t *__restrict__ s_gm{nullptr};
    __gm__ uint8_t *__restrict__ p_gm{nullptr};
    __gm__ uint8_t *__restrict__ o_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ upo_tmp_gm{nullptr};
    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
    __gm__ uint8_t *__restrict__ deq_qk_gm{nullptr};
    __gm__ uint8_t *__restrict__ deq_pv_gm{nullptr};
    __gm__ uint8_t *__restrict__ quant_p_gm{nullptr};

    const uint32_t ls_ubuf_offset = 0;
    const uint32_t lp_ubuf_offset = 0;
    const uint32_t ls32_ubuf_offset = 2 * UB_UINT8_BLOCK_SIZE;
    const uint32_t mask_ubuf_offset = 4 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lo_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE;
    const uint32_t lm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE;
    const uint32_t hm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 1 * UB_UINT8_LINE_SIZE;
    const uint32_t gm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 2 * UB_UINT8_LINE_SIZE;
    const uint32_t dm_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 4 * UB_UINT8_LINE_SIZE;
    const uint32_t ll_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 8 * UB_UINT8_LINE_SIZE;
    const uint32_t gl_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 16 * UB_UINT8_LINE_SIZE;
    const uint32_t scale_ubuf_offset = 8 * UB_UINT8_BLOCK_SIZE + 18 * UB_UINT8_LINE_SIZE;
    const uint32_t tv_ubuf_offset = 11 * UB_UINT8_BLOCK_SIZE;
    const uint32_t go_ubuf_offset = 9 * UB_UINT8_BLOCK_SIZE;
    const uint32_t ls32_quant_ubuf_offset = 6 * UB_UINT8_BLOCK_SIZE;

    AsdopsBuffer<ArchType::ASCEND_V220> buf;

    AscendC::LocalTensor<half> ls_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(ls_ubuf_offset);
    AscendC::LocalTensor<mm2InputType> lp_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, mm2InputType>(lp_ubuf_offset);
    AscendC::LocalTensor<int8_t> lp_int8_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, int8_t>(lp_ubuf_offset);
    AscendC::LocalTensor<float> lp32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lp_ubuf_offset);
    AscendC::LocalTensor<float> ls32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_ubuf_offset);
    AscendC::LocalTensor<float> ls32_quant_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ls32_quant_ubuf_offset);
    AscendC::LocalTensor<half> mask_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(mask_ubuf_offset);
    AscendC::LocalTensor<half> mask_value_ubuf_tensor =
        buf.GetBuffer<BufferType::ASCEND_UB, half>(11 * UB_UINT8_BLOCK_SIZE);
    AscendC::LocalTensor<uint8_t> mask_u8_ubuf_tensor =
        buf.GetBuffer<BufferType::ASCEND_UB, uint8_t>(11 * UB_UINT8_BLOCK_SIZE);
    AscendC::LocalTensor<float> lo_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(lo_ubuf_offset);
    AscendC::LocalTensor<half> lm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(lm_ubuf_offset);
    AscendC::LocalTensor<half> hm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(hm_ubuf_offset);
    AscendC::LocalTensor<half> gm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(gm_ubuf_offset);
    AscendC::LocalTensor<half> dm_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(dm_ubuf_offset);
    AscendC::LocalTensor<float> ll_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(ll_ubuf_offset);
    AscendC::LocalTensor<float> gl_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(gl_ubuf_offset);
    AscendC::LocalTensor<float> tv_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv_ubuf_offset);
    AscendC::LocalTensor<float> tv32_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(tv_ubuf_offset);
    AscendC::LocalTensor<half> tv16_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, half>(tv_ubuf_offset);
    AscendC::LocalTensor<uint8_t> tv_u8_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, uint8_t>(tv_ubuf_offset);
    AscendC::LocalTensor<float> go_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(go_ubuf_offset);
    AscendC::LocalTensor<float> scale_ubuf_tensor = buf.GetBuffer<BufferType::ASCEND_UB, float>(scale_ubuf_offset);

    AscendC::GlobalTensor<half> mask_gm_tensor;
    AscendC::GlobalTensor<half> o_gm_tensor;
    AscendC::GlobalTensor<half> s_gm_tensor;
    AscendC::GlobalTensor<mm2InputType> p_gm_tensor;
    AscendC::GlobalTensor<mmOutputType> o_tmp_gm_tensor;
    AscendC::GlobalTensor<mmOutputType> upo_gm_tensor;
    AscendC::GlobalTensor<half> deq_qk_gm_tensor;
    AscendC::GlobalTensor<float> deq_pv_gm_tensor;
    AscendC::GlobalTensor<float> quant_p_gm_tensor;
    AscendC::GlobalTensor<half> logN_gm_tensor;

    uint32_t batch_size{0};
    uint32_t max_seqlen{0};
    uint32_t q_heads{0};
    uint32_t embd{0};
    uint32_t embdv{0};
    uint32_t block_size{0};
    half tor{0};
    uint32_t head_stride{0};
    uint32_t mask_stride{0};
    uint32_t total_q_blk_num{0}; 
    uint64_t stride_qo{0};
    uint32_t quantType{0};

    int32_t sub_block_idx{-1};
    uint32_t tiling_head_size{0};
    uint32_t tiling_para_size{0};
    uint32_t long_seq{0};
    uint32_t mask_type{0};
    uint32_t cur_qN_blk_size{0};
    uint32_t cur_qN_blk_num{0};
};

#endif

extern "C" __global__ __aicore__ void paged_multi_latent_attention_multi_token_prediction_mix(
    __gm__ uint8_t *__restrict__ sync,
    __gm__ uint8_t *__restrict__ q_gm,
    __gm__ uint8_t *__restrict__ ctkv_gm,
    __gm__ uint8_t *__restrict__ block_tables_gm,
    __gm__ uint8_t *__restrict__ mask_gm,
    __gm__ uint8_t *__restrict__ o_gm,
    __gm__ uint8_t *__restrict__ s_gm,
    __gm__ uint8_t *__restrict__ p_gm,
    __gm__ uint8_t *__restrict__ o_tmp_gm,
    __gm__ uint8_t *__restrict__ upo_tmp_gm,
    __gm__ uint8_t *__restrict__ tiling_para_gm)
{
    SetFftsBaseAddr((unsigned long)sync);
    SetAtomicnone();
    SetMasknorm();
#ifdef __DAV_C220_VEC__
    SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
#elif __DAV_C220_CUBE__
    SetPadding<uint64_t>(0);
    SetNdpara(1, 0, 0);
#endif
    if (TILING_KEY_IS(0)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<half, half, float, CalcMode::CALC_MODE_576, half, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionAiv<half, half, float, CalcMode::CALC_MODE_576, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(1)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<half, half, float, CalcMode::CALC_MODE_256, half, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionAiv<half, half, float, CalcMode::CALC_MODE_256, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(12)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<__bf16, __bf16, float, CalcMode::CALC_MODE_576, float, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionHpAiv<__bf16, CalcMode::CALC_MODE_576, __bf16, __bf16, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(13)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<__bf16, __bf16, float, CalcMode::CALC_MODE_256, float, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionHpAiv<__bf16, CalcMode::CALC_MODE_256, __bf16, __bf16, FuncType::MULTI_TOKEN_PREDICTION> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(16)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<half, half, float, CalcMode::CALC_MODE_576, half, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionAiv<half, half, float, CalcMode::CALC_MODE_576, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(17)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<half, half, float, CalcMode::CALC_MODE_256, half, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionAiv<half, half, float, CalcMode::CALC_MODE_256, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(28)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<__bf16, __bf16, float, CalcMode::CALC_MODE_576, float, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionHpAiv<__bf16, CalcMode::CALC_MODE_576, __bf16, __bf16, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    } else if (TILING_KEY_IS(29)) {
#ifdef __DAV_C220_CUBE__
        PagedMLAMultiTokenPredictionAic<__bf16, __bf16, float, CalcMode::CALC_MODE_256, float, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aic(sync, q_gm, ctkv_gm, mask_gm, block_tables_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aic.Run();
#elif __DAV_C220_VEC__
        PagedMLAMultiTokenPredictionHpAiv<__bf16, CalcMode::CALC_MODE_256, __bf16, __bf16, FuncType::UNEQUAL_QK_LEN_PREFILL> mla_mtp_aiv(sync, mask_gm,
                o_gm, s_gm, p_gm, o_tmp_gm, upo_tmp_gm, tiling_para_gm);
        mla_mtp_aiv.Run();
#endif
    }
}