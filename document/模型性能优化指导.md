# 模型性能优化指导
-   [简介](#简介md)
-   [统一模型启动脚本](#统一模型启动脚本md)
-   [性能验收标准](#性能验收标准md)
    -   [简单指标](#简单指标md)
    -   [智普指标](#智普指标md)
-   [Operation替换](#Operation替换md)
-   [Layer替换](#Layer替换md)
-   [Model替换](#Model替换md)
    -   [Model包含所有的Layer](#Model包含所有的Layermd)
    -   [Model包含WordEbemdding和Layer后处理](#Model包含WordEbemdding和Layer后处理md)
    -   [PlanExecute异步化](#PlanExecute异步化md)
-   [Sampling替换](#Sampling替换md)
-   [纯Python代码优化](#纯Python代码优化md)
-   [减少算子transdata](#纯Python代码优化md)
-   [Profiling工具使用](#Profiling工具使用md)

<h2 id="简介md">简介</h2>

1. ChatGlm6B基本模型流程图, 每次输入一个会话的处理流程如下：

```
stream_chat
    Tokenizer //把词转换成词向量
    SessionPreProcess //stream_generate到while循环之前
    for i in range(TokenNum)
        ProcessToken
            InputGenerate //生成model输入
            Model
                WordEbemdding //所有Layer前
                for i in range(LayerNum):
                    Layer
                Normal+Linear //所有Layer后
            Sampling
        Decoder //把词向量转换成词
```
2. Transformer模型推理代码使用加速库替换来提高推理性能，替换粒度主要有三层，Operation、Layer、Model级别


<h2 id="统一模型启动脚本md">统一模型启动脚本</h2>
所有模型代码根目录实现有个run.sh脚本，有run.sh启动模型，run.sh参数如下
run.sh model_patch.py --run|--performance|--webdemo|--zhipu|--profiling

<h2 id="性能验收标准md">性能验收标准</h2>
<h3 id="简单指标md">简单指标</h3>
执行命令：

```
run.sh model_patch.py --performance
```

结果：

```
SessionPreProcess: 2.9146671295166016ms
TotalModelTime: 7424.642086029053ms
TotalTokenTime: 10349.486827850342ms
IncreaseAverageModelTime: 112.06591318524073ms
IncreaseAverageTokenTime: 115.90785071963356ms
Token_1: InputGenerate: 2679.8880100250244ms, ModelTime: 364.4895553588867ms, Sampling: 2.9146671295166016ms, TokenTime: 3047.2922325134277ms
Token_2: InputGenerate: 0.8156299591064453ms, ModelTime: 112.13254928588867ms, Sampling: 2.783536911010742ms, TokenTime: 115.73171615600586ms
Token_3: InputGenerate: 1.0089874267578125ms, ModelTime: 111.19937896728516ms, Sampling: 2.6197433471679688ms, TokenTime: 114.82810974121094ms
Token_4: InputGenerate: 0.9505748748779297ms, ModelTime: 110.2747917175293ms, Sampling: 2.701282501220703ms, TokenTime: 113.92664909362793ms
Token_5: InputGenerate: 0.9770393371582031ms, ModelTime: 111.2065315246582ms, Sampling: 3.7567615509033203ms, TokenTime: 115.94033241271973ms
Token_6: InputGenerate: 1.2061595916748047ms, ModelTime: 111.480712890625ms, Sampling: 2.8867721557617188ms, TokenTime: 115.57364463806152ms
Token_7: InputGenerate: 0.9982585906982422ms, ModelTime: 110.79072952270508ms, Sampling: 3.008604049682617ms, TokenTime: 114.79759216308594ms
Token_8: InputGenerate: 1.0309219360351562ms, ModelTime: 113.47436904907227ms, Sampling: 3.502368927001953ms, TokenTime: 118.00765991210938ms
Token_9: InputGenerate: 1.0919570922851562ms, ModelTime: 110.71443557739258ms, Sampling: 2.843618392944336ms, TokenTime: 114.65001106262207ms
Token_10: InputGenerate: 0.9634494781494141ms, ModelTime: 110.96429824829102ms, Sampling: 2.6273727416992188ms, TokenTime: 114.55512046813965ms
Token_11: InputGenerate: 0.9577274322509766ms, ModelTime: 110.99648475646973ms, Sampling: 2.7146339416503906ms, TokenTime: 114.6688461303711ms
Token_12: InputGenerate: 0.9348392486572266ms, ModelTime: 110.83579063415527ms, Sampling: 2.763986587524414ms, TokenTime: 114.53461647033691ms
Token_13: InputGenerate: 0.9777545928955078ms, ModelTime: 111.18745803833008ms, Sampling: 2.7532577514648438ms, TokenTime: 114.91847038269043ms
Token_14: InputGenerate: 0.9903907775878906ms, ModelTime: 111.18674278259277ms, Sampling: 2.7647018432617188ms, TokenTime: 114.94183540344238ms
Token_15: InputGenerate: 0.9760856628417969ms, ModelTime: 111.49811744689941ms, Sampling: 2.886533737182617ms, TokenTime: 115.36073684692383ms
Token_16: InputGenerate: 2.998828887939453ms, ModelTime: 116.7757511138916ms, Sampling: 3.11279296875ms, TokenTime: 122.88737297058105ms
Token_17: InputGenerate: 1.064300537109375ms, ModelTime: 111.31024360656738ms, Sampling: 2.7489662170410156ms, TokenTime: 115.12351036071777ms
Token_18: InputGenerate: 0.9405612945556641ms, ModelTime: 111.44351959228516ms, Sampling: 6.0214996337890625ms, TokenTime: 118.40558052062988ms
Token_19: InputGenerate: 1.1065006256103516ms, ModelTime: 111.7851734161377ms, Sampling: 2.8045177459716797ms, TokenTime: 115.69619178771973ms
Token_20: InputGenerate: 0.9589195251464844ms, ModelTime: 111.1440658569336ms, Sampling: 2.6521682739257812ms, TokenTime: 114.75515365600586ms
Token_21: InputGenerate: 0.9436607360839844ms, ModelTime: 111.79947853088379ms, Sampling: 2.7437210083007812ms, TokenTime: 115.48686027526855ms
Token_22: InputGenerate: 0.9694099426269531ms, ModelTime: 119.95887756347656ms, Sampling: 4.0493011474609375ms, TokenTime: 124.97758865356445ms
Token_23: InputGenerate: 1.1529922485351562ms, ModelTime: 112.2884750366211ms, Sampling: 2.8150081634521484ms, TokenTime: 116.2564754486084ms
Token_24: InputGenerate: 0.9744167327880859ms, ModelTime: 111.44447326660156ms, Sampling: 2.8104782104492188ms, TokenTime: 115.22936820983887ms
Token_25: InputGenerate: 0.9641647338867188ms, ModelTime: 114.33100700378418ms, Sampling: 3.252267837524414ms, TokenTime: 118.54743957519531ms
Token_26: InputGenerate: 1.0821819305419922ms, ModelTime: 112.11061477661133ms, Sampling: 2.5305747985839844ms, TokenTime: 115.7233715057373ms
Token_27: InputGenerate: 0.9634494781494141ms, ModelTime: 111.6788387298584ms, Sampling: 3.0629634857177734ms, TokenTime: 115.70525169372559ms
Token_28: InputGenerate: 0.9825229644775391ms, ModelTime: 111.82236671447754ms, Sampling: 2.9044151306152344ms, TokenTime: 115.70930480957031ms
Token_29: InputGenerate: 0.9911060333251953ms, ModelTime: 111.5868091583252ms, Sampling: 2.809286117553711ms, TokenTime: 115.3872013092041ms
Token_30: InputGenerate: 0.9691715240478516ms, ModelTime: 112.39767074584961ms, Sampling: 2.6798248291015625ms, TokenTime: 116.04666709899902ms
Token_31: InputGenerate: 0.9965896606445312ms, ModelTime: 112.05720901489258ms, Sampling: 2.6979446411132812ms, TokenTime: 115.75174331665039ms
Token_32: InputGenerate: 0.9539127349853516ms, ModelTime: 112.04838752746582ms, Sampling: 3.2427310943603516ms, TokenTime: 116.24503135681152ms
Token_33: InputGenerate: 2.901315689086914ms, ModelTime: 116.46318435668945ms, Sampling: 3.018617630004883ms, TokenTime: 122.38311767578125ms
Token_34: InputGenerate: 1.2841224670410156ms, ModelTime: 112.48636245727539ms, Sampling: 3.5550594329833984ms, TokenTime: 117.3255443572998ms
Token_35: InputGenerate: 1.0025501251220703ms, ModelTime: 112.23793029785156ms, Sampling: 2.3953914642333984ms, TokenTime: 115.63587188720703ms
Token_36: InputGenerate: 0.8883476257324219ms, ModelTime: 111.01484298706055ms, Sampling: 2.5098323822021484ms, TokenTime: 114.41302299499512ms
Token_37: InputGenerate: 0.9551048278808594ms, ModelTime: 111.66501045227051ms, Sampling: 2.9997825622558594ms, TokenTime: 115.61989784240723ms
Token_38: InputGenerate: 0.9665489196777344ms, ModelTime: 112.06865310668945ms, Sampling: 2.565145492553711ms, TokenTime: 115.6003475189209ms
Token_39: InputGenerate: 0.8995532989501953ms, ModelTime: 112.02311515808105ms, Sampling: 2.650737762451172ms, TokenTime: 115.57340621948242ms
Token_40: InputGenerate: 0.9319782257080078ms, ModelTime: 112.45536804199219ms, Sampling: 2.428293228149414ms, TokenTime: 115.81563949584961ms
Token_41: InputGenerate: 0.9047985076904297ms, ModelTime: 111.82475090026855ms, Sampling: 2.481698989868164ms, TokenTime: 115.21124839782715ms
Token_42: InputGenerate: 0.9353160858154297ms, ModelTime: 112.03551292419434ms, Sampling: 2.480030059814453ms, TokenTime: 115.45085906982422ms
Token_43: InputGenerate: 0.9326934814453125ms, ModelTime: 112.25509643554688ms, Sampling: 2.588033676147461ms, TokenTime: 115.77582359313965ms
Token_44: InputGenerate: 0.9434223175048828ms, ModelTime: 112.00690269470215ms, Sampling: 2.568960189819336ms, TokenTime: 115.51928520202637ms
Token_45: InputGenerate: 0.9112358093261719ms, ModelTime: 112.1368408203125ms, Sampling: 2.6018619537353516ms, TokenTime: 115.64993858337402ms
Token_46: InputGenerate: 0.9768009185791016ms, ModelTime: 112.28156089782715ms, Sampling: 2.524852752685547ms, TokenTime: 115.7832145690918ms
Token_47: InputGenerate: 0.9670257568359375ms, ModelTime: 112.14947700500488ms, Sampling: 2.553701400756836ms, TokenTime: 115.67020416259766ms
Token_48: InputGenerate: 0.9627342224121094ms, ModelTime: 112.32280731201172ms, Sampling: 2.511739730834961ms, TokenTime: 115.79728126525879ms
Token_49: InputGenerate: 0.9403228759765625ms, ModelTime: 112.09487915039062ms, Sampling: 2.5730133056640625ms, TokenTime: 115.60821533203125ms
Token_50: InputGenerate: 0.9438991546630859ms, ModelTime: 111.87362670898438ms, Sampling: 2.5186538696289062ms, TokenTime: 115.33617973327637ms
Token_51: InputGenerate: 0.9317398071289062ms, ModelTime: 111.98973655700684ms, Sampling: 2.6144981384277344ms, TokenTime: 115.53597450256348ms
Token_52: InputGenerate: 0.9279251098632812ms, ModelTime: 111.39631271362305ms, Sampling: 2.557516098022461ms, TokenTime: 114.88175392150879ms
Token_53: InputGenerate: 0.9479522705078125ms, ModelTime: 111.52768135070801ms, Sampling: 2.5331974029541016ms, TokenTime: 115.00883102416992ms
Token_54: InputGenerate: 0.9260177612304688ms, ModelTime: 111.65237426757812ms, Sampling: 2.5811195373535156ms, TokenTime: 115.15951156616211ms
Token_55: InputGenerate: 0.9458065032958984ms, ModelTime: 111.80615425109863ms, Sampling: 2.4700164794921875ms, TokenTime: 115.22197723388672ms
Token_56: InputGenerate: 0.9582042694091797ms, ModelTime: 111.82117462158203ms, Sampling: 2.4755001068115234ms, TokenTime: 115.25487899780273ms
Token_57: InputGenerate: 0.9276866912841797ms, ModelTime: 111.43183708190918ms, Sampling: 2.4318695068359375ms, TokenTime: 114.7913932800293ms
Token_58: InputGenerate: 0.9214878082275391ms, ModelTime: 111.53197288513184ms, Sampling: 2.466440200805664ms, TokenTime: 114.91990089416504ms
Token_59: InputGenerate: 0.9419918060302734ms, ModelTime: 111.94586753845215ms, Sampling: 2.466917037963867ms, TokenTime: 115.35477638244629ms
Token_60: InputGenerate: 0.9350776672363281ms, ModelTime: 111.89079284667969ms, Sampling: 2.515554428100586ms, TokenTime: 115.3414249420166ms
Token_61: InputGenerate: 0.9500980377197266ms, ModelTime: 112.09940910339355ms, Sampling: 2.9549598693847656ms, TokenTime: 116.00446701049805ms
Token_62: InputGenerate: 0.9782314300537109ms, ModelTime: 112.06626892089844ms, Sampling: 2.521991729736328ms, TokenTime: 115.56649208068848ms
Token_63: InputGenerate: 0.9388923645019531ms, ModelTime: 111.68265342712402ms, Sampling: 2.593517303466797ms, TokenTime: 115.21506309509277ms
Token_64: InputGenerate: 0.9596347808837891ms, ModelTime: 111.98902130126953ms, Sampling: 2.4509429931640625ms, TokenTime: 115.39959907531738ms
```
<h3 id="智普指标md">智普指标</h3>
执行命令：

```
run.sh model_patch.py --zhipu
```

结果：

```
Batch,MaxSeqLen,InputSeqLen(Encoding),OutputSeqLen(Decoding),TokensPerSecond(ms),ResponseTime(ms),FirstTokenTime(ms),TimePerTokens(ms)
1,2048,32,32,73.5,642.12,206.76,13.61
1,2048,32,64,73.48,1082.54,211.55,13.61
1,2048,32,128,72.9,1957.51,201.67,13.72
1,2048,32,256,73.37,3691.59,202.33,13.63
1,2048,32,512,71.45,7367.76,201.84,14.0
1,2048,32,1024,71.68,14486.45,201.44,13.95
1,2048,64,32,73.77,646.61,212.85,13.55
1,2048,64,64,74.36,1063.33,202.67,13.45
1,2048,64,128,72.08,1977.73,201.94,13.87
1,2048,64,256,71.47,3784.34,202.21,13.99
1,2048,64,512,71.62,7359.04,210.25,13.96
1,2048,64,1024,71.65,14494.7,203.8,13.96
1,2048,128,32,72.27,651.53,208.77,13.84
1,2048,128,64,73.26,1075.53,201.96,13.65
1,2048,128,128,73.73,1939.43,203.31,13.56
1,2048,128,256,73.08,3705.31,202.11,13.68
1,2048,128,512,72.73,7242.41,202.77,13.75
1,2048,128,1024,71.07,14610.94,202.63,14.07
1,2048,256,32,73.35,640.39,204.14,13.63
1,2048,256,64,73.35,1075.91,203.35,13.63
1,2048,256,128,73.26,1950.45,203.14,13.65
1,2048,256,256,73.19,3700.33,202.74,13.66
1,2048,256,512,72.36,7279.19,203.06,13.82
1,2048,256,1024,70.41,14748.07,205.11,14.2
1,2048,512,32,71.94,652.2,207.41,13.9
```

<h2 id="Operation替换md">Operation替换</h2>

<h2 id="Layer替换md">Layer替换</h2>

<h2 id="Model替换md">Model替换</h2>

<h3 id="Model包含所有的Layermd">Model包含所有的Layer</h3>

```
void ChatGlm6BDecoderWithoutFusionModel::BuildGraph()
{
    ASD_LOG(INFO) << "Build Graph Start.";
    const int weightTensorSize = WEIGHT_COUNT_PER_LAYER * param_.layerNum; 
    graph_.weightTensors.resize(weightTensorSize);

    graph_.inTensors.resize(IN_TENSOR_PASTK_V_START + 3 * param_.layerNum);
    graph_.outTensors.resize(1 + 2 * param_.layerNum);

    const int nodeSize = param_.layerNum;
    graph_.nodes.resize(nodeSize);

    graph_.internalTensors.resize(graph_.nodes.size() - 1);

    int nodeId = 0;   

    AsdOps::Tensor *firstInTensor = &graph_.inTensors.at(0);
    ASD_LOG(INFO) << "First InTensor Set.";

    for (int layerId = 0; layerId < param_.layerNum; ++layerId) {
        auto &layerNode = graph_.nodes.at(nodeId++);

        ChatGlm6BLayerParam opParam;
        opParam.layerNormEps = param_.layerNormEps;
        opParam.headNum = param_.headNum;
        opParam.transKey = param_.transKey;
        opParam.dk = param_.dk;
        opParam.layerId = layerId;
        opParam.residualAddScale = param_.residualAddScale;
        opParam.beginNormAxis = param_.beginNormAxis;
        opParam.beginParamsAxis = param_.beginParamsAxis;
        layerNode.operation = std::make_shared<ChatGlm6BLayerDecoderWithoutFusionOperation>(opParam);
        layerNode.inTensors.resize(layerNode.operation->GetInTensorCount());

        size_t inTensorId = 0;
        layerNode.inTensors.at(inTensorId++) = firstInTensor;   // hidden states tensor

        for (size_t weightTensorId = 0; weightTensorId < WEIGHT_COUNT_PER_LAYER; ++weightTensorId) {
            layerNode.inTensors.at(inTensorId++) = &graph_.weightTensors.at(layerId * WEIGHT_COUNT_PER_LAYER + weightTensorId);
        }

        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_POSITIONID);       // positionIdTensor
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_COSTABLE);       // costable
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_SINTABLE);       // sintable
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_ATTENTIONMASK);  // attentionMaskTensor
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_PASTK_V_START + layerId);
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_PASTK_V_START + param_.layerNum + layerId);
        
        if (layerId != param_.layerNum - 1) {
            layerNode.outTensors = {&graph_.internalTensors.at(layerId),
                                    &graph_.outTensors.at(2 * layerId + 1),
                                    &graph_.outTensors.at(2 * layerId + 2)};
        } else {
            layerNode.outTensors = {&graph_.outTensors.at(0),
                                    &graph_.outTensors.at(2 * layerId + 1),
                                    &graph_.outTensors.at(2 * layerId + 2)};
        }

        firstInTensor = layerNode.outTensors.at(0);
    }
    ASD_LOG(INFO) << "Build Graph finished.";
}
```

<h3 id="Model包含WordEbemdding和Layer后处理md">Model包含WordEbemdding和Layer后处理</h3>

```
void ChatGlm6BDecoderModel::BuildGraph()
{
    const int weightTensorSize =
        WORDEMBEDDINGNODE_WEIGHT_COUNT + WEIGHT_COUNT_PER_LAYER * param_.layerNum + FINALNORMNODE_WEIGHT_COUNT;
    graph_.weightTensors.resize(weightTensorSize);

    graph_.inTensors.resize(IN_TENSOR_MAX + param_.layerNum);
    graph_.outTensors.resize(1);

    const int nodeSize = param_.layerNum + OPERATION_COUNT_BEFORE_LAYER + OPERATION_COUNT_AFTER_LAYER;
    graph_.nodes.resize(nodeSize);

    graph_.internalTensors.resize(graph_.nodes.size() - 1);

    int nodeId = 0;
    auto &wordEmbeddingNode = graph_.nodes.at(nodeId++);
    wordEmbeddingNode.operation = std::make_shared<EmbeddingOperation>(EmbeddingParam());
    wordEmbeddingNode.inTensors = {&graph_.weightTensors.at(0), &graph_.inTensors.at(0)};
    wordEmbeddingNode.outTensors = {&graph_.internalTensors.at(0)};

    auto &transposeNode = graph_.nodes.at(nodeId++);
    TransposeParam transposeParam = {{1, 0, 2}};
    transposeNode.operation = std::make_shared<TransposeOperation>(transposeParam);
    transposeNode.inTensors = {&graph_.internalTensors.at(0)};
    transposeNode.outTensors = {&graph_.internalTensors.at(1)};

    AsdOps::Tensor *firstInTensor = &graph_.internalTensors.at(1);

    for (int layerId = 0; layerId < param_.layerNum; ++layerId) {
        auto &layerNode = graph_.nodes.at(nodeId++);

        ChatGlm6BLayerDecoderFlashAttentionParam opParam;
        opParam.layerNormEps = param_.layerNormEps;
        opParam.headNum = param_.headNum;
        opParam.transKey = param_.transKey;
        opParam.dk = param_.dk;
        opParam.layerId = layerId;
        opParam.residualAddScale = param_.residualAddScale;
        opParam.tokenOffset = param_.tokenOffset;
        opParam.seqLen = param_.seqLen;
        layerNode.operation = std::make_shared<ChatGlm6BLayerDecoderFlashAttentionOperation>(opParam);
        layerNode.inTensors.resize(layerNode.operation->GetInTensorCount());

        size_t inTensorId = 0;
        layerNode.inTensors.at(inTensorId++) = firstInTensor;
        for (size_t weightTensorId = 0; weightTensorId < WEIGHT_COUNT_PER_LAYER; ++weightTensorId) {
            layerNode.inTensors.at(inTensorId++) = &graph_.weightTensors.at(
                layerId * WEIGHT_COUNT_PER_LAYER + weightTensorId + WORDEMBEDDINGNODE_WEIGHT_COUNT);
        }
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_POSITIONID);    // positionIdTensor
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_COSTABLE);      // cosTable
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_SINTABLE);      // sinTable
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_ATTENTIONMASK); // attentionMaskTensor
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_PASTKEY);
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_PASTVALUE);
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_TOKENOFFSET);
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_SEQLEN); // seqLen
        layerNode.inTensors.at(inTensorId++) = &graph_.inTensors.at(IN_TENSOR_MAX + layerId);

        layerNode.outTensors = {&graph_.internalTensors.at(OPERATION_COUNT_BEFORE_LAYER + layerId)}; 
       firstInTensor = layerNode.outTensors.at(0);
    }

    auto &finalNormNode = graph_.nodes.at(nodeId++);
    NormParam finalNormParam = {param_.layerNormEps};
    finalNormNode.operation = std::make_shared<NormOperation>(finalNormParam);
    const int finalLayerNormWeightTensorId = graph_.weightTensors.size() - FINALNORMNODE_WEIGHT_COUNT;
    const int finalLayerNormBiasTensorId = graph_.weightTensors.size() - 1;
    finalNormNode.inTensors = {firstInTensor, &graph_.weightTensors.at(finalLayerNormWeightTensorId),
                               &graph_.weightTensors.at(finalLayerNormBiasTensorId)};
    finalNormNode.outTensors = {&graph_.outTensors.at(0)};
}
```

<h3 id="PlanExecute异步化md">PlanExecute异步化</h3>

1. Model包含多个Operation和对应的Plan，循环调用Plan的Setup和Execute

```
    for i in plans:
        plan.Setup()
        plan.Execute()
```

2. PlanExuecte异步化, 通过环境变量ACLTRANSFORMER_PLAN_EXECUTE_ASYNC=1开启。当TASK_QUEUE_ENABLE=1时，使用Torch本身的一个后台任务线程处理Plan的运行；
当TASK_QUEUE_ENABLE=0时，使用Model类里的一个后台线程处理Plan的运行

```
    for i in plans:
        plan.Setup()
        push planexecutetask to queue

    for item in queue:
        plan.Execute()
```

<h2 id="Sampling替换md">Sampling替换</h2>
参考：/home/denglian/code/ascend-transformer-acceleration/examples/chatglm6b/patches/operations/modeling_chatglm_post_torch_runner.py

```
while True:
            model_inputs = self.prepare_inputs_for_generation(
                input_ids, **model_kwargs)
            # forward pass to get next token
            torch.npu.synchronize()
            start = time.time()
            outputs = self(
                **model_inputs,
                return_dict=True,
                output_attentions=False,
                output_hidden_states=False,
            )
            torch.npu.synchronize()
            end = time.time()
            self.count += 1
            self.cur_time = (end - start) * 1000
            if self.count == 1:
                self.first = self.cur_time
            else:
                self.total += self.cur_time

            post_processing_start = time.time()
            next_token_logits = outputs.logits[:, -1, :]

            # pre-process distribution
            next_token_scores = logits_processor(input_ids, next_token_logits)

            inter_output = self.post_operation.execute([next_token_scores])
            next_tokens, indices = inter_output[0], inter_output[1]
            for i in next_tokens:
                next_tokens = indices[0][len(indices[0]) - i - 1]
```

<h2 id="纯Python代码优化md">纯Python代码优化</h2>

1. 预先转换好权重文件的npu格式

```
# 优化ND NZ排布，消除transdata
soc_version = torch_npu._C._npu_get_soc_version()
if soc_version in [104, 220, 221, 222, 223]:
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            module.weight.data = module.weight.data.npu_format_cast(2)
    print("soc_version:", soc_version, " is 910B, support ND")
else:
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            module.weight.data = module.weight.data.npu_format_cast(29)
    print("soc_version:", soc_version, " is not 910B, support NZ")
```

2. 预先建好Tensor数组, 后更新，相比每次append，性能提升

```
self.acl_decoder_operation_inputs[0] = input_ids
self.acl_decoder_operation_inputs[1] = position_ids
self.acl_decoder_operation_inputs[2] = self.cos
self.acl_decoder_operation_inputs[3] = self.sin
self.acl_decoder_operation_inputs[4] = self.attention_mask_max
self.acl_decoder_operation_inputs[5] = self.k_cache_input
self.acl_decoder_operation_inputs[6] = self.v_cache_input
self.acl_decoder_operation_inputs[7] = self.token_offset
self.acl_decoder_operation_inputs[8] = self.seq_len
acl_model_out = self.acl_decoder_operation.execute(
    self.acl_decoder_operation_inputs, acl_param)
```

<h2 id="Profiling工具使用md">Profiling工具使用</h2>
执行命令：
```
run.sh model_patch.py --profiling
```