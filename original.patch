diff --git a/src/kernels/kernels/activation/CMakeLists.txt b/src/kernels/kernels/activation/CMakeLists.txt
index 8c574277..3c6a4015 100644
--- a/src/kernels/kernels/activation/CMakeLists.txt
+++ b/src/kernels/kernels/activation/CMakeLists.txt
@@ -46,3 +46,15 @@ add_kernel(faster_gelu_forward ascend910 vector
 add_kernel(faster_gelu_forward ascend910b vector
     faster_gelu_forward/kernel/faster_gelu_forward.cpp
     FasterGeluForwardKernel)
+
+add_kernel(gelu_forward ascend310p vector
+        gelu_forward/kernel/gelu_forward.cpp
+        GeluForwardKernel)
+
+add_kernel(gelu_forward ascend910b vector
+        gelu_forward/kernel/gelu_forward.cpp
+        GeluForwardKernel)
+
+add_kernel(gelu_forward ascend910 vector
+        gelu_forward/kernel/gelu_forward.cpp
+        GeluForwardKernel)
diff --git a/src/kernels/kernels/activation/gelu_forward/kernel/gelu_forward.cpp b/src/kernels/kernels/activation/gelu_forward/kernel/gelu_forward.cpp
new file mode 100644
index 00000000..2ac35cd2
--- /dev/null
+++ b/src/kernels/kernels/activation/gelu_forward/kernel/gelu_forward.cpp
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+
+#include "gelu_forward.h"
+
+#include "kernel_operator.h"
+#include "kernels/activation/gelu_forward/tiling/tiling_data.h"
+#include "kernels/utils/kernel/kernel_utils.h"
+
+using namespace AscendC;
+using namespace AsdOps;
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, GeluForwardTilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->blockLength = (*(const __gm__ uint32_t *)(p_tilingdata + 0));
+    tilingdata->tileNum = (*(const __gm__ uint32_t *)(p_tilingdata + 4));
+    tilingdata->tileLength = (*(const __gm__ uint32_t *)(p_tilingdata + 8));
+    tilingdata->tailLength = (*(const __gm__ uint32_t *)(p_tilingdata + 12));
+    tilingdata->bufferNum = (*(const __gm__ uint32_t *)(p_tilingdata + 16));
+#else
+    TPipe pipe;
+    __ubuf__ uint8_t *tilingdata_in_ub = nullptr;
+    CopyGmTilingToUb(tilingdata_in_ub, p_tilingdata, sizeof(AsdOps::GeluForwardTilingData), &pipe);
+    SetFlag<HardEvent::MTE2_S>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
+    tilingdata->blockLength = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 0));
+    tilingdata->tileNum = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 4));
+    tilingdata->tileLength = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 8));
+    tilingdata->tailLength = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 12));
+    tilingdata->bufferNum = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 16));
+    PipeBarrier<PIPE_ALL>();
+#endif
+}
+
+extern "C" __global__ __aicore__ void gelu_forward(GM_ADDR inputAddr, GM_ADDR outputAddr, GM_ADDR tiling)
+{
+    GeluForwardTilingData tilingData;
+    InitTilingData(tiling, &(tilingData));
+    if (TILING_KEY_IS(1)) { // 1代表fp16 type
+        GeluForward<half, half> op;
+        op.Init(inputAddr, outputAddr, tilingData);
+        op.Process();
+    } else if (TILING_KEY_IS(0)) { // 0代表float type
+        GeluForward<float, float> op;
+        op.Init(inputAddr, outputAddr, tilingData);
+        op.ProcessFP32();
+    }
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220) // 220代表910B
+    if (TILING_KEY_IS(27)) { // 27代表bf16
+        GeluForward<bfloat16_t, bfloat16_t> op;
+        op.Init(inputAddr, outputAddr, tilingData);
+        op.Process();
+    }
+#endif
+}
diff --git a/src/kernels/kernels/activation/gelu_forward/kernel/gelu_forward.h b/src/kernels/kernels/activation/gelu_forward/kernel/gelu_forward.h
new file mode 100644
index 00000000..2a750e74
--- /dev/null
+++ b/src/kernels/kernels/activation/gelu_forward/kernel/gelu_forward.h
@@ -0,0 +1,135 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+
+#ifndef OPP_FASTER_GELU_FORWARD_H
+#define OPP_FASTER_GELU_FORWARD_H
+#include "kernel_operator.h"
+#include "kernels/activation/gelu_forward/tiling/tiling_data.h"
+#include "kernels/utils/kernel/kernel_utils.h"
+using namespace AscendC;
+
+template <typename inType, typename outType, bool highPerformance = true> class GeluForward {
+public:
+    __aicore__ inline GeluForward() {}
+    __aicore__ inline ~GeluForward() {}
+
+    __aicore__ inline void Init(GM_ADDR inputAddr, GM_ADDR outAddr, AsdOps::GeluForwardTilingData &tilingData)
+    {
+        this->blockLength = tilingData.blockLength;
+        this->tileNum = tilingData.tileNum;
+        this->tileLength = tilingData.tileLength;
+        this->tailLength = tilingData.tailLength;
+        this->bufferNum = tilingData.bufferNum;
+
+        inputGM.SetGlobalBuffer((__gm__ inType *)inputAddr + this->blockLength * GetBlockIdx(), this->blockLength);
+        outputGM.SetGlobalBuffer((__gm__ outType *)outAddr + this->blockLength * GetBlockIdx(), this->blockLength);
+
+        // 不能整除的情况下，可能存在浪费
+        pipe.InitBuffer(inQueueX, this->bufferNum, this->tileLength * sizeof(inType));
+        pipe.InitBuffer(outQueueZ, this->bufferNum, this->tileLength * sizeof(outType));
+    }
+
+    __aicore__ inline void Process()
+    {
+        uint64_t offset = 0;
+        pipe.InitBuffer(inFp32Buffer, this->tileLength * sizeof(float));
+        pipe.InitBuffer(outFp32Buffer, this->tileLength * sizeof(float));
+        LocalTensor<float> inLocal = inFp32Buffer.Get<float>();
+        LocalTensor<float> outLocal = outFp32Buffer.Get<float>();
+        for (int32_t i = 0; i < this->tileNum; i++) {
+            CopyInAndCastF32(inLocal, inputGM, inQueueX, offset, this->tileLength);
+            Compute(outLocal, inLocal, this->tileLength);
+            Cast16AndCopyOut(outLocal, outputGM, outQueueZ, offset, this->tileLength);
+            offset = offset + this->tileLength;
+        }
+        // 处理每个核上的尾块
+        if (this->tailLength != 0) {
+            CopyInAndCastF32(inLocal, inputGM, inQueueX, offset, this->tailLength);
+            Compute(outLocal, inLocal, this->tailLength);
+            Cast16AndCopyOut(outLocal, outputGM, outQueueZ, offset, this->tailLength);
+        }
+    }
+
+    __aicore__ inline void ProcessFP32()
+    {
+        uint64_t offset = 0;
+        for (uint32_t i = 0; i < this->tileNum; i++) {
+            CopyIn(inputGM, inQueueX, offset, this->tileLength);
+            LocalTensor<float> inLocal = inQueueX.DeQue<float>();
+            LocalTensor<float> outLocal = outQueueZ.AllocTensor<float>();
+            Compute(outLocal, inLocal, this->tileLength);
+            inQueueX.FreeTensor(inLocal);
+            outQueueZ.EnQue<float>(outLocal);
+            CopyOut(outputGM, outQueueZ, offset, this->tileLength);
+            offset = offset + this->tileLength;
+        }
+        // 处理每个核上的尾块
+        if (this->tailLength != 0) {
+            CopyIn(inputGM, inQueueX, offset, this->tailLength);
+            LocalTensor<float> inLocal = inQueueX.DeQue<float>();
+            LocalTensor<float> outLocal = outQueueZ.AllocTensor<float>();
+            Compute(outLocal, inLocal, this->tailLength);
+            inQueueX.FreeTensor(inLocal);
+            outQueueZ.EnQue<float>(outLocal);
+            CopyOut(outputGM, outQueueZ, offset, this->tailLength);
+        }
+    }
+
+    __aicore__ inline void Compute(LocalTensor<float> &zLocal, LocalTensor<float> &xLocal, uint64_t tileLength)
+    {
+        //  z = x ^ 3
+        Mul(zLocal, xLocal, xLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+        Mul(zLocal, zLocal, xLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = 0.044715 * z
+        Muls(zLocal, zLocal, attr1, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = x + z
+        Add(zLocal, xLocal, zLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = -1.59576912 * z
+        Muls(zLocal, zLocal, attr2, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = e ^ z
+        Exp(zLocal, zLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = 1 + z
+        Adds(zLocal, zLocal, attr3, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = x / z
+        Div(zLocal, xLocal, zLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+    }
+
+private:
+    TBuf<AscendC::TPosition::VECCALC> inFp32Buffer;
+    TBuf<AscendC::TPosition::VECCALC> outFp32Buffer;
+    TQue<QuePosition::VECIN, AsdOps::GELU_FORWARD_BUFF_NUM> inQueueX;
+    TQue<QuePosition::VECOUT, AsdOps::GELU_FORWARD_BUFF_NUM> outQueueZ;
+    GlobalTensor<inType> inputGM;
+    GlobalTensor<outType> outputGM;
+    TPipe pipe;
+    uint32_t blockLength{0};
+    uint32_t tileNum{0};
+    uint32_t tileLength{0};
+    uint32_t tailLength{0};
+    uint32_t bufferNum{2};
+    const float attr1 = 0.044715;
+    const float attr2 = -1.59576912;
+    const float attr3 = 1.0;
+};
+#endif // OPP_FASTER_GELU_FORWARD_H
\ No newline at end of file
diff --git a/src/kernels/kernels/elewise/CMakeLists.txt b/src/kernels/kernels/elewise/CMakeLists.txt
index 54d8917b..fc0c47c2 100644
--- a/src/kernels/kernels/elewise/CMakeLists.txt
+++ b/src/kernels/kernels/elewise/CMakeLists.txt
@@ -41,4 +41,16 @@ add_kernel(quant_per_channel ascend910 vector
 
 add_kernel(quant_per_channel ascend310b vector
     simple_broadcast/kernel/quant_per_channel.cpp
-    QuantPerChannelKernel)
\ No newline at end of file
+    QuantPerChannelKernel)
+
+add_kernel(quant ascend910b vector
+        quant/op_kernel/quant.cpp
+        QuantF16Kernel)
+
+add_kernel(quant ascend310p vector
+        quant/op_kernel/quant.cpp
+        QuantF16Kernel)
+
+add_kernel(quant ascend910 vector
+        quant/op_kernel/quant.cpp
+        QuantF16Kernel)
\ No newline at end of file
diff --git a/src/kernels/kernels/elewise/quant/op_kernel/quant.cpp b/src/kernels/kernels/elewise/quant/op_kernel/quant.cpp
new file mode 100644
index 00000000..fda4a8b3
--- /dev/null
+++ b/src/kernels/kernels/elewise/quant/op_kernel/quant.cpp
@@ -0,0 +1,201 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#include "kernel_operator.h"
+#include "kernels/utils/kernel/kernel_utils.h"
+#include "kernels/elewise/quant/quant_tiling/tiling_data.h"
+static constexpr int32_t BUFFER_NUM = 1; // tensor num for each queue
+static constexpr uint32_t DATA_BYTE = 2;
+static constexpr uint32_t BLOCK_NUMEL = 16;
+
+using AscendC::HardEvent;
+
+class KernelQuantization {
+public:
+    __aicore__ inline KernelQuantization() {}
+
+    __aicore__ inline uint32_t CEIL_DIV(uint32_t x, uint32_t y)
+    {
+        if (y == 0) {
+            return UINT32_MAX;
+        }
+        return (x + y - 1) / y;
+    }
+
+    __aicore__ inline uint32_t ROUND_UP(uint32_t x) { return (x + BLOCK_NUMEL - 1) / BLOCK_NUMEL * BLOCK_NUMEL; }
+
+    __aicore__ inline uint32_t MIN(uint32_t x, uint32_t y) { return x < y ? x : y; }
+    __aicore__ inline uint32_t MAX(uint32_t x, uint32_t y) { return x > y ? x : y; }
+
+    __aicore__ inline void Init(__gm__ uint8_t *x, __gm__ uint8_t *z, uint32_t num_core_, uint32_t num_Last_dim_,
+                                uint32_t num_first_dim_, uint32_t nl_first_dim_per_core_,
+                                uint32_t l_first_dim_per_core_, uint32_t first_dim_per_times_, uint32_t scale_,
+                                uint32_t offset_, int32_t quant_min_)
+    {
+        // 一次搬入多行
+        num_core = num_core_;
+        num_last_dim = num_Last_dim_;
+        num_first_dim = num_first_dim_;
+        nl_first_dim_per_core = nl_first_dim_per_core_;
+        l_first_dim_per_core = l_first_dim_per_core_;
+        first_dim_per_times = first_dim_per_times_;
+        quant_min = static_cast<half>(quant_min_);
+
+        input_scale = *reinterpret_cast<float *>(&scale_);
+        input_offset = *reinterpret_cast<int32_t *>(&offset_);
+
+        if (AscendC::GetBlockIdx() != num_core - 1) {
+            row_work = nl_first_dim_per_core;
+            row_step = first_dim_per_times;
+        } else {
+            row_work = l_first_dim_per_core;
+            row_step = MIN(first_dim_per_times, row_work);
+        }
+        row_tail_ = (row_work % first_dim_per_times == 0) ? first_dim_per_times : (row_work % first_dim_per_times);
+        gm_offset_ = nl_first_dim_per_core * num_last_dim;
+        x_gm.SetGlobalBuffer((__gm__ half *)x + AscendC::GetBlockIdx() * gm_offset_);
+        z_gm.SetGlobalBuffer((__gm__ int8_t *)z + AscendC::GetBlockIdx() * gm_offset_);
+        pipe.InitBuffer(x_que, BUFFER_NUM, row_step * ROUND_UP(num_last_dim) * DATA_BYTE);
+        pipe.InitBuffer(z_que, BUFFER_NUM, row_step * ROUND_UP(num_last_dim) * sizeof(int8_t));
+    }
+
+    __aicore__ inline void Process()
+    {
+        uint32_t move_cnt = CEIL_DIV(row_work, row_step); // 一个核需要做多少次
+        for (uint64_t i = 0; i < move_cnt; ++i) {
+            if (i < move_cnt - 1) {
+                CopyIn(i, row_step * num_last_dim);
+
+                AscendC::SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                AscendC::WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+
+                Compute(row_step);
+
+                AscendC::SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                AscendC::WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+
+                CopyOut(i, row_step * num_last_dim);
+            } else {
+                CopyIn(i, row_tail_ * num_last_dim);
+
+                AscendC::SetFlag<HardEvent::MTE2_V>(EVENT_ID1);
+                AscendC::WaitFlag<HardEvent::MTE2_V>(EVENT_ID1);
+
+                Compute(row_tail_);
+
+                AscendC::SetFlag<HardEvent::V_MTE2>(EVENT_ID1);
+                AscendC::WaitFlag<HardEvent::V_MTE2>(EVENT_ID1);
+
+                CopyOut(i, row_tail_ * num_last_dim);
+            }
+        }
+    }
+
+private:
+    __aicore__ inline void CopyIn(uint64_t proc_id, int32_t size)
+    {
+        // alloc tensor from queue memory
+        AscendC::LocalTensor<half> x_local = x_que.AllocTensor<half>();
+        uint64_t offset = proc_id * row_step * num_last_dim;
+        DataCopy(x_local, x_gm[offset], size);
+        x_que.EnQue(x_local);
+    }
+
+    __aicore__ inline void Compute(int32_t nums)
+    {
+        AscendC::LocalTensor<half> x_local = x_que.DeQue<half>();
+        AscendC::LocalTensor<int8_t> z_local = z_que.AllocTensor<int8_t>();
+
+        for (int32_t rid = 0; rid < nums; ++rid) {
+            AscendC::PipeBarrier<PIPE_V>();
+            Muls(x_local[rid * num_last_dim], x_local[rid * num_last_dim], static_cast<half>(input_scale),
+                 num_last_dim);
+            AscendC::PipeBarrier<PIPE_V>();
+            Adds(x_local[rid * num_last_dim], x_local[rid * num_last_dim], static_cast<half>(input_offset),
+                 num_last_dim);
+            AscendC::PipeBarrier<PIPE_V>();
+            CastFromF16ToI8(z_local[rid * num_last_dim], x_local[rid * num_last_dim], quant_min, num_last_dim);
+        }
+
+        z_que.EnQue(z_local);
+        x_que.FreeTensor(x_local);
+    }
+
+    __aicore__ inline void CopyOut(uint64_t proc_id, int32_t size)
+    {
+        AscendC::LocalTensor<int8_t> z = z_que.DeQue<int8_t>();
+        uint64_t offset = proc_id * row_step * num_last_dim; // 单核一次总共做了多少。
+        DataCopy(z_gm[offset], z, size);
+        z_que.FreeTensor(z);
+    }
+
+private:
+    AscendC::TPipe pipe;
+    AscendC::TQue<AscendC::QuePosition::VECIN, BUFFER_NUM> x_que;
+    AscendC::TQue<AscendC::QuePosition::VECOUT, BUFFER_NUM> z_que;
+    AscendC::GlobalTensor<half> x_gm;
+    AscendC::GlobalTensor<int8_t> z_gm;
+    float input_scale{0};
+    int32_t input_offset{0};
+    uint32_t num_core{0};       // 一共激活多少AICORE
+    uint32_t num_first_dim{0};  // 输入的列数
+    uint32_t num_last_dim{0};   // 输入的列数
+    uint32_t row_work{0};       // 每个AICORE需要计算多少行
+    uint32_t row_step{0};       // 除最后一次，每次搬入多少行
+    uint32_t row_tail_{0};      // 最后一次搬入多少行数据
+    uint64_t gm_offset_{0};     // GM数据起始位置偏移量
+    uint32_t nl_first_dim_per_core{0};
+    uint32_t l_first_dim_per_core{0};
+    uint32_t first_dim_per_times{0};
+    half quant_min = -128;
+};
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, AsdOps::QuantF16TilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->numCore = (*(const __gm__ uint32_t *)(p_tilingdata + 0));
+    tilingdata->numLastDim = (*(const __gm__ uint32_t *)(p_tilingdata + 4));
+    tilingdata->numFirstDim = (*(const __gm__ uint32_t *)(p_tilingdata + 8));
+    tilingdata->nlFirstdimPerCore = (*(const __gm__ uint32_t *)(p_tilingdata + 12));
+    tilingdata->lFirstdimPerCore = (*(const __gm__ uint32_t *)(p_tilingdata + 16));
+    tilingdata->firstDimPerTimes = (*(const __gm__ uint32_t *)(p_tilingdata + 20));
+    tilingdata->inputScale = (*(const __gm__ uint32_t *)(p_tilingdata + 24));
+    tilingdata->inputOffset = (*(const __gm__ uint32_t *)(p_tilingdata + 28));
+    tilingdata->quantMin = (*(const __gm__ float *)(p_tilingdata + 32));
+#else
+    AscendC::TPipe pipe;
+    __ubuf__ uint8_t *tilingdata_in_ub = nullptr;
+    CopyGmTilingToUb(tilingdata_in_ub, p_tilingdata, sizeof(AsdOps::QuantF16TilingData), &pipe);
+    AscendC::PipeBarrier<PIPE_ALL>();
+    tilingdata->numCore = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 0));
+    tilingdata->numLastDim = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 4));
+    tilingdata->numFirstDim = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 8));
+    tilingdata->nlFirstdimPerCore = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 12));
+    tilingdata->lFirstdimPerCore = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 16));
+    tilingdata->firstDimPerTimes = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 20));
+    tilingdata->inputScale = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 24));
+    tilingdata->inputOffset = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 28));
+    tilingdata->quantMin = (*(__ubuf__ float *)((__ubuf__ uint8_t *)tilingdata_in_ub + 32));
+    AscendC::PipeBarrier<PIPE_ALL>();
+#endif
+}
+
+#define GET_TILING_DATA(tiling_data, tiling_arg)                                                                       \
+    AsdOps::QuantF16TilingData tiling_data;                                                                            \
+    InitTilingData(tiling_arg, &(tiling_data))
+
+extern "C" __global__ __aicore__ void quant(GM_ADDR x, GM_ADDR z, GM_ADDR tiling)
+{
+    GET_TILING_DATA(tiling_data, tiling);
+    KernelQuantization op;
+    op.Init(x, z, tiling_data.numCore, tiling_data.numLastDim, tiling_data.numFirstDim,
+            tiling_data.nlFirstdimPerCore, tiling_data.lFirstdimPerCore, tiling_data.firstDimPerTimes,
+            tiling_data.inputScale, tiling_data.inputOffset, tiling_data.quantMin);
+    op.Process();
+}
\ No newline at end of file
diff --git a/src/kernels/kernels/logprobs_sample/CMakeLists.txt b/src/kernels/kernels/logprobs_sample/CMakeLists.txt
index 29600342..01529720 100644
--- a/src/kernels/kernels/logprobs_sample/CMakeLists.txt
+++ b/src/kernels/kernels/logprobs_sample/CMakeLists.txt
@@ -13,3 +13,12 @@ set(logprobssample_srcs
 )
 
 add_operation(LogprobsSampleOperation "${logprobssample_srcs}")
+
+add_kernel(logprobs_sample ascend910b vector
+        op_kernel/logprobs_sample.cpp
+        LogprobsSampleKernel)
+
+add_kernel(logprobs_sample ascend310p vector
+        op_kernel/logprobs_sample.cpp
+        LogprobsSampleKernel)
+
diff --git a/src/kernels/kernels/logprobs_sample/op_kernel/logprobs_sample.cpp b/src/kernels/kernels/logprobs_sample/op_kernel/logprobs_sample.cpp
new file mode 100644
index 00000000..c437f332
--- /dev/null
+++ b/src/kernels/kernels/logprobs_sample/op_kernel/logprobs_sample.cpp
@@ -0,0 +1,322 @@
+/*
+ * Copyright (c) Huawei Technologies Co., Ltd. 2025. All rights reserved.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#include "kernel_operator.h"
+#include "mixkernels/utils/common/kernel/kernel_utils.h"
+
+#include "kernels/logprobs_sample/tiling/tiling_data.h"
+
+static constexpr int32_t BUFFER_NUM = 1;               // 不使用double buffer，仅使用queue接口来简化同步逻辑
+static constexpr float DEFAULT_LOG_VALUE = -9999.0f;
+static constexpr int32_t INPUT_PAD_SIZE = 16;          // 输入sortedProbs为half或者bf16
+static constexpr int32_t OUTPUT_PAD_SIZE = 8;          // 输出logprobs为float32
+static constexpr int32_t SYNC_OUT_MIN_SIZE = 32;
+static constexpr int32_t SYNC_OUT_EVENT_ID = 0;
+
+template<typename T>
+class LogprobsSample {
+public:
+    __aicore__ inline LogprobsSample() {}
+    __aicore__ inline void Init(GM_ADDR sortedProbs, GM_ADDR cumsumedProbs, GM_ADDR selectRange, GM_ADDR logprobsSize,
+                                GM_ADDR syncWorkspace, GM_ADDR outputLogprobs,
+                                const AsdOps::LogprobsSampleTilingData &tilingData)
+    {
+        coreNums_ = AscendC::GetBlockNum();
+        isMultiCore_ = coreNums_ > 1;
+        currentCoreId_ = AscendC::GetBlockIdx();
+        currentSelectRange_ = 0;
+        currentSelectProb_ = 0.0f;
+        batchSize_ = tilingData.batchSize;
+        logprobsSize_ = *((__gm__ int32_t*)logprobsSize);
+        calculateSizePad_ = (logprobsSize_ + INPUT_PAD_SIZE - 1) / INPUT_PAD_SIZE * INPUT_PAD_SIZE;
+        outputSizePad_ = (logprobsSize_ + OUTPUT_PAD_SIZE - 1) / OUTPUT_PAD_SIZE * OUTPUT_PAD_SIZE;
+        isOutputAligned_ = outputSizePad_ == logprobsSize_;
+        collectOutputLocally_ = logprobsSize_ < OUTPUT_PAD_SIZE;
+        probsSize_ = tilingData.probsSize;
+        notLastCoreRun_ = (batchSize_ + coreNums_ - 1) / coreNums_;
+        lastCoreRun_ = batchSize_ - (coreNums_ - 1) * notLastCoreRun_;
+        coreRun_ = (currentCoreId_ == coreNums_ - 1) ? lastCoreRun_ : notLastCoreRun_;
+
+        cumsumedProbsGm_.SetGlobalBuffer((__gm__ T *)cumsumedProbs);
+        selectRangeGm_.SetGlobalBuffer((__gm__ int32_t *)selectRange);
+
+        pipe_.InitBuffer(calcTmpBuf_, calculateSizePad_ * sizeof(float));
+        sortedProbsGm_.SetGlobalBuffer((__gm__ T *)sortedProbs);
+        logprobsOutputGm_.SetGlobalBuffer((__gm__ float *)outputLogprobs);
+
+        pipe_.InitBuffer(inQueueSortedProbs_, BUFFER_NUM,  calculateSizePad_ * sizeof(T));
+        pipe_.InitBuffer(outQueueLogprobs_, BUFFER_NUM, outputSizePad_ * sizeof(float));
+
+#if (__CCE_AICORE__ != 220)
+        if (isMultiCore_) {
+            if (collectOutputLocally_) {
+                pipe_.InitBuffer(outputLocalBuf_, coreRun_ * OUTPUT_PAD_SIZE * sizeof(float));
+                syncGm_.SetGlobalBuffer((__gm__ int32_t *)syncWorkspace, coreNums_ * SYNC_OUT_MIN_SIZE);
+                pipe_.InitBuffer(syncOutQueue_, BUFFER_NUM, SYNC_OUT_MIN_SIZE);
+            } else {
+                pipe_.InitBuffer(gatherMaskPattenBuf_, sizeof(uint32_t));
+                pipe_.InitBuffer(outQueueLogprobsTail_, BUFFER_NUM, OUTPUT_PAD_SIZE * sizeof(float));
+            }
+        }
+#endif
+    }
+    __aicore__ inline void Process()
+    {
+        if (logprobsSize_ == 0) {
+            return;
+        }
+        for (int32_t relativeBatchId = 0; relativeBatchId < coreRun_; ++relativeBatchId) {
+            int32_t absoluteBatchId = currentCoreId_ * notLastCoreRun_ + relativeBatchId;
+            InitSelectRangeAndProb(absoluteBatchId);
+            CopyIn(absoluteBatchId);
+            Compute();
+            CopyOut(relativeBatchId, absoluteBatchId);
+        }
+#if (__CCE_AICORE__ != 220)
+        if (isMultiCore_ && collectOutputLocally_) {
+            SyncOut();
+        }
+#endif
+    }
+
+private:
+    __aicore__ inline void InitSelectRangeAndProb(int32_t absoluteBatchId)
+    {
+        currentSelectRange_ = selectRangeGm_.GetValue(absoluteBatchId);
+        // 当选择范围为0，为异常情况。根本原因是ToppSample应当选择the minimum that >= topp，而不是the maximum that <= topp
+        // 这里强制设置为1，意为选择范围只有第一个值
+        if (currentSelectRange_ == 0)
+            currentSelectRange_ = 1;
+        int64_t selectProbOffset = currentSelectRange_ - 1;
+        selectProbOffset += probsSize_ * absoluteBatchId;
+#if (__CCE_AICORE__ == 220)
+        if constexpr(AscendC::IsSameType<T, bfloat16_t>::value) {
+            currentSelectProb_ = AscendC::ToFloat(cumsumedProbsGm_.GetValue(selectProbOffset));
+        } else {
+            currentSelectProb_ = (float)cumsumedProbsGm_.GetValue(selectProbOffset);
+        }
+#else
+        currentSelectProb_ = (float)cumsumedProbsGm_.GetValue(selectProbOffset);
+#endif
+    }
+
+    __aicore__ inline void CopyIn(int32_t absoluteBatchId)
+    {
+        AscendC::LocalTensor<T> sortedProbsLocal = inQueueSortedProbs_.AllocTensor<T>();
+        int32_t blockOffset = absoluteBatchId * probsSize_;
+        AscendC::DataCopy(sortedProbsLocal, sortedProbsGm_[blockOffset], calculateSizePad_);
+        inQueueSortedProbs_.EnQue(sortedProbsLocal);
+    }
+
+    __aicore__ inline void Compute()
+    {
+        AscendC::LocalTensor<T> sortedProbsLocal = inQueueSortedProbs_.DeQue<T>();
+        AscendC::LocalTensor<float> logprobsLocal = outQueueLogprobs_.AllocTensor<float>();
+        if (logprobsSize_ <= currentSelectRange_) {
+            CalculateLogOutput(sortedProbsLocal, logprobsLocal, outputSizePad_);
+        } else {
+            uint32_t fullBlockSize = (currentSelectRange_ / INPUT_PAD_SIZE) * INPUT_PAD_SIZE;
+            uint32_t tailSize = currentSelectRange_ - fullBlockSize;
+            if (fullBlockSize > 0) {
+                CalculateLogOutput(sortedProbsLocal, logprobsLocal, fullBlockSize);
+            }
+            if (fullBlockSize < logprobsSize_) {
+                AscendC::Duplicate(logprobsLocal[fullBlockSize], DEFAULT_LOG_VALUE, logprobsSize_ - fullBlockSize);
+                AscendC::PipeBarrier<PIPE_V>();
+            }
+            if (tailSize > 0) {
+                CalculateLogOutput(sortedProbsLocal[fullBlockSize], logprobsLocal[fullBlockSize], tailSize);
+            }
+        }
+
+#if (__CCE_AICORE__ != 220)
+        if (!isOutputAligned_ && isMultiCore_ && !collectOutputLocally_) {
+            AscendC::LocalTensor<uint32_t> gatherMaskPattern = gatherMaskPattenBuf_.Get<uint32_t>();
+            /* 比如logprobsSize为20，对应的索引为[0, 19]
+             * 则在拷出时，需要先把[0, 15]拷出，再把[16, 19]拷出
+             * 这里通过在[8, 19]的输入里，GatherMask [12, 19]的位置拷贝到tailLocal，对应的pattern为
+             * 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
+             * 0  0  0  0  1  1  1  1  1  1  1  1  0  0  0  0
+             * 注意8为低地址，[12, 15]拷贝了两次
+             * */
+            int32_t remainCount = logprobsSize_ + OUTPUT_PAD_SIZE - outputSizePad_;
+            uint32_t pattern = ((1u << OUTPUT_PAD_SIZE) - 1) << remainCount;
+            gatherMaskPattern.SetValue(0, pattern);
+
+            AscendC::LocalTensor<float> tailLocal = outQueueLogprobsTail_.AllocTensor<float>();
+            uint32_t mask = OUTPUT_PAD_SIZE * 2;
+            uint32_t mainOffset = outputSizePad_ - mask;
+            uint64_t rsvdCnt = 0;
+            AscendC::GatherMask(tailLocal, logprobsLocal[mainOffset], gatherMaskPattern, true, mask, {1, 1, 8, 8},
+                                rsvdCnt);
+            outQueueLogprobsTail_.EnQue(tailLocal);
+        }
+#endif
+        outQueueLogprobs_.EnQue(logprobsLocal);
+        inQueueSortedProbs_.FreeTensor(sortedProbsLocal);
+    }
+
+    __aicore__ inline void CalculateLogOutput(const AscendC::LocalTensor<T> &srcTensor,
+                                              const AscendC::LocalTensor<float> &dstTensor,
+                                              int calcCount)
+    {
+        AscendC::LocalTensor<float> dividendBuf = calcTmpBuf_.Get<float>();
+        // topp select prob不可能为0，因为cumsumed prob是逆序概率求部分和算出来的
+        AscendC::Duplicate(dividendBuf, currentSelectProb_, calcCount);
+        AscendC::PipeBarrier<PIPE_V>();
+        AscendC::Cast(dstTensor, srcTensor, AscendC::RoundMode::CAST_NONE, calcCount);
+        AscendC::PipeBarrier<PIPE_V>();
+        AscendC::Div(dstTensor, dstTensor, dividendBuf, calcCount);
+        AscendC::PipeBarrier<PIPE_V>();
+        AscendC::Log(dstTensor, dstTensor, calcCount);
+        AscendC::PipeBarrier<PIPE_V>();
+    }
+
+    __aicore__ inline void CopyOut(int32_t relativeBatchId, int32_t absoluteBatchId)
+    {
+        AscendC::LocalTensor<float> logprobsLocal = outQueueLogprobs_.DeQue<float>();
+        uint32_t offsetMain = absoluteBatchId * logprobsSize_;
+        if (isOutputAligned_ || !isMultiCore_) {
+            // 对齐场景，全量拷贝；单核场景，正常覆盖
+            AscendC::DataCopy(logprobsOutputGm_[offsetMain], logprobsLocal, outputSizePad_);
+        } else {
+#if (__CCE_AICORE__ == 220)
+            AscendC::DataCopyExtParams copyParams{1, logprobsSize_ * (uint32_t)sizeof(float), 0, 0, 0};
+            AscendC::DataCopyPad(logprobsOutputGm_[offsetMain], logprobsLocal, copyParams);
+#else
+            // 非对齐场景
+            if (collectOutputLocally_) {
+                AscendC::LocalTensor<float> outputLocal = outputLocalBuf_.Get<float>();
+                AscendC::DataCopy(outputLocal[relativeBatchId * OUTPUT_PAD_SIZE], logprobsLocal, OUTPUT_PAD_SIZE);
+            } else {
+                uint32_t mainBlockSize = logprobsSize_ / OUTPUT_PAD_SIZE * OUTPUT_PAD_SIZE;
+                AscendC::DataCopy(logprobsOutputGm_[offsetMain], logprobsLocal, mainBlockSize);
+                AscendC::PipeBarrier<PIPE_MTE3>();
+                uint32_t offsetTail = offsetMain + logprobsSize_ - OUTPUT_PAD_SIZE;
+                AscendC::LocalTensor<float> tailLocal = outQueueLogprobsTail_.DeQue<float>();
+                AscendC::DataCopy(logprobsOutputGm_[offsetTail], tailLocal, OUTPUT_PAD_SIZE);
+                outQueueLogprobsTail_.FreeTensor(tailLocal);
+            }
+#endif
+        }
+        outQueueLogprobs_.FreeTensor(logprobsLocal);
+    }
+
+    __aicore__ inline void SyncOut()
+    {
+        if (currentCoreId_ == 0) {
+            auto syncBuf = syncOutQueue_.AllocTensor<int32_t>();
+            CopyLocalBufOut();
+            AscendC::IBSet(syncGm_, syncBuf, 0, SYNC_OUT_EVENT_ID);
+            syncOutQueue_.FreeTensor(syncBuf);
+        } else if (currentCoreId_ == coreNums_ - 1) {
+            auto syncBuf = syncOutQueue_.AllocTensor<int32_t>();
+            AscendC::IBWait(syncGm_, syncBuf, currentCoreId_ - 1, SYNC_OUT_EVENT_ID);
+            CopyLocalBufOut();
+            syncOutQueue_.FreeTensor(syncBuf);
+        } else {
+            auto syncBuf = syncOutQueue_.AllocTensor<int32_t>();
+            AscendC::IBWait(syncGm_, syncBuf, currentCoreId_ - 1, SYNC_OUT_EVENT_ID);
+            CopyLocalBufOut();
+            AscendC::IBSet(syncGm_, syncBuf, currentCoreId_, SYNC_OUT_EVENT_ID);
+            syncOutQueue_.FreeTensor(syncBuf);
+        }
+    }
+
+    __aicore__ inline void CopyLocalBufOut()
+    {
+        AscendC::LocalTensor<float> localBuf = outputLocalBuf_.Get<float>();
+        for (int32_t relativeBatchId = 0; relativeBatchId < coreRun_; ++relativeBatchId) {
+            int32_t absoluteBatchId = currentCoreId_ * notLastCoreRun_ + relativeBatchId;
+            uint32_t offsetMain = absoluteBatchId * logprobsSize_;
+            // GM里的数据位置有重叠，需要同步
+            AscendC::PipeBarrier<PIPE_MTE3>();
+            AscendC::DataCopy(logprobsOutputGm_[offsetMain], localBuf[relativeBatchId * OUTPUT_PAD_SIZE],
+                              OUTPUT_PAD_SIZE);
+        }
+    }
+
+private:
+    AscendC::TPipe pipe_;
+    AscendC::TQue<AscendC::TPosition::VECIN, BUFFER_NUM> inQueueSortedProbs_;
+    AscendC::TQue<AscendC::TPosition::VECOUT, BUFFER_NUM> outQueueLogprobs_;
+    AscendC::TQue<AscendC::TPosition::VECOUT, BUFFER_NUM> outQueueLogprobsTail_;
+    AscendC::TQue<AscendC::TPosition::VECOUT, BUFFER_NUM> syncOutQueue_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> calcTmpBuf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> gatherMaskPattenBuf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> outputLocalBuf_;
+    AscendC::GlobalTensor<T> sortedProbsGm_;
+    AscendC::GlobalTensor<T> cumsumedProbsGm_;
+    AscendC::GlobalTensor<int32_t> selectRangeGm_;
+    AscendC::GlobalTensor<float> logprobsOutputGm_;
+    AscendC::GlobalTensor<int32_t> syncGm_;
+    uint32_t coreNums_;
+    uint32_t currentCoreId_;
+    int32_t currentSelectRange_;
+    float currentSelectProb_;
+    uint32_t batchSize_;
+    uint32_t logprobsSize_;
+    uint32_t calculateSizePad_;
+    uint32_t outputSizePad_;
+    bool isOutputAligned_;
+    uint32_t probsSize_;
+    uint32_t notLastCoreRun_;
+    uint32_t lastCoreRun_;
+    uint32_t coreRun_;
+    bool collectOutputLocally_ = false;
+    bool isMultiCore_;
+};
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, AsdOps::LogprobsSampleTilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->batchSize = (*(const __gm__ uint32_t *)(p_tilingdata + 0));
+    tilingdata->probsSize = (*(const __gm__ uint32_t *)(p_tilingdata + 4));
+#else
+    AscendC::TPipe pipe_;
+    __ubuf__ uint8_t *tilingdata_in_ub = nullptr;
+    CopyGmTilingToUb(tilingdata_in_ub, p_tilingdata, sizeof(AsdOps::LogprobsSampleTilingData), &pipe_);
+    AscendC::PipeBarrier<PIPE_ALL>();
+    tilingdata->batchSize = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 0));
+    tilingdata->probsSize = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 4));
+    AscendC::PipeBarrier<PIPE_ALL>();
+#endif
+}
+
+
+/**
+  * @brief  计算logprobs的kernel入口函数
+  * @param  sortedProbs:    逆序排序概率的，shape为(batch, voc_size)
+  * @param  cumsumedProbs:  逆序排序概率l累积和，shape为(batch, vec_size)
+  * @param  selectRange:    topp筛选出的范围，shape为(batch, 1)，每个值的值域为[0, voc_size]
+  * @param  logprobsSize:   logprobs输出第二维的大小，shape为(1)
+  * @param  outputLogprobs: logprobs的计算结果，shape为(batch, logprobsSize)
+  * @param  syncWorkspace:  用于拷出时的核间同步
+  * @param  tiling:         logprobs tiling数据
+  * @retval None
+ */
+extern "C" __global__ __aicore__ void logprobs_sample(GM_ADDR sortedProbs, GM_ADDR cumsumedProbs, GM_ADDR selectRange,
+                                                      GM_ADDR logprobsSize, GM_ADDR outputLogprobs,
+                                                      GM_ADDR syncWorkspace, GM_ADDR tiling)
+{
+    AsdOps::LogprobsSampleTilingData tiling_data;
+    InitTilingData(tiling, &tiling_data);
+    if (TILING_KEY_IS(0)) {
+        LogprobsSample<half> op;
+        op.Init(sortedProbs, cumsumedProbs, selectRange, logprobsSize, syncWorkspace, outputLogprobs, tiling_data);
+        op.Process();
+    }
+#if (__CCE_AICORE__ == 220)
+    if (TILING_KEY_IS(1)) {
+        LogprobsSample<bfloat16_t> op;
+        op.Init(sortedProbs, cumsumedProbs, selectRange, logprobsSize, syncWorkspace, outputLogprobs, tiling_data);
+        op.Process();
+    }
+#endif
+}
diff --git a/src/kernels/mixkernels/CMakeLists.txt b/src/kernels/mixkernels/CMakeLists.txt
index 4d9b06dd..8766d9f8 100644
--- a/src/kernels/mixkernels/CMakeLists.txt
+++ b/src/kernels/mixkernels/CMakeLists.txt
@@ -74,20 +74,20 @@ list(APPEND BINARY_SRC_LIST ${CMAKE_CURRENT_LIST_DIR}/param_to_json.cpp
 add_library(atb_mixops SHARED ${BINARY_SRC_LIST})
 add_dependencies(atb_mixops MIX_BINARY_SRC_TARGET)
 target_link_libraries(atb_mixops PRIVATE ${ops_objects} mki tbe_adapter tiling_api register cann_ops_adapter)
-target_link_libraries(atb_mixops PRIVATE
-    -Wl,--whole-archive
-    exp_mixops_static
-    -Wl,--no-whole-archive
-)
+#target_link_libraries(atb_mixops PRIVATE
+#    -Wl,--whole-archive
+#    exp_mixops_static
+#    -Wl,--no-whole-archive
+#)
 
 add_library(atb_mixops_static STATIC $<TARGET_OBJECTS:atb_mixops>)
 add_dependencies(atb_mixops_static MIX_BINARY_SRC_TARGET)
 target_link_libraries(atb_mixops_static PRIVATE ${ops_objects} mki tbe_adapter tiling_api register cann_ops_adapter)
-target_link_libraries(atb_mixops_static PRIVATE
-    -Wl,--whole-archive
-    exp_mixops_static
-    -Wl,--no-whole-archive
-)
+#target_link_libraries(atb_mixops_static PRIVATE
+#    -Wl,--whole-archive
+#    exp_mixops_static
+#    -Wl,--no-whole-archive
+#)
 
 add_executable(atbops_sym_check sym_check.cpp)
 target_link_libraries(atbops_sym_check PRIVATE atb_mixops mki)
diff --git a/src/kernels/mixkernels/fastsoftmax/CMakeLists.txt b/src/kernels/mixkernels/fastsoftmax/CMakeLists.txt
index 6ac9d281..02ba3889 100644
--- a/src/kernels/mixkernels/fastsoftmax/CMakeLists.txt
+++ b/src/kernels/mixkernels/fastsoftmax/CMakeLists.txt
@@ -13,3 +13,7 @@ set(fastsoftmax_srcs
 )
 
 add_operation(FastSoftMaxOperation "${fastsoftmax_srcs}")
+
+add_kernel(fastsoftmax ascend910b vector
+        op_kernel/fast_softmax.cpp
+        FastSoftMaxKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/fastsoftmax/op_kernel/fast_softmax.cpp b/src/kernels/mixkernels/fastsoftmax/op_kernel/fast_softmax.cpp
new file mode 100644
index 00000000..afee768b
--- /dev/null
+++ b/src/kernels/mixkernels/fastsoftmax/op_kernel/fast_softmax.cpp
@@ -0,0 +1,156 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#include "kernel_operator.h"
+#include "mixkernels/fastsoftmax/tiling/tiling_data.h"
+
+using namespace AscendC;
+using namespace AtbOps;
+
+class FastSoftmax {
+public:
+    __aicore__ inline FastSoftmax() {}
+    __aicore__ inline void Init(GM_ADDR dataInput, GM_ADDR dataOutput, const FastSoftMaxSampleTilingData &tilingData)
+    {
+        sampleSeqLenOrigin = tilingData.sampleSeqLenOrigin;
+        sampleSeqLen = tilingData.sampleSeqLen;
+        tileRowNum = tilingData.tileRowNum;
+        tailTileRowNum = tilingData.tailTileRowNum;
+        tileLengthOrigin = tileRowNum * sampleSeqLenOrigin;
+        if (GetBlockIdx() < tilingData.formerCoreNum) {
+            coreTileNum = tilingData.formerCoreTileNum;
+            innerDataOffset = static_cast<uint64_t>(GetBlockIdx()) * tilingData.formerCoreTileNum * tileLengthOrigin;
+        } else {
+            coreTileNum = tilingData.latterCoreTileNum;
+            innerDataOffset = static_cast<uint64_t>((GetBlockIdx() * tilingData.latterCoreTileNum +
+                tilingData.formerCoreNum) * tileLengthOrigin);
+        }
+        isLastCore = (tailTileRowNum > 0) &&
+            (GetBlockIdx() == tilingData.formerCoreNum + tilingData.latterCoreNum - 1);
+
+        dataInputGm.SetGlobalBuffer((__gm__ half*)dataInput + tilingData.dataOffset, tilingData.dataLength);
+        dataOutputGm.SetGlobalBuffer((__gm__ half*)dataOutput + tilingData.dataOffset, tilingData.dataLength);
+
+        softMaxTiling = *reinterpret_cast<const SoftMaxTiling *>(tilingData.softMaxTilingBuffer);
+        tailSoftMaxTiling = *reinterpret_cast<const SoftMaxTiling *>(tilingData.tailSoftMaxTilingBuffer);
+
+        pipe.InitBuffer(dataInputQueue, BUFFER_NUM, tileRowNum * tilingData.innerSize);
+        pipe.InitBuffer(dataOutputQueue, BUFFER_NUM, tileRowNum * tilingData.innerSize);
+        pipe.InitBuffer(sharedTmpBuffer, SHARED_TMP_SIZE);
+        softmaxTmpUb = sharedTmpBuffer.Get<uint8_t>();
+    }
+
+    __aicore__ inline void Process()
+    {
+        for (uint32_t tileIndex = 0; tileIndex < coreTileNum; ++tileIndex) {
+            CopyIn(tileRowNum);
+            Compute(tileRowNum, softMaxTiling);
+            CopyOut(tileRowNum);
+            innerDataOffset += tileLengthOrigin;
+        }
+        if (isLastCore) {
+            CopyIn(tailTileRowNum);
+            Compute(tailTileRowNum, tailSoftMaxTiling);
+            CopyOut(tailTileRowNum);
+        }
+    }
+
+private:
+    __aicore__ inline void CopyIn(uint32_t currentTileRowNum)
+    {
+        LocalTensor<half> dataInputUb = dataInputQueue.AllocTensor<half>();
+        DataCopyPad(dataInputUb, dataInputGm[innerDataOffset],
+            DataCopyParams(currentTileRowNum, sampleSeqLenOrigin * DATA_BYTESIZE, 0, 0), DataCopyPadParams());
+        dataInputQueue.EnQue<half>(dataInputUb);
+    }
+
+    __aicore__ inline void Compute(uint32_t currentTileRowNum, const SoftMaxTiling &currentSoftMaxTiling)
+    {
+        LocalTensor<half> dataInputUb = dataInputQueue.DeQue<half>();
+        LocalTensor<half> dataOutputUb = dataOutputQueue.AllocTensor<half>();
+
+        SoftMaxShapeInfo srcShape = { currentTileRowNum, sampleSeqLen, currentTileRowNum, sampleSeqLenOrigin };
+        SoftMax<half, false>(dataOutputUb, dataInputUb, softmaxTmpUb, currentSoftMaxTiling, srcShape);
+
+        dataOutputQueue.EnQue<half>(dataOutputUb);
+        dataInputQueue.FreeTensor<half>(dataInputUb);
+    }
+
+    __aicore__ inline void CopyOut(uint32_t currentTileRowNum)
+    {
+        LocalTensor<half> dataOutputUb = dataOutputQueue.DeQue<half>();
+        DataCopyPad(dataOutputGm[innerDataOffset], dataOutputUb,
+            DataCopyParams(currentTileRowNum, sampleSeqLenOrigin * DATA_BYTESIZE, 0, 0));
+        dataOutputQueue.FreeTensor(dataOutputUb);
+    }
+
+    TPipe pipe;
+    TQue<QuePosition::VECIN, BUFFER_NUM> dataInputQueue;
+    TQue<QuePosition::VECOUT, BUFFER_NUM> dataOutputQueue;
+    TBuf<TPosition::VECCALC> sharedTmpBuffer;
+    LocalTensor<uint8_t> softmaxTmpUb;
+    GlobalTensor<half> dataInputGm;
+    GlobalTensor<half> dataOutputGm;
+    uint32_t sampleSeqLenOrigin;
+    uint32_t sampleSeqLen;
+    uint32_t tileRowNum;
+    uint32_t tailTileRowNum;
+    uint32_t coreTileNum;
+    uint64_t innerDataOffset;
+    uint32_t tileLengthOrigin;
+    bool isLastCore;
+    SoftMaxTiling softMaxTiling;
+    SoftMaxTiling tailSoftMaxTiling;
+};
+
+inline __aicore__ FastSoftMaxTilingData GetTilingData(const GM_ADDR tiling)
+{
+    auto tilingDataPointer = reinterpret_cast<const __gm__ FastSoftMaxTilingData *>(tiling);
+    FastSoftMaxTilingData tilingData;
+    tilingData.batchSize = tilingDataPointer->batchSize;
+    tilingData.headNum = tilingDataPointer->headNum;
+    return tilingData;
+}
+
+inline __aicore__ FastSoftMaxSampleTilingData GetSampleTilingData(const GM_ADDR tiling)
+{
+    auto sampleTilingDataPointer = reinterpret_cast<const __gm__ FastSoftMaxSampleTilingData *>(tiling);
+    FastSoftMaxSampleTilingData sampleTilingData;
+    sampleTilingData.sampleSeqLenOrigin = sampleTilingDataPointer->sampleSeqLenOrigin;
+    sampleTilingData.sampleSeqLen = sampleTilingDataPointer->sampleSeqLen;
+    sampleTilingData.dataOffset = sampleTilingDataPointer->dataOffset;
+    sampleTilingData.dataLength = sampleTilingDataPointer->dataLength;
+    sampleTilingData.outerSize = sampleTilingDataPointer->outerSize;
+    sampleTilingData.innerSize = sampleTilingDataPointer->innerSize;
+    sampleTilingData.tileRowNum = sampleTilingDataPointer->tileRowNum;
+    sampleTilingData.tailTileRowNum = sampleTilingDataPointer->tailTileRowNum;
+    sampleTilingData.formerCoreNum = sampleTilingDataPointer->formerCoreNum;
+    sampleTilingData.latterCoreNum = sampleTilingDataPointer->latterCoreNum;
+    sampleTilingData.formerCoreTileNum = sampleTilingDataPointer->formerCoreTileNum;
+    sampleTilingData.latterCoreTileNum = sampleTilingDataPointer->latterCoreTileNum;
+    for (uint32_t i = 0; i < SOFTMAX_TILING_SIZE; ++i) {
+        sampleTilingData.softMaxTilingBuffer[i] = sampleTilingDataPointer->softMaxTilingBuffer[i];
+        sampleTilingData.tailSoftMaxTilingBuffer[i] = sampleTilingDataPointer->tailSoftMaxTilingBuffer[i];
+    }
+    return sampleTilingData;
+}
+
+extern "C" __global__ __aicore__ void fastsoftmax(GM_ADDR dataInput, GM_ADDR dataOutput, GM_ADDR tiling)
+{
+    FastSoftMaxTilingData tilingData = GetTilingData(tiling);
+    tiling += sizeof(FastSoftMaxTilingData);
+    for (uint32_t sampleIndex = 0; sampleIndex < tilingData.batchSize; ++sampleIndex) {
+        FastSoftMaxSampleTilingData sampleTilingData = GetSampleTilingData(tiling);
+        FastSoftmax op;
+        op.Init(dataInput, dataOutput, sampleTilingData);
+        op.Process();
+        tiling += sizeof(FastSoftMaxSampleTilingData);
+    }
+}
diff --git a/src/kernels/mixkernels/fastsoftmaxgrad/CMakeLists.txt b/src/kernels/mixkernels/fastsoftmaxgrad/CMakeLists.txt
index cacfe934..e6b45728 100644
--- a/src/kernels/mixkernels/fastsoftmaxgrad/CMakeLists.txt
+++ b/src/kernels/mixkernels/fastsoftmaxgrad/CMakeLists.txt
@@ -12,3 +12,7 @@ set(fastsoftmax_grad_srcs
 )
 
 add_operation(FastSoftMaxGradOperation "${fastsoftmax_grad_srcs}")
+
+add_kernel(fastsoftmaxgrad ascend910b vector
+        op_kernel/fast_softmax_grad.cpp
+        FastSoftMaxGradKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/fastsoftmaxgrad/op_kernel/fast_softmax_grad.cpp b/src/kernels/mixkernels/fastsoftmaxgrad/op_kernel/fast_softmax_grad.cpp
new file mode 100644
index 00000000..dd0e2704
--- /dev/null
+++ b/src/kernels/mixkernels/fastsoftmaxgrad/op_kernel/fast_softmax_grad.cpp
@@ -0,0 +1,168 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#include "kernel_operator.h"
+#include "mixkernels/fastsoftmaxgrad/tiling/tiling_data.h"
+
+using namespace AscendC;
+using namespace AtbOps;
+
+class FastSoftmaxGrad {
+public:
+    __aicore__ inline FastSoftmaxGrad() {}
+    __aicore__ inline void Init(GM_ADDR yInput, GM_ADDR yGrad, GM_ADDR xGrad,
+        const FastSoftMaxGradSampleTilingData &tilingData)
+    {
+        sampleSeqLenOrigin = tilingData.sampleSeqLenOrigin;
+        sampleSeqLen = tilingData.sampleSeqLen;
+        tileRowNum = tilingData.tileRowNum;
+        tailTileRowNum = tilingData.tailTileRowNum;
+        tileLengthOrigin = tileRowNum * sampleSeqLenOrigin;
+        if (GetBlockIdx() < tilingData.formerCoreNum) {
+            coreTileNum = tilingData.formerCoreTileNum;
+            innerDataOffset = static_cast<uint64_t>(GetBlockIdx()) * tilingData.formerCoreTileNum * tileLengthOrigin;
+        } else {
+            coreTileNum = tilingData.latterCoreTileNum;
+            innerDataOffset = static_cast<uint64_t>((GetBlockIdx() * tilingData.latterCoreTileNum +
+                tilingData.formerCoreNum) * tileLengthOrigin);
+        }
+        isLastCore =
+            (tailTileRowNum > 0) && (GetBlockIdx() == tilingData.formerCoreNum + tilingData.latterCoreNum - 1);
+
+        yInputGm.SetGlobalBuffer((__gm__ half*)yInput + tilingData.dataOffset, tilingData.dataLength);
+        yGradGm.SetGlobalBuffer((__gm__ half*)yGrad + tilingData.dataOffset, tilingData.dataLength);
+        xGradGm.SetGlobalBuffer((__gm__ half*)xGrad + tilingData.dataOffset, tilingData.dataLength);
+
+        softMaxGradTiling = *reinterpret_cast<const SoftMaxTiling *>(tilingData.softMaxGradTilingBuffer);
+        tailSoftMaxGradTiling = *reinterpret_cast<const SoftMaxTiling *>(tilingData.tailSoftMaxGradTilingBuffer);
+
+        pipe.InitBuffer(yInputQueue, BUFFER_NUM, tileRowNum * tilingData.innerSize);
+        pipe.InitBuffer(yGradQueue, BUFFER_NUM, tileRowNum * tilingData.innerSize);
+        pipe.InitBuffer(xGradQueue, BUFFER_NUM, tileRowNum * tilingData.innerSize);
+        pipe.InitBuffer(sharedTmpBuffer, SHARED_TMP_SIZE);
+        softmaxTmpUb = sharedTmpBuffer.Get<uint8_t>();
+    }
+
+    __aicore__ inline void Process()
+    {
+        for (uint32_t tileIndex = 0; tileIndex < coreTileNum; ++tileIndex) {
+            CopyIn(tileRowNum);
+            Compute(tileRowNum, softMaxGradTiling);
+            CopyOut(tileRowNum);
+            innerDataOffset += tileLengthOrigin;
+        }
+        if (isLastCore) {
+            CopyIn(tailTileRowNum);
+            Compute(tailTileRowNum, tailSoftMaxGradTiling);
+            CopyOut(tailTileRowNum);
+        }
+    }
+
+private:
+    __aicore__ inline void CopyIn(uint32_t currentTileRowNum)
+    {
+        LocalTensor<half> yInputUb = yInputQueue.AllocTensor<half>();
+        LocalTensor<half> yGradUb = yGradQueue.AllocTensor<half>();
+        DataCopyPad(yInputUb, yInputGm[innerDataOffset],
+            DataCopyParams(currentTileRowNum, sampleSeqLenOrigin * DATA_BYTESIZE, 0, 0), DataCopyPadParams());
+        DataCopyPad(yGradUb, yGradGm[innerDataOffset],
+            DataCopyParams(currentTileRowNum, sampleSeqLenOrigin * DATA_BYTESIZE, 0, 0), DataCopyPadParams());
+        yInputQueue.EnQue<half>(yInputUb);
+        yGradQueue.EnQue<half>(yGradUb);
+    }
+
+    __aicore__ inline void Compute(uint32_t currentTileRowNum, const SoftMaxTiling &currentSoftMaxTiling)
+    {
+        LocalTensor<half> yInputUb = yInputQueue.DeQue<half>();
+        LocalTensor<half> yGradUb = yGradQueue.DeQue<half>();
+        LocalTensor<half> xGradUb = xGradQueue.AllocTensor<half>();
+
+        SoftMaxShapeInfo srcShape = { currentTileRowNum, sampleSeqLen, currentTileRowNum, sampleSeqLenOrigin };
+        SoftmaxGrad<half, false>(xGradUb, yGradUb, yInputUb, softmaxTmpUb, currentSoftMaxTiling, false, srcShape);
+
+        xGradQueue.EnQue<half>(xGradUb);
+        yInputQueue.FreeTensor<half>(yInputUb);
+        yGradQueue.FreeTensor<half>(yGradUb);
+    }
+
+    __aicore__ inline void CopyOut(uint32_t currentTileRowNum)
+    {
+        LocalTensor<half> xGradUb = xGradQueue.DeQue<half>();
+        DataCopyPad(xGradGm[innerDataOffset], xGradUb,
+            DataCopyParams(currentTileRowNum, sampleSeqLenOrigin * DATA_BYTESIZE, 0, 0));
+        xGradQueue.FreeTensor(xGradUb);
+    }
+
+    int64_t headNum;
+    TPipe pipe;
+    TQue<QuePosition::VECIN, BUFFER_NUM> yInputQueue;
+    TQue<QuePosition::VECIN, BUFFER_NUM> yGradQueue;
+    TQue<QuePosition::VECOUT, BUFFER_NUM> xGradQueue;
+    TBuf<TPosition::VECCALC> sharedTmpBuffer;
+    LocalTensor<uint8_t> softmaxTmpUb;
+    GlobalTensor<half> yInputGm;
+    GlobalTensor<half> yGradGm;
+    GlobalTensor<half> xGradGm;
+    uint32_t sampleSeqLenOrigin;
+    uint32_t sampleSeqLen;
+    uint32_t tileRowNum;
+    uint32_t tailTileRowNum;
+    uint32_t coreTileNum;
+    uint64_t innerDataOffset;
+    uint32_t tileLengthOrigin;
+    bool isLastCore;
+    SoftMaxTiling softMaxGradTiling;
+    SoftMaxTiling tailSoftMaxGradTiling;
+};
+
+inline __aicore__ FastSoftMaxGradTilingData GetTilingData(const GM_ADDR tiling)
+{
+    auto tilingDataPointer = reinterpret_cast<const __gm__ FastSoftMaxGradTilingData *>(tiling);
+    FastSoftMaxGradTilingData tilingData;
+    tilingData.batchSize = tilingDataPointer->batchSize;
+    tilingData.headNum = tilingDataPointer->headNum;
+    return tilingData;
+}
+
+inline __aicore__ FastSoftMaxGradSampleTilingData GetSampleTilingData(const GM_ADDR tiling)
+{
+    auto sampleTilingDataPointer = reinterpret_cast<const __gm__ FastSoftMaxGradSampleTilingData *>(tiling);
+    FastSoftMaxGradSampleTilingData sampleTilingData;
+    sampleTilingData.sampleSeqLenOrigin = sampleTilingDataPointer->sampleSeqLenOrigin;
+    sampleTilingData.sampleSeqLen = sampleTilingDataPointer->sampleSeqLen;
+    sampleTilingData.dataOffset = sampleTilingDataPointer->dataOffset;
+    sampleTilingData.dataLength = sampleTilingDataPointer->dataLength;
+    sampleTilingData.outerSize = sampleTilingDataPointer->outerSize;
+    sampleTilingData.innerSize = sampleTilingDataPointer->innerSize;
+    sampleTilingData.tileRowNum = sampleTilingDataPointer->tileRowNum;
+    sampleTilingData.tailTileRowNum = sampleTilingDataPointer->tailTileRowNum;
+    sampleTilingData.formerCoreNum = sampleTilingDataPointer->formerCoreNum;
+    sampleTilingData.latterCoreNum = sampleTilingDataPointer->latterCoreNum;
+    sampleTilingData.formerCoreTileNum = sampleTilingDataPointer->formerCoreTileNum;
+    sampleTilingData.latterCoreTileNum = sampleTilingDataPointer->latterCoreTileNum;
+    for (uint32_t i = 0; i < SOFTMAX_TILING_SIZE; ++i) {
+        sampleTilingData.softMaxGradTilingBuffer[i] = sampleTilingDataPointer->softMaxGradTilingBuffer[i];
+        sampleTilingData.tailSoftMaxGradTilingBuffer[i] = sampleTilingDataPointer->tailSoftMaxGradTilingBuffer[i];
+    }
+    return sampleTilingData;
+}
+
+extern "C" __global__ __aicore__ void fastsoftmaxgrad(GM_ADDR yInput, GM_ADDR yGrad, GM_ADDR xGrad, GM_ADDR tiling)
+{
+    FastSoftMaxGradTilingData tilingData = GetTilingData(tiling);
+    tiling += sizeof(FastSoftMaxGradTilingData);
+    for (uint32_t sampleIndex = 0; sampleIndex < tilingData.batchSize; ++sampleIndex) {
+        FastSoftMaxGradSampleTilingData sampleTilingData = GetSampleTilingData(tiling);
+        FastSoftmaxGrad op;
+        op.Init(yInput, yGrad, xGrad, sampleTilingData);
+        op.Process();
+        tiling += sizeof(FastSoftMaxGradSampleTilingData);
+    }
+}
diff --git a/src/kernels/mixkernels/ffn/CMakeLists.txt b/src/kernels/mixkernels/ffn/CMakeLists.txt
index 19553bad..423169e9 100644
--- a/src/kernels/mixkernels/ffn/CMakeLists.txt
+++ b/src/kernels/mixkernels/ffn/CMakeLists.txt
@@ -12,3 +12,7 @@ set(ffn_srcs
 )
 
 add_operation(FFNOperation "${ffn_srcs}")
+
+add_kernel(ffn ascend310p cube
+        op_kernel/ffn.cpp
+        FFNKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/ffn/op_kernel/ffn.cpp b/src/kernels/mixkernels/ffn/op_kernel/ffn.cpp
new file mode 100644
index 00000000..21de767d
--- /dev/null
+++ b/src/kernels/mixkernels/ffn/op_kernel/ffn.cpp
@@ -0,0 +1,51 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+/*!
+ * \file ffn.cpp
+ * \brief
+ */
+#include "kernel_operator.h"
+#include "ffn_quant.h"
+#include "mixkernels/ffn/tiling/tiling_data.h"
+
+template <class T>
+__aicore__ inline void InitPrivateTilingData(const __gm__ uint8_t *tiling, T *const_data)
+{
+    const __gm__ uint32_t *src = (const __gm__ uint32_t *)tiling;
+    uint32_t *dst = (uint32_t *)const_data;
+    for (auto i = 0; i < sizeof(T) / 4; i++) {
+        *(dst + i) = *(src + i);
+    }
+}
+
+extern "C" __global__ __aicore__ void ffn(GM_ADDR x, GM_ADDR weight1, GM_ADDR weight2,
+    GM_ADDR scale_quant, GM_ADDR scale_dequant1, GM_ADDR scale_dequant2, GM_ADDR bias_quant, GM_ADDR bias_dequant1,
+    GM_ADDR bias_dequant2, GM_ADDR y, GM_ADDR workspace, GM_ADDR tiling)
+{
+    AscendC::SetLoadDataPaddingValue<uint64_t>((uint64_t)0x0);
+    AscendC::SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
+    AscendC::SetAtomicNone();
+
+    AtbOps::FFNTilingData tilingData;
+    InitPrivateTilingData<AtbOps::FFNTilingData>(tiling, &tilingData);
+    if (TILING_KEY_IS(0)) {
+        FFN::DefaultFFN<ArchType::ASCEND_V200, int8_t, uint64_t, int32_t, half, half, CubeFormat::ND, CubeFormat::ND,
+            false, true, true, false, false> kernel_int8_nd_prec;
+        kernel_int8_nd_prec.Init(x, weight1, weight2, scale_quant, scale_dequant1, scale_dequant2,
+            bias_quant, bias_dequant1, bias_dequant2, y, workspace, &tilingData);
+        kernel_int8_nd_prec.Process();
+    } else if (TILING_KEY_IS(1)) {
+        FFN::DefaultFFN<ArchType::ASCEND_V200, int8_t, uint64_t, int32_t, half, half, CubeFormat::ND, CubeFormat::ND,
+            false, true, true, false, true> kernel_int8_nd_perf;
+        kernel_int8_nd_perf.Init(x, weight1, weight2, scale_quant, scale_dequant1, scale_dequant2,
+            bias_quant, bias_dequant1, bias_dequant2, y, workspace, &tilingData);
+        kernel_int8_nd_perf.Process();
+    }
+}
\ No newline at end of file
diff --git a/src/kernels/mixkernels/ffn/op_kernel/ffn_common.h b/src/kernels/mixkernels/ffn/op_kernel/ffn_common.h
new file mode 100644
index 00000000..09ea85d0
--- /dev/null
+++ b/src/kernels/mixkernels/ffn/op_kernel/ffn_common.h
@@ -0,0 +1,68 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#ifndef ASCEND_OPS_FFN_COMMON_H
+#define ASCEND_OPS_FFN_COMMON_H
+
+#include "kernel_operator.h"
+#include "lib/matrix/matmul/matmul.h"
+#include "lib/matmul_intf.h"
+#include "kernels/utils/kernel/hardware.h"
+
+namespace FFN {
+constexpr uint32_t BLOCK_SIZE_16 = 16;
+constexpr uint32_t BLOCK_SIZE_32 = 32;
+constexpr uint32_t CONST_1 = 1;
+constexpr uint32_t CONST_2 = 2;
+constexpr uint32_t CONST_3 = 3;
+constexpr uint32_t CONST_4 = 4;
+constexpr uint32_t CONST_8 = 8;
+constexpr uint32_t CONST_16 = 16;
+constexpr uint32_t CONST_32 = 32;
+constexpr uint32_t CONST_64 = 64;
+constexpr uint32_t CONST_128 = 128;
+constexpr uint32_t CONST_256 = 256;
+constexpr uint32_t BLOCK_NUM_PER_FRACTAL = 16;
+constexpr uint32_t BLOCK_NUM_PER_VEC = 8;
+constexpr uint32_t FRACTAL_SIZE = 512;
+constexpr uint32_t MAX_REPEAT_LIMIT = 255;
+
+enum class ActivationType {
+    GELU = 0,
+    FASTGELU,
+    FASTGELUV2,
+    INVALID_TYPE
+};
+
+template <ArchType ARCH_TAG, typename DTYPE_A, CubeFormat FORMAT_A, typename DTYPE_B, CubeFormat FORMAT_B,
+    typename DTYPE_C, CubeFormat FORMAT_C, typename DTYPE_ACCUMULATOR, typename DTYPE_BIAS, typename DTYPE_SCALE,
+    bool TRANS_A, bool TRANS_B, bool WITH_BIAS, bool EN_L0C_DB
+>
+struct DefaultMatmul;
+
+template <
+    ArchType ARCH_TAG,
+    typename IN_DTYPE,
+    typename DESCALE_DTYPE,
+    typename BIAS_DTYPE,
+    typename ACCUMULATOR_DTYPE,
+    typename OUT_DTYPE,
+    CubeFormat IN_FORMAT,
+    CubeFormat OUT_FORMAT,
+    bool TRANSPOSE_X,
+    bool TRANSPOSE_W1,
+    bool TRANSPOSE_W2,
+    bool HIGH_PRECISION,
+    bool HIGH_PERFORMANCE
+>
+struct DefaultFFN;
+} // namespace FFN
+
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/ffn/op_kernel/ffn_gelu.h b/src/kernels/mixkernels/ffn/op_kernel/ffn_gelu.h
new file mode 100644
index 00000000..308244c6
--- /dev/null
+++ b/src/kernels/mixkernels/ffn/op_kernel/ffn_gelu.h
@@ -0,0 +1,148 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef ASCEND_OPS_FFN_GELU_H
+#define ASCEND_OPS_FFN_GELU_H
+
+#ifdef __CCE_KT_TEST__
+#define __aicore__
+#else
+#define __aicore__ [aicore]
+#endif
+
+#include "kernel_operator.h"
+#include "ffn_common.h"
+
+namespace FFN {
+template <typename T>
+__aicore__ inline void FastGeluV2ClipParams(const AscendC::LocalTensor<T>& tempTensorA,
+    const AscendC::LocalTensor<T>& srcLocal, const AscendC::GeluParams<T>& params)
+{
+    const T coefficientsA = -0.1444;
+    const T coefficientsB = -1.769;
+    const T coefficientsBInv = 1.769;
+    const T coefficientsC = 0.7071;
+    const T coefficientsD = 0.5;
+
+    const AscendC::UnaryRepeatParams unaryParams;
+    const AscendC::BinaryRepeatParams binaryParams;
+
+    AscendC::Muls<T, false>(
+        tempTensorA, srcLocal, coefficientsC, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Abs<T, false>(
+        tempTensorA, tempTensorA, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Mins<T, false>(
+        tempTensorA, tempTensorA, coefficientsBInv, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Adds<T, false>(
+        tempTensorA, tempTensorA, coefficientsB, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Mul<T, false>(
+        tempTensorA, tempTensorA, tempTensorA, AscendC::MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Muls<T, false>(
+        tempTensorA, tempTensorA, coefficientsA, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Adds<T, false>(
+        tempTensorA, tempTensorA, coefficientsD, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+}
+
+template <typename T, bool HIGH_PERFORMANCE = false>
+__aicore__ inline void FastGeluV2SgnParams(const AscendC::LocalTensor<T>& tempTensorB,
+    const AscendC::LocalTensor<T>& tempTensorC, const AscendC::LocalTensor<T>& srcLocal,
+    const AscendC::GeluParams<T>& params)
+{
+    T coefficients;
+    if constexpr (AscendC::IsSameType<T, half>::value) {
+        coefficients = 0.0000001;
+    } else {
+        coefficients = 0.000000000001;
+    }
+
+    const AscendC::UnaryRepeatParams unaryParams;
+    const AscendC::BinaryRepeatParams binaryParams;
+
+    AscendC::Adds<T, false>(
+        tempTensorB, srcLocal, coefficients, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Abs<T, false>(
+        tempTensorC, tempTensorB, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    if constexpr (HIGH_PERFORMANCE) {
+        AscendC::Reciprocal<T, false>(
+            tempTensorC, tempTensorC, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+        AscendC::PipeBarrier<PIPE_V>();
+
+        AscendC::Mul<T, false>(
+            tempTensorB, tempTensorB, tempTensorC, AscendC::MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
+        AscendC::PipeBarrier<PIPE_V>();
+    } else {
+        AscendC::Div<T, false>(
+            tempTensorB, tempTensorB, tempTensorC, AscendC::MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
+        AscendC::PipeBarrier<PIPE_V>();
+    }
+}
+
+template <typename T>
+__aicore__ inline void FastGeluV2Params(const AscendC::LocalTensor<T>& tempTensorA,
+    const AscendC::LocalTensor<T>& tempTensorB, const AscendC::LocalTensor<T>& dstLocal,
+    const AscendC::LocalTensor<T>& srcLocal, const AscendC::GeluParams<T>& params)
+{
+    const T coefficientsHalf = 0.5;
+
+    const AscendC::UnaryRepeatParams unaryParams;
+    const AscendC::BinaryRepeatParams binaryParams;
+
+    AscendC::Mul<T, false>(
+        tempTensorA, tempTensorA, tempTensorB, AscendC::MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Adds<T, false>(
+        tempTensorA, tempTensorA, coefficientsHalf, AscendC::MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+    AscendC::Mul<T, false>(
+        dstLocal, srcLocal, tempTensorA, AscendC::MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
+    AscendC::PipeBarrier<PIPE_V>();
+}
+
+template <typename T, bool HIGH_PERFORMANCE = false>
+__aicore__ inline void FasterGeluV2Func(const AscendC::LocalTensor<T>& dstLocal,
+    const AscendC::LocalTensor<T>& srcLocal, const AscendC::GeluParams<T>& params)
+{
+    const AscendC::UnaryRepeatParams unaryParams;
+    const AscendC::LocalTensor<T>& tempTensorA = params.tempTensorA;
+    const AscendC::LocalTensor<T>& tempTensorB = params.tempTensorB;
+    const AscendC::LocalTensor<T>& tempTensorC = params.tempTensorC;
+    const AscendC::LocalTensor<T>& tempTensorConv = params.tempTensorConv;
+
+    // x1 = (-0.1444) * (clip(|0.7071 * x|, max=1.769) - 1.769) ^ 2 + 0.5
+    FFN::FastGeluV2ClipParams(tempTensorA, srcLocal, params);
+    // x2 = (x + 0.0000001) / |(x + 0.0000001)|
+    FFN::FastGeluV2SgnParams<T, HIGH_PERFORMANCE>(tempTensorB, tempTensorC, srcLocal, params);
+    // fast_gelu_v2(x) = x * (x2 * x1 + 0.5)
+    FFN::FastGeluV2Params(tempTensorA, tempTensorB, dstLocal, srcLocal, params);
+}
+
+template <typename T, bool HIGH_PRECISION = false, bool HIGH_PERFORMANCE = false>
+__aicore__ inline void FasterGeluV2(const AscendC::LocalTensor<T>& dstLocal,
+    const AscendC::LocalTensor<T>& srcLocal, const AscendC::LocalTensor<uint8_t>& sharedTmpBuffer,
+    const uint32_t dataSize)
+{
+    if constexpr (HIGH_PRECISION && (AscendC::IsSameType<T, half>::value)) {
+        AscendC::GeluClass<CONST_3>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
+            FasterGeluV2Func<float, HIGH_PERFORMANCE>);
+    } else {
+        AscendC::GeluClass<T, CONST_3>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
+            FasterGeluV2Func<T, HIGH_PERFORMANCE>);
+    }
+}
+} // namespace FFN
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/ffn/op_kernel/ffn_matmul.h b/src/kernels/mixkernels/ffn/op_kernel/ffn_matmul.h
new file mode 100644
index 00000000..b062f495
--- /dev/null
+++ b/src/kernels/mixkernels/ffn/op_kernel/ffn_matmul.h
@@ -0,0 +1,498 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef ASCEND_OPS_FFN_MATMUL_H
+#define ASCEND_OPS_FFN_MATMUL_H
+
+#ifdef __CCE_KT_TEST__
+#define __aicore__
+#else
+#define __aicore__ [aicore]
+#endif
+
+#include "kernel_operator.h"
+#include "ffn_common.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+namespace FFN {
+constexpr uint32_t L0AB_PINGPONG_BUFFER_LEN_INT8 = 32768;
+constexpr uint32_t CUBE_MATRIX_SIZE_256 = 256;
+constexpr uint32_t CUBE_MATRIX_SIZE_512 = 16 * 32;
+constexpr uint32_t L1_PINGPONG_BUFFER_LEN_INT8 = 131072;
+constexpr uint32_t L1_DESCALE_BUFFER_LEN = 40960;
+constexpr uint32_t L1_BIAS_BUFFER_LEN = 20480;
+constexpr uint32_t UB_HALF_BUFFER_LEN = 131072;
+constexpr uint32_t MATRIX_C_PINGPONG_LEN = 256 * 128;
+
+struct matmulInfo {
+    uint32_t mIdx{0};
+    uint32_t nIdx{0};
+    uint32_t m{0};
+    uint32_t k{0};
+    uint32_t n{0};
+    uint32_t baseM{0};
+    uint32_t baseK{0};
+    uint32_t baseN{0};
+    uint32_t mLoops{0};
+    uint32_t kLoops{0};
+    uint32_t nLoops{0};
+    uint32_t mActual{0};
+    uint32_t nActual{0};
+    uint32_t mRound{0};
+    uint32_t nRound{0};
+    __aicore__ inline void updateIdx(uint32_t newMIdx, uint32_t newNIdx) {
+        mIdx = newMIdx;
+        nIdx = newNIdx;
+        mActual = (mIdx == (mLoops - 1)) ? (m - mIdx * baseM) : baseM;
+        nActual = (nIdx == (nLoops - 1)) ? (n - nIdx * baseN) : baseN;
+        mRound = RoundUp(mActual, BLOCK_SIZE_16);
+        nRound = RoundUp(nActual, BLOCK_SIZE_16);
+    }
+};
+
+template <typename DTYPE_A, CubeFormat FORMAT_A, typename DTYPE_B, CubeFormat FORMAT_B,
+    typename DTYPE_C, CubeFormat FORMAT_C, typename DTYPE_ACCUMULATOR, typename DTYPE_BIAS,
+    typename DTYPE_SCALE, bool TRANS_A, bool TRANS_B, bool WITH_BIAS, bool EN_L0C_DB
+>
+struct DefaultMatmul<ArchType::ASCEND_V200,
+    DTYPE_A, FORMAT_A, DTYPE_B, FORMAT_B, DTYPE_C, FORMAT_C,
+    DTYPE_ACCUMULATOR, DTYPE_BIAS, DTYPE_SCALE,
+    TRANS_A, TRANS_B, WITH_BIAS, EN_L0C_DB> {
+    __aicore__ explicit DefaultMatmul() {};
+
+    __aicore__ inline void IterateAll(
+        AscendC::GlobalTensor<DTYPE_A> refA, AscendC::GlobalTensor<DTYPE_B> refB, AscendC::LocalTensor<DTYPE_C> refC,
+        AscendC::GlobalTensor<DTYPE_SCALE> refScale, const matmulInfo &mmInfo, const uint32_t &l0cPingPong)
+    {
+        AsdopsBuffer<ArchType::ASCEND_V200> buf;
+        AscendC::LocalTensor<DTYPE_A> l1aPingTensor =
+            buf.GetBuffer<BufferType::ASCEND_CB, DTYPE_A>(0);
+        AscendC::LocalTensor<DTYPE_A> l1aPongTensor =
+            buf.GetBuffer<BufferType::ASCEND_CB, DTYPE_A>(L1_PINGPONG_BUFFER_LEN_INT8);
+        AscendC::LocalTensor<DTYPE_B> l1bPingTensor =
+            buf.GetBuffer<BufferType::ASCEND_CB, DTYPE_B>(L1_PINGPONG_BUFFER_LEN_INT8 * CONST_2);
+        AscendC::LocalTensor<DTYPE_B> l1bPongTensor =
+            buf.GetBuffer<BufferType::ASCEND_CB, DTYPE_B>(L1_PINGPONG_BUFFER_LEN_INT8 * CONST_3);
+        AscendC::LocalTensor<DTYPE_A> l0aPingTensor =
+            buf.GetBuffer<BufferType::ASCEND_L0A, DTYPE_A>(0);
+        AscendC::LocalTensor<DTYPE_A> l0aPongTensor =
+            buf.GetBuffer<BufferType::ASCEND_L0A, DTYPE_A>(L0AB_PINGPONG_BUFFER_LEN_INT8);
+        AscendC::LocalTensor<DTYPE_B> l0bPingTensor =
+            buf.GetBuffer<BufferType::ASCEND_L0B, DTYPE_B>(0);
+        AscendC::LocalTensor<DTYPE_B> l0bPongTensor =
+            buf.GetBuffer<BufferType::ASCEND_L0B, DTYPE_B>(L0AB_PINGPONG_BUFFER_LEN_INT8);
+        AscendC::LocalTensor<DTYPE_SCALE> ubScaleDequant =
+            buf.GetBuffer<BufferType::ASCEND_UB, DTYPE_SCALE>(0);
+        AscendC::LocalTensor<DTYPE_BIAS> l0cTensor =
+            buf.GetBuffer<BufferType::ASCEND_UB, DTYPE_BIAS>(0);
+        event_t l0cEvent;
+        if constexpr (EN_L0C_DB) {
+            l0cTensor = buf.GetBuffer<BufferType::ASCEND_L0C, DTYPE_BIAS>(
+                l0cPingPong * MATRIX_C_PINGPONG_LEN * sizeof(float));
+            l0cEvent = l0cPingPong ? EVENT_ID0 : EVENT_ID1;
+        } else {
+            l0cTensor = buf.GetBuffer<BufferType::ASCEND_L0C, DTYPE_BIAS>(0);
+            l0cEvent = EVENT_ID0;
+        }
+
+        uint32_t kActual = (mmInfo.kLoops == 1) ? mmInfo.k : mmInfo.baseK;
+        uint32_t kRound = RoundUp(kActual, BLOCK_SIZE_32);
+
+        uint32_t mOrgUp = RoundUp(mmInfo.m, BLOCK_SIZE_16);
+        uint32_t nOrgUp = RoundUp(mmInfo.n, BLOCK_SIZE_16);
+        uint32_t kOrgUp = RoundUp(mmInfo.k, BLOCK_SIZE_32);
+
+        AscendC::LocalTensor<DTYPE_A> l1aTensor = l1PingPong ? l1aPingTensor : l1aPongTensor;
+        AscendC::LocalTensor<DTYPE_B> l1bTensor = l1PingPong ? l1bPingTensor : l1bPongTensor;
+        event_t l1aEvent = l1PingPong ? EVENT_ID0 : EVENT_ID1;
+        event_t l1bEvent = l1PingPong ? EVENT_ID2 : EVENT_ID3;
+        WAIT_FLAG(MTE1, MTE2, l1aEvent);
+        if constexpr (FORMAT_A == CubeFormat::NZ) {
+            gm_to_l1<ArchType::ASCEND_V200, DTYPE_A, DataFormat::NZ, DataFormat::NZ>(
+                l1aTensor, refA[mmInfo.mIdx * mmInfo.baseM * BLOCK_SIZE_32],
+                mmInfo.mRound, mmInfo.mRound, mOrgUp, kRound, kRound, 0);
+        } else {
+            CopyND2NZOnTheFly(l1aTensor, refA, mmInfo.mIdx * mmInfo.baseM, 0, mmInfo.mActual, kActual, mmInfo.k);
+        }
+        SET_FLAG(MTE2, MTE1, l1aEvent);
+
+        WAIT_FLAG(MTE1, MTE2, l1bEvent);
+        if constexpr (FORMAT_B == CubeFormat::NZ) {
+            gm_to_l1<ArchType::ASCEND_V200, DTYPE_B, DataFormat::NZ, DataFormat::NZ>(
+                l1bTensor, refB[mmInfo.nIdx * mmInfo.baseN * BLOCK_SIZE_32],
+                mmInfo.nRound, mmInfo.nRound, nOrgUp, kRound, kRound, 0);
+        } else {
+            CopyND2NZOnTheFly(l1bTensor, refB, mmInfo.nIdx * mmInfo.baseN, 0, mmInfo.nActual, kActual, mmInfo.k);
+        }
+        SET_FLAG(MTE2, MTE1, l1bEvent);
+
+        uint32_t srcOffset = 0;
+        uint32_t mnMax = mmInfo.mRound > mmInfo.nRound ? mmInfo.mRound : mmInfo.nRound;
+        uint32_t kPartLen = L0AB_PINGPONG_BUFFER_LEN_INT8 / mnMax / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+
+        uint32_t lastKLoop = mmInfo.kLoops - 1;
+        uint32_t offsetANext = 0;
+        uint32_t offsetBNext = 0;
+        uint32_t kActualNext = 0;
+        uint32_t kRoundNext = 0;
+
+        for (uint32_t kIdx = 0; kIdx < mmInfo.kLoops; ++kIdx) {
+            kActual = (kIdx == mmInfo.kLoops - 1) ? mmInfo.k - kIdx * mmInfo.baseK : mmInfo.baseK;
+            kRound = RoundUp(kActual, BLOCK_SIZE_32);
+
+            AscendC::LocalTensor<DTYPE_A> l1aTensor = l1PingPong ? l1aPingTensor : l1aPongTensor;
+            AscendC::LocalTensor<DTYPE_B> l1bTensor = l1PingPong ? l1bPingTensor : l1bPongTensor;
+
+            event_t l1aEvent = l1PingPong ? EVENT_ID0 : EVENT_ID1;
+            event_t l1bEvent = l1PingPong ? EVENT_ID2 : EVENT_ID3;
+
+            if (kIdx < lastKLoop) {
+                AscendC::LocalTensor<DTYPE_A> l1BufANextTensor = (1 - l1PingPong) ? l1aPingTensor : l1aPongTensor;
+                AscendC::LocalTensor<DTYPE_B> l1BufBNextTensor = (1 - l1PingPong) ? l1bPingTensor : l1bPongTensor;
+                event_t l1aEventNext = (1 - l1PingPong) ? EVENT_ID0 : EVENT_ID1;
+                event_t l1bEventNext = (1 - l1PingPong) ? EVENT_ID2 : EVENT_ID3;
+
+                uint32_t kActualNext =
+                    ((kIdx + 1) == lastKLoop) ? (mmInfo.k - (kIdx + 1) * mmInfo.baseK) : mmInfo.baseK;
+                uint32_t kRoundNext = RoundUp(kActualNext, BLOCK_SIZE_32);
+
+                WAIT_FLAG(MTE1, MTE2, l1aEventNext);
+                if constexpr (FORMAT_A == CubeFormat::NZ) {
+                    offsetANext = (kIdx + 1) * mmInfo.baseK * mOrgUp + mmInfo.mIdx * mmInfo.baseM * BLOCK_SIZE_32;
+                    gm_to_l1<ArchType::ASCEND_V200, DTYPE_A, DataFormat::NZ, DataFormat::NZ>(
+                        l1BufANextTensor, refA[offsetANext],
+                        mmInfo.mRound, mmInfo.mRound, mOrgUp, kRoundNext, kRoundNext, 0);
+                } else {
+                    CopyND2NZOnTheFly(l1BufANextTensor, refA, mmInfo.mIdx * mmInfo.baseM, (kIdx + 1) * mmInfo.baseK,
+                        mmInfo.mActual, kActual, mmInfo.k);
+                }
+                SET_FLAG(MTE2, MTE1, l1aEventNext);
+
+                WAIT_FLAG(MTE1, MTE2, l1bEventNext);
+                if constexpr (FORMAT_B == CubeFormat::NZ) {
+                    offsetBNext = (kIdx + 1) * mmInfo.baseK * nOrgUp + mmInfo.nIdx * mmInfo.baseN * BLOCK_SIZE_32;
+                    gm_to_l1<ArchType::ASCEND_V200, DTYPE_B, DataFormat::NZ, DataFormat::NZ>(
+                        l1BufBNextTensor, refB[offsetBNext],
+                        mmInfo.nRound, mmInfo.nRound, nOrgUp, kRoundNext, kRoundNext, 0);
+                } else {
+                    CopyND2NZOnTheFly(l1BufBNextTensor, refB, mmInfo.nIdx * mmInfo.baseN, (kIdx + 1) * mmInfo.baseK,
+                        mmInfo.nActual, kActual, mmInfo.k);
+                }
+                SET_FLAG(MTE2, MTE1, l1bEventNext);
+            }
+
+            uint32_t kPartLoops = CeilDiv(kActual, kPartLen);
+            AscendC::PipeBarrier<PIPE_MTE2>();
+            for (uint32_t kPartIdx = 0; kPartIdx < kPartLoops; ++kPartIdx) {
+                uint32_t k0Round = kPartIdx < kPartLoops - 1 ? kPartLen : kRound - kPartIdx * kPartLen;
+                uint32_t k0Actual = kPartIdx < kPartLoops - 1 ? kPartLen : kActual - kPartIdx * kPartLen;
+                uint32_t l0PingPong = 1 - kPartIdx % 2;
+                event_t l0Event = l0PingPong ? EVENT_ID0 : EVENT_ID1;
+                AscendC::LocalTensor<DTYPE_A> l0aTensor = l0PingPong ? l0aPingTensor : l0aPongTensor;
+                AscendC::LocalTensor<DTYPE_B> l0bTensor = l0PingPong ? l0bPingTensor : l0bPongTensor;
+                if (kPartIdx == 0) {
+                    WAIT_FLAG(MTE2, MTE1, l1aEvent);
+                }
+                WAIT_FLAG(M, MTE1, l0Event);
+                l1_to_l0_a<ArchType::ASCEND_V200, DTYPE_A, false, DataFormat::ZN, DataFormat::ZZ>(
+                    l0aTensor,
+                    l1aTensor[kPartIdx * kPartLen * mmInfo.mRound],
+                    mmInfo.mRound,
+                    k0Round,
+                    1,
+                    mmInfo.mRound / 16,
+                    k0Round / 32,
+                    1
+                );
+                if (kPartIdx == kPartLoops - 1) {
+                    SET_FLAG(MTE1, MTE2, l1aEvent);
+                }
+                if (kPartIdx == 0) {
+                    WAIT_FLAG(MTE2, MTE1, l1bEvent);
+                }
+
+                srcOffset = kPartIdx * kPartLen * mmInfo.nRound;
+                l1_to_l0_b<ArchType::ASCEND_V200, DTYPE_B, false, DataFormat::VECTOR, DataFormat::VECTOR>(
+                    l0bTensor,                                      // dst
+                    l1bTensor[srcOffset],                           // src
+                    0,
+                    mmInfo.nRound * k0Round / CUBE_MATRIX_SIZE_512, // repeat
+                    0,
+                    1,                                              // srcStride
+                    0,
+                    0                                               // dstStride
+                );
+                if (kPartIdx == kPartLoops - 1) {
+                    SET_FLAG(MTE1, MTE2, l1bEvent);
+                }
+                uint32_t mMmadActual = (mmInfo.mActual == 1) ? CONST_2 : mmInfo.mActual;
+                SET_FLAG(MTE1, M, l0Event);
+                WAIT_FLAG(MTE1, M, l0Event);
+                if (kIdx == 0 && kPartIdx == 0) {
+                    WAIT_FLAG(V, M, l0cEvent);
+                }
+                AscendC::PipeBarrier<PIPE_M>();
+                mmad<ArchType::ASCEND_V200, DTYPE_A, DTYPE_B, DTYPE_BIAS, false>(
+                    l0cTensor,
+                    l0aTensor,
+                    l0bTensor,
+                    mMmadActual,        // mmInfo.m
+                    mmInfo.nActual,     // mmInfo.n
+                    k0Actual,           // mmInfo.k
+                    0                   // cmatrixInitVal
+                );
+                SET_FLAG(M, MTE1, l0Event);
+            }
+            l1PingPong = 1 - l1PingPong;
+        }
+        WAIT_FLAG(V, MTE2, EVENT_ID2);
+        gm_to_ub<ArchType::ASCEND_V200, DTYPE_SCALE>(
+            ubScaleDequant,
+            refScale[mmInfo.nIdx * mmInfo.baseN],
+            0,                                              // sid
+            1,                                              // nBurst
+            mmInfo.nRound / 4,                              // lenBurst
+            0,                                              // srcStride
+            0                                               // dstStride
+        );
+        SET_FLAG(MTE2, V, EVENT_ID2);
+        WAIT_FLAG(MTE2, V, EVENT_ID2);
+        SET_FLAG(M, V, l0cEvent);
+        WAIT_FLAG(M, V, l0cEvent);
+        WAIT_FLAG(MTE3, V, l0cEvent);
+        l0c_to_ub<ArchType::ASCEND_V200, DTYPE_BIAS, DTYPE_C>(
+            refC,
+            l0cTensor,
+            (uint16_t)(mmInfo.nRound / BLOCK_SIZE_16),      // nBurst
+            (uint16_t)(mmInfo.mRound / BLOCK_SIZE_16),      // lenBurst
+            (uint16_t)0,                                    // srcStride
+            (uint16_t)0                                     // dstStride
+        );
+        SET_FLAG(V, MTE2, l0cEvent);
+        SET_FLAG(V, MTE2, EVENT_ID2);
+        if (mmInfo.mActual == 1) {
+            DTYPE_C zero = 0;
+            SetVectorMask<int8_t>((uint64_t)0x0, (uint64_t)0xffff);
+            for (uint32_t i = 0; i < mmInfo.nRound / BLOCK_SIZE_16; i++) {
+                uint64_t curr_offset_c = i * mmInfo.mRound * BLOCK_SIZE_16 + mmInfo.mActual * BLOCK_SIZE_16;
+                muls_v<ArchType::ASCEND_V200, DTYPE_C>(refC[curr_offset_c], refC[curr_offset_c], zero, 1, 1, 1,
+                        2, 2);
+            }
+        }
+    }
+
+    __aicore__ inline void LoadBias(AscendC::GlobalTensor<DTYPE_BIAS> refBias,
+        const matmulInfo &mmInfo, const uint32_t &l0cPingPong)
+    {
+        AsdopsBuffer<ArchType::ASCEND_V200> buf;
+        AscendC::LocalTensor<DTYPE_BIAS> ubBiasDequant;
+        event_t l0cEvent;
+        if constexpr (EN_L0C_DB) {
+            ubBiasDequant = buf.GetBuffer<BufferType::ASCEND_UB, DTYPE_BIAS>(mmInfo.baseN * sizeof(DTYPE_SCALE) +
+                l0cPingPong * mmInfo.baseN * (sizeof(DTYPE_SCALE) +sizeof(DTYPE_BIAS)));
+            l0cEvent = l0cPingPong ? EVENT_ID0 : EVENT_ID1;
+        } else {
+            ubBiasDequant = buf.GetBuffer<BufferType::ASCEND_UB, DTYPE_BIAS>(mmInfo.baseN * sizeof(DTYPE_SCALE));
+            l0cEvent = EVENT_ID0;
+        }
+
+        WAIT_FLAG(V, MTE2, l0cEvent);
+        gm_to_ub<ArchType::ASCEND_V200, DTYPE_BIAS>(ubBiasDequant,
+            refBias[mmInfo.nIdx * mmInfo.baseN],
+            0,                                          // sid
+            1,                                          // nBurst
+            mmInfo.nRound / CONST_8,                    // lenBurst
+            0,                                          // srcStride
+            0                                           // dstStride
+        );
+        SET_FLAG(MTE2, V, l0cEvent);
+    }
+
+    __aicore__ inline void BrcBias(const matmulInfo &mmInfo, const uint32_t &l0cPingPong)
+    {
+        AsdopsBuffer<ArchType::ASCEND_V200> buf;
+        AscendC::LocalTensor<DTYPE_BIAS> ubBiasDequant;
+        AscendC::LocalTensor<DTYPE_BIAS> l0cTensor;
+        event_t l0cEvent;
+        if constexpr (EN_L0C_DB) {
+            ubBiasDequant = buf.GetBuffer<BufferType::ASCEND_UB, DTYPE_BIAS>(
+                mmInfo.baseN * sizeof(DTYPE_SCALE) + l0cPingPong * mmInfo.baseN *
+                (sizeof(DTYPE_SCALE) + sizeof(DTYPE_BIAS)));
+            l0cTensor = buf.GetBuffer<BufferType::ASCEND_L0C, DTYPE_BIAS>(
+                l0cPingPong * MATRIX_C_PINGPONG_LEN * sizeof(float));
+            l0cEvent = l0cPingPong ? EVENT_ID0 : EVENT_ID1;
+        } else {
+            ubBiasDequant = buf.GetBuffer<BufferType::ASCEND_UB, DTYPE_BIAS>(
+                mmInfo.baseN * sizeof(DTYPE_SCALE));
+            l0cTensor = buf.GetBuffer<BufferType::ASCEND_L0C, DTYPE_BIAS>(0);
+            l0cEvent = EVENT_ID0;
+        }
+        
+        WAIT_FLAG(MTE2, V, l0cEvent);
+        for (uint32_t i = 0; i < mmInfo.mRound / BLOCK_SIZE_16; i++) {
+            AscendC::BroadCastVecToMM(l0cTensor[i * CONST_256], ubBiasDequant,
+                mmInfo.nRound / BLOCK_SIZE_16, 1, 0, mmInfo.mRound / BLOCK_SIZE_16 - 1);
+        }
+        SET_FLAG(V, M, l0cEvent);
+    }
+
+    template <typename DTYPE>
+    __aicore__ inline void CopyND2NZOnTheFly(const AscendC::LocalTensor<DTYPE> &dst,
+        const AscendC::GlobalTensor<DTYPE> &src, const uint32_t row,
+        const uint32_t col, const uint32_t height, const uint32_t width, const uint32_t gCol)
+    {
+        const uint32_t c0Size = BLOCK_SIZE_32 / sizeof(DTYPE);
+        uint32_t calcWidth = width / c0Size;
+        int64_t dstOffset = 0;
+        int64_t srcOffset = ((int64_t)row * (int64_t)gCol + (int64_t)col);
+        uint32_t calcWidthExr = CeilDiv(width, c0Size);
+        uint32_t calcHeightExr = CeilDiv(height, BLOCK_NUM_PER_FRACTAL);
+        if (height % BLOCK_NUM_PER_FRACTAL != 0) {
+            uint32_t repeat = calcWidthExr * calcHeightExr;
+            AscendC::LocalTensor<int16_t> tmp = dst.template ReinterpretCast<int16_t>();
+            AscendC::InitConstValueParams<int16_t> initConstValueParams;
+            initConstValueParams.repeatTimes = (uint16_t)repeat;
+            initConstValueParams.initValue = 0;
+            InitConstValue(tmp, initConstValueParams);
+            AscendC::PipeBarrier<PIPE_MTE2>();
+        }
+        uint32_t srcGap = gCol * sizeof(DTYPE) / BLOCK_SIZE_32 - 1;
+        if (gCol % c0Size != 0 || srcGap >= UINT16_MAX) {
+            int64_t oriSrcOffset = srcOffset;
+            int64_t oriDstOffset = dstOffset;
+            for (uint32_t i = 0; i < calcWidth; i++) {
+                for (uint32_t j = 0; j < height; j++) {
+                    AscendC::DataCopy(dst[dstOffset], src[srcOffset], { 1, 1, 0, 0 });
+                    dstOffset += c0Size;
+                    srcOffset += gCol;
+                }
+                srcOffset = oriSrcOffset + (i + 1) * c0Size;
+                dstOffset = oriDstOffset + (i + 1) * calcHeightExr * BLOCK_NUM_PER_FRACTAL * c0Size;
+            }
+        } else {
+            for (uint32_t i = 0; i < calcWidth; i++) {
+                AscendC::DataCopy(dst[dstOffset], src[srcOffset],
+                    { static_cast<uint16_t>(height), 1, static_cast<uint16_t>(srcGap), 0 });
+                    dstOffset += calcHeightExr * BLOCK_NUM_PER_FRACTAL * c0Size;
+                    srcOffset += c0Size;
+            }
+        }
+    }
+    
+    template <typename DTYPE>
+    __aicore__ inline void TransNZ2ND(
+        const AscendC::LocalTensor<DTYPE>& dst, const AscendC::LocalTensor<DTYPE>& src,
+        const uint32_t blockHigh, const uint32_t blockWidth, const DTYPE scalar)
+    {
+        uint32_t blockCount = BLOCK_SIZE_32 / sizeof(DTYPE);
+        ASCENDC_ASSERT((blockWidth <= MAX_REPEAT_LIMIT), {
+            KERNEL_LOG(KERNEL_ERROR, "blockWidth is %d, blockCount is %d, repeat time exceed max time %d", blockWidth,
+                blockCount, MAX_REPEAT_LIMIT);
+        });
+        struct AscendC::UnaryRepeatParams intriParams;
+
+        int64_t dstOffset = 0;
+        int64_t srcOffset = 0;
+        uint32_t highBlock = MAX_REPEAT_LIMIT;
+        uint32_t highBlocks = 0;
+        uint32_t highTail = 0;
+        uint32_t srcStride = highBlock * blockCount;
+        uint32_t dstStride = blockWidth * blockCount * highBlock;
+        bool isBeyondMaxStride = false;
+        uint64_t mask[2] = {uint64_t(-1), uint64_t(-1)};
+
+        intriParams.dstBlkStride = blockWidth;
+        intriParams.srcBlkStride = 1;
+        uint32_t dstRepStride = (blockWidth * blockCount * sizeof(DTYPE) / BLOCK_SIZE_32) * BLOCK_NUM_PER_VEC;
+        intriParams.dstRepStride = dstRepStride;
+        if (dstRepStride > 255) {
+            isBeyondMaxStride = true;
+        }
+        intriParams.srcRepStride = (blockCount * sizeof(DTYPE) / BLOCK_SIZE_32) * BLOCK_NUM_PER_VEC;
+        highBlocks = (blockHigh * blockCount) / BLOCK_NUM_PER_VEC / highBlock;
+        highTail = (blockHigh * blockCount) / BLOCK_NUM_PER_VEC % highBlock;
+        srcStride *= BLOCK_NUM_PER_VEC;
+        dstStride *= BLOCK_NUM_PER_VEC;
+        
+        AscendC::SetVectorMask<DTYPE>(mask[1], mask[0]);
+        for (uint32_t i = 0; i < blockWidth; i++) {
+            int64_t dstMulsOffset = dstOffset;
+            for (uint32_t j = 0; j < highBlocks; j++) {
+                AscendC::Muls<DTYPE, false>(dst[dstMulsOffset], src[srcOffset], scalar, mask, highBlock, intriParams);
+                srcOffset += srcStride;
+                dstMulsOffset += dstStride;
+            }
+            if (highTail) {
+                if (isBeyondMaxStride) {
+                    uint32_t srcOffsetStride = blockCount * BLOCK_NUM_PER_VEC;
+                    uint32_t dstOffsetStride = blockWidth * BLOCK_NUM_PER_FRACTAL * BLOCK_NUM_PER_VEC;
+                    for (uint32_t j = 0; j < highTail; j++) {
+                        AscendC::Muls<DTYPE, false>(dst[dstMulsOffset + j * dstOffsetStride],
+                            src[srcOffset + j * srcOffsetStride], scalar, mask, 1, intriParams);
+                    }
+                } else {
+                    AscendC::Muls<DTYPE, false>(
+                        dst[dstMulsOffset], src[srcOffset], scalar, mask, highTail, intriParams);
+                }
+                srcOffset += highTail * blockCount * BLOCK_NUM_PER_VEC;
+            }
+            dstOffset += blockCount;
+        }
+        return;
+    }
+
+    template <typename DTYPE>
+    __aicore__ inline void CopyCo22GMNZ2ND(const AscendC::GlobalTensor<DTYPE>& gmC,
+        const AscendC::LocalTensor<DTYPE>& src, AscendC::LocalTensor<DTYPE> &trans,
+        const uint32_t row, const uint32_t col, const uint32_t height, const uint32_t width,
+        const uint32_t gCol)
+    {
+        const uint32_t blockCount = BLOCK_SIZE_32 / sizeof(DTYPE);
+        uint32_t mRound = RoundUp(height, BLOCK_SIZE_16);
+        uint32_t nRound = RoundUp(width, blockCount);
+        bool isTragetAligned = (width % blockCount) == 0;
+        bool isGmAligned = (gCol % blockCount) == 0;
+        int dstStride = (gCol - nRound) * sizeof(DTYPE) / BLOCK_SIZE_32;
+        int dstOffset = row * gCol + col;
+        ASCENDC_ASSERT((gCol >= nRound),
+            {KERNEL_LOG(KERNEL_ERROR, "n is %d, nRound is %d, n should be no less than nRound", gCol, nRound);});
+        const bool isComputeLineByLine = (!isGmAligned || dstStride >= UINT16_MAX);
+
+        SET_FLAG(MTE3, V, EVENT_ID2);
+        WAIT_FLAG(MTE3, V, EVENT_ID2);
+        TransNZ2ND(trans, src, mRound / BLOCK_NUM_PER_FRACTAL, nRound / blockCount, (DTYPE)1.0);
+        SET_FLAG(V, MTE3, EVENT_ID2);
+        WAIT_FLAG(V, MTE3, EVENT_ID2);
+
+        int32_t blocklen = nRound / blockCount;
+        if (isComputeLineByLine) {
+            bool needPipe = gCol < BLOCK_NUM_PER_FRACTAL;
+            for (int i = 0; i < height; ++i) {
+                AscendC::DataCopy(gmC[dstOffset], trans[i * blocklen * BLOCK_SIZE_32 / sizeof(DTYPE)],
+                    { 1, static_cast<uint16_t>(blocklen), 0, 0 });
+                dstOffset += gCol;
+                AscendC::PipeBarrier<PIPE_MTE3>();
+            }
+        } else {
+            AscendC::DataCopy(gmC[dstOffset], trans,
+                { static_cast<uint16_t>(height), static_cast<uint16_t>(blocklen), 0,
+                static_cast<uint16_t>(dstStride) });
+        }
+    }
+private:
+    uint32_t l1PingPong{0};
+};
+} // namespace FFN
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/ffn/op_kernel/ffn_quant.h b/src/kernels/mixkernels/ffn/op_kernel/ffn_quant.h
new file mode 100644
index 00000000..4cfc0b50
--- /dev/null
+++ b/src/kernels/mixkernels/ffn/op_kernel/ffn_quant.h
@@ -0,0 +1,396 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef ASCEND_OPS_FFN_QUANT_H
+#define ASCEND_OPS_FFN_QUANT_H
+
+#ifdef __CCE_KT_TEST__
+#define __aicore__
+#else
+#define __aicore__ [aicore]
+#endif
+
+#include "kernel_operator.h"
+#include "ffn_common.h"
+#include "ffn_gelu.h"
+#include "ffn_matmul.h"
+#include "mixkernels/ffn/tiling/tiling_data.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+namespace FFN {
+template <
+    typename DESCALE_DTYPE,
+    typename BIAS_DTYPE,
+    typename ACCUMULATOR_DTYPE,
+    typename OUT_DTYPE,
+    CubeFormat IN_FORMAT,
+    CubeFormat OUT_FORMAT,
+    bool HIGH_PRECISION,
+    bool HIGH_PERFORMANCE
+>
+struct DefaultFFN<ArchType::ASCEND_V200, int8_t, DESCALE_DTYPE, BIAS_DTYPE, ACCUMULATOR_DTYPE, OUT_DTYPE,
+    IN_FORMAT, OUT_FORMAT, false, true, true, HIGH_PRECISION, HIGH_PERFORMANCE> {
+    __aicore__ inline explicit DefaultFFN()
+    {
+        AscendC::SetLoadDataPaddingValue<uint64_t>((uint64_t)0x0);
+        AscendC::SetAtomicNone();
+        AscendC::SetMaskNorm();
+    };
+
+    __aicore__ inline void Init(__gm__ uint8_t *x, __gm__ uint8_t *weight1, __gm__ uint8_t *weight2,
+        __gm__ uint8_t *scaleQuant, __gm__ uint8_t *scaleDequant1, __gm__ uint8_t *scaleDequant2,
+        __gm__ uint8_t *biasQuant, __gm__ uint8_t *biasDequant1, __gm__ uint8_t *biasDequant2,
+        __gm__ uint8_t *y, __gm__ uint8_t *workspace, const AtbOps::FFNTilingData *tiling)
+    {
+        gmX.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(x));
+        gmW1.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(weight1));
+        gmW2.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(weight2));
+        gmScaleQuant.SetGlobalBuffer(reinterpret_cast<__gm__ ACCUMULATOR_DTYPE *>(scaleQuant));
+        gmScaleDq1.SetGlobalBuffer(reinterpret_cast<__gm__ DESCALE_DTYPE *>(scaleDequant1));
+        gmScaleDq2.SetGlobalBuffer(reinterpret_cast<__gm__ DESCALE_DTYPE *>(scaleDequant2));
+        gmBiasQuant.SetGlobalBuffer(reinterpret_cast<__gm__ ACCUMULATOR_DTYPE *>(biasQuant));
+        gmBiasDq1.SetGlobalBuffer(reinterpret_cast<__gm__ BIAS_DTYPE *>(biasDequant1));
+        gmBiasDq2.SetGlobalBuffer(reinterpret_cast<__gm__ BIAS_DTYPE *>(biasDequant2));
+        gmYAccu.SetGlobalBuffer(reinterpret_cast<__gm__ ACCUMULATOR_DTYPE *>(y));
+        gmY.SetGlobalBuffer(reinterpret_cast<__gm__ OUT_DTYPE *>(y));
+        gmSync.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(workspace));
+        gmYTmp.SetGlobalBuffer(reinterpret_cast<__gm__ IN_DTYPE *>(workspace + FRACTAL_SIZE));
+        const AtbOps::FFNTilingData *tilingData = tiling;
+        m1 = tilingData->mm1.m;
+        k1 = tilingData->mm1.k;
+        n1 = tilingData->mm1.n;
+        baseM1 = tilingData->mm1.baseM;
+        baseK1 = tilingData->mm1.baseK;
+        baseN1 = tilingData->mm1.baseN;
+        m1Loops = tilingData->mm1.mLoops;
+        k1Loops = tilingData->mm1.kLoops;
+        n1Loops = tilingData->mm1.nLoops;
+        coreLoops1 = tilingData->mm1.coreLoops;
+        swizzlCnt1 = tilingData->mm1.swizzlCount;
+        swizzlDirect1 = tilingData->mm1.swizzlDirect;
+
+        m2 = tilingData->mm2.m;
+        k2 = tilingData->mm2.k;
+        n2 = tilingData->mm2.n;
+        baseM2 = tilingData->mm2.baseM;
+        baseK2 = tilingData->mm2.baseK;
+        baseN2 = tilingData->mm2.baseN;
+        m2Loops = tilingData->mm2.mLoops;
+        k2Loops = tilingData->mm2.kLoops;
+        n2Loops = tilingData->mm2.nLoops;
+        coreLoops2 = tilingData->mm2.coreLoops;
+        swizzlCnt2 = tilingData->mm2.swizzlCount;
+        swizzlDirect2 = tilingData->mm2.swizzlDirect;
+
+        activationType = tilingData->activationType;
+        mOrgUp1 = RoundUp(m1, BLOCK_SIZE_16);
+        nOrgUp1 = RoundUp(n1, BLOCK_SIZE_16);
+        mOrgUp2 = RoundUp(m2, BLOCK_SIZE_16);
+        nOrgUp2 = RoundUp(n2, BLOCK_SIZE_16);
+        
+        ubScaleQuant = buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(tilingData->scaleOffset);
+        ubBiasQuant = buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(tilingData->biasOffset);
+        ubSync = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(tilingData->syncOffset);
+        ubC1 = buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(tilingData->cOffset);
+        ubC2 = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(tilingData->cOffset);
+        ubCQuant = buf.GetBuffer<BufferType::ASCEND_UB, IN_DTYPE>(tilingData->cOffset);
+        ubTrans1 = buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(tilingData->geluOffset);
+        ubTrans2 = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(tilingData->geluOffset);
+        ubGelu = buf.GetBuffer<BufferType::ASCEND_UB, uint8_t>(tilingData->geluOffset);
+        ubGelu.SetSize(UB_HALF_BUFFER_LEN - tilingData->cOffset);
+    }
+
+    __aicore__ void Process()
+    {
+        uint32_t eachCoreNum = BLOCK_SIZE_32 / sizeof(int32_t);
+        Duplicate(ubSync, 0, eachCoreNum);
+        SET_FLAG(V, MTE3, EVENT_ID0);
+        WAIT_FLAG(V, MTE3, EVENT_ID0);
+        DataCopy(gmSync[eachCoreNum * block_idx], ubSync, eachCoreNum);
+        AscendC::PipeBarrier<PIPE_ALL>();
+        Stage1MmVec();
+        SyncAll(gmSync, ubSync);
+        Stage2Mm();
+    }
+
+private:
+    using IN_DTYPE = int8_t;
+    using MM1 = DefaultMatmul<ArchType::ASCEND_V200, IN_DTYPE, IN_FORMAT, IN_DTYPE, IN_FORMAT,
+        half, CubeFormat::NZ, BIAS_DTYPE, BIAS_DTYPE, DESCALE_DTYPE, false, true, true, true>;
+    using MM2 = DefaultMatmul<ArchType::ASCEND_V200, IN_DTYPE, CubeFormat::NZ, IN_DTYPE, IN_FORMAT,
+        OUT_DTYPE, CubeFormat::NZ, BIAS_DTYPE, BIAS_DTYPE, DESCALE_DTYPE, false, true, true, false>;
+
+    __aicore__ inline void GetIdx(uint32_t loopIdx, matmulInfo &mmInfo,
+        const uint32_t &swizzlDirect, const uint32_t &swizzlCnt)
+    {
+        uint32_t inBatchIdx = loopIdx % (mmInfo.mLoops * mmInfo.nLoops);
+        uint32_t mIdx = 0;
+        uint32_t nIdx = 0;
+        if (swizzlDirect == 0) { // Zn
+            uint32_t tileBlockLoop = (mmInfo.mLoops + swizzlCnt - 1) / swizzlCnt;
+            uint32_t tileBlockIdx = inBatchIdx / (swizzlCnt * mmInfo.nLoops);
+            uint32_t inTileBlockIdx = inBatchIdx % (swizzlCnt * mmInfo.nLoops);
+
+            uint32_t nRow = swizzlCnt;
+            if (tileBlockIdx == tileBlockLoop - 1) {
+                nRow = mmInfo.mLoops - swizzlCnt * tileBlockIdx;
+            }
+            mIdx = tileBlockIdx * swizzlCnt + inTileBlockIdx % nRow;
+            nIdx = inTileBlockIdx / nRow;
+            
+        } else if (swizzlDirect == 1) { // Nz
+            uint32_t tileBlockLoop = (mmInfo.nLoops + swizzlCnt - 1) / swizzlCnt;
+            uint32_t tileBlockIdx = inBatchIdx / (swizzlCnt * mmInfo.mLoops);
+            uint32_t inTileBlockIdx = inBatchIdx % (swizzlCnt * mmInfo.mLoops);
+
+            uint32_t nCol = swizzlCnt;
+            if (tileBlockIdx == tileBlockLoop - 1) {
+                nCol = mmInfo.nLoops - swizzlCnt * tileBlockIdx;
+            }
+            mIdx = inTileBlockIdx / nCol;
+            nIdx = tileBlockIdx * swizzlCnt + inTileBlockIdx % nCol;
+        }
+        mmInfo.updateIdx(mIdx, nIdx);
+    }
+
+    __aicore__ inline void Stage2AscendQuantNz(AscendC::LocalTensor<IN_DTYPE> &dst,
+        AscendC::LocalTensor<ACCUMULATOR_DTYPE> &src, AscendC::LocalTensor<ACCUMULATOR_DTYPE> &scaleQuantUb,
+        AscendC::LocalTensor<ACCUMULATOR_DTYPE> &biasQuantUb, const uint32_t &height, const uint32_t &width)
+    {
+        uint64_t mask0 = 128;
+        uint32_t calCount = height * width;
+        WAIT_FLAG(MTE2, V, EVENT_ID3);
+        for (uint32_t i = 0; i < CONST_2; i++) {
+            Mul(src[i * calCount / CONST_2], src[i * calCount / CONST_2], scaleQuantUb[i * calCount / CONST_16],
+                mask0, calCount / CONST_256, { 1, 1, 0, BLOCK_NUM_PER_VEC, BLOCK_NUM_PER_VEC, 1 });
+        }
+
+        for (uint32_t i = 0; i < width / BLOCK_SIZE_16; i += CONST_2) {
+            Add(ubTrans1[i * height * BLOCK_SIZE_16], src[i * height * BLOCK_SIZE_16],
+                biasQuantUb[i * BLOCK_SIZE_16], mask0, height / BLOCK_NUM_PER_VEC,
+                { CONST_2, 1, 0, CONST_2 * BLOCK_NUM_PER_VEC, BLOCK_NUM_PER_VEC, 0 });
+            Add(ubTrans1[i * height * BLOCK_SIZE_16 + BLOCK_SIZE_16], src[(i + 1) * height * BLOCK_SIZE_16],
+                biasQuantUb[(i + 1) * BLOCK_SIZE_16], mask0, height / BLOCK_NUM_PER_VEC,
+                { CONST_2, 1, 0, CONST_2 * BLOCK_NUM_PER_VEC, BLOCK_NUM_PER_VEC, 0 });
+        }
+        SET_FLAG(V, MTE2, EVENT_ID3);
+        
+        Cast(dst, ubTrans1, AscendC::RoundMode::CAST_NONE, calCount);
+    }
+
+    __aicore__ inline void GeluQuant(
+        AscendC::LocalTensor<IN_DTYPE> ubCQuantTensor,  AscendC::LocalTensor<ACCUMULATOR_DTYPE> ubCTensor,
+        const matmulInfo &mmInfo, const uint32_t &l0cPingPong)
+    {
+        WAIT_FLAG(V, MTE2, EVENT_ID3);
+        for (uint32_t i = 0; i < mmInfo.mRound / BLOCK_NUM_PER_VEC; i++) {
+            AscendC::DataCopy(ubScaleQuant[i * BLOCK_SIZE_16], gmScaleQuant[mmInfo.nIdx * baseN1],
+                {static_cast<uint16_t>(mmInfo.nRound / BLOCK_SIZE_16), 1, 0,
+                static_cast<uint16_t>(mmInfo.mRound / BLOCK_NUM_PER_VEC - 1)});
+        }
+        AscendC::DataCopy(ubBiasQuant, gmBiasQuant[mmInfo.nIdx * baseN1], mmInfo.nRound);
+        SET_FLAG(MTE2, V, EVENT_ID3);
+        
+        ActivationType active = static_cast<ActivationType>(activationType);
+        if (active == ActivationType::GELU) {
+            AscendC::Gelu<ACCUMULATOR_DTYPE, HIGH_PRECISION, HIGH_PERFORMANCE>(
+                ubCTensor, ubCTensor, ubGelu, mmInfo.mRound * mmInfo.nRound);
+        } else if (active == ActivationType::FASTGELU) {
+            AscendC::FasterGelu<ACCUMULATOR_DTYPE, HIGH_PRECISION, HIGH_PERFORMANCE>(
+                ubCTensor, ubCTensor, ubGelu, mmInfo.mRound * mmInfo.nRound);
+        } else if (active == ActivationType::FASTGELUV2) {
+            FFN::FasterGeluV2<ACCUMULATOR_DTYPE, HIGH_PRECISION, HIGH_PERFORMANCE>(
+                ubCTensor, ubCTensor, ubGelu, mmInfo.mRound * mmInfo.nRound);
+        }
+        
+        Stage2AscendQuantNz(ubCQuantTensor, ubCTensor, ubScaleQuant, ubBiasQuant, mmInfo.mRound, mmInfo.nRound);
+    }
+
+    __aicore__ inline void CopyOut1(AscendC::LocalTensor<IN_DTYPE> ubCQuantTensor,
+        const matmulInfo &mmInfo, const uint32_t &l0cPingPong)
+    {
+        event_t l0cEvent = l0cPingPong ? EVENT_ID0 : EVENT_ID1;
+        SET_FLAG(V, MTE3, EVENT_ID0);
+        WAIT_FLAG(V, MTE3, EVENT_ID0);
+
+        int64_t mOffset = static_cast<int64_t>(mmInfo.mIdx) * mmInfo.baseM * BLOCK_SIZE_32 +
+            static_cast<int64_t>(mmInfo.nIdx) * mmInfo.baseN * mOrgUp1;
+        uint32_t dstStride = mOrgUp1 - mmInfo.mRound;
+        uint16_t blockCount = CeilDiv(mmInfo.nRound, BLOCK_SIZE_32);
+        ub_to_gm<ArchType::ASCEND_V200, IN_DTYPE, DataFormat::NZ, DataFormat::NZ>(
+            gmYTmp[mOffset], ubCQuantTensor, mmInfo.mRound, mmInfo.mRound, mOrgUp1, mmInfo.nRound, mmInfo.nRound, 0);
+        
+        SET_FLAG(MTE3, V, l0cEvent);
+    }
+
+    __aicore__ inline void Stage1MmVec()
+    {
+        SET_FLAG(MTE1, MTE2, EVENT_ID0);
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(MTE1, MTE2, EVENT_ID2);
+        SET_FLAG(MTE1, MTE2, EVENT_ID3);
+        SET_FLAG(M, MTE1, EVENT_ID0);
+        SET_FLAG(M, MTE1, EVENT_ID1);
+        SET_FLAG(MTE3, V, EVENT_ID0);
+        SET_FLAG(MTE3, V, EVENT_ID1);
+        SET_FLAG(V, MTE2, EVENT_ID0);
+        SET_FLAG(V, MTE2, EVENT_ID1);
+        SET_FLAG(V, MTE2, EVENT_ID2);
+        SET_FLAG(V, MTE2, EVENT_ID3);
+        uint32_t l0cPingPong = 1;
+        matmulInfo mmInfo = {0, 0, m1, k1, n1, baseM1, baseK1, baseN1, m1Loops, k1Loops, n1Loops};
+        matmulInfo nextMmInfo = {0, 0, m1, k1, n1, baseM1, baseK1, baseN1, m1Loops, k1Loops, n1Loops};
+        GetIdx(block_idx, mmInfo, swizzlDirect1, swizzlCnt1);
+        mm1.LoadBias(gmBiasDq1, mmInfo, l0cPingPong);
+        mm1.BrcBias(mmInfo, l0cPingPong);
+        for (uint32_t loopIdx = block_idx; loopIdx < coreLoops1; loopIdx += block_num) {
+            AscendC::LocalTensor<ACCUMULATOR_DTYPE> ubCTensor = l0cPingPong ? ubC1 : ubC1[MATRIX_C_PINGPONG_LEN];
+            AscendC::LocalTensor<IN_DTYPE> ubCQuantTensor =
+                l0cPingPong ? ubCQuant : ubCQuant[MATRIX_C_PINGPONG_LEN * CONST_2];
+            if (loopIdx + block_num < coreLoops1) {
+                GetIdx(loopIdx + block_num, nextMmInfo, swizzlDirect1, swizzlCnt1);
+                mm1.LoadBias(gmBiasDq1, nextMmInfo, 1 - l0cPingPong);
+            }
+            mm1.IterateAll(gmX, gmW1, ubCTensor, gmScaleDq1, mmInfo, l0cPingPong);
+            if (loopIdx + block_num < coreLoops1) {
+                mm1.BrcBias(nextMmInfo, 1 - l0cPingPong);
+            }
+            GeluQuant(ubCQuantTensor, ubCTensor, mmInfo, l0cPingPong);
+            CopyOut1(ubCQuantTensor, mmInfo, l0cPingPong);
+            mmInfo.updateIdx(nextMmInfo.mIdx, nextMmInfo.nIdx);
+            l0cPingPong = 1 - l0cPingPong;
+        }
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID0);
+        WAIT_FLAG(M, MTE1, EVENT_ID1);
+        WAIT_FLAG(MTE3, V, EVENT_ID0);
+        WAIT_FLAG(MTE3, V, EVENT_ID1);
+        WAIT_FLAG(V, MTE2, EVENT_ID0);
+        WAIT_FLAG(V, MTE2, EVENT_ID1);
+        WAIT_FLAG(V, MTE2, EVENT_ID2);
+        WAIT_FLAG(V, MTE2, EVENT_ID3);
+    }
+
+    __aicore__ inline void CopyOut2(const matmulInfo &mmInfo)
+    {
+        SET_FLAG(V, MTE3, EVENT_ID0);
+        WAIT_FLAG(V, MTE3, EVENT_ID0);
+        if constexpr (OUT_FORMAT == CubeFormat::ND) {
+            mm2.CopyCo22GMNZ2ND(gmY, ubC2, ubTrans2, mmInfo.mIdx * mmInfo.baseM, mmInfo.nIdx * mmInfo.baseN,
+                mmInfo.mActual, mmInfo.nActual, mmInfo.n);
+        } else {
+            int64_t dstOffset = static_cast<int64_t>(mmInfo.nIdx) * mmInfo.baseN * mOrgUp2 +
+                static_cast<int64_t>(mmInfo.mIdx) * mmInfo.baseM * BLOCK_SIZE_16;
+            ub_to_gm<ArchType::ASCEND_V200, OUT_DTYPE, DataFormat::NZ, DataFormat::NZ>(
+                gmY[dstOffset], ubC2, mmInfo.mRound, mmInfo.mRound, mOrgUp2, mmInfo.nRound, mmInfo.nRound, 0);
+        }
+        SET_FLAG(MTE3, V, EVENT_ID0);
+    }
+
+    __aicore__ inline void Stage2Mm()
+    {
+        ubC2 = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(UB_HALF_BUFFER_LEN);
+        ubTrans2 = buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(
+            baseN2 * sizeof(DESCALE_DTYPE) + baseN2 * sizeof(BIAS_DTYPE));
+        SET_FLAG(MTE1, MTE2, EVENT_ID0);
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(MTE1, MTE2, EVENT_ID2);
+        SET_FLAG(MTE1, MTE2, EVENT_ID3);
+        SET_FLAG(M, MTE1, EVENT_ID0);
+        SET_FLAG(M, MTE1, EVENT_ID1);
+        SET_FLAG(MTE3, V, EVENT_ID0);
+        SET_FLAG(V, MTE2, EVENT_ID0);
+        SET_FLAG(V, MTE2, EVENT_ID2);
+        matmulInfo mmInfo = {0, 0, m2, k2, n2, baseM2, baseK2, baseN2, m2Loops, k2Loops, n2Loops};
+        for (uint32_t loopIdx = block_idx; loopIdx < coreLoops2; loopIdx += block_num) {
+            GetIdx(loopIdx, mmInfo, swizzlDirect2, swizzlCnt2);
+            mm2.LoadBias(gmBiasDq2, mmInfo, 0);
+            mm2.BrcBias(mmInfo, 0);
+            mm2.IterateAll(gmYTmp, gmW2, ubC2, gmScaleDq2, mmInfo, 0);
+            CopyOut2(mmInfo);
+        }
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID0);
+        WAIT_FLAG(M, MTE1, EVENT_ID1);
+        WAIT_FLAG(MTE3, V, EVENT_ID0);
+        WAIT_FLAG(V, MTE2, EVENT_ID0);
+        WAIT_FLAG(V, MTE2, EVENT_ID2);
+    }
+
+    AsdopsBuffer<ArchType::ASCEND_V200> buf;
+    AscendC::GlobalTensor<IN_DTYPE> gmX;
+    AscendC::GlobalTensor<IN_DTYPE> gmW1;
+    AscendC::GlobalTensor<IN_DTYPE> gmW2;
+    AscendC::GlobalTensor<ACCUMULATOR_DTYPE> gmScaleQuant;
+    AscendC::GlobalTensor<DESCALE_DTYPE> gmScaleDq1;
+    AscendC::GlobalTensor<DESCALE_DTYPE> gmScaleDq2;
+    AscendC::GlobalTensor<ACCUMULATOR_DTYPE> gmBiasQuant;
+    AscendC::GlobalTensor<BIAS_DTYPE> gmBiasDq1;
+    AscendC::GlobalTensor<BIAS_DTYPE> gmBiasDq2;
+    AscendC::GlobalTensor<OUT_DTYPE> gmY;
+    AscendC::GlobalTensor<ACCUMULATOR_DTYPE> gmYAccu;
+    AscendC::GlobalTensor<IN_DTYPE> gmYTmp;
+    AscendC::GlobalTensor<int32_t> gmSync;
+
+    AscendC::LocalTensor<ACCUMULATOR_DTYPE> ubC1{buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(0)};
+    AscendC::LocalTensor<OUT_DTYPE> ubC2{buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(0)};
+    AscendC::LocalTensor<IN_DTYPE> ubCQuant{buf.GetBuffer<BufferType::ASCEND_UB, IN_DTYPE>(0)};
+    AscendC::LocalTensor<int32_t> ubSync{buf.GetBuffer<BufferType::ASCEND_UB, int32_t>(0)};
+    AscendC::LocalTensor<ACCUMULATOR_DTYPE> ubScaleQuant{buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(0)};
+    AscendC::LocalTensor<ACCUMULATOR_DTYPE> ubBiasQuant{buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(0)};
+    AscendC::LocalTensor<ACCUMULATOR_DTYPE> ubTrans1{buf.GetBuffer<BufferType::ASCEND_UB, ACCUMULATOR_DTYPE>(0)};
+    AscendC::LocalTensor<OUT_DTYPE> ubTrans2{buf.GetBuffer<BufferType::ASCEND_UB, OUT_DTYPE>(0)};
+    AscendC::LocalTensor<uint8_t> ubGelu{buf.GetBuffer<BufferType::ASCEND_UB, uint8_t>(0)};
+    MM1 mm1;
+    MM2 mm2;
+    
+    uint32_t m1{0};
+    uint32_t k1{0};
+    uint32_t n1{0};
+    uint32_t baseM1{0};
+    uint32_t baseK1{0};
+    uint32_t baseN1{0};
+    uint32_t m1Loops{0};
+    uint32_t k1Loops{0};
+    uint32_t n1Loops{0};
+    uint32_t coreLoops1{0};
+    uint32_t swizzlCnt1{0};
+    uint32_t swizzlDirect1{0};
+    uint32_t mOrgUp1{0};
+    uint32_t nOrgUp1{0};
+    uint32_t mOrgUp2{0};
+    uint32_t nOrgUp2{0};
+    uint32_t m2{0};
+    uint32_t k2{0};
+    uint32_t n2{0};
+    uint32_t baseM2{0};
+    uint32_t baseK2{0};
+    uint32_t baseN2{0};
+    uint32_t m2Loops{0};
+    uint32_t k2Loops{0};
+    uint32_t n2Loops{0};
+    uint32_t coreLoops2{0};
+    uint32_t swizzlCnt2{0};
+    uint32_t swizzlDirect2{0};
+    uint32_t activationType{0};
+};
+} // namespace FFN
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/genattentionmask/CMakeLists.txt b/src/kernels/mixkernels/genattentionmask/CMakeLists.txt
index d5ed03be..3ef7303b 100644
--- a/src/kernels/mixkernels/genattentionmask/CMakeLists.txt
+++ b/src/kernels/mixkernels/genattentionmask/CMakeLists.txt
@@ -12,3 +12,7 @@ set(genattentionmask_srcs
 )
 
 add_operation(GenAttentionMaskOperation "${genattentionmask_srcs}")
+
+add_kernel(genattentionmask ascend910b vector
+        op_kernel/genattentionmask.cpp
+        GenAttentionMaskKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/genattentionmask/op_kernel/genattentionmask.cpp b/src/kernels/mixkernels/genattentionmask/op_kernel/genattentionmask.cpp
new file mode 100644
index 00000000..eac96a16
--- /dev/null
+++ b/src/kernels/mixkernels/genattentionmask/op_kernel/genattentionmask.cpp
@@ -0,0 +1,148 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#include "kernel_operator.h"
+#include "mixkernels/genattentionmask/tiling/tiling_data.h"
+
+using namespace AscendC;
+namespace {
+    constexpr uint32_t MAX_PROCESS_NUM = 192 * 1024 / sizeof(half) / 4;
+    constexpr int32_t BUFFER_NUM = 1;                                     // tensor num for each queue
+} // namespace
+
+class GenAttentionMask {
+public:
+    __aicore__ inline GenAttentionMask(uint32_t batch, uint32_t maxSeqLen,
+    uint32_t blockNum, uint32_t headNum, int32_t *qSeqLen)
+    {
+        this->batch = batch;
+        this->maxSeqLen = maxSeqLen;
+        this->blockNum = blockNum;
+        this->headNum = headNum;
+
+        for (int i = 0; i < batch; i++) {
+            this->qSeqLen[i] = *(qSeqLen + i);
+        }
+    }
+
+    __aicore__ inline void Init(GM_ADDR attentionMask, GM_ADDR attentionMask2)
+    {
+        pipe.InitBuffer(attentionMaskUb, MAX_PROCESS_NUM * sizeof(half));
+
+        this->currentSeqLen = this->qSeqLen[GetBlockIdx()];
+        int64_t offset = 0;
+        for (int i = 0; i < GetBlockIdx(); i++) {
+            offset += static_cast<int64_t>(this->qSeqLen[i]) * this->qSeqLen[i] * this->headNum;
+        }
+
+        // get start index for current core, core parallel
+        attentionMaskGm.SetGlobalBuffer((__gm__ half*)attentionMask +
+        this->maxSeqLen * this->maxSeqLen * GetBlockIdx(), this->currentSeqLen * this->maxSeqLen);
+        attentionMask2Gm.SetGlobalBuffer((__gm__ half*)attentionMask2 + offset,
+        this->currentSeqLen * this->currentSeqLen * this->headNum);
+
+        // pipe alloc memory to queue, the unit is Bytes
+        int currentSeqLenRound = (this->currentSeqLen * sizeof(half) + 31) / 32 * 32 / sizeof(half);
+
+        this->rowsPerLoop = MAX_PROCESS_NUM / currentSeqLenRound;
+        int rowsRepeat = this->currentSeqLen / rowsPerLoop;
+        int rowsRemain = this->currentSeqLen % rowsPerLoop;
+
+        for (int i = 0; i < rowsRepeat; i++) {
+            Process(i, this->rowsPerLoop);
+        }
+        if (rowsRemain > 0) {
+            Process(rowsRepeat, rowsRemain);
+        }
+    }
+
+private:
+    __aicore__ inline void Process(int32_t progress, int rows)
+    {
+        int currentSeqLenRound = (this->currentSeqLen * sizeof(half) + 31) / 32 * 32 / sizeof(half);
+        // alloc tensor from queue memory
+        LocalTensor<half> attentionMaskUbPreUb = attentionMaskUb.Get<half>(MAX_PROCESS_NUM * sizeof(half));
+
+        // copy progress_th tile from global tensor to local tensor
+        pipe_barrier(PIPE_ALL);
+        copy_gm_to_ubuf_align_b16(
+            (__ubuf__ half *)attentionMaskUbPreUb.GetPhyAddr(),
+            (__gm__ half *)attentionMaskGm.GetPhyAddr() + progress * this->rowsPerLoop * this->maxSeqLen,
+            0,
+            rows,
+            this->currentSeqLen * sizeof(half),
+            0,
+            0,
+            (this->maxSeqLen - this->currentSeqLen) * sizeof(half),
+            0);
+        pipe_barrier(PIPE_ALL);
+
+        for (int i = 0; i < this->headNum; i++) {
+            copy_ubuf_to_gm_align_b16(
+                (__gm__ half *)attentionMask2Gm.GetPhyAddr() + i * this->currentSeqLen * this->currentSeqLen +
+                progress * this->rowsPerLoop * this->currentSeqLen,
+                (__ubuf__ half *)attentionMaskUbPreUb.GetPhyAddr(),
+                0,
+                rows,
+                this->currentSeqLen * sizeof(half),
+                0,
+                0,
+                0,
+                0);
+        }
+        pipe_barrier(PIPE_ALL);
+    }
+private:
+    int32_t batch = 0;
+    int32_t maxSeqLen = 0;
+    int32_t blockNum = 0;
+    int32_t headNum = 0;
+    int32_t qSeqLen[AtbOps::GEN_ATTENTION_MASK_MAX_BATCH];
+    int32_t currentSeqLen = 0;
+    int32_t rowsPerLoop = 0;
+    TPipe pipe;
+    TBuf<> attentionMaskUb;
+    GlobalTensor<half> attentionMaskGm, attentionMask2Gm;
+};
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata,
+AtbOps::GenAttentionMaskTilingData *tilingData)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingData->batch = (*(const __gm__ uint32_t *)(p_tilingdata + 0));
+    tilingData->maxSeqLen = (*(const __gm__ uint32_t *)(p_tilingdata + 4));
+    tilingData->blockNum = (*(const __gm__ uint32_t *)(p_tilingdata + 8));
+    tilingData->headNum = (*(const __gm__ uint32_t *)(p_tilingdata + 12));
+    for (int i = 0; i < tilingData->batch; i++) {
+        tilingData->qSeqLen[i] = (*(const __gm__ uint32_t *)(p_tilingdata + 16 + 4 * i));
+    }
+#else
+    __ubuf__ uint8_t *tilingdata_in_ub = (__ubuf__ uint8_t *)get_imm(0);
+    copy_gm_to_ubuf(((__ubuf__ uint8_t *)tilingdata_in_ub), p_tilingdata, 0, 1, 2, 0, 0);
+    pipe_barrier(PIPE_ALL);
+    tilingData->batch = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 0));
+    tilingData->maxSeqLen = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 4));
+    tilingData->blockNum = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 8));
+    tilingData->headNum = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 12));
+    for (int i = 0; i < tilingData->batch; i++) {
+        tilingData->qSeqLen[i] = (__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 16 + 4 * i);
+    }
+    pipe_barrier(PIPE_ALL);
+#endif
+}
+
+// implementation of kernel function
+extern "C" __global__ __aicore__ void genattentionmask(GM_ADDR attentionMask, GM_ADDR attentionMask2, GM_ADDR tiling)
+{
+    AtbOps::GenAttentionMaskTilingData tiling_data;
+    InitTilingData(tiling, &tiling_data);
+    GenAttentionMask op(tiling_data.batch, tiling_data.maxSeqLen,
+        tiling_data.blockNum, tiling_data.headNum, tiling_data.qSeqLen);
+    op.Init(attentionMask, attentionMask2);
+}
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/CMakeLists.txt b/src/kernels/mixkernels/laser_attention/CMakeLists.txt
index db986008..63e376a3 100644
--- a/src/kernels/mixkernels/laser_attention/CMakeLists.txt
+++ b/src/kernels/mixkernels/laser_attention/CMakeLists.txt
@@ -14,3 +14,7 @@ set(laser_attention_srcs
 )
 
 add_operation(LaserAttentionOperation "${laser_attention_srcs}")
+
+add_kernel(laser_attention ascend910b mix
+        op_kernel/laser_attention.cpp
+        LaserAttentionKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/AddressMappingForward.h b/src/kernels/mixkernels/laser_attention/op_kernel/AddressMappingForward.h
new file mode 100644
index 00000000..29ba8ce3
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/AddressMappingForward.h
@@ -0,0 +1,529 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef ADDRESS_MODULE_ADDRESSMAPPING_FORWARD_H
+#define ADDRESS_MODULE_ADDRESSMAPPING_FORWARD_H
+
+#include "address_const_192.h"
+
+namespace Address_192 {
+    template<typename TYPE>
+    class AddressMappingForward {
+    public:
+        // B N S D 格式的基础信息
+        int64_t batchSize;                    // batch批次的大小
+        int64_t headNum;                      // head数量
+        int64_t gqaGroupNum;                 // GQA场景的组数
+        int64_t querySequenceLen;            // query序列长度
+        int64_t keyValueSequenceLen;        // key、value序列长度
+        int64_t outputSequenceLen;           // 输出O的序列长度
+        int64_t rowSumSequenceLen;          // row_sum的序列长度
+        int64_t headDim;                      // head_dim的长度
+
+        // 核数、核组的信息
+        int64_t coreNum;                      // 使用的核心数量
+        int64_t coreGroupNum;                // 核数数量
+        int64_t coreNumPerGroup;            // 每一核组的核心数量
+        int64_t curCoreIndex;                // 当前核心的序号
+        int64_t curGroupIndex;               // 当前组的序号
+        int64_t groupLocalIndex;             // 组内核心的序号
+
+        // 偏移相关信息
+        bool isTriangle;                      // 倒三角mask的标志
+        int64_t forwardBlockNumPerCol;     // 负载均衡后attention的行数
+        int64_t forwardBlockNumPerRow;     // 负载均衡后attention的列数
+        int64_t forwardBlockRowsPerHead;   // 前向每个head的基本行块数量
+        int64_t forwardBlockRowsPerBatch;  // 前向每个batch的基本行块数量
+        int64_t forwardTotalRows;            // 前向计算的总行块数
+        int64_t forwardTotalRounds;          // 前向总共的轮次
+        int64_t blockNumPerCoreForward;    // 前向每个核心平均处理的基本块数量
+        int64_t remainBlockNumForward;      // 前向尾块的数量
+
+    public:
+        __aicore__ __inline__ void set_balance_info()
+        {
+            // no-mask场景
+            this->forwardBlockNumPerCol = this->querySequenceLen / SIZE_128;
+            this->forwardBlockNumPerRow = this->keyValueSequenceLen / SIZE_128;
+
+            // 倒三角mask场景：
+            if (this->isTriangle) {
+                this->forwardBlockNumPerCol = this->querySequenceLen / SIZE_128 / DIVISOR_2;
+                this->forwardBlockNumPerRow = this->keyValueSequenceLen / SIZE_128 + 1;
+            }
+
+            // 计算head、batch的基本块
+            this->forwardBlockRowsPerHead = this->forwardBlockNumPerCol;
+            this->forwardBlockRowsPerBatch = this->forwardBlockRowsPerHead * this->headNum;
+            this->forwardTotalRows = this->forwardBlockRowsPerBatch * this->batchSize;
+
+            // 平均块和尾块数量的计算
+            this->blockNumPerCoreForward = this->forwardBlockNumPerRow / this->coreNumPerGroup;
+            this->remainBlockNumForward = this->forwardBlockNumPerRow % this->coreNumPerGroup;
+
+            // 轮次
+            this->forwardTotalRounds =
+                    (this->forwardTotalRows + this->coreGroupNum - 1) / this->coreGroupNum;
+        }
+
+    public:
+        __aicore__ __inline__ void
+        init(int64_t batchSize, int64_t headNum, int64_t gqaGroupNum, int64_t querySequenceLen,
+            int64_t keyValueSequenceLen, int64_t headDim, bool isTriangle)
+        {
+            this->batchSize = batchSize;
+            this->headNum = headNum;
+            this->gqaGroupNum = gqaGroupNum;
+            this->querySequenceLen = querySequenceLen;
+            this->keyValueSequenceLen = keyValueSequenceLen;
+            this->headDim = headDim;
+            this->isTriangle = isTriangle;
+
+            this->outputSequenceLen = this->querySequenceLen;
+            this->rowSumSequenceLen = this->querySequenceLen;
+        }
+
+        __aicore__ __inline__ void
+        set_core_info(int64_t coreNum, int64_t coreGroupNum, int64_t coreNumPerGroup, int64_t curCoreIndex,
+                      int64_t curGroupIndex, int64_t groupLocalIndex)
+        {
+            this->coreNum = coreNum;
+            this->coreGroupNum = coreGroupNum;
+            this->coreNumPerGroup = coreNumPerGroup;
+            this->curCoreIndex = curCoreIndex;
+            this->curGroupIndex = curGroupIndex;
+            this->groupLocalIndex = groupLocalIndex;
+
+            // 计算轮次等信息
+            set_balance_info();
+        }
+
+        __aicore__ __inline__ int64_t get_total_round()
+        {
+            return this->forwardTotalRounds;
+        }
+
+        __aicore__ __inline__ bool is_running(int64_t round_id)
+        {
+            return (round_id * this->coreGroupNum + this->curGroupIndex) < this->forwardTotalRows;
+        }
+
+        template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+        __aicore__ __inline__ void addrMapping_cube1(__gm__ T_LEFT *__restrict__ left,
+                                                     __gm__ T_RIGHT *__restrict__ right, __gm__
+                                                     T_OUTPUT *__restrict__ out,
+                                                     int64_t round_id,
+                                                     PHY_ADDR_FORWARD_CUBE1<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+                                                     PHY_ADDR_FORWARD_CUBE1<T_LEFT, T_RIGHT, T_OUTPUT> *remain)
+        {
+            if (this->isTriangle) {
+                addrMapping_cube1_mask(left, right, out, round_id, src, remain);
+            } else {
+                addrMapping_cube1_nomask(left, right, out, round_id, src, remain);
+            }
+        }
+
+        template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+        __aicore__ __inline__ void addrMapping_cube1_nomask(__gm__ T_LEFT *__restrict__ left,
+                                                            __gm__ T_RIGHT *__restrict__ right, __gm__
+                                                            T_OUTPUT *__restrict__ out,
+                                                            int64_t round_id,
+                                                            PHY_ADDR_FORWARD_CUBE1<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+                                                            PHY_ADDR_FORWARD_CUBE1<T_LEFT, T_RIGHT, T_OUTPUT> *remain)
+            {
+            // 当前的row
+            int64_t curCalcRow =
+                    round_id * this->coreGroupNum + this->curGroupIndex;
+            // workspace
+            int64_t curCalcRowP =
+                    this->coreGroupNum * (round_id % 2) + this->curGroupIndex;
+            // 当前行所在的batch号
+            int64_t b = curCalcRow /
+                        this->forwardBlockRowsPerBatch;
+            // 当前batch下的head号
+            int64_t n = curCalcRow % this->forwardBlockRowsPerBatch /
+                        this->forwardBlockNumPerCol;
+            // 当前head下的行号
+            int64_t i_r = curCalcRow % this->forwardBlockRowsPerBatch %
+                          this->forwardBlockNumPerCol;
+            // 当前core处理的起始列
+            int64_t i_c =
+                    this->groupLocalIndex *
+                    this->blockNumPerCoreForward;
+            // 尾块的起始列
+            int64_t begin_remain =
+                    this->forwardBlockNumPerRow - this->remainBlockNumForward;
+
+            // batch head_num的偏移量
+            int64_t left_bn_offset = (b * this->headNum + n) * this->querySequenceLen * this->headDim;
+            int64_t g = n / (this->headNum / this->gqaGroupNum);
+            int64_t right_bn_offset_GQA =
+                    (b * this->gqaGroupNum + g) * this->keyValueSequenceLen * SIZE_256;
+            int64_t out_bn_offset = (curCalcRowP * this->forwardBlockNumPerRow) * ATTENTION_SCORE_BLOCK_SIZE;
+
+            src[0].left = left + left_bn_offset + i_r * QUERY_BLOCK_SIZE;
+            src[0].right = right + right_bn_offset_GQA + i_c * KEY_BLOCK_SIZE;
+            src[0].out = out + out_bn_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+            src[0].k = this->blockNumPerCoreForward;
+
+            if (this->remainBlockNumForward != 0) {
+                remain[0].left = src[0].left;
+                remain[0].right = right + right_bn_offset_GQA + begin_remain * KEY_BLOCK_SIZE;
+                remain[0].out = out + out_bn_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE;
+                remain[0].k = this->remainBlockNumForward;
+            }
+        }
+
+        template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+        __aicore__ __inline__ void addrMapping_cube1_mask(__gm__ T_LEFT *__restrict__ left,
+                                                          __gm__ T_RIGHT *__restrict__ right, __gm__
+                                                          T_OUTPUT *__restrict__ out,
+                                                          int64_t round_id,
+                                                          PHY_ADDR_FORWARD_CUBE1<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+                                                          PHY_ADDR_FORWARD_CUBE1<T_LEFT, T_RIGHT, T_OUTPUT> *remain)
+        {
+            // 当前的row
+            int64_t curCalcRow =
+                    round_id * this->coreGroupNum + this->curGroupIndex;
+            // workspace
+            int64_t curCalcRowP =
+                    this->coreGroupNum * (round_id % 2) + this->curGroupIndex;
+            // 当前行所在的batch号
+            int64_t b = curCalcRow /
+                        this->forwardBlockRowsPerBatch;
+            // 当前batch下的head号
+            int64_t n = curCalcRow % this->forwardBlockRowsPerBatch /
+                        this->forwardBlockNumPerCol;
+            // 当前head下的行号
+            int64_t i_r = curCalcRow % this->forwardBlockRowsPerBatch %
+                          this->forwardBlockNumPerCol;
+            // 当前core处理的起始列
+            int64_t i_c =
+                    this->groupLocalIndex *
+                    this->blockNumPerCoreForward;
+            // 尾块的起始列
+            int64_t begin_remain =
+                    this->forwardBlockNumPerRow - this->remainBlockNumForward;
+
+            // 跳变点的位置
+            int64_t q_left_index = this->forwardBlockNumPerCol + i_r;
+            int64_t q_right_index = this->forwardBlockNumPerCol - 1 - i_r;
+            int64_t switch_index = q_left_index;
+
+            // batch、head_num的偏移量
+            int64_t left_bn_offset = (b * this->headNum + n) * this->querySequenceLen * this->headDim;
+            int64_t g = n / (this->headNum / this->gqaGroupNum);
+            int64_t right_bn_offset_GQA =
+                    (b * this->gqaGroupNum + g) * this->keyValueSequenceLen * SIZE_256;
+            int64_t out_bn_offset = curCalcRowP * this->forwardBlockNumPerRow * ATTENTION_SCORE_BLOCK_SIZE;
+
+            if (switch_index < i_c) {
+                src[0].left = left + left_bn_offset + q_right_index * QUERY_BLOCK_SIZE;
+                src[0].right = right + right_bn_offset_GQA + (i_c - switch_index - 1) * KEY_BLOCK_SIZE;
+                src[0].out = out + out_bn_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+                src[0].k = this->blockNumPerCoreForward;
+
+                if (this->remainBlockNumForward != 0) {
+                    remain[0].left = src[0].left;
+                    remain[0].right =
+                            right + right_bn_offset_GQA + (begin_remain - switch_index - 1) * KEY_BLOCK_SIZE;
+                    remain[0].out = out + out_bn_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE;
+                    remain[0].k = this->remainBlockNumForward;
+                }
+            } else if (i_c <= switch_index && i_c + this->blockNumPerCoreForward > switch_index + 1) {
+                src[0].left = left + left_bn_offset + q_left_index * QUERY_BLOCK_SIZE;
+                src[0].right = right + right_bn_offset_GQA + i_c * KEY_BLOCK_SIZE;
+                src[0].out = out + out_bn_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+                src[0].k = switch_index - i_c + 1;
+
+                src[1].left = left + left_bn_offset + q_right_index * QUERY_BLOCK_SIZE;
+                src[1].right = right + right_bn_offset_GQA;
+                src[1].out = src[0].out + src[0].k * ATTENTION_SCORE_BLOCK_SIZE;
+                src[1].k = this->blockNumPerCoreForward - src[0].k;
+
+                if (this->remainBlockNumForward != 0) {
+                    remain[0].left = src[1].left;
+                    remain[0].right =
+                            right + right_bn_offset_GQA + (begin_remain - switch_index - 1) * KEY_BLOCK_SIZE;
+                    remain[0].out = out + out_bn_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE;
+                    remain[0].k = this->remainBlockNumForward;
+                }
+            } else {
+                src[0].left = left + left_bn_offset + q_left_index * QUERY_BLOCK_SIZE;
+                src[0].right = right + right_bn_offset_GQA + i_c * KEY_BLOCK_SIZE;
+                src[0].out = out + out_bn_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+                src[0].k = this->blockNumPerCoreForward;
+
+                if (this->remainBlockNumForward != 0) {
+                    if (switch_index < begin_remain) {
+                        remain[0].left = left + left_bn_offset + q_right_index * QUERY_BLOCK_SIZE;
+                        remain[0].right =
+                                right + right_bn_offset_GQA + (begin_remain - switch_index - 1) * KEY_BLOCK_SIZE;
+                        remain[0].out = out + out_bn_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE;
+                        remain[0].k = this->remainBlockNumForward;
+                    } else {
+                        remain[0].left = left + left_bn_offset + q_left_index * QUERY_BLOCK_SIZE;
+                        remain[0].right = right + right_bn_offset_GQA + begin_remain * KEY_BLOCK_SIZE;
+                        remain[0].out = out + out_bn_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE;
+                        remain[0].k = switch_index - begin_remain + 1;
+
+                        remain[1].left = left + left_bn_offset + q_right_index * QUERY_BLOCK_SIZE;
+                        remain[1].right = right + right_bn_offset_GQA;
+                        remain[1].out = remain[0].out + remain[0].k * ATTENTION_SCORE_BLOCK_SIZE;
+                        remain[1].k = this->remainBlockNumForward - remain[0].k;
+                    }
+                }
+            }
+        }
+
+        template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+        __aicore__ __inline__ void addrMapping_cube2_rowsum(
+                __gm__ T_LEFT *__restrict__ left,
+                __gm__ T_RIGHT *__restrict__ right, __gm__
+                T_OUTPUT *__restrict__ out,
+                __gm__ T_OUTPUT *__restrict__ rowsumOut,
+                int64_t round_id, int64_t &src_length,
+                int64_t &remain_length,
+                PHY_ADDR_FORWARD_ROWSUM_CUBE2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+                PHY_ADDR_FORWARD_ROWSUM_CUBE2<T_LEFT, T_RIGHT, T_OUTPUT> *remain)
+        {
+            if (this->isTriangle) {
+                addrMapping_cube2_rowsum_mask(left, right, out, rowsumOut, round_id, src_length, remain_length, src,
+                                              remain);
+            } else {
+                addrMapping_cube2_rowsum_nomask(left, right, out, rowsumOut, round_id, src_length, remain_length, src,
+                                                remain);
+            }
+        }
+
+        template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+        __aicore__ __inline__ void addrMapping_cube2_rowsum_nomask(
+                __gm__ T_LEFT *__restrict__ left,
+                __gm__ T_RIGHT *__restrict__ right, __gm__
+                T_OUTPUT *__restrict__ out,
+                __gm__ T_OUTPUT *__restrict__ rowsumOut,
+                int64_t round_id, int64_t &src_length,
+                int64_t &remain_length,
+                PHY_ADDR_FORWARD_ROWSUM_CUBE2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+                PHY_ADDR_FORWARD_ROWSUM_CUBE2<T_LEFT, T_RIGHT, T_OUTPUT> *remain)
+        {
+            // 当前的row
+            int64_t curCalcRow =
+                    round_id * this->coreGroupNum + this->curGroupIndex;
+            // workspace
+            int64_t curCalcRowP =
+                    this->coreGroupNum * (round_id % 2) + this->curGroupIndex;
+            // 当前行所在的batch号
+            int64_t b = curCalcRow /
+                        this->forwardBlockRowsPerBatch;
+            // 当前batch下的head号
+            int64_t n = curCalcRow % this->forwardBlockRowsPerBatch /
+                        this->forwardBlockNumPerCol;
+            // 当前head下的行号
+            int64_t i_r = curCalcRow % this->forwardBlockRowsPerBatch %
+                          this->forwardBlockNumPerCol;
+            // 当前core处理的起始列
+            int64_t i_c =
+                    this->groupLocalIndex *
+                    this->blockNumPerCoreForward;
+            // 尾块的起始列
+            int64_t begin_remain =
+                    this->forwardBlockNumPerRow - this->remainBlockNumForward;
+
+            // batch、head_num的偏移量
+            int64_t g = n / (this->headNum / this->gqaGroupNum);
+            int64_t left_bn_out_offset =
+                    (curCalcRowP * this->forwardBlockNumPerRow) * ATTENTION_SCORE_BLOCK_SIZE;
+            int64_t right_bn_offset_GQA = (b * this->gqaGroupNum + g) * keyValueSequenceLen * SIZE_128;
+            int64_t cube2_out_bn_offset = (b * this->headNum + n) * outputSequenceLen * SIZE_128;
+            int64_t rowsum_out_bn_offset = (b * this->headNum + n) * rowSumSequenceLen;
+
+            src[0].left = left + left_bn_out_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+            src[0].right = right + right_bn_offset_GQA + i_c * VALUE_BLOCK_SIZE;
+            src[0].out = out + cube2_out_bn_offset + i_r * OUTPUT_BLOCK_SIZE;
+            src[0].rowsumOut = rowsumOut + rowsum_out_bn_offset + i_r * ROWSUM_BLOCK_SIZE;
+            src[0].k = this->blockNumPerCoreForward;
+            src_length = 1;
+            if (this->remainBlockNumForward != 0) {
+                remain[0].left = left + left_bn_out_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE +
+                                 (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                 BASE_BLOCK_LENGTH;
+                remain[0].right = right + right_bn_offset_GQA + begin_remain * VALUE_BLOCK_SIZE;
+                remain[0].out = out + cube2_out_bn_offset + i_r * OUTPUT_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                BASE_BLOCK_LENGTH;
+                remain[0].rowsumOut =
+                        rowsumOut + rowsum_out_bn_offset + i_r * ROWSUM_BLOCK_SIZE +
+                        (BASE_BLOCK_LENGTH / this->coreGroupNum) * this->groupLocalIndex;
+                remain[0].k = this->remainBlockNumForward;
+                remain_length = 1;
+            }
+        }
+
+        template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+        __aicore__ __inline__ void addrMapping_cube2_rowsum_mask(
+            __gm__ T_LEFT *__restrict__ left,
+            __gm__ T_RIGHT *__restrict__ right,
+            __gm__ T_OUTPUT *__restrict__ out,
+            __gm__ T_OUTPUT *__restrict__ rowsumOut,
+            int64_t round_id, int64_t &src_length,
+            int64_t &remain_length,
+            PHY_ADDR_FORWARD_ROWSUM_CUBE2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+            PHY_ADDR_FORWARD_ROWSUM_CUBE2<T_LEFT, T_RIGHT, T_OUTPUT> *remain) {
+            // 当前的row
+            int64_t curCalcRow =
+                    round_id * this->coreGroupNum + this->curGroupIndex;
+            // workspace
+            int64_t curCalcRowP =
+                    this->coreGroupNum * (round_id % 2) + this->curGroupIndex;
+            // 当前行所在的batch号
+            int64_t b = curCalcRow /
+                        this->forwardBlockRowsPerBatch;
+            // 当前batch下的head号
+            int64_t n = curCalcRow % this->forwardBlockRowsPerBatch /
+                        this->forwardBlockNumPerCol;
+            // 当前head下的行号
+            int64_t i_r = curCalcRow % this->forwardBlockRowsPerBatch %
+                          this->forwardBlockNumPerCol;
+            // 当前core处理的起始列
+            int64_t i_c =
+                    this->groupLocalIndex *
+                    this->blockNumPerCoreForward;
+            // 尾块的起始列
+            int64_t begin_remain =
+                    this->forwardBlockNumPerRow - this->remainBlockNumForward;
+
+            // 跳变点的位置
+            int64_t q_left_index = this->forwardBlockNumPerCol + i_r;
+            int64_t q_right_index = this->forwardBlockNumPerCol - 1 - i_r;
+            int64_t switch_index = q_left_index;
+
+            // batch、head_num的偏移量
+            int64_t g = n / (this->headNum / this->gqaGroupNum);
+            int64_t left_bn_out_offset =
+                    (curCalcRowP * this->forwardBlockNumPerRow) * ATTENTION_SCORE_BLOCK_SIZE;
+            int64_t right_bn_offset_GQA = (b * this->gqaGroupNum + g) * keyValueSequenceLen * SIZE_128;
+            int64_t cube2_out_bn_offset = (b * this->headNum + n) * outputSequenceLen * SIZE_128;
+            int64_t rowsum_out_bn_offset = (b * this->headNum + n) * rowSumSequenceLen;
+
+            if (switch_index < i_c) {
+                src[0].left = left + left_bn_out_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+                src[0].right = right + right_bn_offset_GQA + (i_c - switch_index - 1) * VALUE_BLOCK_SIZE;
+                src[0].out = out + cube2_out_bn_offset + q_right_index * OUTPUT_BLOCK_SIZE;
+                src[0].rowsumOut = rowsumOut + rowsum_out_bn_offset + q_right_index * ROWSUM_BLOCK_SIZE;
+                src[0].k = this->blockNumPerCoreForward;
+                src_length = 1;
+
+                if (this->remainBlockNumForward != 0) {
+                    remain[0].left =
+                            left + left_bn_out_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE +
+                            (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                            BASE_BLOCK_LENGTH;
+                    remain[0].right =
+                            right + right_bn_offset_GQA + (begin_remain - switch_index - 1) * VALUE_BLOCK_SIZE;
+                    remain[0].out = src[0].out +
+                                    (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                    BASE_BLOCK_LENGTH;
+                    remain[0].rowsumOut =
+                            src[0].rowsumOut +
+                            (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex;
+                    remain[0].k = this->remainBlockNumForward;
+                    remain_length = 1;
+                }
+            } else if (i_c <= switch_index && i_c + this->blockNumPerCoreForward > switch_index + 1) {
+                src[0].left = left + left_bn_out_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+                src[0].right = right + right_bn_offset_GQA + i_c * VALUE_BLOCK_SIZE;
+                src[0].out = out + cube2_out_bn_offset + q_left_index * OUTPUT_BLOCK_SIZE;
+                src[0].rowsumOut = rowsumOut + rowsum_out_bn_offset + q_left_index * ROWSUM_BLOCK_SIZE;
+                src[0].k = switch_index - i_c + 1;
+
+                src[1].left = src[0].left +
+                              src[0].k * ATTENTION_SCORE_BLOCK_SIZE;
+                src[1].right = right + right_bn_offset_GQA;
+                src[1].out = out + cube2_out_bn_offset + q_right_index * OUTPUT_BLOCK_SIZE;
+                src[1].rowsumOut = rowsumOut + rowsum_out_bn_offset + q_right_index * ROWSUM_BLOCK_SIZE;
+                src[1].k = this->blockNumPerCoreForward - src[0].k;
+
+                src_length = 2;
+
+                if (this->remainBlockNumForward != 0) {
+                    remain[0].left =
+                            left + left_bn_out_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE +
+                            (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                            BASE_BLOCK_LENGTH;
+                    remain[0].right =
+                            right + right_bn_offset_GQA + (begin_remain - switch_index - 1) * VALUE_BLOCK_SIZE;
+                    remain[0].out =
+                            src[1].out + (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                         BASE_BLOCK_LENGTH;
+                    remain[0].rowsumOut =
+                            src[1].rowsumOut +
+                            (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex;
+                    remain[0].k = this->remainBlockNumForward;
+                    remain_length = 1;
+                }
+            } else {
+                src[0].left = left + left_bn_out_offset + i_c * ATTENTION_SCORE_BLOCK_SIZE;
+                src[0].right = right + right_bn_offset_GQA + i_c * VALUE_BLOCK_SIZE;
+                src[0].out = out + cube2_out_bn_offset + q_left_index * OUTPUT_BLOCK_SIZE;
+                src[0].rowsumOut = rowsumOut + rowsum_out_bn_offset + q_left_index * ROWSUM_BLOCK_SIZE;
+                src[0].k = this->blockNumPerCoreForward;
+                src_length = 1;
+
+                if (this->remainBlockNumForward != 0) {
+                    if (switch_index < begin_remain) {
+                        remain[0].left =
+                                left + left_bn_out_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                BASE_BLOCK_LENGTH;
+                        remain[0].right =
+                                right + right_bn_offset_GQA + (begin_remain - switch_index - 1) * VALUE_BLOCK_SIZE;
+                        remain[0].out =
+                                out + cube2_out_bn_offset + q_right_index * OUTPUT_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                BASE_BLOCK_LENGTH;
+                        remain[0].rowsumOut =
+                                rowsumOut + rowsum_out_bn_offset + q_right_index * ROWSUM_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex;
+                        remain[0].k = this->remainBlockNumForward;
+                        remain_length = 1;
+                    } else {
+                        remain[0].left =
+                                left + left_bn_out_offset + begin_remain * ATTENTION_SCORE_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                BASE_BLOCK_LENGTH;
+                        remain[0].right = right + right_bn_offset_GQA + begin_remain * VALUE_BLOCK_SIZE;
+                        remain[0].out =
+                                out + cube2_out_bn_offset + q_left_index * OUTPUT_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                BASE_BLOCK_LENGTH;
+                        remain[0].rowsumOut =
+                                rowsumOut + rowsum_out_bn_offset + q_left_index * ROWSUM_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex;
+                        remain[0].k = switch_index - begin_remain + 1;
+
+                        remain[1].left = remain[0].left + remain[0].k * ATTENTION_SCORE_BLOCK_SIZE;
+                        remain[1].right = right + right_bn_offset_GQA;
+                        remain[1].out =
+                                out + cube2_out_bn_offset + q_right_index * OUTPUT_BLOCK_SIZE +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex *
+                                BASE_BLOCK_LENGTH;
+                        remain[1].rowsumOut =
+                                rowsumOut + rowsum_out_bn_offset + q_right_index * BASE_BLOCK_LENGTH +
+                                (BASE_BLOCK_LENGTH / this->coreNumPerGroup) * this->groupLocalIndex;
+                        remain[1].k = this->remainBlockNumForward - remain[0].k;
+
+                        remain_length = 2;
+                    }
+                }
+            }
+        }
+    };
+}
+#endif
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/CubeForward.h b/src/kernels/mixkernels/laser_attention/op_kernel/CubeForward.h
new file mode 100644
index 00000000..5c3c4bae
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/CubeForward.h
@@ -0,0 +1,1193 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __CUBEFORWARD_H__
+#define __CUBEFORWARD_H__
+
+#define USE_ASCENDC
+#include "ppmatmul_const.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/utils.h"
+#include "kernels/utils/kernel/iterator.h"
+
+using namespace AscendC;
+#define SET_FLAG(trigger, waiter, e) AscendC::SetFlag<AscendC::HardEvent::trigger##_##waiter>((e))
+#define WAIT_FLAG(trigger, waiter, e) AscendC::WaitFlag<AscendC::HardEvent::trigger##_##waiter>((e))
+
+#ifdef __DAV_C220_CUBE__
+
+constexpr int32_t SIZE_16 = 16;
+constexpr int32_t SIZE_32 = 32;
+constexpr int32_t SIZE_64 = 64;
+constexpr int32_t SIZE_128 = 128;
+constexpr int32_t SIZE_256 = 256;
+constexpr int32_t SIZE_384 = 384;
+constexpr int32_t SIZE_504 = 504;
+constexpr int32_t SIZE_ONE_K = 1024;
+constexpr int64_t BASE_LEN = 128;
+constexpr int64_t BASE_BLOCK = 128 * 128;
+constexpr int64_t REPEAT_TIME_8 = 8;
+constexpr int64_t REPEAT_TIME_64 = 64;
+constexpr int64_t SRC_STRIDE_1 = 1;
+constexpr int64_t SRC_STRIDE_8 = 8;
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+class CubeForward {
+public:
+    __aicore__ inline CubeForward(){};
+    __aicore__ inline void Init(__gm__ uint8_t *__restrict__ a_cube1, __gm__ uint8_t *__restrict__ b_cube1,
+        __gm__ uint8_t *__restrict__ b_cube2, __gm__ uint8_t *__restrict__ c_cube1, __gm__ float *__restrict__ c_cube2,
+        __gm__ uint8_t *__restrict__ ones_rowsum, __gm__ float *__restrict__ gm_rowsum, int32_t Y, int32_t F, int32_t B,
+        int32_t N, int32_t S1, int32_t S2, int32_t D, int32_t nG, int32_t isTriangle, int32_t sparseMode,
+        int32_t windowLength);
+    __aicore__ inline void Run();
+    __aicore__ inline void PresetFlag();
+    __aicore__ inline void ClearFlag();
+    __aicore__ inline void SetHighPrecision(bool isHighPrecision)
+    {
+        this->isHighPrecision = isHighPrecision;
+    };
+
+protected:
+    __aicore__ __inline__ void unified_addrMapping(
+        int32_t roundId, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain);
+    __aicore__ __inline__ void addrMapping(
+        int32_t roundId, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain);
+    __aicore__ __inline__ void addrMapping_nomask(
+        int32_t roundId, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain);
+    __aicore__ __inline__ void cube1(
+        PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain);
+    __aicore__ __inline__ void base_block_mad(PhyAddrCube1<TYPE, WORKSPACE_TYPE> addr_1,
+        PhyAddrCube1<TYPE, WORKSPACE_TYPE> addr_2, int32_t l0a_offset_remain = -1);
+
+    // 以下方法用于混算：
+    __aicore__ __inline__ void loadRowSumOnesblock();  // 加载rowsum右全1矩阵
+    __aicore__ __inline__ void addrMapping_cube2_rowsum(int32_t roundId, int64_t &src_length, int64_t &remain_length,
+        PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+        PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain);  // 混算中cube2、rowsum寻址（倒三角+sparse）
+    __aicore__ __inline__ void addrMapping_cube2_rowsum_nomask(int32_t roundId, int64_t &src_length,
+        int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+        PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain);  // 混算中cube2、rowsum寻址（nomask+不等长）
+    __aicore__ __inline__ void unified_addrMapping_cube2mix(int32_t roundId, int64_t &src_length,
+        int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+        PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain);  // cube2和rowsum混算的寻址
+    __aicore__ __inline__ void cube2_rowsum_mix_only(const PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *cube_addr,
+        int32_t cube_length, int32_t m_length, int32_t vcore_num_per_head);  // cube2和row_sum混算的基本单元
+    __aicore__ __inline__ void mix_cube2_rowsum(int32_t cube2_roundId);      // cube2、rowsum的混算
+
+protected:
+    __gm__ TYPE *__restrict__ gm_a_cube1;
+    __gm__ TYPE *__restrict__ gm_b_cube1;
+    __gm__ TYPE *__restrict__ gm_b_cube2;
+    __gm__ WORKSPACE_TYPE *__restrict__ gm_c_cube1;
+    __gm__ float *__restrict__ gm_c_cube2;
+    __gm__ TYPE *__restrict__ gm_ones;
+    __gm__ float *__restrict__ rowsum_gm;
+
+    AsdopsBuffer<ArchType::ASCEND_V220> asdopsBuf;
+
+    // init gm input tensor
+    GlobalTensor<TYPE> gm_Q_tensor;
+    GlobalTensor<TYPE> gm_K_tensor;
+    GlobalTensor<TYPE> gm_V_tensor;
+    GlobalTensor<WORKSPACE_TYPE> gm_c_cube1_tensor;
+    GlobalTensor<TYPE> gm_c_cube1_tensor_type;
+    GlobalTensor<float> gm_c_cube2_tensor;
+    GlobalTensor<TYPE> gm_ones_tensor;
+    GlobalTensor<float> gm_rowsum_tensor;
+
+    LocalTensor<TYPE> l1_base_b_cube1_tensor;
+    LocalTensor<TYPE> l1_base_a_cube2_tensor;
+    LocalTensor<TYPE> l1_base_b_cube2_tensor;
+
+    LocalTensor<TYPE> l1_a_ping_tensor;
+    LocalTensor<TYPE> l1_a_pong_tensor;
+    LocalTensor<TYPE> l1_b_ping_tensor;
+    LocalTensor<TYPE> l1_b_pong_tensor;
+    LocalTensor<TYPE> l1_row_sum_1_buf_tensor;
+
+    LocalTensor<TYPE> l0_a_ping_tensor;
+    LocalTensor<TYPE> l0_a_pong_tensor;
+    LocalTensor<TYPE> l0_b_ping_tensor;
+    LocalTensor<TYPE> l0_b_pong_tensor;
+    LocalTensor<float> l0_c_ping_tensor;
+    LocalTensor<float> l0_c_pong_tensor;
+
+    uint32_t l1_a_ping_pong_flag_ = 0;
+    uint32_t l1_b_ping_pong_flag_ = 0;
+
+    uint32_t l0_a_ping_pong_flag_ = 0;
+    uint32_t l0_b_ping_pong_flag_ = 0;
+    uint32_t l0_c_ping_pong_flag_ = 0;
+
+    // Y个core处理一个完整行，所有core分成 F 组， block_dim = F * Y
+    int32_t Y;
+    int32_t F;
+
+    // Q K V shape : [B,N,S,D] 其中 D 固定为128
+    int32_t B;
+    int32_t N;
+    // int32_t S; // 256 - 128k (256的倍数) 方阵的行
+    int32_t S1;  // 256 - 128k (256的倍数) 方阵的行
+    int32_t S2;  // 256 - 128k (256的倍数) 方阵的行
+    int32_t D;
+    int32_t nG;
+    int32_t G;              // GQA 场景  k v shape [B,G,S,D]
+    int32_t qk_triangle;    // GQA 场景  k v shape [B,G,S,D]
+    int32_t sparseMode;     // sparseMode: 0:dense, 1:sparse
+    int32_t windowLength;  // sparse场景下，滑动窗口的长度
+
+    // int32_t H;
+    int32_t H1;
+    int32_t H2;
+    int32_t L;  // 负责均衡
+    int32_t W;
+
+    int32_t curCoreIndex;
+    int32_t local_block_index;  // 组内 core id [0, Y-1], 组内第几个 0
+    int32_t core_group_index;   // [0, F-1], 第几组
+
+    int32_t row_per_batch;
+    int32_t column_per_core;
+    int32_t column_remain;
+
+    bool isHighPrecision = true;
+};
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::Init(__gm__ uint8_t *__restrict__ a_cube1,
+    __gm__ uint8_t *__restrict__ b_cube1, __gm__ uint8_t *__restrict__ b_cube2, __gm__ uint8_t *__restrict__ c_cube1,
+    __gm__ float *__restrict__ c_cube2, __gm__ uint8_t *__restrict__ ones_rowsum, __gm__ float *__restrict__ gm_rowsum,
+    int32_t y_cube_num_per_line, int32_t group_num, int32_t batch, int32_t n_batch,
+    // int32_t total_length,
+    int32_t qSeqLength, int32_t kSeqLength, int32_t base_length, int32_t headGroupNum, int32_t isTriangle,
+    int32_t sparseMode, int32_t windowLength)
+{
+    this->gm_a_cube1 = (__gm__ TYPE *__restrict__)a_cube1;
+    this->gm_b_cube1 = (__gm__ TYPE *__restrict__)b_cube1;
+    this->gm_b_cube2 = (__gm__ TYPE *__restrict__)b_cube2;
+    this->gm_c_cube1 = (__gm__ WORKSPACE_TYPE *__restrict__)c_cube1;
+    this->gm_c_cube2 = c_cube2;
+    this->gm_ones = (__gm__ TYPE *__restrict__)ones_rowsum;
+    this->rowsum_gm = gm_rowsum;
+
+    this->Y = y_cube_num_per_line;
+    this->F = group_num;
+    this->B = batch;
+    this->N = n_batch;
+    this->S1 = qSeqLength;
+    this->S2 = kSeqLength;
+    this->D = base_length;
+    this->nG = headGroupNum;
+    this->G = this->N / this->nG;
+
+    this->qk_triangle = isTriangle;
+    this->sparseMode = sparseMode;
+    this->windowLength = windowLength;
+
+    // 无mask
+    if (this->qk_triangle == 0) {
+        this->H1 = this->S1 / SIZE_128;
+        this->H2 = this->S2 / SIZE_128;
+        this->L = this->H1;  // 负责均衡
+        this->column_per_core = this->H2 / this->Y;
+        this->column_remain = this->H2 % this->Y;
+    }
+    // 有mask
+    else {
+        this->H1 = this->S1 / SIZE_128;
+        this->H2 = this->S2 / SIZE_128;
+        this->L = this->H1 / 2;  // 负责均衡
+        this->column_per_core = (this->H2 + 1) / this->Y;
+        this->column_remain = (this->H2 + 1) % this->Y;
+
+        // sparse场景：行数、列数以及尾块需要重新设置 update
+        if (this->sparseMode == 1) {
+            this->H1 = this->S1 / SIZE_128;
+            this->H2 = this->S2 / SIZE_128;
+            this->W = this->windowLength / SIZE_128;
+            this->L = this->H1 - this->W / 2;
+            this->column_per_core = (this->W + 1) / this->Y;
+            this->column_remain = (this->W + 1) % this->Y;
+        }
+    }
+
+    this->curCoreIndex = get_block_idx();
+    this->core_group_index = this->curCoreIndex / this->Y;
+    this->local_block_index = this->curCoreIndex % this->Y;
+    this->row_per_batch = this->N * this->L;
+
+    // init input gm tensor
+    gm_Q_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_a_cube1));
+    gm_K_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_b_cube1));
+    gm_V_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_b_cube2));
+
+    // init gm output tensor
+    gm_c_cube1_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ WORKSPACE_TYPE *>(this->gm_c_cube1));
+    gm_c_cube1_tensor_type.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_c_cube1));
+    gm_c_cube2_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->gm_c_cube2));
+    gm_ones_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_ones));
+    gm_rowsum_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->rowsum_gm));
+
+    // init L1 tensor
+    l1_base_b_cube1_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_128 * SIZE_ONE_K);
+    l1_base_a_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_256 * SIZE_ONE_K);
+    l1_base_b_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_384 * SIZE_ONE_K);
+
+    l1_a_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(0);
+    l1_a_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_128 * SIZE_ONE_K);
+    l1_b_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_256 * SIZE_ONE_K);
+    l1_b_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_384 * SIZE_ONE_K);
+    l1_row_sum_1_buf_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_504 * SIZE_ONE_K);
+
+    // init L0A/L0B/L0C tensor
+    l0_a_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(0);
+    l0_a_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(SIZE_32 * SIZE_ONE_K);
+    l0_b_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(0);
+    l0_b_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(SIZE_32 * SIZE_ONE_K);
+    l0_c_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+    l0_c_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(SIZE_64 * SIZE_ONE_K);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::PresetFlag()
+{
+    SET_FLAG(MTE1, MTE2, EVENT_ID0);
+    SET_FLAG(MTE1, MTE2, EVENT_ID1);
+    SET_FLAG(MTE1, MTE2, EVENT_ID2);
+    SET_FLAG(MTE1, MTE2, EVENT_ID3);
+
+    SET_FLAG(M, MTE1, EVENT_ID0);
+    SET_FLAG(M, MTE1, EVENT_ID1);
+    SET_FLAG(M, MTE1, EVENT_ID2);
+    SET_FLAG(M, MTE1, EVENT_ID3);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::ClearFlag()
+{
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+
+    WAIT_FLAG(M, MTE1, EVENT_ID0);
+    WAIT_FLAG(M, MTE1, EVENT_ID1);
+    WAIT_FLAG(M, MTE1, EVENT_ID2);
+    WAIT_FLAG(M, MTE1, EVENT_ID3);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::Run()
+{
+    SetPadding(0);
+    uint64_t config = 0x1;
+    AscendC::SetNdParaImpl(config);
+
+    // 等待vector workspace的初始化
+    WaitFlagDev(AIV2AICFLAGID);
+    loadRowSumOnesblock();  // 先加载全1矩阵，使其常驻L1
+
+    PresetFlag();
+    // 计算每个核的总轮次 Z，  注意：存在余数需要计算当前核释放参与尾行的计算
+    int32_t Z = B * N * L / F;
+    int32_t remain = B * N * L % F;
+    if (remain != 0) {
+        Z += 1;
+    }
+
+    if (Z == 1) {
+        if (core_group_index >= B * N * L) {
+            if (Y > 1) {
+                FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                WaitFlagDev(AICFLAGID);
+            }
+        } else {
+            PhyAddrCube1<TYPE, WORKSPACE_TYPE> src[2];
+            PhyAddrCube1<TYPE, WORKSPACE_TYPE> remain[2];
+            unified_addrMapping(0, src, remain);
+            cube1(src, remain);
+
+            if (Y > 1) {
+                FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID); // 核间同步
+                WaitFlagDev(AICFLAGID);
+            }
+
+            // aic sync aiv
+            FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            WaitFlagDev(AIV2AICFLAGID);
+
+            // cube2与rowsum的混算
+            mix_cube2_rowsum(0);
+        }
+    } else {
+        int32_t cube2_index = 0;
+        /************************************CUBE1*2******************************************************/
+        for (int32_t i = 0; i < 2; i++) {
+            if (i * F + core_group_index < B * N * L) {
+                PhyAddrCube1<TYPE, WORKSPACE_TYPE> src[2];
+                PhyAddrCube1<TYPE, WORKSPACE_TYPE> remain[2];
+                unified_addrMapping(i, src, remain);
+                cube1(src, remain);
+
+                if (Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+                if (i == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            } else if (i == (Z - 1)) {
+                if (Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+            }
+        }
+        /****************************************** CUBE2 + CUBE1 ***************************************/
+        for (int32_t i = 2; i < Z; i++) {
+            WaitFlagDev(AIV2AICFLAGID);
+            FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+
+            // cube2与rowsum的混算
+            mix_cube2_rowsum(cube2_index);
+            cube2_index += 1;
+
+            if (i * F + core_group_index < B * N * L) {
+                PhyAddrCube1<TYPE, WORKSPACE_TYPE> src[2];
+                PhyAddrCube1<TYPE, WORKSPACE_TYPE> remain[2];
+                unified_addrMapping(i, src, remain);
+                cube1(src, remain);
+
+                // Wait Vec
+                // cube2 load some data to l1
+
+                // 多核处理一个完整行，需要全核同步
+                if (Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+            } else {
+                if (Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+            }
+        }
+        /************************************CUBE2*2******************************************************/
+        for (int32_t i = 0; i < 2; i++) {
+            if ((Z - 2 + i) * F + core_group_index < B * N * L) {
+                WaitFlagDev(AIV2AICFLAGID);
+                if (i == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+
+                // cube2与rowsum的混算
+                mix_cube2_rowsum(cube2_index);
+                cube2_index += 1;
+            }
+        }
+    }
+    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+    WaitFlagDev(AICFLAGID);
+
+
+    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+
+    ClearFlag();
+}
+
+// --------------------------------------
+// [+] Unified addressing function entrance
+// @unified_addrMapping
+// @unified_addrMapping_cube2
+// @unified_addrMapping_rowsum
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::unified_addrMapping(
+    int32_t roundId, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain)
+{
+    if (this->qk_triangle == 1) {  // 倒三角
+        addrMapping(roundId, src, remain);
+    } else if (this->qk_triangle == 0) {  // 非倒三角
+        addrMapping_nomask(roundId, src, remain);
+    }
+}
+
+// --------------------------------------
+// [+] No mask cases + qk不等长
+// L = H
+// column_per_core = H2 / Y
+// column_remain = H2 % Y
+// --------------------------------------
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::addrMapping_nomask(
+    int32_t roundId, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t curCalcRow = roundId * F + core_group_index;  // 当前的row
+    int32_t curCalcRowP = F * (roundId % 2) + core_group_index;
+    int32_t b = curCalcRow / row_per_batch;               // 当前行所在的batch号
+    int32_t n = (curCalcRow % row_per_batch) / L;         // 当前batch下的head号
+    int32_t i_r = (curCalcRow % row_per_batch) % L;       // 当前head下的行号
+    int32_t i_c =  local_block_index * column_per_core ;    // 当前core处理的起始列
+    int32_t begin_remian = H2 - column_remain;           // MHA + 无mask
+
+    int32_t q_left_index = i_r;           // MHA + 无mask
+    int32_t q_right_index = L - 1 - i_r;  // MHA + 无mask
+    int32_t switch_index = H2;            // MHA + 无mask
+
+    int64_t bn_offset_1 = (b * N * H1 + n * H1) * BASE_BLOCK_SIZE;
+    int32_t g = n / (N / G);
+    int64_t bn_offset_GQA = (b * G * H2 + g * H2) * BASE_BLOCK_SIZE;
+    int64_t out_offset = (curCalcRowP * H2) * BASE_BLOCK_SIZE;
+
+    src[0].left = gm_a_cube1 + bn_offset_1 + q_left_index * BASE_BLOCK_SIZE;
+    src[0].right = gm_b_cube1 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+    src[0].out = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+    src[0].k = column_per_core;
+
+    if (column_remain != 0) {
+        remain[0].left = gm_a_cube1 + bn_offset_1 + q_left_index * BASE_BLOCK_SIZE;
+        remain[0].right = gm_b_cube1 + bn_offset_GQA + begin_remian * BASE_BLOCK_SIZE;
+        remain[0].out = gm_c_cube1 + out_offset + begin_remian * BASE_BLOCK_SIZE;
+        remain[0].k = column_remain;
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::addrMapping(
+    int32_t roundId, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t curCalcRow = roundId * F + core_group_index;  // 当前的row
+    int32_t curCalcRowP = F * (roundId % 2) + core_group_index;
+    int32_t b = curCalcRow / row_per_batch;           // 当前行所在的batch号
+    int32_t n = (curCalcRow % row_per_batch) / L;     // 当前batch下的head号
+    int32_t i_r = (curCalcRow % row_per_batch) % L;   // 当前head下的行号
+    int32_t i_c = local_block_index * column_per_core;  // 当前core处理的起始列
+    int32_t begin_remain = this->sparseMode == 1 ? (W + 1 - column_remain) : (H1 + 1 - column_remain);  // update
+
+    int32_t q_left_index = this->sparseMode == 1 ? (W / 2 + i_r) : (L + i_r);
+    int32_t q_right_index = this->sparseMode == 1 ? (W / 2 - 1 - i_r) : (L - 1 - i_r);
+    int32_t switch_index = q_left_index;
+
+    // 设置sparse的标志位 以及 sparse场景非倒三角部分的偏移
+    int32_t sparse_flag = false;
+    if (this->sparseMode == 1) {
+        sparse_flag = i_r > ((W - 1) / 2) ? true : false;
+    }
+    int32_t row_down = i_r + W / 2;
+    int32_t col_down = i_r + i_c - W / 2;
+    int32_t col_down_remain = i_r + begin_remain - W / 2;
+
+    int64_t bn_offset = (b * N * H1 + n * H1) * BASE_BLOCK_SIZE;
+    int32_t g = n / (N / G);
+    int64_t bn_offset_GQA = (b * G * H1 + g * H1) * BASE_BLOCK_SIZE;
+    int64_t q_left_offset = bn_offset + q_left_index * BASE_BLOCK_SIZE;
+    int64_t q_right_offset = bn_offset + q_right_index * BASE_BLOCK_SIZE;
+    // int64_t out_offset = (curCalcRow * (H + 1)) * BASE_BLOCK_SIZE;
+    // workspace的大小(F * (H + 1) * BASE_BLOCK_SIZE) * (roundId % 2) + core_group_index * (H + 1) * BASE_BLOCK_SIZE
+    // 反向workspace的大小  coreNum * len * BASE_BLOCK_SIZE,     curCoreIndex * len * BASE_BLOCK_SIZE
+    int64_t out_offset = this->sparseMode == 1 ? ((curCalcRowP * (W + 1)) * BASE_BLOCK_SIZE)
+                                               : ((curCalcRowP * (H1 + 1)) * BASE_BLOCK_SIZE);
+
+    if (!sparse_flag && switch_index < i_c) {
+        src[0].left = gm_a_cube1 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube1 + bn_offset_GQA + (i_c - switch_index - 1) * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].k = column_per_core;
+
+        if (column_remain != 0) {
+            remain[0].left = src[0].left;
+            remain[0].right = gm_b_cube1 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+            remain[0].out = gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE;
+            remain[0].k = column_remain;
+        }
+    } else if (!sparse_flag && i_c <= switch_index && i_c + column_per_core > switch_index) {
+        src[0].left = gm_a_cube1 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube1 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].k = switch_index - i_c + 1;
+
+        src[1].left = gm_a_cube1 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+        src[1].right = gm_b_cube1 + bn_offset_GQA;
+        src[1].out = src[0].out + src[0].k * BASE_BLOCK_SIZE;
+        src[1].k = column_per_core - src[0].k;
+
+        if (column_remain != 0) {
+            remain[0].left = src[1].left;
+            remain[0].right = gm_b_cube1 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+            remain[0].out = gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE;
+            remain[0].k = column_remain;
+        }
+    } else if (!sparse_flag && i_c <= switch_index && i_c + column_per_core <= switch_index) {
+        src[0].left = gm_a_cube1 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube1 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].k = column_per_core;
+
+        if (column_remain != 0) {
+            if (switch_index < begin_remain) {
+                remain[0].left = gm_a_cube1 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+                remain[0].right = gm_b_cube1 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+                remain[0].out = gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE;
+                remain[0].k = column_remain;
+            } else {
+                remain[0].left = gm_a_cube1 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+                remain[0].right = gm_b_cube1 + bn_offset_GQA + begin_remain * BASE_BLOCK_SIZE;
+                remain[0].out = gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE;
+                remain[0].k = switch_index - begin_remain + 1;
+
+                remain[1].left = gm_a_cube1 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+                remain[1].right = gm_b_cube1 + bn_offset_GQA;
+                remain[1].out = remain[0].out + remain[0].k * BASE_BLOCK_SIZE;
+                remain[1].k = column_remain - remain[0].k;
+            }
+        }
+    } else {
+        src[0].left = gm_a_cube1 + bn_offset + row_down * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube1 + bn_offset_GQA + col_down * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].k = column_per_core;
+
+        if (column_remain != 0) {
+            remain[0].left = src[0].left;
+            remain[0].right = gm_b_cube1 + bn_offset_GQA + col_down_remain * BASE_BLOCK_SIZE;
+            remain[0].out = gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE;
+            remain[0].k = column_remain;
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::cube1(
+    PhyAddrCube1<TYPE, WORKSPACE_TYPE> *src, PhyAddrCube1<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t switch_flag = 0;
+    int32_t l0a_offset_remain = 0;
+
+    auto l1_base_a_cube1_tensor = l1_a_ping_pong_flag_ ? l1_a_pong_tensor : l1_a_ping_tensor;
+    WAIT_FLAG(MTE1, MTE2, l1_a_ping_pong_flag_);
+
+    auto commonNd2NzParams = AscendC::Nd2NzParams(
+        1,            // ndNum
+        BASE_LEN,     // nValue
+        BASE_LEN,     // dValue
+        0,            // srcNdMatrixStride, unused
+        BASE_LEN,     // srcDValue
+        BASE_LEN,     // dstNzC0Stride
+        1,            // dstNzNStride
+        0             // dstNzMatrixStride, unused
+    );
+    for (int i = 0; i < 2; i++) {
+        if (src[i].k == 0) {
+            continue;
+        }
+        AscendC::DataCopy(
+            l1_base_a_cube1_tensor[BASE_BLOCK_SIZE * i],
+            this->gm_Q_tensor[(uint32_t(src[i].left - this->gm_a_cube1))],
+            commonNd2NzParams
+        );
+        switch_flag++;
+    }
+
+    if (switch_flag == 2) {
+        l0a_offset_remain = BASE_BLOCK_SIZE;
+    } else if (src[0].k == 0 && src[1].k == 0) {
+        for (int i = 0; i < 2; i++) {
+            if (remain[i].k == 0) {
+                continue;
+            }
+            AscendC::DataCopy(
+                l1_base_a_cube1_tensor[BASE_BLOCK_SIZE * i],
+                this->gm_Q_tensor[(uint32_t(remain[i].left - this->gm_a_cube1))],
+                commonNd2NzParams
+            );
+            switch_flag++;
+        }
+    } else if (remain[0].k != 0 && src[0].left != remain[0].left) {
+        l0a_offset_remain = BASE_BLOCK_SIZE;
+        AscendC::DataCopy(
+            l1_base_a_cube1_tensor[BASE_BLOCK_SIZE],
+            this->gm_Q_tensor[(uint32_t(remain[0].left - this->gm_a_cube1))],
+            commonNd2NzParams
+        );
+        switch_flag++;
+    } else if (remain[1].k != 0) {
+        AscendC::DataCopy(
+            l1_base_a_cube1_tensor[BASE_BLOCK_SIZE],
+            this->gm_Q_tensor[(uint32_t(remain[1].left - this->gm_a_cube1))],
+            commonNd2NzParams
+        );
+        switch_flag++;
+    }
+
+    SET_FLAG(MTE2, MTE1, l1_a_ping_pong_flag_);
+    WAIT_FLAG(MTE2, MTE1, l1_a_ping_pong_flag_);
+
+    // 处理完整基本块
+    base_block_mad(src[0], src[1]);
+    // 处理尾块
+    if (remain[0].k + remain[1].k != 0) {
+        base_block_mad(remain[0], remain[1], l0a_offset_remain);
+    }
+
+    SET_FLAG(MTE1, MTE2, l1_a_ping_pong_flag_);
+    l1_a_ping_pong_flag_ = 1 - l1_a_ping_pong_flag_;
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::base_block_mad(
+    PhyAddrCube1<TYPE, WORKSPACE_TYPE> addr_1, PhyAddrCube1<TYPE, WORKSPACE_TYPE> addr_2,
+     int32_t l0a_offset_remain)
+{
+    auto remain_offset = (l0a_offset_remain == -1) ? 0 : 128 / Y * 128 * local_block_index;
+
+    // 判断循环的次数
+    int32_t loopTimes = 1;
+    if (addr_2.k != 0) {
+        loopTimes = 2;
+    }
+    // todo:
+    for (int32_t k = 0; k < loopTimes; k++) {
+
+        auto l0_a_buf_tensor = l0_a_ping_pong_flag_ ? l0_a_pong_tensor : l0_a_ping_tensor;
+        auto l1_base_a_cube1_tensor = l1_a_ping_pong_flag_ ? l1_a_pong_tensor : l1_a_ping_tensor;
+        // 左矩阵加载进L0A
+        WAIT_FLAG(M, MTE1, l0_a_ping_pong_flag_);
+        auto commonLoadData2dParamsNoTranspose = AscendC::LoadData2dParams(
+            0,                // startIndex
+            REPEAT_TIME_8,    // repeatTimes
+            SRC_STRIDE_8,     // srcStride
+            0,                // sid
+            0,                // dstGap
+            false,            // ifTranspose
+            0                 // addrMode
+        );
+        if (l0a_offset_remain == -1 || l0a_offset_remain == 0) {
+            for (int32_t i = 0; i < SIZE_128 / BLOCK_SIZE; i++) {  // 按照Q的行16为单位循环搬入
+                AscendC::LoadData(
+                    l0_a_buf_tensor[i * SIZE_128 * BLOCK_SIZE],
+                    l1_base_a_cube1_tensor[k * BASE_BLOCK + i * CUBE_MATRIX_SIZE],
+                    commonLoadData2dParamsNoTranspose
+                );
+            }
+        } else {
+            for (int32_t i = 0; i < SIZE_128 / BLOCK_SIZE; i++) {  // 按照Q的行16为单位循环搬入
+                AscendC::LoadData(
+                    l0_a_buf_tensor[i * SIZE_128 * BLOCK_SIZE],
+                    l1_base_a_cube1_tensor[BASE_BLOCK + i * CUBE_MATRIX_SIZE],
+                    commonLoadData2dParamsNoTranspose
+                );
+            }
+        }
+
+        // 加载右矩阵
+        auto addr_temp = k == 0 ? addr_1 : addr_2;
+        auto gm_k = addr_temp.right;
+        auto gm_c = addr_temp.out + remain_offset;
+        for (int32_t j = 0; j < addr_temp.k; j++) {  // (H+1)/Y
+            // 右矩阵：GM -> L1
+            WAIT_FLAG(MTE1, MTE2, l1_b_ping_pong_flag_ + 2);
+
+            auto l1_buf_b_tensor = l1_b_ping_pong_flag_ ? l1_b_pong_tensor : l1_b_ping_tensor;
+            AscendC::DataCopy(
+                l1_buf_b_tensor,
+                this->gm_K_tensor[(uint32_t(gm_k - this->gm_b_cube1))],
+                AscendC::Nd2NzParams(
+                    1,         // ndNum
+                    SIZE_128,  // nValue
+                    SIZE_128,  // dValue
+                    0,         // srcNdMatrixStride, unused
+                    SIZE_128,  // srcDValue
+                    SIZE_128,  // dstNzC0Stride
+                    1,         // dstNzNStride
+                    0          // dstNzMatrixStride, unused
+                )
+            );
+
+            SET_FLAG(MTE2, MTE1, l1_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE2, MTE1, l1_b_ping_pong_flag_ + 2);
+
+            auto l0_b_buf_tensor = l0_b_ping_pong_flag_ ? l0_b_pong_tensor : l0_b_ping_tensor;
+            auto l0_c_buf_tensor = l0_c_ping_pong_flag_ ? l0_c_pong_tensor : l0_c_ping_tensor;
+
+            WAIT_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            commonLoadData2dParamsNoTranspose.repeatTimes = REPEAT_TIME_64;
+            commonLoadData2dParamsNoTranspose.srcStride = SRC_STRIDE_1;
+            AscendC::LoadData(
+                l0_b_buf_tensor,
+                l1_buf_b_tensor,
+                commonLoadData2dParamsNoTranspose
+            );
+
+            SET_FLAG(MTE1, MTE2, l1_b_ping_pong_flag_ + 2);
+            SET_FLAG(MTE1, M, l0_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE1, M, l0_b_ping_pong_flag_ + 2);
+
+            auto l0_a_remain_offset = 0;
+            auto m_mad = BASE_LEN;
+            if (l0a_offset_remain != -1) {
+                l0_a_remain_offset = remain_offset;
+                m_mad = BASE_LEN / Y;
+            }
+
+            int unit_flag = 0b11;
+            AscendC::Mmad(
+                l0_c_buf_tensor,                     // dstLocal
+                l0_a_buf_tensor[l0_a_remain_offset], // fmLocal
+                l0_b_buf_tensor,                     // filterLocal
+                AscendC::MmadParams(
+                    m_mad,       // m
+                    SIZE_128,    // n
+                    SIZE_128,    // k
+                    unit_flag,   // unitFlag
+                    false,       // cmatrixSource
+                    true         // cmatrixInitVal
+                )
+            );
+
+            SET_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            auto intriParams = AscendC::FixpipeParamsV220(
+                SIZE_128,   // nSize
+                m_mad,      // mSize
+                m_mad,      // srcStride
+                SIZE_128,   // dstStride
+                false       // reluEn
+            );
+            intriParams.quantPre = QuantMode_t::NoQuant;
+            intriParams.unitFlag = unit_flag;
+            AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                gm_c_cube1_tensor[gm_c - this->gm_c_cube1], // dstGlobal
+                l0_c_buf_tensor,                            // srcLocal
+                intriParams                                 // intriParams
+            );
+
+            l0_b_ping_pong_flag_ = 1 - l0_b_ping_pong_flag_;
+            l0_c_ping_pong_flag_ = 1 - l0_c_ping_pong_flag_;
+            l1_b_ping_pong_flag_ = 1 - l1_b_ping_pong_flag_;
+
+            gm_k += BASE_BLOCK_SIZE;
+            gm_c += BASE_BLOCK_SIZE;
+        }
+
+        SET_FLAG(M, MTE1, l0_a_ping_pong_flag_);
+        l0_a_ping_pong_flag_ = 1 - l0_a_ping_pong_flag_;
+    }
+}
+
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::loadRowSumOnesblock()
+{
+    AscendC::DataCopy(
+        l1_row_sum_1_buf_tensor,
+        this->gm_ones_tensor[(uint32_t(gm_ones - this->gm_ones))],
+        AscendC::Nd2NzParams(
+            1,          // ndNum
+            BASE_LEN,   // nValue
+            SIZE_16,    // dValue
+            0,          // srcNdMatrixStride, unused
+            BASE_LEN,   // srcDValue
+            BASE_LEN,   // dstNzC0Stride
+            1,          // dstNzNStride
+            0           // dstNzMatrixStride, unused
+        )
+    );
+
+    SET_FLAG(MTE2, MTE1, EVENT_ID7);
+    WAIT_FLAG(MTE2, MTE1, EVENT_ID7);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::addrMapping_cube2_rowsum(int32_t roundId,
+    int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t curCalcRow = roundId * F + core_group_index;  // 当前的row
+    int32_t curCalcRowP = F * (roundId % 2) + core_group_index;
+    int32_t b = curCalcRow / row_per_batch;           // 当前行所在的batch号
+    int32_t n = (curCalcRow % row_per_batch) / L;     // 当前batch下的head号
+    int32_t i_r = (curCalcRow % row_per_batch) % L;   // 当前head下的行号
+    int32_t i_c = local_block_index * column_per_core;  // 当前core处理的起始列
+    int32_t begin_remain =
+        this->sparseMode == 1 ? (W + 1 - column_remain) : (H1 + 1 - column_remain);  // sparse or triangle
+
+    int32_t q_left_index = this->sparseMode == 1 ? (W / 2 + i_r) : (L + i_r);           // sparse or triangle
+    int32_t q_right_index = this->sparseMode == 1 ? (W / 2 - 1 - i_r) : (L - 1 - i_r);  // sparse or triangle
+    int32_t switch_index = q_left_index;
+
+    // 设置sparse的标志位 以及 sparse场景非倒三角部分的偏移
+    int32_t sparse_flag = false;
+    if (this->sparseMode == 1) {
+        sparse_flag = i_r > ((W - 1) / 2) ? true : false;
+    }
+    int32_t row_down = i_r + W / 2;
+    int32_t col_down = i_r + i_c - W / 2;
+    int32_t col_down_remain = i_r + begin_remain - W / 2;
+
+    int32_t g = n / nG;
+    int64_t bn_offset = (b * N * H1 + n * H1) * BASE_BLOCK_SIZE;
+    int64_t bn_offset_rowsum = (b * N * H1 + n * H1) * 128;
+    int64_t bn_offset_GQA = (b * G * H1 + g * H1) * BASE_BLOCK_SIZE;
+    int64_t q_left_offset = bn_offset + q_left_index * BASE_BLOCK_SIZE;
+    int64_t q_right_offset = bn_offset + q_right_index * BASE_BLOCK_SIZE;
+    // int64_t out_offset = (curCalcRow * (H + 1)) * BASE_BLOCK_SIZE;
+    int64_t out_offset = this->sparseMode == 1 ? ((curCalcRowP * (W + 1)) * BASE_BLOCK_SIZE)
+                                               : ((curCalcRowP * (H1 + 1)) * BASE_BLOCK_SIZE);  // sparse or triangle
+
+    if (!sparse_flag && switch_index < i_c) {
+        src[0].left = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube2 + bn_offset_GQA + (i_c - switch_index - 1) * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = rowsum_gm + bn_offset_rowsum + q_right_index * 128;
+        src[0].k = column_per_core;
+        src_length = 1;
+
+        if (column_remain != 0) {
+            remain[0].left =
+                gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+            remain[0].right = gm_b_cube2 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+            remain[0].out = src[0].out + (128 / Y) * local_block_index * 128;
+            remain[0].rowsumOut = src[0].rowsumOut + (128 / Y) * local_block_index;
+            remain[0].k = column_remain;
+            remain_length = 1;
+        }
+    } else if (!sparse_flag && i_c <= switch_index && i_c + column_per_core > switch_index + 1) {
+        src[0].left = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube2 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube2 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = rowsum_gm + bn_offset_rowsum + q_left_index * 128;
+        src[0].k = switch_index - i_c + 1;
+
+        src[1].left = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE + src[0].k * BASE_BLOCK_SIZE;
+        src[1].right = gm_b_cube2 + bn_offset_GQA;
+        src[1].out = gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+        src[1].rowsumOut = rowsum_gm + bn_offset_rowsum + q_right_index * 128;
+        src[1].k = column_per_core - src[0].k;
+
+        src_length = 2;
+
+        if (column_remain != 0) {
+            remain[0].left =
+                gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+            remain[0].right = gm_b_cube2 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+            remain[0].out = src[1].out + (128 / Y) * local_block_index * 128;
+            remain[0].rowsumOut = src[1].rowsumOut + (128 / Y) * local_block_index;
+            remain[0].k = column_remain;
+            remain_length = 1;
+        }
+    } else if (!sparse_flag && i_c <= switch_index && i_c + column_per_core <= switch_index + 1) {
+        src[0].left = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube2 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube2 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = rowsum_gm + bn_offset_rowsum + q_left_index * 128;
+        src[0].k = column_per_core;
+        src_length = 1;
+
+        if (column_remain != 0) {
+            if (switch_index < begin_remain) {
+                remain[0].left =
+                    gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+                remain[0].right = gm_b_cube2 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+                remain[0].out =
+                    gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+                remain[0].rowsumOut =
+                    rowsum_gm + bn_offset_rowsum + q_right_index * 128 + (128 / Y) * local_block_index;
+                remain[0].k = column_remain;
+                remain_length = 1;
+            } else {
+                remain[0].left =
+                    gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+                remain[0].right = gm_b_cube2 + bn_offset_GQA + begin_remain * BASE_BLOCK_SIZE;
+                remain[0].out =
+                    gm_c_cube2 + bn_offset + q_left_index * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+                remain[0].rowsumOut =
+                    rowsum_gm + bn_offset_rowsum + q_left_index * 128 + (128 / Y) * local_block_index;
+                remain[0].k = switch_index - begin_remain + 1;
+
+                remain[1].left = remain[0].left + remain[0].k * BASE_BLOCK_SIZE;
+                remain[1].right = gm_b_cube2 + bn_offset_GQA;
+                remain[1].out =
+                    gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+                remain[1].rowsumOut =
+                    rowsum_gm + bn_offset_rowsum + q_right_index * 128 + (128 / Y) * local_block_index;
+                remain[1].k = column_remain - remain[0].k;
+
+                remain_length = 2;
+            }
+        }
+    } else {
+        src[0].left = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = gm_b_cube2 + bn_offset_GQA + col_down * BASE_BLOCK_SIZE;
+        src[0].out = gm_c_cube2 + bn_offset + row_down * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = rowsum_gm + bn_offset_rowsum + row_down * 128;
+        src[0].k = column_per_core;
+        src_length = 1;
+
+        if (column_remain != 0) {
+            remain[0].left =
+                gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+            remain[0].right = gm_b_cube2 + bn_offset_GQA + col_down_remain * BASE_BLOCK_SIZE;
+            remain[0].out = src[0].out + (128 / Y) * local_block_index * 128;
+            remain[0].rowsumOut = src[0].rowsumOut + (128 / Y) * local_block_index;
+            remain[0].k = column_remain;
+            remain_length = 1;
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::addrMapping_cube2_rowsum_nomask(int32_t roundId,
+    int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t curCalcRow = roundId * F + core_group_index;  // 当前的row 0
+    int32_t curCalcRowP = F * (roundId % 2) + core_group_index;
+    int32_t b = curCalcRow / row_per_batch;               // 当前行所在的batch号
+    int32_t n = (curCalcRow % row_per_batch) / L;         // 当前batch下的head号
+    int32_t i_r = (curCalcRow % row_per_batch) % L;       // 当前head下的行号
+    int32_t i_c =  local_block_index * column_per_core ;    // 当前core处理的起始列
+    int32_t begin_remian = H2 - column_remain;
+
+    int32_t q_left_index = i_r;
+    int32_t q_right_index = L - 1 - i_r;
+    int32_t switch_index = H2;
+
+    int32_t g = n / nG;
+    int64_t bn_offset_1 = (b * N * H1 + n * H1) * BASE_BLOCK_SIZE;
+    int64_t bn_offset_rowsum = (b * N * H1 + n * H1) * 128;
+    int64_t bn_offset_2 = (b * G * H2 + g * H2) * BASE_BLOCK_SIZE;  // GQA version of bn_offset_2
+    int64_t out_offset = (curCalcRowP * H2) * BASE_BLOCK_SIZE;
+
+    src[0].left = gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+    src[0].right = gm_b_cube2 + bn_offset_2 + i_c * BASE_BLOCK_SIZE;
+    src[0].out = gm_c_cube2 + bn_offset_1 + q_left_index * BASE_BLOCK_SIZE;
+    src[0].rowsumOut = rowsum_gm + bn_offset_rowsum + q_left_index * 128;
+    src[0].k = column_per_core;
+    src_length = 1;
+    if (column_remain != 0) {
+        remain[0].left = gm_c_cube1 + out_offset + begin_remian *
+            BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+        remain[0].right = gm_b_cube2 + bn_offset_2 + begin_remian * BASE_BLOCK_SIZE;
+        remain[0].out = gm_c_cube2 + bn_offset_1 + q_left_index *
+            BASE_BLOCK_SIZE + (128 / Y) * local_block_index * 128;
+        remain[0].rowsumOut = rowsum_gm + bn_offset_rowsum +
+            q_left_index * 128 + (128 / Y) * local_block_index;
+        remain[0].k = column_remain;
+        remain_length = 1;
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::unified_addrMapping_cube2mix(int32_t roundId,
+    int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain)
+{
+    if (this->qk_triangle == 1) {  // 倒三角
+        addrMapping_cube2_rowsum(roundId, src_length, remain_length, src, remain);
+    } else if (this->qk_triangle == 0) {  // 非倒三角
+        addrMapping_cube2_rowsum_nomask(roundId, src_length, remain_length, src, remain);
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::cube2_rowsum_mix_only(
+    const PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *cube_addr, int32_t cube_length, int32_t m_length,
+    int32_t vcore_num_per_head) {
+    int32_t l1_m = CUBE2_LENGTH_M;    // 128
+    int32_t l1_k = CUBE2_LENGTH_K;
+    int32_t l1_n = CUBE2_LENGTH_N;
+    int32_t l0_k = BASE_BLOCK_LENGTH; // 128
+    int32_t l0_m = BASE_BLOCK_LENGTH; // 128
+    int32_t l0_n = BASE_BLOCK_LENGTH; // 128
+
+    for (int32_t i = 0; i < cube_length; i++) {
+        __gm__ TYPE *__restrict__ left_start_addr = (__gm__ TYPE *)(cube_addr[i].left);  // 高精度是需要将float转为TYPE
+        __gm__ TYPE *__restrict__ right_start_addr = cube_addr[i].right;
+        __gm__ float *__restrict__ result_gm = cube_addr[i].out;
+        __gm__ float *__restrict__ rowsum_out_gm = cube_addr[i].rowsumOut;
+
+        int32_t k_core_gm = cube_addr[i].k * l0_k;
+        l1_k = k_core_gm < l1_k ? k_core_gm : l1_k;
+        int32_t loop_l1_times = k_core_gm / l1_k;
+        int32_t l0_k_block_num = l0_k / BLOCK_SIZE; // 128/16
+        int32_t l0_m_block_num = m_length / BLOCK_SIZE; // 128/16
+        int32_t l0_n_block_num = l0_n / BLOCK_SIZE; // 8
+
+        for (int32_t l1_loop_index = 0; l1_loop_index < loop_l1_times; l1_loop_index++) {
+            auto l1_buf_a_cube2_tensor = l1_a_ping_pong_flag_ ? l1_a_pong_tensor : l1_a_ping_tensor;
+            auto l1_buf_b_cube2_tensor = l1_b_ping_pong_flag_ ? l1_b_pong_tensor : l1_b_ping_tensor;
+            auto l0a_buf_tensor = l0_a_ping_pong_flag_ ? l0_a_pong_tensor: l0_a_ping_tensor;
+            auto l0b_buf_tensor = l0_b_ping_pong_flag_ ? l0_b_pong_tensor: l0_b_ping_tensor;
+            auto l0c_buf_tensor = l0_c_ping_pong_flag_ ? l0_c_pong_tensor: l0_c_ping_tensor;
+
+            WAIT_FLAG(MTE1, MTE2, l1_a_ping_pong_flag_);
+
+            AscendC::DataCopy(
+                l1_buf_a_cube2_tensor,
+                this->gm_c_cube1_tensor_type[(uint32_t(left_start_addr - (__gm__ TYPE *)this->gm_c_cube1)
+                + l1_loop_index * l0_m * l0_k * 2)],
+                AscendC::Nd2NzParams(
+                    vcore_num_per_head,
+                    m_length / vcore_num_per_head,
+                    l0_k,
+                    m_length * l0_k / vcore_num_per_head * 2,
+                    l0_k,
+                    m_length,
+                    1,
+                    m_length / vcore_num_per_head * BLOCK_SIZE
+                )
+            );
+
+            SET_FLAG(MTE2, MTE1, l1_a_ping_pong_flag_);
+            WAIT_FLAG(MTE2, MTE1, l1_a_ping_pong_flag_);
+            WAIT_FLAG(M, MTE1, l0_a_ping_pong_flag_);
+
+            for (int32_t l0a_load_idx = 0; l0a_load_idx < l0_k_block_num; l0a_load_idx++) {
+                AscendC::LoadData(
+                    l0a_buf_tensor[l0a_load_idx * CUBE_MATRIX_SIZE],
+                    l1_buf_a_cube2_tensor[l0a_load_idx * l0_m_block_num * CUBE_MATRIX_SIZE],
+                    AscendC::LoadData2dParams(
+                        0,                  // startIndex
+                        l0_m_block_num,     // repeatTimes
+                        1,                  // srcStride
+                        0,                  // sid
+                        l0_k_block_num - 1, // dstGap
+                        false,              // ifTranspose
+                        0                   // addrMode
+                    )
+                );
+            }
+
+            SET_FLAG(MTE1, MTE2, l1_a_ping_pong_flag_);
+            WAIT_FLAG(MTE1, MTE2, l1_b_ping_pong_flag_ + 2);
+
+            AscendC::DataCopy(
+                l1_buf_b_cube2_tensor,
+                this->gm_V_tensor[(uint32_t(right_start_addr - this->gm_b_cube2) + l1_loop_index * l0_k * l0_n)],
+                AscendC::Nd2NzParams(
+                    1,            // ndNum
+                    l0_k,         // nValue
+                    l0_n,         // dValue
+                    l0_k * l0_n,  // srcNdMatrixStride
+                    l0_n,         // srcDValue
+                    l0_k,         // dstNzC0Stride
+                    1,            // dstNzNStride
+                    l0_k * l0_n   // dstNzMatrixStride
+                )
+            );
+
+            SET_FLAG(MTE2, MTE1, l1_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE2, MTE1, l1_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            for (int32_t l0b_load_idx = 0; l0b_load_idx < l0_k_block_num; l0b_load_idx++) {
+                AscendC::LoadData(
+                    l0b_buf_tensor[l0b_load_idx * l0_n * BLOCK_SIZE],
+                    l1_buf_b_cube2_tensor[l0b_load_idx * CUBE_MATRIX_SIZE],
+                    AscendC::LoadData2dParams(
+                        0,              // startIndex
+                        l0_n_block_num, // repeatTimes
+                        l0_k_block_num, // srcStride
+                        0,              // sid
+                        0,              // dstGap
+                        true,           // ifTranspose
+                        0               // addrMode
+                    )
+                );
+            }
+
+            SET_FLAG(MTE1, MTE2, l1_b_ping_pong_flag_ + 2);
+            SET_FLAG(MTE1, M, l0_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE1, M, l0_b_ping_pong_flag_ + 2);
+
+            bool init_c = (l1_loop_index == 0);
+            bool out_c = (l1_loop_index == loop_l1_times - 1);
+            int unit_flag = 0b10;
+            if (out_c) {
+                unit_flag = 0b11;
+            }
+
+            AscendC::Mmad(
+                l0c_buf_tensor,                  // dstLocal
+                l0a_buf_tensor,                  // fmLocal
+                l0b_buf_tensor,                  // filterLocal
+                AscendC::MmadParams(
+                    m_length,                    // m
+                    l0_n,                        // n
+                    l0_k,                        // k
+                    unit_flag,                   // unitFlag
+                    false,                       // cmatrixSource
+                    (init_c == 1) ? true : false // cmatrixInitVal
+                )
+            );
+
+            SET_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            if (out_c) {
+                AscendC::SetAtomicAdd<float>();
+                AscendC::SetAtomicType<float>();
+                auto intriParams = AscendC::FixpipeParamsV220(
+                    l0_n,     // nSize
+                    m_length, // mSize
+                    m_length, // srcStride
+                    l0_n,     // dstStride
+                    false     // reluEn
+                );
+                intriParams.quantPre = QuantMode_t::NoQuant;
+                intriParams.unitFlag = unit_flag;
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    gm_c_cube2_tensor[result_gm - this->gm_c_cube2], // dstGlobal
+                    l0c_buf_tensor,                                  // srcLocal
+                    intriParams                                      // intriParams
+                );
+                AscendC::SetAtomicNone();
+            }
+
+            l0_b_ping_pong_flag_ = 1 - l0_b_ping_pong_flag_;
+            l0_c_ping_pong_flag_ = 1 - l0_c_ping_pong_flag_;
+            l0b_buf_tensor = l0_b_ping_pong_flag_ ? l0_b_pong_tensor: l0_b_ping_tensor;
+            l0c_buf_tensor = l0_c_ping_pong_flag_ ? l0_c_pong_tensor: l0_c_ping_tensor;
+
+            // rowsum: 将全1矩阵从L1搬运到L0b
+            WAIT_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            SET_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+            SET_FLAG(M, MTE1, l0_a_ping_pong_flag_);
+
+            l1_a_ping_pong_flag_ = 1 - l1_a_ping_pong_flag_;
+            l1_b_ping_pong_flag_ = 1 - l1_b_ping_pong_flag_;
+            l0_a_ping_pong_flag_ = 1 - l0_a_ping_pong_flag_;
+            l0_b_ping_pong_flag_ = 1 - l0_b_ping_pong_flag_;
+            l0_c_ping_pong_flag_ = 1 - l0_c_ping_pong_flag_;
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward<TYPE, IF_BF16, WORKSPACE_TYPE>::mix_cube2_rowsum(int32_t cube2_roundId)
+{
+    // 混算寻址：
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> src[2];
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> remain[2];
+    int64_t src_length = 0;
+    int64_t remain_length = 0;
+    unified_addrMapping_cube2mix(cube2_roundId, src_length, remain_length, src, remain);  // cube2和rowsum的寻址
+
+    // 开始混算：
+    cube2_rowsum_mix_only(src, src_length, BASE_BLOCK_LENGTH, this->Y * 2);
+    if (remain_length > 0) {
+        cube2_rowsum_mix_only(remain, remain_length, BASE_BLOCK_LENGTH / this->Y, 2);
+    }
+}
+
+#endif
+
+#endif  // __CUBEFORWARD_H__
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/CubeForward_192.h b/src/kernels/mixkernels/laser_attention/op_kernel/CubeForward_192.h
new file mode 100644
index 00000000..a6e3088d
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/CubeForward_192.h
@@ -0,0 +1,1035 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef __CUBEFORWARD_192_H__
+#define __CUBEFORWARD_192_H__
+
+#define USE_ASCENDC
+#include "ppmatmul_const.h"
+#include "AddressMappingForward.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/utils.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/iterator.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_CUBE__
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE> class CubeForward_192 {
+public:
+    __aicore__ inline CubeForward_192(){};
+    __aicore__ inline void Init(__gm__ uint8_t *__restrict__ a_cube1, __gm__ uint8_t *__restrict__ b_cube1,
+                                __gm__ uint8_t *__restrict__ b_cube2, __gm__ uint8_t *__restrict__ c_cube1,
+                                __gm__ float *__restrict__ c_cube2, __gm__ uint8_t *__restrict__ ones_rowsum,
+                                __gm__ float *__restrict__ gm_rowsum, int32_t Y, int32_t F, int32_t B, int32_t N,
+                                int32_t S1, int32_t S2, int32_t D, int32_t nG, int32_t qk_triangle, int32_t sparseMode,
+                                int32_t windowLength);
+    __aicore__ inline void PresetFlag();
+    __aicore__ inline void ClearFlag();
+    __aicore__ inline void SetHighPrecision(bool isHighPrecision)
+    {
+        this->isHighPrecision = isHighPrecision;
+    };
+    __aicore__ inline void Run();
+
+    // headDim = 192的cube1模块
+    __aicore__ __inline__ void
+    cube1_headDim192(Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> *src,
+                     Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> *remain);
+    __aicore__ __inline__ void
+    base_block_mad_headDim192(Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> addr_1,
+                              Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> addr_2,
+                              int32_t l0a_offset_remain = -1);
+
+protected:
+    __aicore__ __inline__ void loadRowSumOnesblock(); // 加载rowsum右全1矩阵
+    __aicore__ __inline__ void addrMapping_cube2_rowsum_192(
+        int32_t roundId, int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+        PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain); // 混算中cube2、rowsum寻址（倒三角+sparse）
+    __aicore__ __inline__ void addrMapping_cube2_rowsum_nomask_192(
+        int32_t roundId, int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+        PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain); // 混算中cube2、rowsum寻址（nomask+不等长）
+    __aicore__ __inline__ void
+    unified_addrMapping_cube2mix_192(int32_t roundId, int64_t &src_length, int64_t &remain_length,
+                                     PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+                                     PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain); // cube2和rowsum混算的寻址
+    __aicore__ __inline__ void cube2_rowsum_mix_only_192(const PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *cube_addr,
+                                                         int32_t cube_length, int32_t m_length,
+                                                         int32_t vcore_num_per_head); // cube2和row_sum混算的基本单元
+    __aicore__ __inline__ void mix_cube2_rowsum_192(int32_t cube2_roundId);           // cube2、rowsum的混算
+protected:
+    Address_192::AddressMappingForward<TYPE> address; // 注入前向寻址模块
+
+    AsdopsBuffer<ArchType::ASCEND_V220> asdopsBuf;
+
+    __gm__ TYPE *__restrict__ gm_a_cube1;
+    __gm__ TYPE *__restrict__ gm_b_cube1;
+    __gm__ TYPE *__restrict__ gm_b_cube2;
+    __gm__ WORKSPACE_TYPE *__restrict__ gm_c_cube1;
+    __gm__ float *__restrict__ gm_c_cube2;
+    __gm__ TYPE *__restrict__ gm_ones;
+    __gm__ float *__restrict__ rowsum_gm;
+
+    // init gm input tensor
+    GlobalTensor<TYPE> gm_Q_tensor;
+    GlobalTensor<TYPE> gm_K_tensor;
+    GlobalTensor<TYPE> gm_V_tensor;
+
+    // init gm output tensor
+    GlobalTensor<WORKSPACE_TYPE> gm_c_cube1_tensor;
+    GlobalTensor<TYPE> gm_c_cube1_tensor_type;
+    GlobalTensor<float> gm_c_cube2_tensor;
+    GlobalTensor<TYPE> gm_ones_tensor;
+    GlobalTensor<float> gm_rowsum_tensor;
+
+    LocalTensor<TYPE> l1_base_b_cube1_tensor;
+    LocalTensor<TYPE> l1_base_a_cube2_tensor;
+    LocalTensor<TYPE> l1_base_b_cube2_tensor;
+    LocalTensor<TYPE> l1_a_ping_tensor;
+    LocalTensor<TYPE> l1_a_pong_tensor;
+    LocalTensor<TYPE> l1_b_ping_tensor;
+    LocalTensor<TYPE> l1_b_pong_tensor;
+    LocalTensor<TYPE> l1_row_sum_1_buf_tensor;
+
+    uint32_t l1_a_ping_pong_flag_ = 0;
+    uint32_t l1_b_ping_pong_flag_ = 0;
+
+    uint32_t l0_a_ping_pong_flag_ = 0;
+    uint32_t l0_b_ping_pong_flag_ = 0;
+    uint32_t l0_c_ping_pong_flag_ = 0;
+
+    LocalTensor<TYPE> l0_a_ping_tensor;
+    LocalTensor<TYPE> l0_a_pong_tensor;
+    LocalTensor<TYPE> l0_b_ping_tensor;
+    LocalTensor<TYPE> l0_b_pong_tensor;
+    LocalTensor<float> l0_c_ping_tensor;
+    LocalTensor<float> l0_c_pong_tensor;
+
+    // Y个core处理一个完整行，所有core分成 F 组， block_dim = F * Y
+    int32_t Y;
+    int32_t F;
+
+    // Q K V shape : [B,N,S,D] 其中 D 固定为128
+    int32_t B;
+    int32_t N;
+    // int32_t S; //256 - 128k (256的倍数) 方阵的行
+    int32_t S1; // 256 - 128k (256的倍数) 方阵的行
+    int32_t S2; // 256 - 128k (256的倍数) 方阵的行
+    int32_t D;
+    int32_t nG;
+    int32_t G;            // GQA 场景  k v shape [B,G,S,D]
+    int32_t qk_triangle;  // GQA 场景  k v shape [B,G,S,D]
+    int32_t sparseMode;   // sparseMode: 0:dense, 1:sparse
+    int32_t windowLength; // sparse场景下，滑动窗口的长度
+
+    // int32_t H;
+    int32_t H1;
+    int32_t H2;
+    int32_t L; // 负责均衡
+    int32_t W;
+
+    int32_t curCoreIndex;
+    int32_t local_block_index; // 组内 core id [0, Y-1], 组内第几个 0
+    int32_t core_group_index;  // [0, F-1], 第几组
+
+    int32_t row_per_batch;
+    int32_t column_per_core;
+    int32_t column_remain;
+
+    bool isHighPrecision = true;
+};
+
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::PresetFlag()
+{
+    SET_FLAG(MTE1, MTE2, EVENT_ID0);
+    SET_FLAG(MTE1, MTE2, EVENT_ID1);
+    SET_FLAG(MTE1, MTE2, EVENT_ID2);
+    SET_FLAG(MTE1, MTE2, EVENT_ID3);
+
+    SET_FLAG(M, MTE1, EVENT_ID0);
+    SET_FLAG(M, MTE1, EVENT_ID1);
+    SET_FLAG(M, MTE1, EVENT_ID2);
+    SET_FLAG(M, MTE1, EVENT_ID3);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::ClearFlag()
+{
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+
+    WAIT_FLAG(M, MTE1, EVENT_ID0);
+    WAIT_FLAG(M, MTE1, EVENT_ID1);
+    WAIT_FLAG(M, MTE1, EVENT_ID2);
+    WAIT_FLAG(M, MTE1, EVENT_ID3);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::loadRowSumOnesblock()
+{
+    AscendC::DataCopy(l1_row_sum_1_buf_tensor, this->gm_ones_tensor[(uint32_t(gm_ones - this->gm_ones))],
+                      AscendC::Nd2NzParams(1,   // ndNum
+                                           128, // nValue
+                                           16,  // dValue
+                                           0,   // srcNdMatrixStride, unused
+                                           128, // srcDValue
+                                           128, // dstNzC0Stride
+                                           1,   // dstNzNStride
+                                           0    // dstNzMatrixStride, unused
+                                           ));
+    SET_FLAG(MTE2, MTE1, EVENT_ID7);
+    WAIT_FLAG(MTE2, MTE1, EVENT_ID7);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::Init(
+    __gm__ uint8_t *__restrict__ a_cube1, __gm__ uint8_t *__restrict__ b_cube1, __gm__ uint8_t *__restrict__ b_cube2,
+    __gm__ uint8_t *__restrict__ c_cube1, __gm__ float *__restrict__ c_cube2, __gm__ uint8_t *__restrict__ ones_rowsum,
+    __gm__ float *__restrict__ gm_rowsum, int32_t y_cube_num_per_line, int32_t group_num, int32_t batch,
+    int32_t n_batch,
+    // int32_t total_length,
+    int32_t qSeqLength, int32_t kSeqLength, int32_t base_length, int32_t headGroupNum, int32_t isTriangle,
+    int32_t sparseMode, int32_t windowLength)
+{
+    this->gm_a_cube1 = (__gm__ TYPE *__restrict__)a_cube1;
+    this->gm_b_cube1 = (__gm__ TYPE *__restrict__)b_cube1;
+    this->gm_b_cube2 = (__gm__ TYPE *__restrict__)b_cube2;
+    this->gm_c_cube1 = (__gm__ WORKSPACE_TYPE *__restrict__)c_cube1;
+    this->gm_c_cube2 = c_cube2;
+    this->gm_ones = (__gm__ TYPE *__restrict__)ones_rowsum;
+    this->rowsum_gm = gm_rowsum;
+
+    this->Y = y_cube_num_per_line;
+    this->F = group_num;
+    this->B = batch;
+    this->N = n_batch;
+    this->S1 = qSeqLength;
+    this->S2 = kSeqLength;
+    this->D = base_length;
+    this->nG = headGroupNum;
+    this->G = this->N / this->nG;
+
+    this->qk_triangle = isTriangle;
+    this->sparseMode = sparseMode;
+    this->windowLength = windowLength;
+
+    // 无mask
+    if (this->qk_triangle == 0) {
+        this->H1 = this->S1 / 128;
+        this->H2 = this->S2 / 128;
+        this->L = this->H1; // 负责均衡
+        this->column_per_core = this->H2 / this->Y;
+        this->column_remain = this->H2 % this->Y;
+    }
+    // 有mask
+    else {
+        this->H1 = this->S1 / 128;
+        this->H2 = this->S2 / 128;
+        this->L = this->H1 / 2; // 负责均衡
+        this->column_per_core = (this->H2 + 1) / this->Y;
+        this->column_remain = (this->H2 + 1) % this->Y;
+
+        // sparse场景：行数、列数以及尾块需要重新设置 update
+        if (this->sparseMode == 1) {
+            this->H1 = this->S1 / 128;
+            this->H2 = this->S2 / 128;
+            this->W = this->windowLength / 128;
+            this->L = this->H1 - this->W / 2;
+            this->column_per_core = (this->W + 1) / this->Y;
+            this->column_remain = (this->W + 1) % this->Y;
+        }
+    }
+
+    this->curCoreIndex = get_block_idx();
+    this->core_group_index = this->curCoreIndex / this->Y;
+    this->local_block_index = this->curCoreIndex % this->Y;
+
+    this->row_per_batch = this->N * this->L;
+
+    // 寻址模块的初始化
+    address.init(this->B, this->N, this->G, this->S1, this->S2, this->D, this->qk_triangle);
+
+    // 设置寻址模块核组信息
+    address.set_core_info(this->Y * this->F, this->F, this->Y, this->curCoreIndex, this->core_group_index,
+                          this->local_block_index);
+
+    // init input gm tensor
+    gm_Q_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_a_cube1));
+    gm_K_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_b_cube1));
+    gm_V_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_b_cube2));
+
+    // init gm output tensor
+    gm_c_cube1_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ WORKSPACE_TYPE *>(this->gm_c_cube1));
+    gm_c_cube1_tensor_type.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_c_cube1));
+    gm_c_cube2_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->gm_c_cube2));
+    gm_ones_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(this->gm_ones));
+    gm_rowsum_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->rowsum_gm));
+
+    // init L1 tensor
+    l1_base_b_cube1_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(128 * 1024);
+    l1_base_a_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(256 * 1024);
+    l1_base_b_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(384 * 1024);
+
+    l1_a_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(0);
+    l1_a_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(128 * 1024);
+    l1_b_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(256 * 1024);
+    l1_b_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(384 * 1024);
+
+    l1_row_sum_1_buf_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(504 * 1024);
+
+    // init L0A/L0B/L0C tensor
+    l0_a_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(0);
+    l0_a_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(32 * 1024);
+    l0_b_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(0);
+    l0_b_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(32 * 1024);
+    l0_c_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+    l0_c_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(64 * 1024);
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::Run()
+{
+    SetPadding(0);
+    uint64_t config = 0x1;
+    AscendC::SetNdParaImpl(config);
+
+    // 等待vector workspace的初始化
+    WaitFlagDev(AIV2AICFLAGID);
+    this->loadRowSumOnesblock(); // 先加载全1矩阵，使其常驻L1
+
+    this->PresetFlag();
+    // 计算每个核的总轮次 Z，  注意：存在余数需要计算当前核释放参与尾行的计算
+    int total_rounds = address.get_total_round();
+
+    if (total_rounds == 1) {
+        if (!address.is_running(0)) {
+            if (this->Y > 1) {
+                FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                WaitFlagDev(AICFLAGID);
+            }
+        } else {
+            Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> src[2];
+            Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> remain[2];
+            address.addrMapping_cube1(this->gm_a_cube1, this->gm_b_cube1, this->gm_c_cube1, 0, src, remain);
+            cube1_headDim192(src, remain);
+
+            if (this->Y > 1) {
+                FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                WaitFlagDev(AICFLAGID);
+            }
+
+            // aic sync aiv
+            FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            WaitFlagDev(AIV2AICFLAGID);
+
+            // cube2与rowsum的混算
+            mix_cube2_rowsum_192(0);
+        }
+    } else {
+        int32_t cube2_index = 0;
+        /************************************CUBE1*2******************************************************/
+        for (int32_t i = 0; i < 2; i++) {
+            if (address.is_running(i)) {
+                Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> src[2];
+                Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> remain[2];
+                address.addrMapping_cube1(this->gm_a_cube1, this->gm_b_cube1, this->gm_c_cube1, i, src, remain);
+                cube1_headDim192(src, remain);
+
+                if (this->Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+                if (i == 0) {
+                    // aic sync aiv
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            } else if (i == (total_rounds - 1)) {
+                if (this->Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+            }
+        }
+        /****************************************** CUBE2 + CUBE1 ***************************************/
+        for (int32_t i = 2; i < total_rounds; i++) {
+            WaitFlagDev(AIV2AICFLAGID);
+            FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+
+            // cube2与rowsum的混算
+            this->mix_cube2_rowsum_192(cube2_index);
+            cube2_index += 1;
+
+            if (address.is_running(i)) {
+                Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> src[2];
+                Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> remain[2];
+                address.addrMapping_cube1(this->gm_a_cube1, this->gm_b_cube1, this->gm_c_cube1, i, src, remain);
+                cube1_headDim192(src, remain);
+
+                // 多核处理一个完整行，需要全核同步
+                if (this->Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+            } else {
+                if (this->Y > 1) {
+                    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+                    WaitFlagDev(AICFLAGID);
+                }
+            }
+        }
+        /************************************CUBE2*2******************************************************/
+        for (int32_t i = 0; i < 2; i++) {
+            if (address.is_running(total_rounds - 2 + i)) {
+                WaitFlagDev(AIV2AICFLAGID);
+                if (i == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+
+                // cube2与rowsum的混算
+                this->mix_cube2_rowsum_192(cube2_index);
+                cube2_index += 1;
+            }
+        }
+    }
+    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+    WaitFlagDev(AICFLAGID);
+
+    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+
+    this->ClearFlag();
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::base_block_mad_headDim192(
+    Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> addr_1,
+    Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> addr_2, int32_t l0a_offset_remain)
+{
+    // 整体思路：将128*192左矩阵（A）切为128*96两块(A1,A2)，将192*128右矩阵(B)切为96*128两块（B1,B2）,
+    // 则C = A * B = A1 * B1 + A2 * B2, 通过两次循环分别计算A1 * B1 和 A2 * B2
+    int32_t headDim = 192;
+    int32_t head_dim_half = 192 / 2;
+    auto gc_remain_offset = (l0a_offset_remain == -1) ? 0 : 128 / this->Y * 128 * this->local_block_index;
+
+    // 判断循环的次数
+    int32_t loopTimes = 1;
+    if (addr_2.k != 0) {
+        loopTimes = 2;
+    }
+
+    for (int32_t k = 0; k < loopTimes; k++) {
+        // 加载右矩阵
+        auto addr_temp = k == 0 ? addr_1 : addr_2;
+        auto gm_k = addr_temp.right;
+        auto gm_c = addr_temp.out + gc_remain_offset;
+        for (int32_t j = 0; j < addr_temp.k; j++) {
+            // 右矩阵：GM -> L1
+            WAIT_FLAG(MTE1, MTE2, this->l1_b_ping_pong_flag_ + 2);
+            auto l1_buf_b_tensor = this->l1_b_ping_pong_flag_ ? this->l1_b_pong_tensor : this->l1_b_ping_tensor;
+
+            AscendC::DataCopy(l1_buf_b_tensor, this->gm_K_tensor[(uint32_t(gm_k - this->gm_b_cube1))],
+                              AscendC::Nd2NzParams(1,   // ndNum
+                                                   128, // nValue
+                                                   256, // dValue
+                                                   0,   // srcNdMatrixStride, unused
+                                                   256, // srcDValue
+                                                   128, // dstNzC0Stride
+                                                   1,   // dstNzNStride
+                                                   0    // dstNzMatrixStride, unused
+                                                   ));
+            SET_FLAG(MTE2, MTE1, this->l1_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE2, MTE1, this->l1_b_ping_pong_flag_ + 2);
+
+            auto l0_c_buf_tensor = this->l0_c_ping_tensor;
+            for (int32_t m = 0; m < 2; m++) {
+                auto l0_a_buf_tensor = this->l0_a_ping_pong_flag_ ? this->l0_a_pong_tensor : this->l0_a_ping_tensor;
+                auto l1_base_a_cube1_tensor =
+                    this->l1_a_ping_pong_flag_ ? this->l1_a_pong_tensor : this->l1_a_ping_tensor;
+                // 左矩阵加载进L0A
+                WAIT_FLAG(M, MTE1, this->l0_a_ping_pong_flag_);
+                // 偏移量的设置
+                auto l1a_offset = m == 0 ? 0 : 128 * 128;
+                auto l0a_offset = m == 0 ? (128 * BLOCK_SIZE) : (64 * BLOCK_SIZE);
+                auto l0a_repeat_times = m == 0 ? 8 : 4;
+                if (l0a_offset_remain == -1 || l0a_offset_remain == 0) {
+                    for (int32_t i = 0; i < 128 / BLOCK_SIZE; i++) {
+                        AscendC::LoadData(l0_a_buf_tensor[i * l0a_offset],
+                                          l1_base_a_cube1_tensor[k * 128 * headDim + l1a_offset + i * CUBE_MATRIX_SIZE],
+                                          AscendC::LoadData2dParams(0,                // startIndex
+                                                                    l0a_repeat_times, // repeatTimes
+                                                                    8,                // srcStride
+                                                                    0,                // sid
+                                                                    0,                // dstGap
+                                                                    false,            // ifTranspose
+                                                                    0                 // addrMode
+                                                                    ));
+                    }
+                } else {
+                    for (int32_t i = 0; i < 128 / BLOCK_SIZE; i++) {
+                        AscendC::LoadData(l0_a_buf_tensor[i * l0a_offset],
+                                          l1_base_a_cube1_tensor[128 * headDim + l1a_offset + i * CUBE_MATRIX_SIZE],
+                                          AscendC::LoadData2dParams(0,                // startIndex
+                                                                    l0a_repeat_times, // repeatTimes
+                                                                    8,                // srcStride
+                                                                    0,                // sid
+                                                                    0,                // dstGap
+                                                                    false,            // ifTranspose
+                                                                    0                 // addrMode
+                                                                    ));
+                    }
+                }
+
+                // 将K基本块从L1搬到L0B
+                auto l0_b_buf_tensor = this->l0_b_ping_pong_flag_ ? this->l0_b_pong_tensor : this->l0_b_ping_tensor;
+                auto l1b_offset = m == 0 ? 0 : 128 * 128;
+                auto l0b_repeat_times = m == 0 ? 64 : 32;
+                WAIT_FLAG(M, MTE1, this->l0_b_ping_pong_flag_ + 2);
+                AscendC::LoadData(l0_b_buf_tensor, l1_buf_b_tensor[l1b_offset],
+                                  AscendC::LoadData2dParams(0,                // startIndex
+                                                            l0b_repeat_times, // repeatTimes
+                                                            1,                // srcStride
+                                                            0,                // sid
+                                                            0,                // dstGap
+                                                            false,            // ifTranspose
+                                                            0                 // addrMode
+                                                            ));
+
+                if (m == 1) {
+                    SET_FLAG(MTE1, MTE2, this->l1_b_ping_pong_flag_ + 2);
+                }
+
+                SET_FLAG(MTE1, M, this->l0_b_ping_pong_flag_ + 2);
+                WAIT_FLAG(MTE1, M, this->l0_b_ping_pong_flag_ + 2);
+
+                auto l0_a_remain_offset = 0;
+                auto remain_offset = 0;
+                if (l0a_offset_remain != -1) {
+                    remain_offset = m == 0 ? (128 / this->Y * 128 * this->local_block_index) :
+                                             (128 / this->Y * 64 * this->local_block_index);
+                }
+                auto m_mad = 128;
+                if (l0a_offset_remain != -1) {
+                    l0_a_remain_offset = remain_offset;
+                    m_mad = 128 / this->Y;
+                }
+                int unit_flag = 0b10; // 搬出时等于0b11， 累加时0b10
+                if (m == 1) {
+                    unit_flag = 0b11;
+                }
+                bool init_c = (m == 0);
+                auto d_value = m == 0 ? 128 : 64;
+                AscendC::Mmad(l0_c_buf_tensor,                                 // dstLocal
+                              l0_a_buf_tensor[l0_a_remain_offset],             // fmLocal
+                              l0_b_buf_tensor,                                 // filterLocal
+                              AscendC::MmadParams(m_mad,                       // m
+                                                  128,                         // n
+                                                  d_value,                     // k
+                                                  unit_flag,                   // unitFlag
+                                                  false,                       // cmatrixSource
+                                                  (init_c == 1) ? true : false // cmatrixInitVal
+                                                  ));
+                SET_FLAG(M, MTE1, this->l0_b_ping_pong_flag_ + 2);
+
+                this->l0_b_ping_pong_flag_ = 1 - this->l0_b_ping_pong_flag_;
+                this->l0_c_ping_pong_flag_ = 1 - this->l0_c_ping_pong_flag_;
+
+                SET_FLAG(M, MTE1, this->l0_a_ping_pong_flag_);
+                this->l0_a_ping_pong_flag_ = 1 - this->l0_a_ping_pong_flag_;
+            }
+            int unit_flag = 0b11;
+            auto m_mad = 128;
+            if (l0a_offset_remain != -1) {
+                m_mad = 128 / this->Y;
+            }
+            auto intriParams = AscendC::FixpipeParamsV220(128,   // nSize
+                                                          m_mad, // mSize
+                                                          m_mad, // srcStride
+                                                          128,   // dstStride
+                                                          false  // reluEn
+            );
+            intriParams.quantPre = QuantMode_t::NoQuant;
+            intriParams.unitFlag = unit_flag;
+            AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                gm_c_cube1_tensor[gm_c - this->gm_c_cube1], // dstGlobal
+                l0_c_buf_tensor,                            // srcLocal
+                intriParams                                 // intriParams
+            );
+            this->l1_b_ping_pong_flag_ = 1 - this->l1_b_ping_pong_flag_;
+
+            gm_k += BASE_BLOCK_SIZE_256;
+            gm_c += BASE_BLOCK_SIZE;
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ __inline__ void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::cube1_headDim192(
+    Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> *src,
+    Address_192::PHY_ADDR_FORWARD_CUBE1<TYPE, TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t switch_flag = 0;
+    int32_t l0a_offset_remain = 0;
+    int32_t loop_times_Q = 2; // 循环两次搬运Q基本块（非mask场景1块，mask场景liang）
+    int32_t headDim = 192;
+
+    auto l1_base_a_cube1_tensor = this->l1_a_ping_pong_flag_ ? this->l1_a_pong_tensor : this->l1_a_ping_tensor;
+
+    WAIT_FLAG(MTE1, MTE2, this->l1_a_ping_pong_flag_);
+    for (int i = 0; i < loop_times_Q; i++) {
+        if (src[i].k == 0) {
+            continue;
+        }
+        AscendC::DataCopy(l1_base_a_cube1_tensor[BASE_BLOCK_SIZE_192 * i],
+                          this->gm_Q_tensor[(uint32_t(src[i].left - this->gm_a_cube1))],
+                          AscendC::Nd2NzParams(1,       // ndNum
+                                               128,     // nValue
+                                               headDim, // dValue
+                                               0,       // srcNdMatrixStride, unused
+                                               headDim, // srcDValue
+                                               128,     // dstNzC0Stride
+                                               1,       // dstNzNStride
+                                               0        // dstNzMatrixStride, unused
+                                               ));
+        switch_flag++;
+    }
+
+    if (switch_flag == 2) {
+        l0a_offset_remain = BASE_BLOCK_SIZE_192;
+    } else if (src[0].k == 0 && src[1].k == 0) {
+        for (int i = 0; i < 2; i++) {
+            if (remain[i].k == 0) {
+                continue;
+            }
+            AscendC::DataCopy(l1_base_a_cube1_tensor[BASE_BLOCK_SIZE_192 * i],
+                              this->gm_Q_tensor[(uint32_t(remain[i].left - this->gm_a_cube1))],
+                              AscendC::Nd2NzParams(1,       // ndNum
+                                                   128,     // nValue
+                                                   headDim, // dValue
+                                                   0,       // srcNdMatrixStride, unused
+                                                   headDim, // srcDValue
+                                                   128,     // dstNzC0Stride
+                                                   1,       // dstNzNStride
+                                                   0        // dstNzMatrixStride, unused
+                                                   ));
+            switch_flag++;
+        }
+    } else if (remain[0].k != 0 && src[0].left != remain[0].left) {
+        l0a_offset_remain = BASE_BLOCK_SIZE_192;
+        AscendC::DataCopy(l1_base_a_cube1_tensor[BASE_BLOCK_SIZE_192],
+                          this->gm_Q_tensor[(uint32_t(remain[0].left - this->gm_a_cube1))],
+                          AscendC::Nd2NzParams(1,       // ndNum
+                                               128,     // nValue
+                                               headDim, // dValue
+                                               0,       // srcNdMatrixStride, unused
+                                               headDim, // srcDValue
+                                               128,     // dstNzC0Stride
+                                               1,       // dstNzNStride
+                                               0        // dstNzMatrixStride, unused
+                                               ));
+        switch_flag++;
+    } else if (remain[1].k != 0) {
+        AscendC::DataCopy(l1_base_a_cube1_tensor[BASE_BLOCK_SIZE_192],
+                          this->gm_Q_tensor[(uint32_t(remain[1].left - this->gm_a_cube1))],
+                          AscendC::Nd2NzParams(1,       // ndNum
+                                               128,     // nValue
+                                               headDim, // dValue
+                                               0,       // srcNdMatrixStride, unused
+                                               headDim, // srcDValue
+                                               128,     // dstNzC0Stride
+                                               1,       // dstNzNStride
+                                               0        // dstNzMatrixStride, unused
+                                               ));
+        switch_flag++;
+    }
+
+    SET_FLAG(MTE2, MTE1, this->l1_a_ping_pong_flag_);
+    WAIT_FLAG(MTE2, MTE1, this->l1_a_ping_pong_flag_);
+
+    // 处理完整基本块
+    base_block_mad_headDim192(src[0], src[1]);
+    // 处理尾块
+    if (remain[0].k + remain[1].k != 0) {
+        base_block_mad_headDim192(remain[0], remain[1], l0a_offset_remain);
+    }
+
+    SET_FLAG(MTE1, MTE2, this->l1_a_ping_pong_flag_);
+    this->l1_a_ping_pong_flag_ = 1 - this->l1_a_ping_pong_flag_;
+}
+
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::addrMapping_cube2_rowsum_192(
+    int32_t roundId, int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t curCalcRow = roundId * this->F + this->core_group_index; // 当前的row
+    int32_t curCalcRowP = this->F * (roundId % 2) + this->core_group_index;
+    int32_t b = curCalcRow / this->row_per_batch;                  // 当前行所在的batch号
+    int32_t n = (curCalcRow % this->row_per_batch) / this->L;      // 当前batch下的head号
+    int32_t i_r = (curCalcRow % this->row_per_batch) % this->L;    // 当前head下的行号
+    int32_t i_c = this->local_block_index * this->column_per_core; // 当前core处理的起始列
+    int32_t begin_remain = this->sparseMode == 1 ? (this->W + 1 - this->column_remain) :
+                                                   (this->H1) + 1 - this->column_remain; // sparse or triangle
+
+    int32_t q_left_index = this->sparseMode == 1 ? (this->W / 2 + i_r) : (this->L + i_r);          // sparse or triangle
+    int32_t q_right_index = this->sparseMode == 1 ? (this->W / 2 - 1 - i_r) : (this->L - 1 - i_r); // sparse or triangle
+    int32_t switch_index = q_left_index;
+
+    // 设置sparse的标志位 以及 sparse场景非倒三角部分的偏移
+    int32_t sparse_flag = false;
+    if (this->sparseMode == 1) {
+        sparse_flag = i_r > ((this->W - 1) / 2) ? true : false;
+    }
+    int32_t row_down = i_r + this->W / 2;
+    int32_t col_down = i_r + i_c - this->W / 2;
+    int32_t col_down_remain = i_r + begin_remain - this->W / 2;
+
+    int32_t g = n / this->nG;
+    int64_t bn_offset = (b * this->N * this->H1 + n * this->H1) * BASE_BLOCK_SIZE;
+    int64_t bn_offset_rowsum = (b * this->N * this->H1 + n * this->H1) * 128;
+    int64_t bn_offset_GQA = (b * this->G * this->H1 + g * this->H1) * BASE_BLOCK_SIZE;
+    int64_t q_left_offset = bn_offset + q_left_index * BASE_BLOCK_SIZE;
+    int64_t q_right_offset = bn_offset + q_right_index * BASE_BLOCK_SIZE;
+    int64_t out_offset = this->sparseMode == 1 ? ((curCalcRowP * (this->W + 1)) * BASE_BLOCK_SIZE) // sparse or triangle
+                                                 :
+                                                 ((curCalcRowP * (this->H1 + 1)) * BASE_BLOCK_SIZE);
+
+    if (!sparse_flag && switch_index < i_c) {
+        src[0].left = this->gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = this->gm_b_cube2 + bn_offset_GQA + (i_c - switch_index - 1) * BASE_BLOCK_SIZE;
+        src[0].out = this->gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_right_index * 128;
+        src[0].k = this->column_per_core;
+        src_length = 1;
+
+        if (this->column_remain != 0) {
+            remain[0].left = this->gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE +
+                             (128 / this->Y) * this->local_block_index * 128;
+            remain[0].right = this->gm_b_cube2 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+            remain[0].out = src[0].out + (128 / this->Y) * this->local_block_index * 128;
+            remain[0].rowsumOut = src[0].rowsumOut + (128 / this->Y) * this->local_block_index;
+            remain[0].k = this->column_remain;
+            remain_length = 1;
+        }
+    } else if (!sparse_flag && i_c <= switch_index && i_c + this->column_per_core > switch_index + 1) {
+        src[0].left = this->gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = this->gm_b_cube2 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+        src[0].out = this->gm_c_cube2 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_left_index * 128;
+        src[0].k = switch_index - i_c + 1;
+
+        src[1].left = this->gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE + src[0].k * BASE_BLOCK_SIZE;
+        src[1].right = this->gm_b_cube2 + bn_offset_GQA;
+        src[1].out = this->gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE;
+        src[1].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_right_index * 128;
+        src[1].k = this->column_per_core - src[0].k;
+
+        src_length = 2;
+
+        if (this->column_remain != 0) {
+            remain[0].left = this->gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE +
+                             (128 / this->Y) * this->local_block_index * 128;
+            remain[0].right = this->gm_b_cube2 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+            remain[0].out = src[1].out + (128 / this->Y) * this->local_block_index * 128;
+            remain[0].rowsumOut = src[1].rowsumOut + (128 / this->Y) * this->local_block_index;
+            remain[0].k = this->column_remain;
+            remain_length = 1;
+        }
+    } else if (!sparse_flag && i_c <= switch_index && i_c + this->column_per_core <= switch_index + 1) {
+        src[0].left = this->gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = this->gm_b_cube2 + bn_offset_GQA + i_c * BASE_BLOCK_SIZE;
+        src[0].out = this->gm_c_cube2 + bn_offset + q_left_index * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_left_index * 128;
+        src[0].k = this->column_per_core;
+        src_length = 1;
+
+        if (this->column_remain != 0) {
+            if (switch_index < begin_remain) {
+                remain[0].left = this->gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE +
+                                 (128 / this->Y) * this->local_block_index * 128;
+                remain[0].right =
+                    this->gm_b_cube2 + bn_offset_GQA + (begin_remain - switch_index - 1) * BASE_BLOCK_SIZE;
+                remain[0].out = this->gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE +
+                                (128 / this->Y) * this->local_block_index * 128;
+                remain[0].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_right_index * 128 +
+                                      (128 / this->Y) * this->local_block_index;
+                remain[0].k = this->column_remain;
+                remain_length = 1;
+            } else {
+                remain[0].left = this->gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE +
+                                 (128 / this->Y) * this->local_block_index * 128;
+                remain[0].right = this->gm_b_cube2 + bn_offset_GQA + begin_remain * BASE_BLOCK_SIZE;
+                remain[0].out = this->gm_c_cube2 + bn_offset + q_left_index * BASE_BLOCK_SIZE +
+                                (128 / this->Y) * this->local_block_index * 128;
+                remain[0].rowsumOut =
+                    this->rowsum_gm + bn_offset_rowsum + q_left_index * 128 + (128 / this->Y) * this->local_block_index;
+                remain[0].k = switch_index - begin_remain + 1;
+
+                remain[1].left = remain[0].left + remain[0].k * BASE_BLOCK_SIZE;
+                remain[1].right = this->gm_b_cube2 + bn_offset_GQA;
+                remain[1].out = this->gm_c_cube2 + bn_offset + q_right_index * BASE_BLOCK_SIZE +
+                                (128 / this->Y) * this->local_block_index * 128;
+                remain[1].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_right_index * 128 +
+                                      (128 / this->Y) * this->local_block_index;
+                remain[1].k = this->column_remain - remain[0].k;
+
+                remain_length = 2;
+            }
+        }
+    } else {
+        src[0].left = this->gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+        src[0].right = this->gm_b_cube2 + bn_offset_GQA + col_down * BASE_BLOCK_SIZE;
+        src[0].out = this->gm_c_cube2 + bn_offset + row_down * BASE_BLOCK_SIZE;
+        src[0].rowsumOut = this->rowsum_gm + bn_offset_rowsum + row_down * 128;
+        src[0].k = this->column_per_core;
+        src_length = 1;
+
+        if (this->column_remain != 0) {
+            remain[0].left = this->gm_c_cube1 + out_offset + begin_remain * BASE_BLOCK_SIZE +
+                             (128 / this->Y) * this->local_block_index * 128;
+            remain[0].right = this->gm_b_cube2 + bn_offset_GQA + col_down_remain * BASE_BLOCK_SIZE;
+            remain[0].out = src[0].out + (128 / this->Y) * this->local_block_index * 128;
+            remain[0].rowsumOut = src[0].rowsumOut + (128 / this->Y) * this->local_block_index;
+            remain[0].k = this->column_remain;
+            remain_length = 1;
+        }
+    }
+}
+
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::addrMapping_cube2_rowsum_nomask_192(
+    int32_t roundId, int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain)
+{
+    int32_t curCalcRow = roundId * this->F + this->core_group_index; // 当前的row 0
+    int32_t curCalcRowP = this->F * (roundId % 2) + this->core_group_index;
+    int32_t b = curCalcRow / this->row_per_batch;                  // 当前行所在的batch号
+    int32_t n = (curCalcRow % this->row_per_batch) / this->L;      // 当前batch下的head号
+    int32_t i_r = (curCalcRow % this->row_per_batch) % this->L;    // 当前head下的行号
+    int32_t i_c = this->local_block_index * this->column_per_core; // 当前core处理的起始列
+    int32_t begin_remian = this->H2 - this->column_remain;
+
+    int32_t q_left_index = i_r;
+    int32_t q_right_index = this->L - 1 - i_r;
+    int32_t switch_index = this->H2;
+
+    int32_t g = n / this->nG;
+    int64_t bn_offset_1 = (b * this->N * this->H1 + n * this->H1) * BASE_BLOCK_SIZE;
+    int64_t bn_offset_rowsum = (b * this->N * this->H1 + n * this->H1) * 128;
+    int64_t bn_offset_2 = (b * this->G * this->H2 + g * this->H2) * BASE_BLOCK_SIZE; // GQA version of bn_offset_2
+    int64_t out_offset = (curCalcRowP * this->H2) * BASE_BLOCK_SIZE;
+
+    src[0].left = this->gm_c_cube1 + out_offset + i_c * BASE_BLOCK_SIZE;
+    src[0].right = this->gm_b_cube2 + bn_offset_2 + i_c * BASE_BLOCK_SIZE;
+    src[0].out = this->gm_c_cube2 + bn_offset_1 + q_left_index * BASE_BLOCK_SIZE;
+    src[0].rowsumOut = this->rowsum_gm + bn_offset_rowsum + q_left_index * 128;
+    src[0].k = this->column_per_core;
+    src_length = 1;
+    if (this->column_remain != 0) {
+        remain[0].left = this->gm_c_cube1 + out_offset + begin_remian * BASE_BLOCK_SIZE +
+                         (128 / this->Y) * this->local_block_index * 128;
+        remain[0].right = this->gm_b_cube2 + bn_offset_2 + begin_remian * BASE_BLOCK_SIZE;
+        remain[0].out = this->gm_c_cube2 + bn_offset_1 + q_left_index * BASE_BLOCK_SIZE +
+                        (128 / this->Y) * this->local_block_index * 128;
+        remain[0].rowsumOut =
+            this->rowsum_gm + bn_offset_rowsum + q_left_index * 128 + (128 / this->Y) * this->local_block_index;
+        remain[0].k = this->column_remain;
+        remain_length = 1;
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::unified_addrMapping_cube2mix_192(
+    int32_t roundId, int64_t &src_length, int64_t &remain_length, PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *src,
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *remain)
+{
+    if (this->qk_triangle == 1) { // 倒三角
+        addrMapping_cube2_rowsum_192(roundId, src_length, remain_length, src, remain);
+    } else if (this->qk_triangle == 0) { // 非倒三角
+        addrMapping_cube2_rowsum_nomask_192(roundId, src_length, remain_length, src, remain);
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::mix_cube2_rowsum_192(int32_t cube2_roundId)
+{
+    // 混算寻址：
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> src[2];
+    PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> remain[2];
+    int64_t src_length = 0;
+    int64_t remain_length = 0;
+    unified_addrMapping_cube2mix_192(cube2_roundId, src_length, remain_length, src, remain); // cube2和rowsum的寻址
+
+    // 开始混算：
+    cube2_rowsum_mix_only_192(src, src_length, BASE_BLOCK_LENGTH, this->Y * 2);
+    if (remain_length > 0) {
+        cube2_rowsum_mix_only_192(remain, remain_length, BASE_BLOCK_LENGTH / this->Y, 2);
+    }
+}
+
+template <typename TYPE, bool IF_BF16, typename WORKSPACE_TYPE>
+__aicore__ inline void CubeForward_192<TYPE, IF_BF16, WORKSPACE_TYPE>::cube2_rowsum_mix_only_192(
+    const PhyAddrCube2Rowsum<TYPE, WORKSPACE_TYPE> *cube_addr, int32_t cube_length, int32_t m_length,
+    int32_t vcore_num_per_head)
+{
+    int32_t l1_m = CUBE2_LENGTH_M; // 128
+    int32_t l1_k = CUBE2_LENGTH_K;
+    int32_t l1_n = CUBE2_LENGTH_N;
+    int32_t l0_k = BASE_BLOCK_LENGTH; // 128
+    int32_t l0_m = BASE_BLOCK_LENGTH; // 128
+    int32_t l0_n = BASE_BLOCK_LENGTH; // 128
+
+    for (int32_t i = 0; i < cube_length; i++) {
+        __gm__ TYPE *__restrict__ left_start_addr = (__gm__ TYPE *)(cube_addr[i].left); // 高精度是需要将float转为TYPE
+        __gm__ TYPE *__restrict__ right_start_addr = cube_addr[i].right;
+        __gm__ float *__restrict__ result_gm = cube_addr[i].out;
+        __gm__ float *__restrict__ rowsum_out_gm = cube_addr[i].rowsumOut;
+
+        int32_t k_core_gm = cube_addr[i].k * l0_k;
+        l1_k = k_core_gm < l1_k ? k_core_gm : l1_k;
+        int32_t loop_l1_times = k_core_gm / l1_k;
+        int32_t l0_k_block_num = l0_k / BLOCK_SIZE;     // 128/16
+        int32_t l0_m_block_num = m_length / BLOCK_SIZE; // 128/16
+        int32_t l0_n_block_num = l0_n / BLOCK_SIZE;     // 8
+
+        for (int32_t l1_loop_index = 0; l1_loop_index < loop_l1_times; l1_loop_index++) {
+            auto l1_buf_a_cube2_tensor = l1_a_ping_pong_flag_ ? l1_a_pong_tensor : l1_a_ping_tensor;
+            auto l1_buf_b_cube2_tensor = l1_b_ping_pong_flag_ ? l1_b_pong_tensor : l1_b_ping_tensor;
+            auto l0a_buf_tensor = l0_a_ping_pong_flag_ ? l0_a_pong_tensor : l0_a_ping_tensor;
+            auto l0b_buf_tensor = l0_b_ping_pong_flag_ ? l0_b_pong_tensor : l0_b_ping_tensor;
+            auto l0c_buf_tensor = l0_c_ping_pong_flag_ ? l0_c_pong_tensor : l0_c_ping_tensor;
+            WAIT_FLAG(MTE1, MTE2, l1_a_ping_pong_flag_);
+
+            AscendC::DataCopy(
+                l1_buf_a_cube2_tensor,
+                this->gm_c_cube1_tensor_type[(uint32_t(left_start_addr - (__gm__ TYPE *)this->gm_c_cube1) +
+                                              l1_loop_index * l0_m * l0_k * 2)],
+                AscendC::Nd2NzParams(vcore_num_per_head, m_length / vcore_num_per_head, l0_k,
+                                     m_length * l0_k / vcore_num_per_head * 2, l0_k, m_length, 1,
+                                     m_length / vcore_num_per_head * BLOCK_SIZE));
+
+            SET_FLAG(MTE2, MTE1, l1_a_ping_pong_flag_);
+            WAIT_FLAG(MTE2, MTE1, l1_a_ping_pong_flag_);
+            WAIT_FLAG(M, MTE1, l0_a_ping_pong_flag_);
+
+            for (int32_t l0a_load_idx = 0; l0a_load_idx < l0_k_block_num; l0a_load_idx++) {
+                AscendC::LoadData(l0a_buf_tensor[l0a_load_idx * CUBE_MATRIX_SIZE],
+                                  l1_buf_a_cube2_tensor[l0a_load_idx * l0_m_block_num * CUBE_MATRIX_SIZE],
+                                  AscendC::LoadData2dParams(0,                  // startIndex
+                                                            l0_m_block_num,     // repeatTimes
+                                                            1,                  // srcStride
+                                                            0,                  // sid
+                                                            l0_k_block_num - 1, // dstGap
+                                                            false,              // ifTranspose
+                                                            0                   // addrMode
+                                                            ));
+            }
+
+            SET_FLAG(MTE1, MTE2, l1_a_ping_pong_flag_);
+            WAIT_FLAG(MTE1, MTE2, l1_b_ping_pong_flag_ + 2);
+
+            AscendC::DataCopy(
+                l1_buf_b_cube2_tensor,
+                this->gm_V_tensor[(uint32_t(right_start_addr - this->gm_b_cube2) + l1_loop_index * l0_k * l0_n)],
+                AscendC::Nd2NzParams(1,           // ndNum
+                                     l0_k,        // nValue
+                                     l0_n,        // dValue
+                                     l0_k * l0_n, // srcNdMatrixStride
+                                     l0_n,        // srcDValue
+                                     l0_k,        // dstNzC0Stride
+                                     1,           // dstNzNStride
+                                     l0_k * l0_n  // dstNzMatrixStride
+                                     ));
+
+            SET_FLAG(MTE2, MTE1, l1_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE2, MTE1, l1_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            for (int32_t l0b_load_idx = 0; l0b_load_idx < l0_k_block_num; l0b_load_idx++) {
+                AscendC::LoadData(l0b_buf_tensor[l0b_load_idx * l0_n * BLOCK_SIZE],
+                                  l1_buf_b_cube2_tensor[l0b_load_idx * CUBE_MATRIX_SIZE],
+                                  AscendC::LoadData2dParams(0,              // startIndex
+                                                            l0_n_block_num, // repeatTimes
+                                                            l0_k_block_num, // srcStride
+                                                            0,              // sid
+                                                            0,              // dstGap
+                                                            true,           // ifTranspose
+                                                            0               // addrMode
+                                                            ));
+            }
+
+            SET_FLAG(MTE1, MTE2, l1_b_ping_pong_flag_ + 2);
+            SET_FLAG(MTE1, M, l0_b_ping_pong_flag_ + 2);
+            WAIT_FLAG(MTE1, M, l0_b_ping_pong_flag_ + 2);
+
+            bool init_c = (l1_loop_index == 0);
+            bool out_c = (l1_loop_index == loop_l1_times - 1);
+            int unit_flag = 0b10;
+            if (out_c) {
+                unit_flag = 0b11;
+            }
+            AscendC::Mmad(l0c_buf_tensor,                                  // dstLocal
+                          l0a_buf_tensor,                                  // fmLocal
+                          l0b_buf_tensor,                                  // filterLocal
+                          AscendC::MmadParams(m_length,                    // m
+                                              l0_n,                        // n
+                                              l0_k,                        // k
+                                              unit_flag,                   // unitFlag
+                                              false,                       // cmatrixSource
+                                              (init_c == 1) ? true : false // cmatrixInitVal
+                                              ));
+
+            SET_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            if (out_c) {
+                AscendC::SetAtomicAdd<float>();
+                AscendC::SetAtomicType<float>();
+                auto intriParams = AscendC::FixpipeParamsV220(l0_n,     // nSize
+                                                              m_length, // mSize
+                                                              m_length, // srcStride
+                                                              l0_n,     // dstStride
+                                                              false     // reluEn
+                );
+                intriParams.quantPre = QuantMode_t::NoQuant;
+                intriParams.unitFlag = unit_flag;
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    gm_c_cube2_tensor[result_gm - this->gm_c_cube2], // dstGlobal
+                    l0c_buf_tensor,                                  // srcLocal
+                    intriParams                                      // intriParams
+                );
+                AscendC::SetAtomicNone();
+            }
+
+            l0_b_ping_pong_flag_ = 1 - l0_b_ping_pong_flag_;
+            l0_c_ping_pong_flag_ = 1 - l0_c_ping_pong_flag_;
+            l0b_buf_tensor = l0_b_ping_pong_flag_ ? l0_b_pong_tensor : l0_b_ping_tensor;
+            l0c_buf_tensor = l0_c_ping_pong_flag_ ? l0_c_pong_tensor : l0_c_ping_tensor;
+            // rowsum: 将全1矩阵从L1搬运到L0b
+            WAIT_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+
+            SET_FLAG(M, MTE1, l0_b_ping_pong_flag_ + 2);
+            SET_FLAG(M, MTE1, l0_a_ping_pong_flag_);
+
+            l1_a_ping_pong_flag_ = 1 - l1_a_ping_pong_flag_;
+            l1_b_ping_pong_flag_ = 1 - l1_b_ping_pong_flag_;
+            l0_a_ping_pong_flag_ = 1 - l0_a_ping_pong_flag_;
+            l0_b_ping_pong_flag_ = 1 - l0_b_ping_pong_flag_;
+            l0_c_ping_pong_flag_ = 1 - l0_c_ping_pong_flag_;
+        }
+    }
+}
+
+#endif
+
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/TransposeCustom.h b/src/kernels/mixkernels/laser_attention/op_kernel/TransposeCustom.h
new file mode 100644
index 00000000..a2bee676
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/TransposeCustom.h
@@ -0,0 +1,259 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */                                                                                                                    \
+#ifndef __TRANSPOSE_CUSTOM_H__
+#define __TRANSPOSE_CUSTOM_H__
+
+#define USE_ASCENDC
+
+#include "ppmatmul_const.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_VEC__
+
+template <typename TYPE, bool IF_BF16> class TransposeCustom {
+public:
+    __aicore__ inline TransposeCustom() {}
+    __aicore__ inline void Init(__gm__ uint8_t *__restrict__ gm_in_tensor,  // input
+                                __gm__ uint8_t *__restrict__ gm_out_tensor, // output
+                                int32_t batch_size, int32_t head_num, int32_t seq_size, int32_t head_dim,
+                                int32_t transpose_type);
+    __aicore__ inline void Run();
+    __aicore__ inline void transposeSBHToBNSD1(int32_t vector_num, int32_t cur_vector_idx, int S, int B, int N, int D);
+    __aicore__ inline void transposeSBHToBNSD2(int32_t vector_num, int32_t cur_vector_idx, int S, int B, int N, int D);
+
+private:
+    GlobalTensor<TYPE> gm_in_tensor;
+    GlobalTensor<TYPE> gm_out_tensor;
+    LocalTensor<TYPE> ub_tensor;
+    int32_t batch_size;     // B
+    int32_t head_num;       // N
+    int32_t seq_size;       // S
+    int32_t head_dim;       // D
+    int32_t transpose_type; // shape类型转换  0：SBH->BNSD
+    AsdopsBuffer<ArchType::ASCEND_V220> calBuf;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeCustom<TYPE, IF_BF16>::Init(__gm__ uint8_t *__restrict__ gm_in,  // input
+                                                            __gm__ uint8_t *__restrict__ gm_out, // output
+                                                            int32_t batch_size, int32_t head_num, int32_t seq_size,
+                                                            int32_t head_dim, int32_t transpose_type)
+{
+    gm_in_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ __bf16 *>(gm_in));
+    gm_out_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ __bf16 *>(gm_out));
+    ub_tensor = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(0);
+    this->batch_size = batch_size;         // B
+    this->head_num = head_num;             // N
+    this->seq_size = seq_size;             // S
+    this->head_dim = head_dim;             // D
+    this->transpose_type = transpose_type; // shape类型转换  0：SBH->BNSD  1:BSH->BNSD  2:BNSD->SBH 3:BNSD->BSH
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ inline void TransposeCustom<TYPE, IF_BF16>::Run()
+{
+    int32_t vector_num = get_subblockdim() * get_block_num();                        // vector的数量
+    int32_t cur_vector_idx = get_block_idx() * get_subblockdim() + get_subblockid(); // 当前编号
+    int S = seq_size;
+    int B = batch_size;
+    int N = head_num;
+    int D = head_dim;
+    int ub_max = 96 * 1024 / sizeof(TYPE); // 48 * 1024  一半拷贝进来，一半排序
+    int segmentation = ub_max / D;
+    // SBH  -> BNSD
+    if (transpose_type == 0) {
+        if (B * N > segmentation) {
+            transposeSBHToBNSD2(vector_num, cur_vector_idx, S, B, N, D);
+        } else {
+            transposeSBHToBNSD1(vector_num, cur_vector_idx, S, B, N, D);
+        }
+    }
+}
+
+// 典型场景测试  S<=8192且B*N <=192
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeCustom<TYPE, IF_BF16>::transposeSBHToBNSD1(int32_t vector_num, int32_t cur_vector_idx,
+                                                                           int S, int B, int N, int D)
+{
+    int H = N * D;
+    // UB 192K
+    // UB 192K   5_16_8192_128
+    int ub_size = 96 * 1024 / sizeof(TYPE); // 48 * 1024  一半拷贝进来，一半排序
+    int ub_max = 96 * 1024 / sizeof(TYPE);  // 48 * 1024  一半拷贝进来，一半排序
+    int length = ub_max / (B * H);          // 3 每次处理3行
+    if (length >= 1) {
+        ub_max = length * (B * H); // ub_max设置为B*H的整数倍
+    }
+    int seq_pre_core_length = S / (vector_num * length);
+    int seq_pre_core = seq_pre_core_length * length;                             // 168
+    int tail = S - seq_pre_core * vector_num;                                    // 8192 - 168*48 = 128
+    int tail_core = tail / length;                                               // 128/3=42余数为2
+    int inOffset = (seq_pre_core * cur_vector_idx + tail_core * length) * B * H; //
+    int outOffset = (seq_pre_core * cur_vector_idx + tail_core * length) * D;
+    if (cur_vector_idx < tail_core) {
+        seq_pre_core += length; // 168+3=171
+        inOffset = seq_pre_core * cur_vector_idx * B * H;
+        outOffset = seq_pre_core * cur_vector_idx * D;
+    }
+
+    int times = seq_pre_core / length; // 56或者57
+
+    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    for (int i = 0; i < times; i++) {
+        
+        WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+
+        AscendC::DataCopy(ub_tensor, gm_in_tensor[inOffset + ub_max * i],
+                          AscendC::DataCopyParams(1,
+                                                  ub_max / 16, // 一个block=32B N*2/32
+                                                  0, 0));
+
+        SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        
+        WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+        for (int j = 0; j < length; j++) {
+            AscendC::DataCopy(ub_tensor[ub_size + D * j], ub_tensor[B * H * j],
+                              AscendC::DataCopyParams(B * H / D, D / 16, 0, (length - 1) * D / 16));
+        }
+
+        SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+
+        SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+        WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+        AscendC::DataCopyPad(gm_out_tensor[outOffset + length * i * D], ub_tensor[ub_size],
+                             AscendC::DataCopyExtParams(B * H / D, D * length * 2, 0, (S - length) * D * 2, 0));
+        SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    }
+    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    // 处理不能整除的数据
+    int remain = tail - tail_core * length; // 128-42 * 3 =2
+    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    if (remain > 0 && cur_vector_idx == (vector_num - 1)) {
+        inOffset = (seq_pre_core * cur_vector_idx + tail_core * length + seq_pre_core) * B * H; // seq_pre_core=168
+        outOffset = (seq_pre_core * cur_vector_idx + tail_core * length + seq_pre_core) * D;
+
+        WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+
+        AscendC::DataCopy(ub_tensor, gm_in_tensor[inOffset], AscendC::DataCopyParams(1, remain * (B * H) / 16, 0, 0));
+
+        SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+        for (int j = 0; j < remain; j++) {
+            AscendC::DataCopy(ub_tensor[ub_size + D * j], ub_tensor[B * H * j],
+                              AscendC::DataCopyParams(B * H / D, D / 16, 0, (remain - 1) * D / 16));
+        }
+
+        SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+        SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+        WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+        AscendC::DataCopyPad(gm_out_tensor[outOffset], ub_tensor[ub_size],
+                             AscendC::DataCopyExtParams(B * H / D, D * remain * 2, 0, (S - remain) * D * 2, 0));
+        SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    }
+    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+}
+
+// 分段拷入，连续拷出  SBH  -> BNSD  应用场景  S>8192或B*N > 192
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeCustom<TYPE, IF_BF16>::transposeSBHToBNSD2(int32_t vector_num, int32_t cur_vector_idx,
+                                                                           int S, int B, int N, int D)
+{
+    int H = N * D;
+    // UB 192K  10_256_32_128
+    int ub_max = 96 * 1024 / sizeof(TYPE);    // 48 * 1024  一半拷贝进来，一半排序
+    int seq_pre_core = S / vector_num;        // 每个核平均处理个数5
+    int tail = S - seq_pre_core * vector_num; // 16
+    int inOffset = (seq_pre_core * cur_vector_idx + tail) * B * H;
+    int outOffset = (seq_pre_core * cur_vector_idx + tail) * D;
+    if (cur_vector_idx < tail) {
+        seq_pre_core += 1;
+        inOffset = seq_pre_core * cur_vector_idx * B * H;
+        outOffset = seq_pre_core * cur_vector_idx * D;
+    }
+    for (int batchSize = 0; batchSize < B; batchSize++) {
+        for (int headSize = 0; headSize < N; headSize++) {
+            int times = seq_pre_core * D / ub_max;
+            if (times > 0) {
+
+                SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+                for (int i = 0; i < times; i++) {
+                    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                    AscendC::DataCopy(
+                        ub_tensor, gm_in_tensor[inOffset + ub_max / D * i * B * H + headSize * D + batchSize * N * D],
+                        AscendC::DataCopyParams(ub_max / D, D / 16, (B * N - 1) * D / 16, 0));
+
+                    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+                    AscendC::DataCopy(ub_tensor[ub_max], ub_tensor, AscendC::DataCopyParams(1, ub_max / 16, 0, 0));
+
+                    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+                    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+                    AscendC::DataCopy(gm_out_tensor[outOffset + ub_max * i + headSize * S * D + batchSize * N * S * D],
+                                      ub_tensor[ub_max], AscendC::DataCopyParams(1, ub_max / 16, 0, 0));
+                    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                }
+                WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+            }
+            // 处理尾块
+            int tail_block = seq_pre_core * D - ub_max * times; // 5 * 128 = 640
+            SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+            SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+            if (tail_block > 0) {
+                WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                AscendC::DataCopy(
+                    ub_tensor, gm_in_tensor[inOffset + ub_max / D * B * H * times + headSize * D + batchSize * N * D],
+                    AscendC::DataCopyParams(tail_block / D, D / 16, (B * N - 1) * D / 16, 0));
+
+                SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+                AscendC::DataCopy(ub_tensor[ub_max], ub_tensor, AscendC::DataCopyParams(1, tail_block / 16, 0, 0));
+
+                SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+                WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+                AscendC::DataCopy(gm_out_tensor[outOffset + ub_max * times + headSize * S * D + batchSize * N * S * D],
+                                  ub_tensor[ub_max], AscendC::DataCopyParams(1, tail_block / 16, 0, 0));
+                SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+            }
+            WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+            WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+        }
+    }
+}
+
+#endif
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/TransposeWithDtype.h b/src/kernels/mixkernels/laser_attention/op_kernel/TransposeWithDtype.h
new file mode 100644
index 00000000..a1e51f5f
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/TransposeWithDtype.h
@@ -0,0 +1,436 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */                                                                                                                    \
+#ifndef __TRANSPOSE_WITH_DTYPE_H__
+#define __TRANSPOSE_WITH_DTYPE_H__
+
+#define USE_ASCENDC
+#include "ppmatmul_const.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_VEC__
+
+template <typename TYPE, bool IF_BF16> class TransposeWithDtype {
+public:
+    __aicore__ inline TransposeWithDtype() {}
+    __aicore__ inline void Init(__gm__ float *__restrict__ gm_in,     // input
+                                __gm__ float *__restrict__ gm_rowsum, // rowsum
+                                __gm__ TYPE *__restrict__ gm_out,     // output
+                                int32_t batch_size, int32_t head_num, int32_t seq_size, int32_t head_dim,
+                                int32_t transpose_type, int32_t is_forward);
+    __aicore__ inline void Run();
+
+private:
+    __aicore__ __inline__ void SBHForward();
+    __aicore__ __inline__ void SBHBackward();
+    __aicore__ __inline__ void ComputeBackward(int32_t in_offset, int32_t out_offset, int32_t lenth, int32_t row);
+    __aicore__ __inline__ void ComputeForward(int32_t in_offset, int32_t rowsum_offset, int32_t out_offset,
+                                              int32_t lenth, int32_t row);
+
+private:
+    AsdopsBuffer<ArchType::ASCEND_V220> calBuf;
+    LocalTensor<TYPE> cur_tensor;
+    LocalTensor<float> cur_tensor_f32;
+    LocalTensor<float> cur_tensor_out;
+    LocalTensor<TYPE> tensor_transpose;
+    LocalTensor<float> tensor_transpose_f32;
+    LocalTensor<TYPE> tensor_rowsum_brcb;
+    LocalTensor<float> tensor_rowsum_brcb_f32;
+    LocalTensor<uint32_t> tensor_mask;
+    GlobalTensor<float> gm_in_tensor;
+    GlobalTensor<float> gm_rowsum_tensor;
+    GlobalTensor<TYPE> gm_out_tensor;
+    int32_t B;              // B
+    int32_t N;              // N
+    int32_t S;              // S
+    int32_t D;              // D
+    int32_t H;              // H
+    int32_t transpose_type; // shape类型转换
+    int32_t is_forward;     // 前向还是反向
+    int32_t vector_num;     // vector的数量
+    int32_t cur_vector_idx; // 当前编号
+    int UB_MAX = 192 * 1024 / sizeof(float) / 3;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeWithDtype<TYPE, IF_BF16>::Init(__gm__ float *__restrict__ gm_in,     // input
+                                                               __gm__ float *__restrict__ gm_rowsum, // rowsum
+                                                               __gm__ TYPE *__restrict__ gm_out,     // output
+                                                               int32_t batch_size, int32_t head_num, int32_t seq_size,
+                                                               int32_t head_dim, int32_t transpose_type,
+                                                               int32_t is_forward)
+{
+    gm_in_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_in));
+    gm_rowsum_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_rowsum));
+    gm_out_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_out));
+
+    cur_tensor = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(0);
+    cur_tensor_f32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    cur_tensor_out = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    tensor_transpose = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(192 * 1024 / 3 * 2);
+    tensor_transpose_f32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(192 * 1024 / 3 * 2);
+    tensor_rowsum_brcb = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(192 * 1024 / 6 * 5);
+    tensor_rowsum_brcb_f32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(192 * 1024 / 6 * 5);
+    tensor_mask = calBuf.GetBuffer<BufferType::ASCEND_UB, uint32_t>(192 * 1024 / sizeof(TYPE) / 6 * 5);
+
+
+    this->B = batch_size;
+    this->N = head_num;
+    this->S = seq_size;
+    this->D = head_dim;
+    this->H = head_num * head_dim;
+    this->transpose_type = transpose_type;
+    this->is_forward = is_forward;
+    this->vector_num = get_block_num() * 2;
+    this->cur_vector_idx = get_block_idx() * 2 + get_subblockid(); // 当前编号
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ inline void TransposeWithDtype<TYPE, IF_BF16>::Run()
+{
+    if (transpose_type == 0) {
+        if (is_forward == 1) {
+            SBHForward();
+        } else {
+            SBHBackward();
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::SBHBackward()
+{
+    int length, times, remain, inOffset, outOffset = 0;
+    int length_per_core = S / vector_num / 8 * 8;
+    int tail_length = S - length_per_core * vector_num;
+    int tail = tail_length / 8;
+
+    if (cur_vector_idx < tail) {
+        length_per_core += 8;
+        inOffset = length_per_core * cur_vector_idx * D;
+        outOffset = length_per_core * cur_vector_idx * B * N * D;
+    } else {
+        inOffset = (length_per_core * cur_vector_idx + tail_length) * D;
+        outOffset = (length_per_core * cur_vector_idx + tail_length) * B * N * D;
+    }
+
+    if (UB_MAX >= B * N * D * length_per_core) {
+        length = length_per_core;
+        times = 1;
+        remain = 0;
+    } else {
+        if (UB_MAX / (B * N * D) < 8) {
+            // B * N > 16 走这个分支
+            length = 8;
+        } else {
+            length = UB_MAX / (B * N * D) / 8 * 8;
+        }
+        times = length_per_core / length;
+        remain = length_per_core % length;
+    }
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (UB_MAX / (B * N * D) < 8) {
+        auto row = (UB_MAX / D / length); // UB_MAX = 16384
+        auto loopBN = (B * N) / row;
+        auto remainBN = (B * N) % row; // 尾行
+        for (int j = 0; j < loopBN; j++) {
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeBackward(inOffset + i * length * D + j * S * D * row,
+                                outOffset + i * length * B * N * D + j * row * D,
+                                // outOffset + i * length * D + j * S * D * row, // no transpose copy
+                                length, row);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+        if (remainBN > 0) {
+            // 存在尾行时 要单独处理
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeBackward(inOffset + i * length * D + loopBN * S * D * row,
+                                outOffset + i * length * B * N * D + loopBN * row * D,
+                                // outOffset + i * length * D + j * S * D * remainBN, // no transpose copy
+                                length, remainBN);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+    } else {
+        for (int i = 0; i < times; i++) {
+            WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            ComputeBackward(inOffset + i * length * D, outOffset + i * length * B * N * D, length, 0);
+            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+        }
+    }
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (remain > 0) {
+        ComputeBackward(inOffset + times * length * D, outOffset + times * length * B * N * D, remain, 0);
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::ComputeBackward(int32_t in_offset, int32_t out_offset,
+                                                                              int32_t length, int32_t row)
+{
+    int32_t nBurst = 0;
+    if (transpose_type == 0) {
+        // SBH
+        nBurst = (UB_MAX / (B * N * D) < 8) ? row : B * N;
+    }
+
+    AscendC::DataCopyPad(cur_tensor_f32, gm_in_tensor[in_offset],
+                         AscendC::DataCopyExtParams(nBurst, D * length * 4, (S - length) * D * 4, 0, 0),
+                         AscendC::DataCopyPadExtParams<float>(false, 0, 0, 0));
+
+    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+    int repeatTimes =
+        nBurst * length * D / (256 / sizeof(float)); // B * N * length * D <= UB_MAX (16384)  即 repeatTimes <=256
+    if constexpr (IF_BF16) {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                        halfTimes, 1, 1, 4, 8);
+        }
+    } else {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                       halfTimes, 1, 1, 4, 8);
+        }
+    }
+
+    pipe_barrier(PIPE_V);
+    if (nBurst >= length) {
+        for (int j = 0; j < length; j++) {
+            AscendC::DataCopy(tensor_transpose[j * nBurst * D], cur_tensor[j * D],
+                              AscendC::DataCopyParams(nBurst, D / 16, (length - 1) * D / 16, 0));
+        }
+    } else {
+        for (int j = 0; j < nBurst; j++) {
+            AscendC::DataCopy(tensor_transpose[j * D], cur_tensor[j * D * length],
+                              AscendC::DataCopyParams(length, D / 16, 0, (nBurst - 1) * D / 16));
+        }
+    }
+    pipe_barrier(PIPE_V);
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    if (transpose_type == 0) {
+        if (UB_MAX / (B * N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (B * N - row) * D * 2, 0));
+        } else {
+            int ub_size = B * N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    } else if (transpose_type == 1) {
+        if (UB_MAX / (N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (N - row) * D * 2, 0));
+        } else {
+            int ub_size = N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::SBHForward()
+{
+    int length, times, remain, inOffset, rowsumOffset, outOffset = 0;
+    int length_per_core = S / vector_num / 8 * 8;       // 16
+    int tail_length = S - length_per_core * vector_num; // 256 288
+    int tail = tail_length / 8;                         // 32
+
+    if (cur_vector_idx < tail) {
+        length_per_core += 8;
+        inOffset = length_per_core * cur_vector_idx * D;
+        rowsumOffset = length_per_core * cur_vector_idx;
+        outOffset = length_per_core * cur_vector_idx * B * N * D;
+    } else {
+        inOffset = (length_per_core * cur_vector_idx + tail_length) * D;
+        rowsumOffset = length_per_core * cur_vector_idx + tail_length;
+        outOffset = (length_per_core * cur_vector_idx + tail_length) * B * N * D;
+    }
+
+    if (UB_MAX >= B * N * D * length_per_core) {
+        length = length_per_core;
+        times = 1;
+        remain = 0;
+    } else {
+        if (UB_MAX / (B * N * D) < 8) {
+            length = 8;
+        } else {
+            length = UB_MAX / (B * N * D) / 8 * 8; // 16
+        }
+        times = length_per_core / length;  // 1
+        remain = length_per_core % length; // 8
+    }
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (UB_MAX / (B * N * D) < 8) {
+        auto row = (UB_MAX / D / length); // UB_MAX = 16384
+        auto loopBN = (B * N) / row;
+        auto remainBN = (B * N) % row;
+        for (int j = 0; j < loopBN; j++) {
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeForward(inOffset + i * length * D + j * S * D * row, rowsumOffset + i * length + j * S * row,
+                               outOffset + i * length * B * N * D + j * row * D,
+                               // outOffset + i * length * D + j * S * D * row, // no transpose copy
+                               length, row);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+        // 尾行逻辑
+        if (remainBN > 0) {
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeForward(inOffset + i * length * D + loopBN * S * D * row,
+                               rowsumOffset + i * length + loopBN * S * row,
+                               outOffset + i * length * B * N * D + loopBN * row * D,
+                               // outOffset + i * length * D + j * S * D * remainBN, // no transpose copy
+                               length, remainBN);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+    } else {
+        for (int i = 0; i < times; i++) {
+            WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            ComputeForward(inOffset + i * length * D, rowsumOffset + i * length, outOffset + i * length * B * N * D,
+                           length, 0);
+            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+        }
+    }
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (remain > 0) {
+        ComputeForward(inOffset + times * length * D, rowsumOffset + times * length,
+                       outOffset + times * length * B * N * D, remain, 0);
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::ComputeForward(int32_t in_offset, int32_t rowsum_offset,
+                                                                             int32_t out_offset, int32_t length,
+                                                                             int32_t row)
+{
+    int32_t nBurst = 0;
+    if (transpose_type == 0) {
+        // SBH
+        nBurst = (UB_MAX / (B * N * D) < 8) ? row : B * N;
+    }
+
+    AscendC::DataCopyPad(cur_tensor_out, gm_in_tensor[in_offset],
+                         AscendC::DataCopyExtParams(nBurst, D * length * 4, (S - length) * D * 4, 0, 0),
+                         AscendC::DataCopyPadExtParams<float>(false, 0, 0, 0));
+
+    AscendC::DataCopyPad(
+        tensor_rowsum_brcb_f32, gm_rowsum_tensor[rowsum_offset],
+        AscendC::DataCopyExtParams(nBurst, length * sizeof(float), (S - length) * 1 * sizeof(float), 0, 0),
+        AscendC::DataCopyPadExtParams<float>(false, 0, 0, 0));
+
+    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+
+    auto lines_per_loop = nBurst * length;
+    brcb_v<ArchType::ASCEND_V220, uint32_t>(tensor_transpose.template ReinterpretCast<uint32_t>(),
+                                            tensor_rowsum_brcb.template ReinterpretCast<uint32_t>(), 1, 8,
+                                            lines_per_loop);
+    pipe_barrier(PIPE_V);
+    // D个FP32 需要两个repeat； 因此，需要先算前半段，再算后半段
+    AscendC::Div<float, false>(cur_tensor_out, cur_tensor_out, tensor_transpose_f32, (uint64_t)0, lines_per_loop,
+                               AscendC::BinaryRepeatParams(1, 1, 0, 16, 16, 1));
+    pipe_barrier(PIPE_V);
+    AscendC::Div<float, false>(cur_tensor_out[HEAD_DIM / 2], cur_tensor_out[HEAD_DIM / 2], tensor_transpose_f32,
+                               (uint64_t)0, lines_per_loop, AscendC::BinaryRepeatParams(1, 1, 0, 16, 16, 1));
+    pipe_barrier(PIPE_V);
+
+    int repeatTimes =
+        nBurst * length * D / (256 / sizeof(float)); // B * N * length * D <= UB_MAX (16384)  即 repeatTimes <=256
+    if constexpr (IF_BF16) {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                        halfTimes, 1, 1, 4, 8);
+        }
+    } else {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                       halfTimes, 1, 1, 4, 8);
+        }
+    }
+    pipe_barrier(PIPE_V);
+
+    if (nBurst >= length) {
+        for (int j = 0; j < length; j++) {
+            AscendC::DataCopy(tensor_transpose[j * nBurst * D], cur_tensor[j * D],
+                              AscendC::DataCopyParams(nBurst, D / 16, (length - 1) * D / 16, 0));
+        }
+    } else {
+        for (int j = 0; j < nBurst; j++) {
+            AscendC::DataCopy(tensor_transpose[j * D], cur_tensor[j * D * length],
+                              AscendC::DataCopyParams(length, D / 16, 0, (nBurst - 1) * D / 16));
+        }
+    }
+    pipe_barrier(PIPE_V);
+
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+    if (transpose_type == 0) {
+        if (UB_MAX / (B * N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (B * N - row) * D * 2, 0));
+        } else {
+            int ub_size = B * N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    } else if (transpose_type == 1) {
+        if (UB_MAX / (N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (N - row) * D * 2, 0));
+        } else {
+            int ub_size = N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    }
+}
+
+#endif
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/VectorForward.h b/src/kernels/mixkernels/laser_attention/op_kernel/VectorForward.h
new file mode 100644
index 00000000..d655dbc0
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/VectorForward.h
@@ -0,0 +1,3030 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef __VECTORFORWARD_H__
+#define __VECTORFORWARD_H__
+
+#define USE_ASCENDC
+#include "ppmatmul_const.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+using namespace AscendC;
+#ifdef __DAV_C220_VEC__
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T> class VectorForward {
+public:
+    __aicore__ inline VectorForward(){};
+    __aicore__ inline void Init(__gm__ uint8_t *__restrict__ aCube1, __gm__ uint8_t *__restrict__ bCube1,
+                                __gm__ uint8_t *__restrict__ bCube2, __gm__ uint8_t *__restrict__ maskGm,
+                                __gm__ uint8_t *__restrict__ onesGm, __gm__ float *__restrict__ zerosGm,
+                                __gm__ uint8_t *__restrict__ scoreGm, __gm__ float *__restrict__ cCube2,
+                                __gm__ uint8_t *__restrict__ cube2_out, __gm__ float *__restrict__ logSumGm,
+                                __gm__ float *__restrict__ gmRowsum, int32_t qSeqLength, int32_t kSeqLength, int32_t H,
+                                int32_t B, int32_t Y, bool qk, int32_t windowsBlockNum, int32_t maskSeqLength,
+                                float scale, int32_t transposeType);
+    __aicore__ inline void Run();
+    __aicore__ inline void SetHighPrecision(bool isHighPrecision)
+    {
+        this->isHighPrecision = isHighPrecision;
+    };
+
+    struct UB_FOR_SHORT_LEN_ATTN_SCORE {
+        LocalTensor<float> tensorForLoadAttnScoreFp32{};
+        LocalTensor<float> tensorForStoreSecondAttnScoreFp32{};
+        LocalTensor<float> tensorForLoadOneBlockTriMaskFp32{};
+        LocalTensor<float> tensorForCaclRowmaxFp32{};
+        LocalTensor<float> tensorForCaclFinalRowmaxFp32{};
+        LocalTensor<float> tensorForCaclSecondFinalRowmaxFp32{};
+        LocalTensor<float> tensorForCaclFinalRowsumFp32{};
+        LocalTensor<float> tensorForCaclSecondFinalRowsumFp32{};
+        LocalTensor<float> tensorForVbrcbRowmaxFp32{};
+        LocalTensor<float> tensorForVbrcbSecondRowmaxFp32{};
+        LocalTensor<float> tensorForRecordRowmaxFp32{};
+        LocalTensor<float> tensorForRecordRowsumFp32{};
+    };
+
+    struct UB_FOR_LONG_SEQ_ATTN_SCORE {
+        LocalTensor<float> tensorForLoadAttnScoreFp32{};
+        LocalTensor<float> tensorForLoadExtraFirstAttnScoreFp32{};
+        LocalTensor<float> tensorForLoadOneBlockTriMaskFp32{};
+        LocalTensor<float> tensorForCaclRowmaxFp32{};
+        LocalTensor<float> tensorForCaclFinalRowmaxFp32{};
+        LocalTensor<float> tensorForCaclFinalRowsumFp32{};
+        LocalTensor<float> tensorForVbrcbRowmaxFp32{};
+        LocalTensor<float> tensorForRecordRowmaxFp32{};
+        LocalTensor<float> tensorForRecordRowsumFp32{};
+    };
+
+    struct UB_FOR_MDDIUM_SEQ_ATTN_SCORE {
+        LocalTensor<float> tensorForStoreOneLineAttnScoreFp32{};
+        LocalTensor<float> tensorForLoadAttnScoreFp32{};
+        LocalTensor<float> tensorForCaclRowmaxFp32{};
+        LocalTensor<float> tensorForCaclFinalRowmaxFp32{};
+        LocalTensor<float> tensorForCaclFinalRowsumFp32{};
+        LocalTensor<float> tensorForVbrcbRowmaxFp32{};
+        LocalTensor<float> tensorForRecordRowmaxFp32{};
+        LocalTensor<float> tensorForRecordRowsumFp32{};
+        LocalTensor<float> tensorForLoadOneBlockTriMaskFp32{};
+    };
+
+    // UB空间划分，求归一化
+    // bufForLoadOFp32：装载计算好的O
+    // bufForLoadRowsumFp32：装载计算好的rowsum
+    // bufForBrcbRowsumFp32：32字节对齐的rowsum
+    struct UB_FOR_NORMALIZE {
+        LocalTensor<float> tensorForLoadOFp32{};
+        LocalTensor<float> tensorForLoadRowsumFp32{};
+        LocalTensor<float> tensorForBrcbRowsumFp32{};
+
+        int32_t oPingPongInterval;
+        int32_t rowsumPingPongInterval;
+        int32_t rowsumBrcbPingPongInterval;
+    };
+
+    // blockNumPerStep：当前处理的块数量（对齐MAX_BLOCK_PER_ONE_PROC）
+    // blockNumForLast：最后一次ping-pong（不满MAX_BLOCK_PER_ONE_PROC时）
+    // lastPaddingBlockNum：当前需要padding的块数量
+    // sectionStartLineOffset：记录当前行的起始地址
+    // totalFragNum：总分段数量
+    // curFrag：当前分段id
+    // triMatrixNum：三角阵的数量  0-非三角阵；1-三角阵，非unirow；2-三角阵，unirow
+    // bufOffset：存score * scal的偏移
+    struct PARAM_MEDIUM_SEQ_EXP {
+        int32_t blockNumPerStep;
+        int32_t blockNumForLast;
+
+        int32_t lastPaddingBlockNum;
+
+        int32_t sectionStartLineOffset;
+        int32_t sectionMaskOffset;
+
+        int32_t totalFragNum;
+        int32_t curFrag;
+
+        bool tailBlock;
+
+        int32_t triMatrixNum;
+        bool applyTriMask;
+
+        int32_t bufOffset;
+
+        int32_t recordRowmaxOffset;
+    };
+
+    // sectionStartLineOffset：attn score 起始地址
+    // sectionMaskOffset：mask的地址 （两个secion的相同）
+    // recordRowmaxOffset：记录rowmax的偏移
+    struct PARAM_SHORT_SEQ_MAX {
+        int32_t sectionOneBlockNum;
+        int32_t sectionTwoBlockNum;
+        int32_t sectionOnePaddingBlockNum;
+        int32_t sectionTwoPaddingBlockNum;
+
+        int32_t sectionStartLineOffset;
+        int32_t sectionMaskOffset;
+
+        int32_t recordRowmaxOffset;
+    };
+
+    // sectionBlockNum：当前处理的块数量
+    // sectionPaddingBlockNum：当前需要padding的块数量
+    // sectionStartLineOffset：attn score 起始地址
+    // sectionMaskOffset：mask的地址 （两个secion的相同）
+    // totalFragNum：总分段数量
+    // curFrag：当前分段id
+    // triMatrixNum：三角阵的数量  0-非三角阵；1-三角阵，非unirow；2-三角阵，unirow
+    struct PARAM_LONG_SEQ_EXP {
+        int32_t sectionBlockNum;
+        int32_t sectionPaddingBlockNum;
+
+        int32_t sectionStartLineOffset;
+        int32_t sectionMaskOffset;
+
+        int32_t totalFragNum;
+        int32_t curFrag;
+
+        int32_t triMatrixNum;
+        int32_t applyTriMask;
+    };
+
+    struct UB_FOR_LN_ROWSUM {
+        __ubuf__ float *ubBufForRowsum;
+        __ubuf__ float *ubBufForRowsumRes;
+    };
+
+private:
+    __aicore__ __inline__ void initCube2WorkSpace();
+    __aicore__ __inline__ void InitWorkSpace();
+    __aicore__ __inline__ void AllocateUbufForNorm(UB_FOR_NORMALIZE *UbNorm);
+    __aicore__ __inline__ void GetGlobalInfo(int32_t batchNum, int32_t headNum, int32_t yNumCubePerLine,
+                                             int32_t qSeqLen, int32_t kSeqLen, bool qkTriangle, int32_t VNum,
+                                             int32_t *cubeNum, int32_t *fCubeGroupNum, int32_t *blockNumPerFullLine,
+                                             int32_t *blockNumPerHead, int32_t *loopTimesForAllFullLine,
+                                             int32_t *loopTailsForAllFullLine);
+    __aicore__ __inline__ void GetLocalInfo(int32_t yNumCubePerLine, int32_t *cubeGroupBelong,
+                                            int32_t *cubeGroupInnderIdx, int32_t *eachVectorProcLineNum);
+    __aicore__ __inline__ void GetInitOffsetForCurrentCore(int32_t cubeGroupBelong, int32_t cubeGroupInnderIdx,
+                                                           int32_t blockNumPerFullLine, int32_t eachVectorProcLineNum,
+                                                           int32_t *eachVectorStartLineOffset,
+                                                           int32_t *triMatrixMaskOffset);
+    __aicore__ __inline__ void GetSubSeqLengthPerProc(int32_t kSeqLen, int32_t blockNumPerFullLine,
+                                                      int32_t *subSeqLengthPerProc);
+    __aicore__ __inline__ void GetPaddingInfoForRowMax(int32_t totalBlockNum, int32_t *paddingBlockNum);
+    __aicore__ __inline__ void PaddingForRowMaxTensor(int32_t totalBlockNum, int32_t paddingBlockNum,
+                                                      const LocalTensor<float> &paddingTensor, float value);
+    __aicore__ __inline__ void GetFullLineOffsetInOneHead(int32_t curLoopIndex, int32_t cubeGroupBelong,
+                                                          int32_t fCubeGroupNum, int32_t blockNumPerHead,
+                                                          int32_t *blockLineOffsetInOneHead);
+    __aicore__ __inline__ void
+    GetTriMatrixSectionBlockOffset(int32_t blockLineOffsetInOneHead, int32_t blockNumPerFullLine,
+                                   int32_t *sectionOneStartBlockOffset, int32_t *sectionTwoStartBlockOffset,
+                                   int32_t *sectionOneEndBlockOffset, int32_t *sectionTwoEndBlockOffset);
+    __aicore__ __inline__ void CaclMaxTensor(const LocalTensor<float> &tensorForCacl, int32_t blockNum);
+    __aicore__ __inline__ __attribute__((noinline)) void
+    ProcessCaclMaxTensor(int32_t basicBlockNum, int32_t paddingBlockNum, bool ppFirstSection,
+                         const LocalTensor<float> &curTensorForAttnScore, const LocalTensor<float> &curTensorForRowmax,
+                         const LocalTensor<float> &tensorForCaclFinalRowmaxFp32);
+    __aicore__ __inline__ void CaclSumTensor(const LocalTensor<float> &tensorForCacl, int32_t blockNum);
+    __aicore__ __inline__ __attribute__((noinline)) void
+    ProcessCaclSumTensor(int32_t basicBlockNum, int32_t paddingBlockNum, bool ppFirstSection,
+                         const LocalTensor<float> &curTensorForAttnScore, const LocalTensor<float> &curTensorForRowsum,
+                         const LocalTensor<float> &tensorForCaclFinalRowsumFp32);
+    __aicore__ __inline__ void ProcessLinePhaseOneForShortSeqMax(bool qkTriangle, PARAM_SHORT_SEQ_MAX Param,
+                                                                 int32_t pingPongFlag, bool firstLine,
+                                                                 __gm__ WORKSPACE_T *attnScoreGm,
+                                                                 __gm__ INPUT_T *attnMaskGm,
+                                                                 UB_FOR_SHORT_LEN_ATTN_SCORE &UbAttn);
+    __aicore__ __inline__ void ProcessLinePhaseOneForShortSeqExp(bool qkTriangle, PARAM_SHORT_SEQ_MAX Param,
+                                                                 int32_t pingPongFlag, __gm__ WORKSPACE_T *attnScoreGm,
+                                                                 __gm__ INPUT_T *attnMaskGm,
+                                                                 UB_FOR_SHORT_LEN_ATTN_SCORE &UbAttn, int32_t offset);
+    __aicore__ __inline__ void ProcessLinePhaseOneForMax(PARAM_LONG_SEQ_EXP Param, int32_t pingPongFlag,
+                                                         __gm__ WORKSPACE_T *attnScoreGm, __gm__ INPUT_T *attnMaskGm,
+                                                         UB_FOR_LONG_SEQ_ATTN_SCORE &UbAttn, bool sparseFlag);
+    __aicore__ __inline__ void ProcessLinePhaseOneForExp(PARAM_LONG_SEQ_EXP Param, int32_t pingPongFlag,
+                                                         __gm__ WORKSPACE_T *attnScoreGm, __gm__ INPUT_T *attnMaskGm,
+                                                         UB_FOR_LONG_SEQ_ATTN_SCORE &UbAttn, int32_t tmp,
+                                                         bool sparseFlag);
+    __aicore__ __inline__ void GetNormLocalInfo(int32_t qSeqLen, int32_t headNum, int32_t batchNum,
+                                                int32_t *eachCoreProcessLines, int32_t *eachCoreOffsetLines);
+    __aicore__ __inline__ void ProcessForNormalize(int32_t linesPerLoop, int32_t pvResultOffset, int32_t rowsumOffset,
+                                                   int32_t pingPongFlag, UB_FOR_NORMALIZE &UbNorm,
+                                                   __gm__ float *__restrict__ gmCCube2,
+                                                   __gm__ INPUT_T *__restrict__ gm_cube2_out,
+                                                   __gm__ float *__restrict__ rowsumGm);
+    __aicore__ __inline__ void AttentionScoreNormalize(int32_t maxProcLen, int32_t curCoreProcessLines,
+                                                       int32_t curCoreOffsetLines, UB_FOR_NORMALIZE &UbNorm,
+                                                       __gm__ float *__restrict__ gmCCube2,
+                                                       __gm__ INPUT_T *__restrict__ gm_cube2_out,
+                                                       __gm__ float *__restrict__ rowsumGm);
+    __aicore__ __inline__ void
+    AttentionScoreShortDoubleLineOne(bool qkTriangle, int32_t sectionOneBlockNums, int32_t sectionTwoBlockNums,
+                                     int32_t triMatrixMaskOffset, int32_t eachVectorProcLineNum,
+                                     int32_t localSectionOneStartLineOffset, int32_t localSectionTwoStartLineOffset,
+                                     __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+                                     __gm__ INPUT_T *__restrict__ attnMaskGm, UB_FOR_SHORT_LEN_ATTN_SCORE &UbAttn);
+    __aicore__ __inline__ void
+    AttentionScoreDoubleLine(bool qkTriangle, int32_t sectionLoopTimes, int32_t sectionOneBlockNums,
+                             int32_t sectionTwoBlockNums, int32_t triMatrixMaskOffset, int32_t eachVectorProcLineNum,
+                             int32_t localSectionOneStartLineOffset, int32_t localSectionTwoStartLineOffset,
+                             int32_t triMatrixNum, int32_t triMatrixOffset[],
+                             __gm__ WORKSPACE_T *__restrict__ attnScoreGm, __gm__ INPUT_T *__restrict__ attnMaskGm,
+                             UB_FOR_LONG_SEQ_ATTN_SCORE &UbAttn, bool sparseFlag);
+    __aicore__ __inline__ void GetUniRowmaxSeqInfoPerProc(int32_t blockNumPerFullLine, int32_t subSeqLengthPerProc,
+                                                          int32_t *pingBlockOffsetNum, int32_t *pongBlockOffsetNum,
+                                                          int32_t *tailBlockOffsetNum, int32_t *tailBlockNum,
+                                                          int32_t *pingPongTimes);
+    __aicore__ __inline__ void ProcessLinePhaseOneForMax(PARAM_MEDIUM_SEQ_EXP Param, int32_t pingPongFlag,
+                                                         bool firstProc, __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+                                                         __gm__ INPUT_T *__restrict__ attnMaskGm,
+                                                         UB_FOR_MDDIUM_SEQ_ATTN_SCORE &UbAttn, int32_t lines);
+    __aicore__ __inline__ void ProcessLinePhaseOneForExp(PARAM_MEDIUM_SEQ_EXP Param, int32_t pingPongFlag,
+                                                         bool firstProc, __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+                                                         __gm__ INPUT_T *__restrict__ attnMaskGm,
+                                                         UB_FOR_MDDIUM_SEQ_ATTN_SCORE &UbAttn, int32_t fp32Offset);
+    __aicore__ __inline__ void
+    AttentionScoreSingleLine(bool qkTriangle, int32_t sectionLoopTimes, int32_t sectionOneBlockNums,
+                             int32_t sectionTwoBlockNums, int32_t triMatrixMaskOffset, int32_t eachVectorProcLineNum,
+                             int32_t localSectionOneStartLineOffset, int32_t localSectionTwoStartLineOffset,
+                             int32_t triMatrixNum, __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+                             __gm__ INPUT_T *__restrict__ attnMaskGm, UB_FOR_MDDIUM_SEQ_ATTN_SCORE &UbAttn);
+    __aicore__ __inline__ void AllocateUbufForLongSeqAttnScore(UB_FOR_LONG_SEQ_ATTN_SCORE *UbAttnScore);
+    __aicore__ __inline__ void AllocateUbufForMediumSeqAttnScore(UB_FOR_MDDIUM_SEQ_ATTN_SCORE *UbAttnScore);
+    __aicore__ __inline__ void AllocateUbufForShortSeqAttnScore(UB_FOR_SHORT_LEN_ATTN_SCORE *UbAttnScore);
+    __aicore__ __inline__ void GetSoftmaxOffset(int32_t sectionNum, int32_t yCubeNumPerLine, int32_t timesSyncCube,
+                                                int32_t qSeqLen, int32_t headNum, bool qkTriangle,
+                                                int32_t *totalOffsetForRowmax);
+    __aicore__ __inline__ void RecordRowmaxAndRowsum(bool sparseFlag, int32_t timesSyncCube,
+                                                     LocalTensor<float> tensorForLong, LocalTensor<float> tensorForMid,
+                                                     LocalTensor<float> tensorForShort,
+                                                     GlobalTensor<float> tensorOutput);
+    __aicore__ __inline__ void InitUbForSoftSync();
+    __aicore__ __inline__ void SoftSyncByGroup(int32_t cubeGroupBelong);
+
+private:
+    __gm__ INPUT_T *__restrict__ gmACube1;
+    __gm__ INPUT_T *__restrict__ gmBCube1;
+    __gm__ INPUT_T *__restrict__ gmBCube2;
+    __gm__ INPUT_T *__restrict__ attnMaskGm;
+    __gm__ INPUT_T *__restrict__ gmOnes;
+    __gm__ INPUT_T *__restrict__ gmZeros;
+    __gm__ WORKSPACE_T *__restrict__ attnScoreGm;
+    __gm__ float *__restrict__ gmCCube2;
+    __gm__ INPUT_T *__restrict__ gm_cube2_out;
+    __gm__ float *__restrict__ logSumMaxGm;
+    __gm__ float *__restrict__ rowsumGm;
+
+    __ubuf__ float *__restrict__ ubForSoftsyncFlags;
+    __ubuf__ float *__restrict__ ubForSoftsyncCheck;
+
+    AsdopsBuffer<ArchType::ASCEND_V220> calBuf;
+    GlobalTensor<INPUT_T> gmACube1Tensor;
+    GlobalTensor<INPUT_T> gmBCube1Tensor;
+    GlobalTensor<INPUT_T> gmBCube2Tensor;
+    GlobalTensor<INPUT_T> attnMaskGmTensor;
+    GlobalTensor<INPUT_T> gmOnesTensor;
+    GlobalTensor<INPUT_T> gmZerosTensor;
+    GlobalTensor<float> attnScoreGmTensor;
+    GlobalTensor<float> gmCCube2Tensor;
+    GlobalTensor<INPUT_T> gmCCube2OutTensor;
+    GlobalTensor<float> logSumMaxGmTensor;
+    GlobalTensor<float> rowsumGmTensor;
+    LocalTensor<float> ubForSoftsyncFlagsTensor;
+    LocalTensor<float> ubForSoftsyncCheckTensor;
+    LocalTensor<float> fp32TestTensor;
+    LocalTensor<float> ubScoreZerotensor;
+    // int32_t seq_len;
+    int32_t qSeqLen;
+    int32_t kSeqLen;
+    int32_t headNum;
+    int32_t batchNum;
+    int32_t yCubeNumPerLine;
+    bool qkTriangle;
+    int32_t VNum;
+    int32_t softSyncTimesCount = 0;
+    bool useSoftSync = false;
+    bool isHighPrecision = true;
+    int32_t maskSeqLength;
+    int32_t rowPerTime = 1;
+    float scale;
+    int32_t transposeType;
+};
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ inline void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::Init(
+    __gm__ uint8_t *__restrict__ aCube1, __gm__ uint8_t *__restrict__ bCube1, __gm__ uint8_t *__restrict__ bCube2,
+    __gm__ uint8_t *__restrict__ maskGm, __gm__ uint8_t *__restrict__ onesGm, __gm__ float *__restrict__ zerosGm,
+    __gm__ uint8_t *__restrict__ scoreGm, __gm__ float *__restrict__ cCube2, __gm__ uint8_t *__restrict__ cube2_out,
+    __gm__ float *__restrict__ logSumGm, __gm__ float *__restrict__ gmRowsum,
+    // int32_t S
+    int32_t qSeqLength, int32_t kSeqLength, int32_t H, int32_t B, int32_t Y, bool qk, int32_t windowsBlockNum,
+    int32_t maskSeqLength, float scale, int32_t transposeType)
+{
+    gmACube1 = (__gm__ INPUT_T *__restrict__)aCube1;
+    gmBCube1 = (__gm__ INPUT_T *__restrict__)bCube1;
+    gmBCube2 = (__gm__ INPUT_T *__restrict__)bCube2;
+    attnMaskGm = (__gm__ INPUT_T *__restrict__)maskGm;
+    gmOnes = (__gm__ INPUT_T *__restrict__)onesGm;
+    gmZeros = (__gm__ INPUT_T *__restrict__)zerosGm;
+    attnScoreGm = (__gm__ WORKSPACE_T *__restrict__)scoreGm;
+    gmCCube2 = cCube2;
+    gm_cube2_out = (__gm__ INPUT_T *__restrict__)cube2_out;
+    logSumMaxGm = logSumGm;
+    rowsumGm = gmRowsum;
+
+    gmACube1Tensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(aCube1));
+    gmBCube1Tensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(bCube1));
+    gmBCube2Tensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(bCube2));
+    attnMaskGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(maskGm));
+    gmOnesTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(onesGm));
+    gmZerosTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(zerosGm));
+    attnScoreGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>((__gm__ float *__restrict__)scoreGm));
+    gmCCube2Tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(cCube2));
+    gmCCube2OutTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(cube2_out));
+    logSumMaxGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(logSumGm));
+    rowsumGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmRowsum));
+    fp32TestTensor = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(192 * 1024 - 128 * 4);
+    qSeqLen = qSeqLength;
+    kSeqLen = kSeqLength;
+    headNum = H;
+    batchNum = B;
+    yCubeNumPerLine = Y;
+    qkTriangle = qk;
+    VNum = windowsBlockNum;
+    this->maskSeqLength = maskSeqLength;
+    this->scale = scale;
+    this->transposeType = transposeType;
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::InitWorkSpace()
+{
+    int32_t matrixNumbers = BASE_BLOCK_SIZE;
+    int32_t vectorNum = AscendC::GetBlockNum() * VEC_NUM_PER_CUBE;
+    int32_t vectorId = AscendC::GetBlockIdx();
+
+    if (yCubeNumPerLine >= 1) {
+        vectorNum = 1;
+        vectorId = 0;
+    }
+
+    LocalTensor<INPUT_T> tensorScoreOne = calBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(0);
+
+    AscendC::Duplicate(tensorScoreOne, (INPUT_T)1, 128, 128, 1, 8);
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+    AscendC::DataCopy(gmOnesTensor, tensorScoreOne, AscendC::DataCopyParams(1, matrixNumbers / 16, 0, 0));
+    PipeBarrier<PIPE_ALL>();
+
+    LocalTensor<float> tensorScoreZero = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    int32_t perLength = 256;
+    int32_t numbersPerVector = (batchNum * headNum * qSeqLen + vectorNum - 1) / vectorNum;
+    int32_t repeat = (numbersPerVector + perLength - 1) / perLength;
+    AscendC::Duplicate(tensorScoreZero, (float)0, 64, 4, 1, 8);
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    // Y >=1 时，得到大小为repeat*256*4B的gm空间
+    for (int i = 0; i < repeat; i++) {
+        AscendC::DataCopy(rowsumGmTensor[vectorId * perLength * repeat + i * perLength], tensorScoreZero,
+                          AscendC::DataCopyParams(1, perLength / 8, 0, 0));
+    }
+
+    if (useSoftSync) {
+        // 拷贝0到zerosGm，用于软同步做累加
+        for (int i = 0; i < 16; i++) {
+            GlobalTensor<float> tmpZeroGmTensor;
+            tmpZeroGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmZeros + i * perLength));
+            AscendC::DataCopy(tmpZeroGmTensor, tensorScoreZero, AscendC::DataCopyParams(1, perLength / 8, 0, 0));
+        }
+    }
+
+    PipeBarrier<PIPE_ALL>();
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ inline void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::Run()
+{
+    AscendC::SetAtomicNone();
+    AscendC::SetMaskNorm();
+    AscendC::SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
+
+    // ture: 三角阵，一行统一计算 (还未支持末尾tri mask)；false：一行分两个section计算
+    bool unirowMode = false;
+
+    int32_t cubeNum = 0;
+    int32_t fCubeGroupNum = 0;
+    int32_t blockNumPerFullLine = 0;
+    int32_t blockNumPerHead = 0;
+    int32_t loopTimesForAllFullLine = 0;
+    int32_t loopTailsForAllFullLine = 0;
+
+    GetGlobalInfo(batchNum, headNum, yCubeNumPerLine, qSeqLen, kSeqLen, qkTriangle, VNum, &cubeNum, &fCubeGroupNum,
+                  &blockNumPerFullLine, &blockNumPerHead, &loopTimesForAllFullLine, &loopTailsForAllFullLine);
+
+    int32_t cubeGroupBelong = 0;
+    int32_t cubeGroupInnderIdx = 0;
+    int32_t eachVectorProcLineNum = 0;
+
+    GetLocalInfo(yCubeNumPerLine, &cubeGroupBelong, &cubeGroupInnderIdx, &eachVectorProcLineNum);
+
+    int32_t eachVectorStartLineOffset = 0;
+    int32_t triMatrixMaskOffset = 0;
+
+    GetInitOffsetForCurrentCore(cubeGroupBelong, cubeGroupInnderIdx, blockNumPerFullLine, eachVectorProcLineNum,
+                                &eachVectorStartLineOffset, &triMatrixMaskOffset);
+
+    UB_FOR_NORMALIZE UbNorm;
+    // 归一化在EXP计算完成之后，这里重新分配UB地址，不会影响rowmax计算
+    AllocateUbufForNorm(&UbNorm);
+
+    // attention score （取代UB_FOR_EXP）
+    UB_FOR_SHORT_LEN_ATTN_SCORE UbShortSeqAttn;
+    UB_FOR_LONG_SEQ_ATTN_SCORE UbLongSeqAttn;
+    UB_FOR_MDDIUM_SEQ_ATTN_SCORE UbMediumSeqAttn;
+
+    // 三角阵拼接，左侧部分的起始block
+    int32_t sectionOneStartBlockOffset = 0;
+    // 三角阵拼接，右侧部分的起始block
+    int32_t sectionTwoStartBlockOffset = 0;
+
+    // 三角阵拼接, 左侧最后一个block的偏移
+    int32_t sectionOneEndBlockOffset = 0;
+    // 三角阵拼接, 右侧最后一个block的偏移
+    int32_t sectionTwoEndBlockOffset = 0;
+
+    int32_t sectionOneBlockNums = 0;
+    int32_t sectionTwoBlockNums = 0;
+
+    // 当前处理的完整行，在当前Head中位于第几行
+    int32_t blockLineOffsetInOneHead = 0;
+
+    // 循环内部使用
+    int32_t localSectionOneStartLineOffset = 0;
+    int32_t localSectionTwoStartLineOffset = 0;
+
+    // 三角块的offset
+    int32_t triMatrixOffset[2] = {0};
+    int32_t triMatrixNum = (qkTriangle == false ? 0 : 2);
+
+    initCube2WorkSpace();
+
+    FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+    WaitFlagDev(AIVFLAGID);
+
+    FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+
+    if (useSoftSync) {
+        InitUbForSoftSync();
+    }
+
+    // 大循环
+    for (int timesSyncCube = 0; timesSyncCube < loopTimesForAllFullLine; timesSyncCube++) {
+        // workspace double buffer
+        if (timesSyncCube % 2 == 0 && timesSyncCube != 0) {
+            attnScoreGm -= blockNumPerFullLine * fCubeGroupNum * BASE_BLOCK_DATA_NUM;
+        }
+        if (timesSyncCube % 2 == 1) {
+            attnScoreGm += blockNumPerFullLine * fCubeGroupNum * BASE_BLOCK_DATA_NUM;
+        }
+
+        bool sparseFlag = false;
+        if (VNum > 0) {
+            GetFullLineOffsetInOneHead(timesSyncCube, cubeGroupBelong, fCubeGroupNum, blockNumPerHead,
+                                       &blockLineOffsetInOneHead);
+            if (blockLineOffsetInOneHead >= VNum / 2) {
+                sparseFlag = true;
+                qkTriangle = false;
+            } else {
+                sparseFlag = false;
+                qkTriangle = true;
+            }
+        }
+        // Section loop times
+        int32_t sectionLoopTimes = qkTriangle == false ? 1 : 2;
+        if (qkTriangle == true) {
+            GetFullLineOffsetInOneHead(timesSyncCube, cubeGroupBelong, fCubeGroupNum, blockNumPerHead,
+                                       &blockLineOffsetInOneHead);
+        }
+
+        if (loopTailsForAllFullLine > 0 && timesSyncCube == loopTimesForAllFullLine - 1) {
+            // 处理尾块行（如果有）；部分vector会被闲置 -- 当前128一定能整除num(vector)
+            if (cubeGroupBelong >= loopTailsForAllFullLine) {
+                if (yCubeNumPerLine > 1) {
+                    if (useSoftSync) {
+                        SoftSyncByGroup(cubeGroupBelong);
+                    } else {
+                        FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+                        WaitFlagDev(AIVFLAGID);
+                    }
+                }
+                continue;
+            }
+        }
+
+        if (qkTriangle == true) {
+            // 只要是三角阵，都需要计算两段地址；非三角阵没有必要计算
+            GetTriMatrixSectionBlockOffset(blockLineOffsetInOneHead, blockNumPerFullLine, &sectionOneStartBlockOffset,
+                                           &sectionTwoStartBlockOffset, &sectionOneEndBlockOffset,
+                                           &sectionTwoEndBlockOffset);
+
+            triMatrixOffset[0] = sectionOneEndBlockOffset;
+            triMatrixOffset[1] = sectionTwoEndBlockOffset;
+
+            // 每个Section， 如果有tail，则一定是tri； 否则，ping不会有， pong最后一次的最后一个
+            sectionTwoBlockNums = sectionTwoEndBlockOffset - sectionTwoStartBlockOffset + 1;
+            // 非三角阵， 该参数没用
+            localSectionTwoStartLineOffset =
+                eachVectorStartLineOffset + BASE_BLOCK_DATA_NUM * sectionTwoStartBlockOffset;
+        } else {
+            sectionOneStartBlockOffset = 0;
+            // 每行最后一个block
+            sectionOneEndBlockOffset = blockNumPerFullLine - 1;
+        }
+
+        // section one其实偏移一定是block，不用再累加
+        localSectionOneStartLineOffset = eachVectorStartLineOffset;
+
+        sectionOneBlockNums = sectionOneEndBlockOffset - sectionOneStartBlockOffset + 1;
+
+        WaitFlagDev(AIC2AIVFLAGID);
+        if (kSeqLen > MDDIUM_SEQ_THRESHOLD || (kSeqLen > SHORT_SEQ_THRESHOLD && qkTriangle == true) ||
+            sparseFlag == true) {
+            rowPerTime = 1;
+            AllocateUbufForLongSeqAttnScore(&UbLongSeqAttn);
+            AttentionScoreDoubleLine(qkTriangle, sectionLoopTimes, sectionOneBlockNums, sectionTwoBlockNums,
+                                     triMatrixMaskOffset, eachVectorProcLineNum, localSectionOneStartLineOffset,
+                                     localSectionTwoStartLineOffset, triMatrixNum, triMatrixOffset, attnScoreGm,
+                                     attnMaskGm, UbLongSeqAttn, sparseFlag);
+        } else if (kSeqLen > SHORT_SEQ_THRESHOLD) {
+            rowPerTime = 1;
+            AllocateUbufForMediumSeqAttnScore(&UbMediumSeqAttn);
+            AttentionScoreSingleLine(qkTriangle, sectionLoopTimes, sectionOneBlockNums, sectionTwoBlockNums,
+                                     triMatrixMaskOffset, eachVectorProcLineNum, localSectionOneStartLineOffset,
+                                     localSectionTwoStartLineOffset, triMatrixNum, attnScoreGm, attnMaskGm,
+                                     UbMediumSeqAttn);
+        } else {
+            if (kSeqLen == 4096 || kSeqLen == 2048 || kSeqLen == 1024 || kSeqLen == 512 || kSeqLen == 256) {
+                rowPerTime = 2;
+            }
+            AllocateUbufForShortSeqAttnScore(&UbShortSeqAttn);
+            AttentionScoreShortDoubleLineOne(qkTriangle, sectionOneBlockNums, sectionTwoBlockNums, triMatrixMaskOffset,
+                                             eachVectorProcLineNum, localSectionOneStartLineOffset,
+                                             localSectionTwoStartLineOffset, attnScoreGm, attnMaskGm, UbShortSeqAttn);
+        }
+
+        // 同步
+        // 多核处理一个完整行，需要全核同步
+        if (yCubeNumPerLine > 1) {
+            if (useSoftSync) {
+                SoftSyncByGroup(cubeGroupBelong);
+            } else {
+                FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+                WaitFlagDev(AIVFLAGID);
+            }
+        }
+
+        FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+
+        SetFlag<HardEvent::V_S>(EVENT_ID0);
+        WaitFlag<HardEvent::V_S>(EVENT_ID0);
+
+        // scalar 捞 rowmax
+        RecordRowmaxAndRowsum(sparseFlag, timesSyncCube, UbLongSeqAttn.tensorForRecordRowmaxFp32,
+                              UbMediumSeqAttn.tensorForRecordRowmaxFp32, UbShortSeqAttn.tensorForRecordRowmaxFp32,
+                              logSumMaxGmTensor);
+        // scalar 捞 rowsum
+        RecordRowmaxAndRowsum(sparseFlag, timesSyncCube, UbLongSeqAttn.tensorForRecordRowsumFp32,
+                              UbMediumSeqAttn.tensorForRecordRowsumFp32, UbShortSeqAttn.tensorForRecordRowsumFp32,
+                              rowsumGmTensor);
+
+    } // 大循环 -- 第一次，将attention exp 计算完毕
+    WaitFlagDev(AIC2AIVFLAGID);
+    // 由cube计算rowsum ，这里需要同步一次，1. rowsum从rowsum_gm读取；2.前期计算得到的exp值，放在gm_c_cube2
+    // 需要逐行将数据读入，除以sum，执行归一化 -- 拷贝多少数据，每行不一样
+
+    // ~~~~~~~~~~~~~~~~ 归一化处理 ~~~~~~~~~~~~~~
+    FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+    WaitFlagDev(AIVFLAGID);
+
+    if (transposeType == 3) {
+        // 归一化时， 每个vector处理的行数
+        int32_t normEachVecProcessLines = 0;
+        // 归一化时， 每个vector处理的起始行
+        int32_t normEachVecOffsetLines = 0;
+
+        // normEachVecOffsetLines：会有尾行，不能被48个核整除，尾行会从第0个核开始补充
+        GetNormLocalInfo(qSeqLen, headNum, batchNum, &normEachVecProcessLines, &normEachVecOffsetLines);
+
+        // 一次处理的最大长度（ping+pong）
+        // 处理的数量
+        int32_t maxProcLen = normEachVecProcessLines * HEAD_DIM;
+        // 保证偶数，可以被pingpong平分
+        if (normEachVecProcessLines % 2 > 0) {
+            maxProcLen += HEAD_DIM;
+        }
+        maxProcLen = maxProcLen > MAX_LENG_PER_UB_PROC * 2 ? MAX_LENG_PER_UB_PROC * 2 : maxProcLen;
+
+        AttentionScoreNormalize(maxProcLen, normEachVecProcessLines, normEachVecOffsetLines, UbNorm, gmCCube2,
+                                gm_cube2_out, rowsumGm);
+    }
+    // ~~~~~~~~~~~~~~~~ 归一化处理 ~~~~~~~~~~~~~~
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::initCube2WorkSpace()
+{
+    int32_t total_block_number = batchNum * headNum * qSeqLen / BASE_BLOCK_LENGTH;
+    int32_t core_num = get_block_num();
+    int32_t vector_num = core_num * 2;
+    int32_t vector_id = get_block_idx() * 2 + get_subblockid(); // [0,47]
+
+    int64_t global_offset = 0;
+    int64_t local_offset = 0;
+    int32_t max_block_per_vector = 3;
+    int32_t total_block_number_per_core = total_block_number / vector_num; // 计算总轮次
+    int32_t tail_block_num = total_block_number % vector_num;
+    int32_t init_offset = vector_id * total_block_number_per_core * BASE_BLOCK_SIZE;
+    if (tail_block_num > 0) {
+        if (vector_id < tail_block_num) {
+            total_block_number_per_core++;
+            init_offset = vector_id * total_block_number_per_core * BASE_BLOCK_SIZE;
+        } else {
+            init_offset = (tail_block_num * (total_block_number_per_core + 1) +
+                           (vector_id - tail_block_num) * total_block_number_per_core) *
+                          BASE_BLOCK_SIZE;
+        }
+    }
+
+    int32_t Z = total_block_number_per_core / max_block_per_vector;
+    int32_t tail = total_block_number_per_core % max_block_per_vector;
+
+    if (tail > 0) {
+        Z++;
+    }
+
+    ubScoreZerotensor = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    // __ubuf__ float *ubScoreZero =  reinterpret_cast<__ubuf__ float *>((uintptr_t)0);
+
+    int32_t dup_times = 0;
+    if (total_block_number_per_core >= max_block_per_vector) {
+        dup_times = max_block_per_vector * 2; // 一次最多dup半块
+    } else {
+        dup_times = total_block_number_per_core * 2;
+    }
+    for (int32_t i = 0; i < dup_times; i++) {
+        AscendC::Duplicate(ubScoreZerotensor[i * BASE_BLOCK_SIZE / 2], (float)0, 64, BASE_BLOCK_SIZE * 2 / 256, 1, 8);
+    }
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+    for (int32_t loop_time = 0; loop_time < Z; loop_time++) {
+        int32_t cur_block_num = max_block_per_vector;
+        if (tail > 0 && loop_time == Z - 1) {
+            cur_block_num = tail;
+        }
+        int32_t local_offset = loop_time * max_block_per_vector * BASE_BLOCK_SIZE;
+
+        GlobalTensor<float> tmpCCube2GmTensorOut;
+        tmpCCube2GmTensorOut.SetGlobalBuffer(
+            reinterpret_cast<__gm__ float *>((__gm__ float *)(gmCCube2 + init_offset + local_offset)));
+
+        AscendC::DataCopy(tmpCCube2GmTensorOut, ubScoreZerotensor,
+                          AscendC::DataCopyParams(1, (cur_block_num * BASE_BLOCK_SIZE) * 4 / 32, 0, 0));
+    }
+    PipeBarrier<PIPE_ALL>();
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::SoftSyncByGroup(int32_t cubeGroupBelong)
+{
+    uint32_t flagSize = 128;
+    PipeBarrier<PIPE_MTE3>();
+    AscendC::SetAtomicType<float>();
+    AscendC::SetAtomicAdd<float>();
+    GlobalTensor<float> tmpZeroGmTensor;
+    tmpZeroGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmZeros));
+    AscendC::DataCopy(tmpZeroGmTensor[cubeGroupBelong * flagSize], ubForSoftsyncFlagsTensor,
+                      AscendC::DataCopyParams(1, 1, 0, 0));
+    AscendC::SetAtomicNone();
+
+    // 2*Y个vector往同一个地址累加1
+    softSyncTimesCount += yCubeNumPerLine * VEC_NUM_PER_CUBE;
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID6);
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID6);
+    PipeBarrier<PIPE_MTE2>();
+
+    while (true) {
+        AscendC::DataCopy(ubForSoftsyncCheckTensor, tmpZeroGmTensor[cubeGroupBelong * flagSize],
+                          AscendC::DataCopyParams(1, 1, 0, 0));
+        SetFlag<HardEvent::MTE2_S>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
+        auto diff = softSyncTimesCount - (float)*(ubForSoftsyncCheck);
+        if (diff < 0.1f) {
+            break;
+        }
+    }
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetSoftmaxOffset(int32_t sectionNum, int32_t yCubeNumPerLine,
+                                                               int32_t timesSyncCube, int32_t qSeqLen, int32_t headNum,
+                                                               bool qkTriangle, int32_t *totalOffsetForRowmax)
+{
+    int32_t cubeNumber = AscendC::GetBlockNum();
+    int32_t cubeIdx = get_block_idx();
+    int32_t vecIdx = get_subblockid();
+
+    int32_t groupNumber = cubeNumber / yCubeNumPerLine;
+    int32_t F = cubeNumber / yCubeNumPerLine;
+    int32_t cubeIdInOneGroup = cubeIdx % yCubeNumPerLine;
+    int32_t coreGroupIndex = cubeIdx / yCubeNumPerLine;
+    // 倒三角:VNum=qSeqLen/128;  非倒三角:VNum=0; sparse:VNum=windowsBlockNum
+    int32_t H = this->VNum;
+    int32_t L = H / 2;
+    int32_t rowPerBatch = headNum * (qSeqLen / BASE_BLOCK_LENGTH - L);
+
+    // 当前在处理哪一行(总行数)(负载均衡后)
+    int32_t curCalcRowBlock = timesSyncCube * F + coreGroupIndex;
+    // Current row's batch number  0
+    int32_t b = curCalcRowBlock / rowPerBatch;
+    // Current batch's head number   0
+    int32_t n = (curCalcRowBlock % rowPerBatch) / (qSeqLen / BASE_BLOCK_LENGTH - L);
+    // 当前head内处理的是哪一行(负载均衡后)
+    int32_t iRow = (curCalcRowBlock % rowPerBatch) % (qSeqLen / BASE_BLOCK_LENGTH - L);
+
+    int32_t aivIRow = iRow * BASE_BLOCK_LENGTH + BASE_BLOCK_LENGTH / yCubeNumPerLine * cubeIdInOneGroup +
+                      BASE_BLOCK_LENGTH / (VEC_NUM_PER_CUBE * yCubeNumPerLine) * vecIdx;
+    int32_t seg0Index = L * BASE_BLOCK_LENGTH + aivIRow;
+
+    if (sectionNum == 0) {
+        *totalOffsetForRowmax = (b * headNum + n) * qSeqLen + seg0Index;
+    } else {
+        // head内 section2 负载均衡前的行数
+        int32_t seg1Index = ((L - 1) - iRow) * BASE_BLOCK_LENGTH +
+                            BASE_BLOCK_LENGTH / yCubeNumPerLine * cubeIdInOneGroup +
+                            BASE_BLOCK_LENGTH / (VEC_NUM_PER_CUBE * yCubeNumPerLine) * vecIdx;
+        *totalOffsetForRowmax = (b * headNum + n) * qSeqLen + seg1Index;
+    }
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::RecordRowmaxAndRowsum(
+    bool sparseFlag, int32_t timesSyncCube, LocalTensor<float> tensorForLong, LocalTensor<float> tensorForMid,
+    LocalTensor<float> tensorForShort, GlobalTensor<float> tensorOutput)
+{
+    int32_t sectionNum = (qkTriangle == true) ? 2 : 1;
+    int32_t totalOffsetForRowmax = 0;
+    int32_t rowmaxUbOffset = 0;
+    for (int section = 0; section < sectionNum; section++) {
+        if (kSeqLen > MDDIUM_SEQ_THRESHOLD || (kSeqLen > SHORT_SEQ_THRESHOLD && qkTriangle == true) ||
+            sparseFlag == true) {
+            LocalTensor<float> tmpTensorForRecordRowmaxFp32 = section == 0 ? tensorForLong : tensorForLong[256];
+            for (int i = 0; i < 32 / yCubeNumPerLine; i++) {
+                fp32TestTensor.SetValue(rowmaxUbOffset, (float)tmpTensorForRecordRowmaxFp32.GetValue(0 + 8 * i));
+                rowmaxUbOffset++;
+                fp32TestTensor.SetValue(rowmaxUbOffset, (float)tmpTensorForRecordRowmaxFp32.GetValue(1 + 8 * i));
+                rowmaxUbOffset++;
+            }
+        } else if (kSeqLen > SHORT_SEQ_THRESHOLD) {
+            for (int i = 0; i < 64 / yCubeNumPerLine; i++) {
+                fp32TestTensor.SetValue(rowmaxUbOffset,
+                                        (float)tensorForMid.GetValue(i * 8 + section * 64 / yCubeNumPerLine * 8));
+                rowmaxUbOffset++;
+            }
+        } else {
+            if (rowPerTime == 1) {
+                for (int i = 0; i < 64 / (yCubeNumPerLine * VEC_NUM_PER_CUBE); i++) {
+                    fp32TestTensor.SetValue(rowmaxUbOffset, (float)tensorForShort.GetValue(section + 16 * i));
+                    rowmaxUbOffset++;
+                    fp32TestTensor.SetValue(rowmaxUbOffset, (float)tensorForShort.GetValue(512 + section + 16 * i));
+                    rowmaxUbOffset++;
+                }
+            } else {
+                for (int i = 0; i < 64 / (yCubeNumPerLine * VEC_NUM_PER_CUBE) / 2; i++) {
+                    fp32TestTensor.SetValue(rowmaxUbOffset, (float)tensorForShort.GetValue(section * 2 + 16 * i));
+                    rowmaxUbOffset++;
+                    fp32TestTensor.SetValue(rowmaxUbOffset, (float)tensorForShort.GetValue(section * 2 + 1 + 16 * i));
+                    rowmaxUbOffset++;
+                    fp32TestTensor.SetValue(rowmaxUbOffset, (float)tensorForShort.GetValue(512 + section * 2 + 16 * i));
+                    rowmaxUbOffset++;
+                    fp32TestTensor.SetValue(rowmaxUbOffset,
+                                            (float)tensorForShort.GetValue(512 + section * 2 + 1 + 16 * i));
+                    rowmaxUbOffset++;
+                }
+            }
+        }
+        SetFlag<HardEvent::S_MTE3>(EVENT_ID0);
+        WaitFlag<HardEvent::S_MTE3>(EVENT_ID0);
+        GetSoftmaxOffset(section, yCubeNumPerLine, timesSyncCube, qSeqLen, headNum, qkTriangle, &totalOffsetForRowmax);
+        DataCopy(tensorOutput[totalOffsetForRowmax],
+                 fp32TestTensor[section * BASE_BLOCK_SIDE_LEN / (yCubeNumPerLine * VEC_NUM_PER_CUBE)],
+                 DataCopyParams(1, 128 / (yCubeNumPerLine * VEC_NUM_PER_CUBE) * B32_SIZE / 32, 0, 0));
+        SetFlag<HardEvent::MTE3_S>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE3_S>(EVENT_ID0);
+    }
+}
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::InitUbForSoftSync()
+{
+    // 等InitWorkSpace完成
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+    ubForSoftsyncFlagsTensor = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    ubForSoftsyncCheckTensor = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(256);
+
+    AscendC::Duplicate(ubForSoftsyncFlagsTensor, (float)1, 64, 1, 1, 8);
+    PipeBarrier<PIPE_V>();
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AllocateUbufForShortSeqAttnScore(UB_FOR_SHORT_LEN_ATTN_SCORE *UbAttnScore)
+{
+    int32_t offset = 512;
+
+    UbAttnScore->tensorForLoadAttnScoreFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += (MAX_LENG_PER_UB_PROC + BASE_BLOCK_SIDE_LEN * rowPerTime * 2) * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForStoreSecondAttnScoreFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += (MAX_LENG_PER_UB_PROC / 2 + BASE_BLOCK_SIDE_LEN) * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForLoadOneBlockTriMaskFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM * rowPerTime;
+
+    UbAttnScore->tensorForCaclRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += MAX_LENG_PER_UB_PROC * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForCaclFinalRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM * rowPerTime;
+
+    UbAttnScore->tensorForCaclSecondFinalRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForCaclFinalRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM * rowPerTime;
+
+    UbAttnScore->tensorForCaclSecondFinalRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForVbrcbRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForVbrcbSecondRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForRecordRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += 4096;
+
+    UbAttnScore->tensorForRecordRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += 4096;
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AllocateUbufForMediumSeqAttnScore(
+    UB_FOR_MDDIUM_SEQ_ATTN_SCORE *UbAttnScore)
+{
+    int32_t offset = 512;
+
+    UbAttnScore->tensorForStoreOneLineAttnScoreFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += MDDIUM_SEQ_THRESHOLD * B32_SIZE;
+
+    auto maxLength = MAX_LENG_PER_UB_PROC > 4096 ? 4096 : MAX_LENG_PER_UB_PROC;
+
+    // 32K
+    UbAttnScore->tensorForLoadAttnScoreFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += maxLength * B32_SIZE * PING_PONG_NUM;
+
+    // 16k
+    UbAttnScore->tensorForCaclRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += maxLength / 2 * B32_SIZE * PING_PONG_NUM;
+
+    // 1K  vmax到最后，只需要64个FP32 （一行计算，不需要ping-pong)
+    UbAttnScore->tensorForCaclFinalRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForCaclFinalRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+    // 1K
+    UbAttnScore->tensorForVbrcbRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    // 单行计算，一个FP32占32字节，最多64 * 32 = 2048， ping-pong翻倍
+    UbAttnScore->tensorForRecordRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += 4096;
+
+    UbAttnScore->tensorForRecordRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += 4096;
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AllocateUbufForLongSeqAttnScore(UB_FOR_LONG_SEQ_ATTN_SCORE *UbAttnScore)
+{
+    int32_t offset = 512;
+
+    // 64K
+    UbAttnScore->tensorForLoadAttnScoreFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += MAX_LENG_PER_UB_PROC * B32_SIZE * PING_PONG_NUM;
+
+    // 64K
+    UbAttnScore->tensorForLoadExtraFirstAttnScoreFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += MAX_LENG_PER_UB_PROC * B32_SIZE * PING_PONG_NUM;
+
+    // sparse双mask
+    UbAttnScore->tensorForLoadOneBlockTriMaskFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * 4;
+
+    // 32k
+    UbAttnScore->tensorForCaclRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += MAX_LENG_PER_UB_PROC / 2 * B32_SIZE * PING_PONG_NUM;
+
+    // 1K  vmax到最后，只需要64个FP32
+    UbAttnScore->tensorForCaclFinalRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B16_SIZE * PING_PONG_NUM;
+
+    UbAttnScore->tensorForCaclFinalRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B16_SIZE * PING_PONG_NUM;
+
+    // 1K
+    UbAttnScore->tensorForVbrcbRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += BASE_BLOCK_SIDE_LEN * B32_SIZE * PING_PONG_NUM;
+
+    // 2K BASE_BLOCK_SIDE_LEN(128)最少被2个vec处理，有64*2个最大值，每2个最大值占32B，需要2K空间
+    UbAttnScore->tensorForRecordRowmaxFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += 2048;
+
+    UbAttnScore->tensorForRecordRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += 2048;
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AllocateUbufForNorm(UB_FOR_NORMALIZE *UbNorm)
+{
+    int32_t offset = 512;
+
+    // 64k 8192个FP32 + ping-pong
+    int32_t sizeofBufForLoadOFp32 = MAX_LENG_PER_UB_PROC * B32_SIZE * PING_PONG_NUM;
+    // 0.5K 128个O对应1个rowsum
+    int32_t sizeofBufForLoadRowsumFp32 = MAX_LENG_PER_UB_PROC / HEAD_DIM * B32_SIZE * PING_PONG_NUM;
+    // 展开成32字节对齐
+    int32_t sizeofBufForBrcbRowsumFp32 = MAX_LENG_PER_UB_PROC / HEAD_DIM * PING_PONG_NUM * B32_SIZE * (32 / B32_SIZE);
+
+    UbNorm->tensorForLoadOFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += sizeofBufForLoadOFp32;
+
+    UbNorm->tensorForLoadRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += sizeofBufForLoadRowsumFp32;
+
+    UbNorm->tensorForBrcbRowsumFp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(offset);
+    offset += sizeofBufForBrcbRowsumFp32;
+
+    UbNorm->oPingPongInterval = MAX_LENG_PER_UB_PROC;
+    UbNorm->rowsumPingPongInterval = MAX_LENG_PER_UB_PROC / HEAD_DIM;
+    UbNorm->rowsumBrcbPingPongInterval = UbNorm->rowsumPingPongInterval * (32 / B32_SIZE);
+}
+
+/**
+ * 获取全局信息
+ * batchNum
+ * headNum：全局处理Head的数量
+ * yNumCubePerLine：Y值，每Y个CUBE CORE处理一个完整行
+ * qkTriangle：QK_T是否是三角阵
+ * VNum：sparse窗口长度
+ * cubeNum：NPU上CUBE Core的数量（20\24\40\48...）
+ * fCubeGroupNum：CUBE Core所有的组数量
+ * blockNumPerFullLine：每个完整行包含基本块数量
+ * blockNumPerHead：每个Head包含的基本块数量，当Q、K不等长时，与block_num_per_full_line值不同
+ * loopTimesForAllFullLine：最外层循环的尾块，剩余的完整行数量（小于cube_group_num）
+ * loopTailsForAllFullLine：最外层循环
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetGlobalInfo(
+    int32_t batchNum, int32_t headNum, int32_t yNumCubePerLine, int32_t qSeqLen, int32_t kSeqLen, bool qkTriangle,
+    int32_t VNum, int32_t *cubeNum, int32_t *fCubeGroupNum, int32_t *blockNumPerFullLine, int32_t *blockNumPerHead,
+    int32_t *loopTimesForAllFullLine, int32_t *loopTailsForAllFullLine)
+{
+    *cubeNum = AscendC::GetBlockNum();
+    *fCubeGroupNum = *cubeNum / yNumCubePerLine;
+
+    int32_t fullLineNum = qSeqLen * headNum * batchNum / BASE_BLOCK_SIDE_LEN;
+    if (VNum == 0) {
+        *blockNumPerFullLine = kSeqLen / BASE_BLOCK_SIDE_LEN;
+
+        if (qkTriangle == true) {
+            fullLineNum = fullLineNum / 2;
+            (*blockNumPerFullLine) += 1;
+        }
+
+        // 尾块计算 要保证是偶数，否则pingpong会把一个基本块拆成几个了
+        *blockNumPerHead = fullLineNum / (headNum * batchNum);
+
+        *loopTimesForAllFullLine = fullLineNum / (*fCubeGroupNum);
+        *loopTailsForAllFullLine = fullLineNum % (*fCubeGroupNum);
+        if (*loopTailsForAllFullLine > 0) {
+            (*loopTimesForAllFullLine) += 1;
+        }
+    } else {
+        *blockNumPerFullLine = VNum + 1;
+        fullLineNum -= headNum * batchNum * VNum / 2;
+        *blockNumPerHead = fullLineNum / (headNum * batchNum);
+        *loopTimesForAllFullLine = fullLineNum / (*fCubeGroupNum);
+        *loopTailsForAllFullLine = fullLineNum % (*fCubeGroupNum);
+        if (*loopTailsForAllFullLine > 0) {
+            (*loopTimesForAllFullLine) += 1;
+        }
+    }
+}
+
+/**
+ * 获取每个vector的局部信息
+ * yNumCubePerLine: Y值，每Y个CUBE CORE处理一个完整行
+ * cubeGroupBelong：所属的CUBE组  -->  决定偏移N个完整行
+ * cubeGroupInnderIdx：组内vector code的顺序  --> 决定完整行内偏移
+ * eachVectorProcLineNum：每个vector处理的数量 （暂确定一定能整除）
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetLocalInfo(int32_t yNumCubePerLine, int32_t *cubeGroupBelong,
+                                                           int32_t *cubeGroupInnderIdx, int32_t *eachVectorProcLineNum)
+{
+    int32_t cubeIdx = get_block_idx();
+    *cubeGroupBelong = cubeIdx / yNumCubePerLine;
+    *cubeGroupInnderIdx = (cubeIdx % yNumCubePerLine) * 2 + get_subblockid();
+
+    *eachVectorProcLineNum = BASE_BLOCK_SIDE_LEN / (yNumCubePerLine * 2);
+    // 若不能整除，最后几个vector core会多处理一些行 - 当前不考虑
+}
+
+/**
+ * 获取当前vector core的地址偏移信息，需要区分左右section
+ * eachVectorStartLineOffset：整体的起始偏移(N块完整行的偏移 + 块内完整行的偏移)
+ * triMatrixMaskOffset：128*128的mask矩阵的偏移
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetInitOffsetForCurrentCore(
+    int32_t cubeGroupBelong, int32_t cubeGroupInnderIdx, int32_t blockNumPerFullLine, int32_t eachVectorProcLineNum,
+    int32_t *eachVectorStartLineOffset, int32_t *triMatrixMaskOffset)
+{
+    // 一块完整行（128行）需要跳过的数据量  （128^2 * 5）
+    int32_t fullLineOffset = BASE_BLOCK_SIDE_LEN * (BASE_BLOCK_SIDE_LEN * blockNumPerFullLine);
+    // 当前vector core属于第cube_group_belong个分组，需要跳过前面所有块的完整行 (0)
+    int32_t interGroupOffset = cubeGroupBelong * fullLineOffset;
+
+    // 当前vector core处理的起始行 (0/64)
+    int32_t innerGroupStartLine = eachVectorProcLineNum * cubeGroupInnderIdx;
+    // 每行第一个基本块的宽度为BASE_BLOCK_SIZE (0*128 / 64*128)
+    int32_t innerGroupStartLineOffset = BASE_BLOCK_SIDE_LEN * innerGroupStartLine;
+
+    *eachVectorStartLineOffset = (interGroupOffset + innerGroupStartLineOffset);
+    *triMatrixMaskOffset = innerGroupStartLine * maskSeqLength;
+}
+
+/**
+ * UB每次处理信号的长度，三角阵拼接后，即使按左右两个section计算，也按最长的预留空间
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ inline void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetSubSeqLengthPerProc(int32_t kSeqLen, int32_t blockNumPerFullLine,
+                                                                     int32_t *subSeqLengthPerProc)
+{
+    // 分两个section处理，不会总是2的幂；求max时用折半的方法，就不行
+    *subSeqLengthPerProc = kSeqLen > MAX_LENG_PER_UB_PROC ? MAX_LENG_PER_UB_PROC : kSeqLen;
+
+    // 序列小于MAX_LENG_PER_UB_PROC, 需要减半以支持ping-pong
+    if (*subSeqLengthPerProc < MAX_LENG_PER_UB_PROC && blockNumPerFullLine > 1) {
+        *subSeqLengthPerProc = *subSeqLengthPerProc / 2; // (256)
+    }
+}
+
+/**
+ * 非2的幂次长度，为了折半求vmax，需要进行padding
+ * totalBlockNum：当前总共需要处理的block数量
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetPaddingInfoForRowMax(int32_t totalBlockNum, int32_t *paddingBlockNum)
+{
+    // 满足最大长度倍数的部分不需要padding
+    auto tailNum = totalBlockNum % BLOCK_NUM_FOR_VMAX;
+
+    if (tailNum == 0) {
+        *paddingBlockNum = 0;
+        return;
+    }
+
+    int32_t totalBlock = 2;
+
+    while (totalBlock < BLOCK_NUM_FOR_VMAX) {
+        if (tailNum <= totalBlock) {
+            break;
+        }
+        totalBlock *= 2;
+    }
+
+    *paddingBlockNum = totalBlock - tailNum;
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::PaddingForRowMaxTensor(
+    int32_t totalBlockNum, int32_t paddingBlockNum, const LocalTensor<float> &paddingTensor, float value)
+{
+    if (paddingBlockNum == 0) {
+        return;
+    }
+
+    auto tailNum = totalBlockNum % MAX_BLOCK_PER_ONE_PROC;
+
+    AscendC::Duplicate(paddingTensor[tailNum * BASE_BLOCK_SIDE_LEN * rowPerTime], float(value), 64,
+                       paddingBlockNum * 2 * rowPerTime, 1, 8);
+    PipeBarrier<PIPE_V>();
+}
+
+/**
+ * 当前处理的完整行在head中属于第几个完整行  --> 找到需要mask的块 （三角阵有两个块）
+ * curLoopIndex：当前的大循环编号
+ * cubeGroupBelong：所属的cube分组
+ * fCubeGroupNum：F, cube总分组数量
+ * blockNumPerHead：一个head包含的块行数量
+ * *blockLineOffsetInOneHead：在Head中的完整行ID(三角阵中是拼接后的行号)
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetFullLineOffsetInOneHead(int32_t curLoopIndex, int32_t cubeGroupBelong,
+                                                                         int32_t fCubeGroupNum, int32_t blockNumPerHead,
+                                                                         int32_t *blockLineOffsetInOneHead)
+{
+    // 所有head依照 B*N顺序排列，每个loop能处理fCubeGroupNum个完整块行，当前处理的完整块行的编号 (全局处理的第x行)
+    int32_t fullLineIdx = curLoopIndex * fCubeGroupNum + cubeGroupBelong;
+
+    // 在Head内的第X个完整行
+    *blockLineOffsetInOneHead = fullLineIdx % blockNumPerHead;
+}
+
+
+/**
+ * 只有三角阵有tri block问题；分为左右两个section，需要分别计算softmax
+ * tri block的位置由section_end表达
+ * blockLineOffsetInOneHead: Head中第x个基本块行
+ * *sectionOneStartBlockOffset: 第一段起始
+ * *sectionTwoStartBlockOffset: 第二段起始
+ * *sectionOneEndBlockOffset:   第一段结束（三角阵位置）
+ * *sectionTwoEndBlockOffset):  第二段结束（三角阵位置）
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetTriMatrixSectionBlockOffset(
+    int32_t blockLineOffsetInOneHead, int32_t blockNumPerFullLine, int32_t *sectionOneStartBlockOffset,
+    int32_t *sectionTwoStartBlockOffset, int32_t *sectionOneEndBlockOffset, int32_t *sectionTwoEndBlockOffset)
+{
+    *sectionOneStartBlockOffset = 0;
+    *sectionTwoStartBlockOffset = blockNumPerFullLine / 2 + 1 + blockLineOffsetInOneHead;
+
+    *sectionOneEndBlockOffset = *sectionTwoStartBlockOffset - 1;
+    *sectionTwoEndBlockOffset = blockNumPerFullLine - 1;
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::CaclMaxTensor(const LocalTensor<float> &tensorForCacl, int32_t blockNum)
+{
+    int32_t curBlockNum = blockNum;
+
+    while (curBlockNum > 1) {
+        // fp32
+        max_v<ArchType::ASCEND_V220>(tensorForCacl, tensorForCacl,
+                                     tensorForCacl[BASE_BLOCK_SIDE_LEN * curBlockNum * rowPerTime],
+                                     curBlockNum * 2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        curBlockNum = curBlockNum / 2;
+    }
+
+    max_v<ArchType::ASCEND_V220>(tensorForCacl, tensorForCacl, tensorForCacl[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                 2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+    PipeBarrier<PIPE_V>();
+}
+
+/**
+ * 64个基本块以下求max
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ __attribute__((noinline)) void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessCaclMaxTensor(
+    int32_t basicBlockNum, int32_t paddingBlockNum, bool ppFirstSection,
+    const LocalTensor<float> &curTensorForAttnScore, const LocalTensor<float> &curTensorForRowmax,
+    const LocalTensor<float> &tensorForCaclFinalRowmaxFp32)
+{
+    int32_t allBlockNum = basicBlockNum + paddingBlockNum;
+    int32_t tailBlockNum = allBlockNum % BLOCK_NUM_FOR_VMAX;
+    int32_t doneBlockNum = allBlockNum / BLOCK_NUM_FOR_VMAX * BLOCK_NUM_FOR_VMAX;
+    bool fromBufForAttnScore = false;
+
+    if (allBlockNum == 64) {
+        // fp32
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 32], 64, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclMaxTensor(curTensorForRowmax, BLOCK_NUM_FOR_VMAX);
+    } else if (allBlockNum >= 48) {
+        // 48(0)\50(2)\52(4)\56(8)  fp32
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 16], 32, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForRowmax,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 32], 32, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForRowmax,
+                                     curTensorForRowmax[BASE_BLOCK_SIDE_LEN * 8], 16, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclMaxTensor(curTensorForRowmax, 4);
+    } else if (allBlockNum >= 32) {
+        // 32(0)\34(2)\36(4)\40(8)  fp32
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 16 * rowPerTime], 32 * rowPerTime, 1,
+                                     1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclMaxTensor(curTensorForRowmax, 8);
+    } else if (allBlockNum >= 16) {
+        // 16(0)\18(2)\20(4)\24(8)  fp32
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 8 * rowPerTime], 16 * rowPerTime, 1, 1,
+                                     1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclMaxTensor(curTensorForRowmax, 4);
+    }
+
+    if (tailBlockNum == 8) {
+        if (allBlockNum < 16) {
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax,
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 4) * rowPerTime],
+                                         8 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForRowmax,
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * 2 * rowPerTime], 4 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        } else {
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 4) * rowPerTime],
+                                         8 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * 3 * rowPerTime], 4 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * 2 * rowPerTime], 2 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+        // fp32  剩下128个FP32, 2组
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForRowmax,
+                                     curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime], 2 * rowPerTime, 1, 1, 1, 8,
+                                     8, 8);
+        PipeBarrier<PIPE_V>();
+    } else if (tailBlockNum == 4) {
+        if (allBlockNum < 16) {
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax,
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 2) * rowPerTime],
+                                         4 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        } else {
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 2) * rowPerTime],
+                                         4 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * 2 * rowPerTime], 2 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForRowmax,
+                                     curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime], 2 * rowPerTime, 1, 1, 1, 8,
+                                     8, 8);
+        PipeBarrier<PIPE_V>();
+    } else if (tailBlockNum == 2) {
+        if (allBlockNum < 16) {
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax,
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 1) * rowPerTime],
+                                         2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        } else {
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 1) * rowPerTime],
+                                         2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            max_v<ArchType::ASCEND_V220>(curTensorForRowmax, curTensorForRowmax,
+                                         curTensorForRowmax[BASE_BLOCK_SIDE_LEN * rowPerTime], 2 * rowPerTime, 1, 1, 1,
+                                         8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+    } // 没有其他分支了
+
+    auto srcBuf = fromBufForAttnScore ? curTensorForAttnScore : curTensorForRowmax;
+
+    if (ppFirstSection) {
+        max_v<ArchType::ASCEND_V220>(tensorForCaclFinalRowmaxFp32, srcBuf, srcBuf[BASE_BLOCK_SIDE_LEN / 2], rowPerTime,
+                                     1, 1, 1, 8, 8 * rowPerTime, 8 * rowPerTime);
+        PipeBarrier<PIPE_V>();
+    } else {
+        max_v<ArchType::ASCEND_V220>(curTensorForRowmax, srcBuf, srcBuf[BASE_BLOCK_SIDE_LEN / 2], 1, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        max_v<ArchType::ASCEND_V220>(tensorForCaclFinalRowmaxFp32, tensorForCaclFinalRowmaxFp32, curTensorForRowmax, 1,
+                                     1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+    }
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void
+VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::CaclSumTensor(const LocalTensor<float> &tensorForCacl, int32_t blockNum)
+{
+    int32_t curBlockNum = blockNum;
+
+    while (curBlockNum > 1) {
+        // fp32
+        add_v<ArchType::ASCEND_V220>(tensorForCacl, tensorForCacl,
+                                     tensorForCacl[BASE_BLOCK_SIDE_LEN * curBlockNum * rowPerTime],
+                                     curBlockNum * 2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        curBlockNum = curBlockNum / 2;
+    }
+
+    add_v<ArchType::ASCEND_V220>(tensorForCacl, tensorForCacl, tensorForCacl[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                 2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+    PipeBarrier<PIPE_V>();
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ __attribute__((noinline)) void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessCaclSumTensor(
+    int32_t basicBlockNum, int32_t paddingBlockNum, bool ppFirstSection,
+    const LocalTensor<float> &curTensorForAttnScore, const LocalTensor<float> &curTensorForRowsum,
+    const LocalTensor<float> &tensorForCaclFinalRowsumFp32)
+{
+    int32_t allBlockNum = basicBlockNum + paddingBlockNum;
+    int32_t tailBlockNum = allBlockNum % BLOCK_NUM_FOR_VMAX;
+    int32_t doneBlockNum = allBlockNum / BLOCK_NUM_FOR_VMAX * BLOCK_NUM_FOR_VMAX;
+    bool fromBufForAttnScore = false;
+
+    if (allBlockNum == 64) {
+        // fp32
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 32], 64, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclSumTensor(curTensorForRowsum, BLOCK_NUM_FOR_VMAX);
+    } else if (allBlockNum >= 48) {
+        // 48(0)\50(2)\52(4)\56(8)  fp32
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 16], 32, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForRowsum,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 32], 32, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForRowsum,
+                                     curTensorForRowsum[BASE_BLOCK_SIDE_LEN * 8], 16, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclSumTensor(curTensorForRowsum, 4);
+    } else if (allBlockNum >= 32) {
+        // 32(0)\34(2)\36(4)\40(8)  fp32
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 16 * rowPerTime], 32 * rowPerTime, 1,
+                                     1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclSumTensor(curTensorForRowsum, 8);
+    } else if (allBlockNum >= 16) {
+        // 16(0)\18(2)\20(4)\24(8)  fp32
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForAttnScore,
+                                     curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * 8 * rowPerTime], 16 * rowPerTime, 1, 1,
+                                     1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+        CaclSumTensor(curTensorForRowsum, 4);
+    }
+
+    if (tailBlockNum == 8) {
+        if (allBlockNum < 16) {
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum,
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 4) * rowPerTime],
+                                         8 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForRowsum,
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * 2 * rowPerTime], 4 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        } else {
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 4) * rowPerTime],
+                                         8 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * 3 * rowPerTime], 4 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * 2 * rowPerTime], 2 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+        // fp32  剩下128个FP32, 2组
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForRowsum,
+                                     curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime], 2 * rowPerTime, 1, 1, 1, 8,
+                                     8, 8);
+        PipeBarrier<PIPE_V>();
+    } else if (tailBlockNum == 4) {
+        if (allBlockNum < 16) {
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum,
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 2) * rowPerTime],
+                                         4 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        } else {
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 2) * rowPerTime],
+                                         4 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * 2 * rowPerTime], 2 * rowPerTime, 1, 1,
+                                         1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForRowsum,
+                                     curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime], 2 * rowPerTime, 1, 1, 1, 8,
+                                     8, 8);
+        PipeBarrier<PIPE_V>();
+    } else if (tailBlockNum == 2) {
+        if (allBlockNum < 16) {
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum,
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 1) * rowPerTime],
+                                         2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        } else {
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * doneBlockNum * rowPerTime],
+                                         curTensorForAttnScore[BASE_BLOCK_SIDE_LEN * (doneBlockNum + 1) * rowPerTime],
+                                         2 * rowPerTime, 1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+
+            add_v<ArchType::ASCEND_V220>(curTensorForRowsum, curTensorForRowsum,
+                                         curTensorForRowsum[BASE_BLOCK_SIDE_LEN * rowPerTime], 2 * rowPerTime, 1, 1, 1,
+                                         8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+    } // 没有其他分支了
+
+    auto srcBuf = fromBufForAttnScore ? curTensorForAttnScore : curTensorForRowsum;
+
+    if (ppFirstSection) {
+        add_v<ArchType::ASCEND_V220>(tensorForCaclFinalRowsumFp32, srcBuf, srcBuf[BASE_BLOCK_SIDE_LEN / 2], rowPerTime,
+                                     1, 1, 1, 8, 8 * rowPerTime, 8 * rowPerTime);
+        PipeBarrier<PIPE_V>();
+    } else {
+        add_v<ArchType::ASCEND_V220>(curTensorForRowsum, srcBuf, srcBuf[BASE_BLOCK_SIDE_LEN / 2], 1, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        add_v<ArchType::ASCEND_V220>(tensorForCaclFinalRowsumFp32, tensorForCaclFinalRowsumFp32, curTensorForRowsum, 1,
+                                     1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+    }
+}
+/**
+ * 8192长度以下的序列计算max
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessLinePhaseOneForShortSeqMax(
+    bool qkTriangle, PARAM_SHORT_SEQ_MAX Param, int32_t pingPongFlag, bool firstLine, __gm__ WORKSPACE_T *attnScoreGm,
+    __gm__ INPUT_T *attnMaskGm, UB_FOR_SHORT_LEN_ATTN_SCORE &UbAttn)
+{
+    // 8192段序列，一定会遇到末尾三角阵
+    auto totalBlockNum = Param.sectionOneBlockNum + Param.sectionTwoBlockNum;
+    auto eventId = (pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1);
+
+    // 原始atten score
+    LocalTensor<float> curTensorForAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadAttnScoreFp32[0] :
+                            UbAttn.tensorForLoadAttnScoreFp32[MAX_LENG_PER_UB_PROC + BASE_BLOCK_SIDE_LEN * rowPerTime];
+
+    LocalTensor<float> curTensorForSecondAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForStoreSecondAttnScoreFp32[0] :
+                            UbAttn.tensorForStoreSecondAttnScoreFp32[MAX_LENG_PER_UB_PROC / 2];
+
+    // 这里可以复用，最后的结果存在 buf_for_cacl_short_(second)_final_rowmax_fp16
+    LocalTensor<float> curTensorForRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclRowmaxFp32[0] : UbAttn.tensorForCaclRowmaxFp32[MAX_LENG_PER_UB_PROC];
+
+    // 得到最后128个最大值 (section one & tow 会连着存放)
+    LocalTensor<float> curTensorForFinalRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclFinalRowmaxFp32[0] :
+                            UbAttn.tensorForCaclFinalRowmaxFp32[BASE_BLOCK_SIDE_LEN * rowPerTime];
+    LocalTensor<float> curTensorForSecondFinalRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclFinalRowmaxFp32[BASE_BLOCK_SIDE_LEN / 2 * rowPerTime] :
+                            UbAttn.tensorForCaclFinalRowmaxFp32[BASE_BLOCK_SIDE_LEN * 3 / 2 * rowPerTime];
+
+    // 32B对齐的最大值
+    LocalTensor<float> curTensorForVbrcbRowmaxFp32 =
+        pingPongFlag == 0 ? UbAttn.tensorForVbrcbRowmaxFp32[0] : UbAttn.tensorForVbrcbRowmaxFp32[BASE_BLOCK_SIDE_LEN];
+
+    // mask
+    LocalTensor<float> curTensorForMaskFp32 =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadOneBlockTriMaskFp32[0] :
+                            UbAttn.tensorForLoadOneBlockTriMaskFp32[BASE_BLOCK_SIDE_LEN * rowPerTime];
+    LocalTensor<INPUT_T> curTensorForMaskFp16 =
+        curTensorForMaskFp32[64 * rowPerTime].template ReinterpretCast<INPUT_T>();
+
+    // 存储最大值，最终需要返回到GM上
+    LocalTensor<float> curTensorForRecordRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForRecordRowmaxFp32[0] : UbAttn.tensorForRecordRowmaxFp32[512];
+    GlobalTensor<float> tmpScoreGmTensor;
+    tmpScoreGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(attnScoreGm + Param.sectionStartLineOffset));
+    int32_t srcGap = rowPerTime == 1 ? 0 : maskSeqLength - BASE_BLOCK_SIDE_LEN;
+    WaitFlag<HardEvent::MTE3_MTE2>(eventId);
+
+    AscendC::DataCopy(curTensorForAttnScore, tmpScoreGmTensor,
+                      AscendC::DataCopyParams(totalBlockNum, BASE_BLOCK_SIDE_LEN * rowPerTime / 8,
+                                              (BASE_BLOCK_DATA_NUM - rowPerTime * BASE_BLOCK_SIDE_LEN) / 8, 0));
+    // 一定有两个tri matrix
+    if (qkTriangle == true) {
+        gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(curTensorForMaskFp16, attnMaskGmTensor[Param.sectionMaskOffset],
+                                                 0,                        // sid
+                                                 rowPerTime,               // nBurst
+                                                 BASE_BLOCK_SIDE_LEN / 16, // lenBurst
+                                                 srcGap / 16,              // srcGap
+                                                 0                         // dstGap
+        );
+    }
+    SetFlag<HardEvent::MTE2_V>(eventId);
+    WaitFlag<HardEvent::MTE2_V>(eventId);
+
+    muls_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, (float)scale,
+                                         totalBlockNum * 2 * rowPerTime, // repeat
+                                         1,                              // dstBlockStride
+                                         1,                              // srcBlockStride
+                                         8,                              // dstRepeatStride
+                                         8                               // srcRepeatStride
+    );
+    PipeBarrier<PIPE_V>();
+    if (qkTriangle == true) {
+        // 处理tri matrix
+        conv_v<ArchType::ASCEND_V220, INPUT_T, float>(curTensorForMaskFp32, curTensorForMaskFp16, rowPerTime * 2, 1, 1,
+                                                      8, 4);
+        PipeBarrier<PIPE_V>();
+
+        // 128个fp32 --> 2个repeat
+        muls_v<ArchType::ASCEND_V220, float>(curTensorForMaskFp32, curTensorForMaskFp32, (float)PADDING_FOR_MAX,
+                                             2 * rowPerTime, // repeat
+                                             1,              // dstBlockStride
+                                             1,              // srcBlockStride
+                                             8,              // dstRepeatStride
+                                             8               // srcRepeatStride
+        );
+        PipeBarrier<PIPE_V>();
+
+        add_v<ArchType::ASCEND_V220, float>(
+            curTensorForAttnScore[rowPerTime * (Param.sectionOneBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+            curTensorForAttnScore[rowPerTime * (Param.sectionOneBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+            curTensorForMaskFp32,
+            rowPerTime * 2, // repeat
+            1,              // dstBlockStride
+            1,              // src0BlockStride
+            1,              // src1BlockStride
+            8,              // dstRepeatStride
+            8,              // src0RepeatStride
+            8               // src1RepeatStride
+        );
+        PipeBarrier<PIPE_V>();
+
+        // 直接写到目的地
+        add_v<ArchType::ASCEND_V220, float>(
+            curTensorForSecondAttnScore[rowPerTime * (Param.sectionTwoBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+            curTensorForAttnScore[rowPerTime * (totalBlockNum - 1) * BASE_BLOCK_SIDE_LEN], curTensorForMaskFp32,
+            rowPerTime * 2, // repeat
+            1,              // dstBlockStride
+            1,              // src0BlockStride
+            1,              // src1BlockStride
+            8,              // dstRepeatStride
+            8,              // src0RepeatStride
+            8               // src1RepeatStride
+        );
+        PipeBarrier<PIPE_V>();
+    }
+
+    // copy section tow
+    if (Param.sectionTwoBlockNum > 1) {
+        // 这个省不掉，相加一次折半的时候，可能会需要用到padding的值
+        AscendC::Copy(curTensorForSecondAttnScore[0],
+                      curTensorForAttnScore[Param.sectionOneBlockNum * BASE_BLOCK_SIDE_LEN * rowPerTime], 64,
+                      rowPerTime * 2 * (Param.sectionTwoBlockNum - 1), {1, 1, 8, 8});
+    }
+    PipeBarrier<PIPE_V>();
+
+    // 计算section one（每次都需要padding，因为会被section two污染）
+    if (Param.sectionOnePaddingBlockNum > 0) {
+        PaddingForRowMaxTensor(Param.sectionOneBlockNum, Param.sectionOnePaddingBlockNum, curTensorForAttnScore,
+                               PADDING_FOR_MAX);
+    }
+
+    // 最后的结果在 curBufForFinalRowmax
+    ProcessCaclMaxTensor(Param.sectionOneBlockNum, Param.sectionOnePaddingBlockNum, true, curTensorForAttnScore,
+                         curTensorForRowmax, curTensorForFinalRowmax);
+
+    if (qkTriangle == true) {
+        // 计算section two （处理第一行的时候padding 0即可，因为所有行的padding都一样，不会被污染）
+        if (Param.sectionTwoPaddingBlockNum > 0) {
+            PaddingForRowMaxTensor(Param.sectionTwoBlockNum, Param.sectionTwoPaddingBlockNum,
+                                   curTensorForSecondAttnScore, PADDING_FOR_MAX);
+        }
+
+        ProcessCaclMaxTensor(Param.sectionTwoBlockNum, Param.sectionTwoPaddingBlockNum, true,
+                             curTensorForSecondAttnScore, curTensorForRowmax, curTensorForSecondFinalRowmax);
+
+        // 接着计算max，有2个最大值(两段)
+        cmax_v<ArchType::ASCEND_V220, float, ReduceOrder::ORDER_ONLY_VALUE>(
+            curTensorForRecordRowmax[Param.recordRowmaxOffset], curTensorForFinalRowmax, rowPerTime * 2, 1, 1, 8);
+        PipeBarrier<PIPE_V>();
+
+        brcb_v<ArchType::ASCEND_V220, uint32_t>(
+            curTensorForVbrcbRowmaxFp32.ReinterpretCast<uint32_t>(),
+            curTensorForRecordRowmax[Param.recordRowmaxOffset].template ReinterpretCast<uint32_t>(),
+            1, // dstBlockStride
+            8, // dstRepeatStride
+            1  // repeat
+        );
+        PipeBarrier<PIPE_V>();
+    } else {
+        // 有2个最大值(两段)
+        cmax_v<ArchType::ASCEND_V220, float, ReduceOrder::ORDER_ONLY_VALUE>(
+            curTensorForRecordRowmax[Param.recordRowmaxOffset], curTensorForFinalRowmax, rowPerTime, 1, 1, 8);
+        PipeBarrier<PIPE_V>();
+
+        brcb_v<ArchType::ASCEND_V220, uint32_t>(
+            curTensorForVbrcbRowmaxFp32.ReinterpretCast<uint32_t>(),
+            curTensorForRecordRowmax[Param.recordRowmaxOffset].template ReinterpretCast<uint32_t>(),
+            1, // dstBlockStride
+            8, // dstRepeatStride
+            1  // repeat
+        );
+        PipeBarrier<PIPE_V>();
+    }
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessLinePhaseOneForShortSeqExp(
+    bool qkTriangle, PARAM_SHORT_SEQ_MAX Param, int32_t pingPongFlag, __gm__ WORKSPACE_T *attnScoreGm,
+    __gm__ INPUT_T *attnMaskGm, UB_FOR_SHORT_LEN_ATTN_SCORE &UbAttn, int32_t offset)
+{
+    auto eventId = (pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1);
+
+    LocalTensor<float> curTensorForAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadAttnScoreFp32[0] :
+                            UbAttn.tensorForLoadAttnScoreFp32[MAX_LENG_PER_UB_PROC + BASE_BLOCK_SIDE_LEN * rowPerTime];
+    LocalTensor<float> curTensorForSecondAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForStoreSecondAttnScoreFp32[0] :
+                            UbAttn.tensorForStoreSecondAttnScoreFp32[MAX_LENG_PER_UB_PROC / 2];
+
+    LocalTensor<float> curTensorForVbrcbRowmaxFp32 =
+        pingPongFlag == 0 ? UbAttn.tensorForVbrcbRowmaxFp32[0] : UbAttn.tensorForVbrcbRowmaxFp32[BASE_BLOCK_SIDE_LEN];
+    // 用rowmax空间计算rowsum
+    LocalTensor<float> curTensorForRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclRowmaxFp32[0] : UbAttn.tensorForCaclRowmaxFp32[MAX_LENG_PER_UB_PROC];
+    LocalTensor<float> curTensorForFinalRowsum =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclFinalRowsumFp32[0] :
+                            UbAttn.tensorForCaclFinalRowsumFp32[BASE_BLOCK_SIDE_LEN * rowPerTime];
+    LocalTensor<float> curTensorForSecondFinalRowsum =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclFinalRowsumFp32[BASE_BLOCK_SIDE_LEN / 2 * rowPerTime] :
+                            UbAttn.tensorForCaclFinalRowsumFp32[BASE_BLOCK_SIDE_LEN * 3 / 2 * rowPerTime];
+
+    LocalTensor<float> curTensorForRecordRowsum =
+        pingPongFlag == 0 ? UbAttn.tensorForRecordRowsumFp32[0] : UbAttn.tensorForRecordRowsumFp32[512];
+    // fp16 result直接用exp空间
+    LocalTensor<INPUT_T> curTensorForResultFp16 = curTensorForAttnScore.template ReinterpretCast<INPUT_T>();
+
+    int32_t totalOffsetFp32 = Param.sectionStartLineOffset - offset + offset / 2;
+    GlobalTensor<INPUT_T> tmpScoreGmTensorOut;
+    tmpScoreGmTensorOut.SetGlobalBuffer(
+        reinterpret_cast<__gm__ INPUT_T *>((__gm__ INPUT_T *)(attnScoreGm + totalOffsetFp32)));
+
+    // 减去最大值
+    if (rowPerTime == 1) {
+        sub_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, curTensorForVbrcbRowmaxFp32,
+                                            Param.sectionOneBlockNum * 2, // repeat
+                                            1,                            // dstBlockStride
+                                            1,                            // src0BlockStride
+                                            0,                            // src1BlockStride
+                                            8,                            // dstRepeatStride
+                                            8,                            // src0RepeatStride
+                                            0                             // src1RepeatStride
+        );
+        PipeBarrier<PIPE_V>();
+    } else {
+        for (int32_t i = 0; i < 2 * rowPerTime; i++) {
+            sub_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore[i * 64], curTensorForAttnScore[i * 64],
+                                                curTensorForVbrcbRowmaxFp32[(i / 2) * 8],
+                                                Param.sectionOneBlockNum, // repeat
+                                                1,                        // dstBlockStride
+                                                1,                        // src0BlockStride
+                                                0,                        // src1BlockStride
+                                                32,                       // dstRepeatStride
+                                                32,                       // src0RepeatStride
+                                                0                         // src1RepeatStride
+            );
+            PipeBarrier<PIPE_V>();
+        }
+    }
+
+    exp_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore,
+                                        Param.sectionOneBlockNum * 2 * rowPerTime, 1, 1, 8, 8);
+    PipeBarrier<PIPE_V>();
+
+    if (Param.sectionOnePaddingBlockNum > 0) {
+        PaddingForRowMaxTensor(Param.sectionOneBlockNum, Param.sectionOnePaddingBlockNum, curTensorForAttnScore, 0);
+    }
+    // 复用rowmax的计算空间
+    ProcessCaclSumTensor(Param.sectionOneBlockNum, Param.sectionOnePaddingBlockNum, true, curTensorForAttnScore,
+                         curTensorForRowmax, curTensorForFinalRowsum);
+    if constexpr (IF_BF16) {
+        convr_v<ArchType::ASCEND_V220, float, INPUT_T>(curTensorForResultFp16, curTensorForAttnScore,
+                                                       Param.sectionOneBlockNum * 2 * rowPerTime, 1, 1, 4, 8);
+    } else {
+        conv_v<ArchType::ASCEND_V220, float, INPUT_T>(curTensorForResultFp16, curTensorForAttnScore,
+                                                      Param.sectionOneBlockNum * 2 * rowPerTime, 1, 1, 4, 8);
+    }
+    PipeBarrier<PIPE_V>();
+
+    if (qkTriangle == true) {
+        if (rowPerTime == 1) {
+            sub_v<ArchType::ASCEND_V220, float>(curTensorForSecondAttnScore, curTensorForSecondAttnScore,
+                                                curTensorForVbrcbRowmaxFp32[8],
+                                                Param.sectionTwoBlockNum * 2, // repeat
+                                                1,                            // dstBlockStride
+                                                1,                            // src0BlockStride
+                                                0,                            // src1BlockStride
+                                                8,                            // dstRepeatStride
+                                                8,                            // src0RepeatStride
+                                                0                             // src1RepeatStride
+            );
+            PipeBarrier<PIPE_V>();
+        } else {
+            for (int32_t i = 0; i < 2 * rowPerTime; i++) {
+                sub_v<ArchType::ASCEND_V220, float>(curTensorForSecondAttnScore[i * 64],
+                                                    curTensorForSecondAttnScore[i * 64],
+                                                    curTensorForVbrcbRowmaxFp32[(i / 2) * 8 + rowPerTime * 8],
+                                                    Param.sectionTwoBlockNum, // repeat
+                                                    1,                        // dstBlockStride
+                                                    1,                        // src0BlockStride
+                                                    0,                        // src1BlockStride
+                                                    32,                       // dstRepeatStride
+                                                    32,                       // src0RepeatStride
+                                                    0                         // src1RepeatStride
+                );
+                PipeBarrier<PIPE_V>();
+            }
+        }
+
+        exp_v<ArchType::ASCEND_V220, float>(curTensorForSecondAttnScore, curTensorForSecondAttnScore,
+                                            Param.sectionTwoBlockNum * 2 * rowPerTime, 1, 1, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        if (Param.sectionTwoPaddingBlockNum > 0) {
+            PaddingForRowMaxTensor(Param.sectionTwoBlockNum, Param.sectionTwoPaddingBlockNum,
+                                   curTensorForSecondAttnScore, 0);
+        }
+        ProcessCaclSumTensor(Param.sectionTwoBlockNum, Param.sectionTwoPaddingBlockNum, true,
+                             curTensorForSecondAttnScore, curTensorForRowmax, curTensorForSecondFinalRowsum);
+
+        if constexpr (IF_BF16) {
+            convr_v<ArchType::ASCEND_V220, float, INPUT_T>(
+                curTensorForResultFp16[Param.sectionOneBlockNum * BASE_BLOCK_SIDE_LEN * rowPerTime],
+                curTensorForSecondAttnScore, Param.sectionTwoBlockNum * 2 * rowPerTime, 1, 1, 4, 8);
+        } else {
+            conv_v<ArchType::ASCEND_V220, float, INPUT_T>(
+                curTensorForResultFp16[Param.sectionOneBlockNum * BASE_BLOCK_SIDE_LEN * rowPerTime],
+                curTensorForSecondAttnScore, Param.sectionTwoBlockNum * 2 * rowPerTime, 1, 1, 4, 8);
+        }
+        PipeBarrier<PIPE_V>();
+    }
+
+    if (qkTriangle == true) {
+        AscendC::RepeatReduceSum<float, false>(curTensorForRecordRowsum[Param.recordRowmaxOffset],
+                                               curTensorForFinalRowsum, rowPerTime * 2, 0, 0, 1, 1, 8);
+    } else {
+        AscendC::RepeatReduceSum<float, false>(curTensorForRecordRowsum[Param.recordRowmaxOffset],
+                                               curTensorForFinalRowsum, rowPerTime, 0, 0, 1, 1, 8);
+    }
+
+    SetFlag<HardEvent::V_MTE3>(eventId);
+    WaitFlag<HardEvent::V_MTE3>(eventId);
+
+    AscendC::DataCopy(tmpScoreGmTensorOut, curTensorForResultFp16,
+                      AscendC::DataCopyParams(
+                          Param.sectionOneBlockNum + Param.sectionTwoBlockNum, BASE_BLOCK_SIDE_LEN * rowPerTime / 16, 0,
+                          ((BASE_BLOCK_DATA_NUM - rowPerTime * BASE_BLOCK_SIDE_LEN) + BASE_BLOCK_DATA_NUM) / 16));
+
+    SetFlag<HardEvent::MTE3_MTE2>(eventId);
+}
+
+/**
+ * 长序列
+ * 处理完整行的阶段一，计算出max值
+ * 对于三角阵，分两段处理（本方法里面只处理一段），尾块是三角阵，需要掩码
+ * 对于非三角阵，完整处理，没有三角阵问题
+ * sparseFlag：记录max的下标
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessLinePhaseOneForMax(
+    PARAM_LONG_SEQ_EXP Param, int32_t pingPongFlag, __gm__ WORKSPACE_T *attnScoreGm, __gm__ INPUT_T *attnMaskGm,
+    UB_FOR_LONG_SEQ_ATTN_SCORE &UbAttn, bool sparseFlag)
+{
+    auto eventId = (pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1);
+    bool firstFrag = Param.curFrag == 0;
+    bool lastFrag = Param.curFrag == Param.totalFragNum - 1;
+
+    // ping-pong， 确定ub上的地址
+    LocalTensor<float> curTensorForAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadAttnScoreFp32 : UbAttn.tensorForLoadAttnScoreFp32[MAX_LENG_PER_UB_PROC];
+    LocalTensor<INPUT_T> curTensorForAttnScoreFp16 =
+        curTensorForAttnScore[32 * BASE_BLOCK_SIDE_LEN].template ReinterpretCast<INPUT_T>();
+
+    LocalTensor<float> curTensorForRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclRowmaxFp32 : UbAttn.tensorForCaclRowmaxFp32[MAX_LENG_PER_UB_PROC / 2];
+
+    LocalTensor<float> curTensorForMask = pingPongFlag == 0 ?
+                                              UbAttn.tensorForLoadOneBlockTriMaskFp32 :
+                                              UbAttn.tensorForLoadOneBlockTriMaskFp32[BASE_BLOCK_SIDE_LEN];
+    LocalTensor<INPUT_T> curTensorForMaskFp16 = curTensorForMask[64].template ReinterpretCast<INPUT_T>();
+
+    LocalTensor<float> curTensorForHeadMask = pingPongFlag == 0 ?
+                                                  UbAttn.tensorForLoadOneBlockTriMaskFp32[256] :
+                                                  UbAttn.tensorForLoadOneBlockTriMaskFp32[BASE_BLOCK_SIDE_LEN + 256];
+    LocalTensor<INPUT_T> curTensorForHeadMaskFp16 = curTensorForHeadMask[64].template ReinterpretCast<INPUT_T>();
+
+    LocalTensor<float> curPpTensorForAttenScore;
+
+    auto extraBufIndex = Param.totalFragNum - 1 - Param.curFrag;
+
+    LocalTensor<float> ppTensorForLoadExtraFirstAttnScoreFp32 =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadExtraFirstAttnScoreFp32 :
+                            UbAttn.tensorForLoadExtraFirstAttnScoreFp32[MAX_LENG_PER_UB_PROC];
+
+    // 使用x个extra buf
+    if (extraBufIndex >= 0) {
+        curPpTensorForAttenScore = curTensorForAttnScore;
+    } else {
+        curTensorForAttnScore = ppTensorForLoadExtraFirstAttnScoreFp32;
+        curPpTensorForAttenScore = ppTensorForLoadExtraFirstAttnScoreFp32;
+    }
+
+    WaitFlag<HardEvent::MTE3_MTE2>(eventId);
+
+    // copy: gm -> ub
+    GlobalTensor<float> tmpScoreGmTensor;
+    tmpScoreGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(attnScoreGm + Param.sectionStartLineOffset));
+    DataCopy(curTensorForAttnScore, tmpScoreGmTensor,
+             DataCopyParams(Param.sectionBlockNum, BASE_BLOCK_SIDE_LEN / 8, BASIC_GAP / 8, 0));
+
+    bool sparseTailMaskFlag =
+        sparseFlag && (Param.applyTriMask == TRI_MATRIX_TAIL || Param.applyTriMask == TRI_MATRIX_HEAD_AND_TAIL);
+    bool sparseHeadMaskFlag =
+        sparseFlag && (Param.applyTriMask == TRI_MATRIX_HEAD || Param.applyTriMask == TRI_MATRIX_HEAD_AND_TAIL);
+    // 尾块 && 三角阵，一定需要mask
+    if ((Param.applyTriMask && Param.triMatrixNum > 0) || sparseTailMaskFlag) {
+        gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(curTensorForMaskFp16, attnMaskGmTensor[Param.sectionMaskOffset], 0, 1,
+                                                 BASE_BLOCK_SIDE_LEN / 16, 0, 0);
+    }
+    if (sparseHeadMaskFlag) {
+        gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(curTensorForHeadMaskFp16,
+                                                 attnMaskGmTensor[Param.sectionMaskOffset + 1], 0, 1,
+                                                 BASE_BLOCK_SIDE_LEN / 16, 0, 0);
+    }
+
+    SetFlag<HardEvent::MTE2_V>(eventId);
+    WaitFlag<HardEvent::MTE2_V>(eventId);
+
+    // repeat num 循环次数  (basic_block_num * 128 / 128)   ~~ FP32 * 2
+    muls_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, (float)scale,
+                                         Param.sectionBlockNum * 2, 1, 1, 8, 8);
+    PipeBarrier<PIPE_V>();
+
+    if (Param.sectionPaddingBlockNum > 0) {
+        PaddingForRowMaxTensor(Param.sectionBlockNum, Param.sectionPaddingBlockNum, curPpTensorForAttenScore,
+                               PADDING_FOR_MAX);
+    }
+
+    if ((Param.applyTriMask && Param.triMatrixNum > 0) || sparseTailMaskFlag) {
+        conv_v<ArchType::ASCEND_V220, INPUT_T, float>(curTensorForMask, curTensorForMaskFp16, 2, 1, 1, 8, 4);
+        PipeBarrier<PIPE_V>();
+
+        // 128个fp32 --> 2 repeats
+        muls_v<ArchType::ASCEND_V220, float>(curTensorForMask, curTensorForMask, (float)PADDING_FOR_MAX, 2, 1, 1, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        // fp32, 2-repeat
+        add_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore[(Param.sectionBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+                                            curTensorForAttnScore[(Param.sectionBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+                                            curTensorForMask, 2, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+    }
+    if (sparseHeadMaskFlag) {
+        conv_v<ArchType::ASCEND_V220, INPUT_T, float>(curTensorForHeadMask, curTensorForHeadMaskFp16, 2, 1, 1, 8, 4);
+        PipeBarrier<PIPE_V>();
+
+        Duplicate(curTensorForMask, (float)1.0, 128); // 2*8*32B / 4B
+        PipeBarrier<PIPE_V>();
+        sub_v<ArchType::ASCEND_V220, float>(curTensorForHeadMask, curTensorForMask, curTensorForHeadMask, 2, 1, 1, 1, 8,
+                                            8, 8);
+        PipeBarrier<PIPE_V>();
+
+        // 128个fp32 --> 2 repeats
+        muls_v<ArchType::ASCEND_V220, float>(curTensorForHeadMask, curTensorForHeadMask, (float)PADDING_FOR_MAX, 2, 1,
+                                             1, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        add_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, curTensorForHeadMask, 2, 1, 1,
+                                            1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+    }
+
+    auto allBlockNum = Param.sectionBlockNum + Param.sectionPaddingBlockNum;
+    int32_t tailBlockNum = allBlockNum % BLOCK_NUM_FOR_VMAX;
+    int32_t doneBlockNum = allBlockNum / BLOCK_NUM_FOR_VMAX * BLOCK_NUM_FOR_VMAX;
+    bool fromBufForAttnScore = false;
+
+    LocalTensor<float> tensorForCaclFinalRowmaxFp32 = pingPongFlag == 0 ?
+                                                          UbAttn.tensorForCaclFinalRowmaxFp32 :
+                                                          UbAttn.tensorForCaclFinalRowmaxFp32[BASE_BLOCK_SIDE_LEN / 2];
+    ProcessCaclMaxTensor(Param.sectionBlockNum, Param.sectionPaddingBlockNum, firstFrag, curTensorForAttnScore,
+                         curTensorForRowmax, tensorForCaclFinalRowmaxFp32);
+
+    if (lastFrag == false) {
+        SetFlag<HardEvent::V_MTE3>(eventId);
+        WaitFlag<HardEvent::V_MTE3>(eventId);
+
+        // 不需要MTE3
+        SetFlag<HardEvent::MTE3_MTE2>(eventId);
+    }
+}
+
+/**
+ * sparseFlag：记录max的下标
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessLinePhaseOneForExp(
+    PARAM_LONG_SEQ_EXP Param, int32_t pingPongFlag, __gm__ WORKSPACE_T *attnScoreGm, __gm__ INPUT_T *attnMaskGm,
+    UB_FOR_LONG_SEQ_ATTN_SCORE &UbAttn, int32_t tmp, bool sparseFlag)
+{
+    auto eventId = pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1;
+    // x-1个 extra空间
+    bool needCopy = Param.curFrag < Param.totalFragNum - 1;
+
+    LocalTensor<float> curTensorForAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadAttnScoreFp32 : UbAttn.tensorForLoadAttnScoreFp32[MAX_LENG_PER_UB_PROC];
+    LocalTensor<INPUT_T> curTensorForAttnScoreFp16 =
+        curTensorForAttnScore[32 * BASE_BLOCK_SIDE_LEN].template ReinterpretCast<INPUT_T>();
+
+    LocalTensor<float> curTensorForHeadMask = pingPongFlag == 0 ?
+                                                  UbAttn.tensorForLoadOneBlockTriMaskFp32[256] :
+                                                  UbAttn.tensorForLoadOneBlockTriMaskFp32[BASE_BLOCK_SIDE_LEN + 256];
+    LocalTensor<INPUT_T> curTensorForHeadMaskFp16 = curTensorForHeadMask[64].template ReinterpretCast<INPUT_T>();
+    LocalTensor<float> curTensorForMask = pingPongFlag == 0 ?
+                                              UbAttn.tensorForLoadOneBlockTriMaskFp32 :
+                                              UbAttn.tensorForLoadOneBlockTriMaskFp32[BASE_BLOCK_SIDE_LEN];
+    LocalTensor<float> curTensorForRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclRowmaxFp32 : UbAttn.tensorForCaclRowmaxFp32[MAX_LENG_PER_UB_PROC / 2];
+    LocalTensor<float> curTensorForCaclFinalRowsumFp32 =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclFinalRowsumFp32 :
+                            UbAttn.tensorForCaclFinalRowsumFp32[BASE_BLOCK_SIDE_LEN / 2];
+    auto extraBufIndex = Param.totalFragNum - 1 - Param.curFrag;
+
+    // 使用x个extra buf
+    if (extraBufIndex < 0) {
+        curTensorForAttnScore = pingPongFlag == 0 ? UbAttn.tensorForLoadExtraFirstAttnScoreFp32 :
+                                                    UbAttn.tensorForLoadExtraFirstAttnScoreFp32[MAX_LENG_PER_UB_PROC];
+    }
+
+    WaitFlag<HardEvent::MTE3_MTE2>(eventId);
+
+    GlobalTensor<float> tmpAttnScoreGmTensor;
+    tmpAttnScoreGmTensor.SetGlobalBuffer(
+        reinterpret_cast<__gm__ float *>((__gm__ float *)(attnScoreGm + Param.sectionStartLineOffset)));
+
+    if (needCopy == true) {
+        // 最后两次由于数据在UB上，不需要拷贝 （暂定仅最后一个在ub上）
+        gm_to_ub<ArchType::ASCEND_V220, float>(curTensorForAttnScore, tmpAttnScoreGmTensor,
+                                               0,                       // sid 一般0
+                                               Param.sectionBlockNum,   // burst_number 搬运几块
+                                               BASE_BLOCK_SIDE_LEN / 8, // FP32
+                                               BASIC_GAP / 8,           // src stride 即burst stride   128*127/16
+                                               0);
+
+        if (Param.curFrag == 0 && sparseFlag) {
+            gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(curTensorForHeadMaskFp16,
+                                                     attnMaskGmTensor[Param.sectionMaskOffset + 1], 0, 1,
+                                                     BASE_BLOCK_SIDE_LEN / 16, 0, 0);
+        }
+    }
+
+    SetFlag<HardEvent::MTE2_V>(eventId);
+    WaitFlag<HardEvent::MTE2_V>(eventId);
+
+    // scale 求max先缩放
+    if (needCopy == true) {
+        if (Param.curFrag == 0 && sparseFlag) {
+            conv_v<ArchType::ASCEND_V220, INPUT_T, float>(curTensorForHeadMask, curTensorForHeadMaskFp16, 2, 1, 1, 8,
+                                                          4);
+            PipeBarrier<PIPE_V>();
+            Duplicate(curTensorForMask, (float)1.0, 128);
+            PipeBarrier<PIPE_V>();
+            sub_v<ArchType::ASCEND_V220, float>(curTensorForHeadMask, curTensorForMask, curTensorForHeadMask, 2, 1, 1,
+                                                1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+            // 128个fp32-->2 repeats
+            muls_v<ArchType::ASCEND_V220, float>(curTensorForHeadMask, curTensorForHeadMask, float(PADDING_FOR_MAX), 2,
+                                                 1, 1, 8, 8);
+            PipeBarrier<PIPE_V>();
+            add_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, curTensorForHeadMask, 2,
+                                                1, 1, 1, 8, 8, 8);
+            PipeBarrier<PIPE_V>();
+        }
+        // repeat num 循环次数  (basic_block_num * 128 / 128)   ~~ FP32 * 2
+        muls_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, (float)scale,
+                                             Param.sectionBlockNum * 2, 1, 1, 8, 8);
+        PipeBarrier<PIPE_V>();
+    }
+
+    sub_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore,
+                                        UbAttn.tensorForVbrcbRowmaxFp32[pingPongFlag == 0 ? 0 : 8],
+                                        Param.sectionBlockNum * 2, 1, 1, 0, 8, 8, 0);
+    PipeBarrier<PIPE_V>();
+
+    // 原地覆盖
+    exp_v<ArchType::ASCEND_V220, float>(curTensorForAttnScore, curTensorForAttnScore, Param.sectionBlockNum * 2, 1, 1,
+                                        8, 8);
+    PipeBarrier<PIPE_V>();
+
+    if (Param.sectionPaddingBlockNum > 0) {
+        auto tailNum = Param.sectionBlockNum % MAX_BLOCK_PER_ONE_PROC;
+        AscendC::Duplicate(curTensorForAttnScore[tailNum * BASE_BLOCK_SIDE_LEN], (float)0, 64,
+                           Param.sectionPaddingBlockNum * 2, 1, 8);
+        PipeBarrier<PIPE_V>();
+    }
+    ProcessCaclSumTensor(Param.sectionBlockNum, Param.sectionPaddingBlockNum, Param.curFrag == Param.totalFragNum - 1,
+                         curTensorForAttnScore, curTensorForRowmax, curTensorForCaclFinalRowsumFp32);
+
+    LocalTensor<INPUT_T> curTensorForAttnScoreRes = curTensorForAttnScore.template ReinterpretCast<INPUT_T>();
+    if constexpr (IF_BF16) {
+        convr_v<ArchType::ASCEND_V220, float, INPUT_T>(curTensorForAttnScoreRes, curTensorForAttnScore,
+                                                       Param.sectionBlockNum * 2, 1, 1, 4, 8);
+    } else {
+        conv_v<ArchType::ASCEND_V220, float, INPUT_T>(curTensorForAttnScoreRes, curTensorForAttnScore,
+                                                      Param.sectionBlockNum * 2, 1, 1, 4, 8);
+    }
+    PipeBarrier<PIPE_V>();
+
+    SetFlag<HardEvent::V_MTE3>(eventId);
+    WaitFlag<HardEvent::V_MTE3>(eventId);
+
+    int32_t totalOffsetFp32 = Param.sectionStartLineOffset - tmp + tmp / 2;
+    GlobalTensor<INPUT_T> tmpScoreGmTensorOut;
+    tmpScoreGmTensorOut.SetGlobalBuffer(
+        reinterpret_cast<__gm__ INPUT_T *>((__gm__ INPUT_T *)(attnScoreGm + totalOffsetFp32)));
+    DataCopy(
+        tmpScoreGmTensorOut, curTensorForAttnScoreRes,
+        DataCopyParams(Param.sectionBlockNum, BASE_BLOCK_SIDE_LEN / 16, 0, (BASIC_GAP + BASE_BLOCK_DATA_NUM) / 16));
+
+    SetFlag<HardEvent::MTE3_MTE2>(eventId);
+}
+
+/**
+ * rowsum的全信息
+ * *eachCoreProcessLines：每个core处理的行数
+ * *eachCoreOffsetLines：每个core偏移的行数
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetNormLocalInfo(
+    int32_t qSeqLen, int32_t headNum, int32_t batchNum, int32_t *eachCoreProcessLines, int32_t *eachCoreOffsetLines)
+{
+    int32_t vectorNum = AscendC::GetBlockNum() * VEC_NUM_PER_CUBE;
+    int32_t curVectorIdx = AscendC::GetBlockIdx();
+    int32_t totalLines = qSeqLen * headNum * batchNum;
+
+    *eachCoreProcessLines = totalLines / vectorNum;
+    int32_t remainLines = totalLines % vectorNum;
+
+    *eachCoreOffsetLines = *eachCoreProcessLines * curVectorIdx;
+
+    if (remainLines > 0 && curVectorIdx < remainLines) {
+        // 对尾数进行分配
+        *eachCoreProcessLines += 1;
+    }
+
+    *eachCoreOffsetLines += curVectorIdx < remainLines ? curVectorIdx : remainLines;
+}
+
+/**
+ * pvResultOffset：PV值的偏移
+ * rowsumOffset：rowsum的偏移
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessForNormalize(
+    int32_t linesPerLoop, int32_t pvResultOffset, int32_t rowsumOffset, int32_t pingPongFlag, UB_FOR_NORMALIZE &UbNorm,
+    __gm__ float *__restrict__ gmCCube2, __gm__ INPUT_T *__restrict__ gm_cube2_out, __gm__ float *__restrict__ rowsumGm)
+{
+    auto eventId = (pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1);
+
+    // 拷贝计算好的exp值 (interval/2 因为是fp32， ub空间申请时，按照fp16申请的)
+    LocalTensor<float> fp32UbTensor =
+        pingPongFlag ? UbNorm.tensorForLoadOFp32[UbNorm.oPingPongInterval] : UbNorm.tensorForLoadOFp32[0];
+
+    // 拷贝由cube计算好的rowsum值 （一次处理）
+    LocalTensor<float> rowsumUbTensor = pingPongFlag ? UbNorm.tensorForLoadRowsumFp32[UbNorm.rowsumPingPongInterval] :
+                                                       UbNorm.tensorForLoadRowsumFp32[0];
+
+    LocalTensor<float> rowsumBrcbUbTensor = pingPongFlag ?
+                                                UbNorm.tensorForBrcbRowsumFp32[UbNorm.rowsumBrcbPingPongInterval] :
+                                                UbNorm.tensorForBrcbRowsumFp32[0];
+
+    auto rowsumCopyLines = (linesPerLoop + 7) / 8 * 8;
+    GlobalTensor<float> tmpGmCCube2Tensor;
+    tmpGmCCube2Tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmCCube2 + pvResultOffset));
+    GlobalTensor<float> tmpRowsumGmTensor;
+    tmpRowsumGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(rowsumGm + rowsumOffset));
+    GlobalTensor<INPUT_T> tmpCCube2GmTensorOut;
+    tmpCCube2GmTensorOut.SetGlobalBuffer(
+        reinterpret_cast<__gm__ INPUT_T *>((__gm__ INPUT_T *)(gm_cube2_out + pvResultOffset)));
+    WaitFlag<HardEvent::MTE3_MTE2>(eventId);
+
+    AscendC::DataCopy(fp32UbTensor, tmpGmCCube2Tensor,
+                      AscendC::DataCopyParams(1, linesPerLoop * HEAD_DIM * 4 / 32, 0, 0));
+
+    AscendC::DataCopy(rowsumUbTensor, tmpRowsumGmTensor, AscendC::DataCopyParams(1, rowsumCopyLines * 4 / 32, 0, 0));
+
+    SetFlag<HardEvent::MTE2_V>(eventId);
+    WaitFlag<HardEvent::MTE2_V>(eventId);
+
+    // 展开linesPerLoop个数据
+    brcb_v<ArchType::ASCEND_V220, uint32_t>(rowsumBrcbUbTensor.template ReinterpretCast<uint32_t>(),
+                                            rowsumUbTensor.template ReinterpretCast<uint32_t>(),
+                                            1,           // dstBlockStride
+                                            8,           // dstRepeatStride
+                                            linesPerLoop // repeat
+    );
+    PipeBarrier<PIPE_V>();
+
+    // 128个FP32 需要两个repeat； 因此，需要先算前半段，再算后半段
+    AscendC::Div<float, false>(fp32UbTensor, fp32UbTensor, rowsumBrcbUbTensor, (uint64_t)0, linesPerLoop,
+                               AscendC::BinaryRepeatParams(1, 1, 0, 16, 16, 1));
+    PipeBarrier<PIPE_V>();
+
+    AscendC::Div<float, false>(fp32UbTensor[HEAD_DIM / 2], fp32UbTensor[HEAD_DIM / 2], rowsumBrcbUbTensor, (uint64_t)0,
+                               linesPerLoop, AscendC::BinaryRepeatParams(1, 1, 0, 16, 16, 1));
+    PipeBarrier<PIPE_V>();
+
+    LocalTensor<INPUT_T> fp16UbTensor = fp32UbTensor.template ReinterpretCast<INPUT_T>();
+    if constexpr (IF_BF16) {
+        convr_v<ArchType::ASCEND_V220, float, INPUT_T>(fp16UbTensor, fp32UbTensor, linesPerLoop * 2, 1, 1, 4, 8);
+    } else {
+        conv_v<ArchType::ASCEND_V220, float, INPUT_T>(fp16UbTensor, fp32UbTensor, linesPerLoop * 2, 1, 1, 4, 8);
+    }
+    PipeBarrier<PIPE_V>();
+
+    SetFlag<HardEvent::V_MTE3>(eventId);
+    WaitFlag<HardEvent::V_MTE3>(eventId);
+
+    AscendC::DataCopy(tmpCCube2GmTensorOut, fp16UbTensor,
+                      AscendC::DataCopyParams(1, linesPerLoop * HEAD_DIM * 2 / 32, 0, 0));
+
+    SetFlag<HardEvent::MTE3_MTE2>(eventId);
+}
+
+/**
+ * 定义函数，处理归一化（exp / rowsum）
+ * 实际放在PV之后（P=softmax(QK_T)）, O=PV/rowsum
+ * O.shape = (seq_len, headDim)，稠密阵（没有三角阵的考虑）
+ *
+ * maxProcLen: 一次处理的子序列最大长度 （FP32的元素个
+ * curCoreProcessLines：总体需要处理的行数（ping + pong）
+ * curCoreOffsetLines
+ * UbNorm
+ * gmCCube2: 归一化的输入和输出
+ * rowsumGm
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AttentionScoreNormalize(
+    int32_t maxProcLen, int32_t curCoreProcessLines, int32_t curCoreOffsetLines, UB_FOR_NORMALIZE &UbNorm,
+    __gm__ float *__restrict__ gmCCube2, __gm__ INPUT_T *__restrict__ gm_cube2_out, __gm__ float *__restrict__ rowsumGm)
+{
+    // 一个loop处理的行数 （单个ping或pong）；一定不会有余数； loop内最大的处理次数，最后一次ping-pong会小于等于该值
+    int32_t linesPerLoop = maxProcLen / HEAD_DIM / 2;
+    // ping-pong分两半，会有奇数行（有余数）
+    int32_t halfLines = curCoreProcessLines / 2;
+
+    // 最后一次ping、pong处理的行数；余数可能是0，表示刚好是整数倍
+    int32_t lastPingLines = halfLines % linesPerLoop;
+    int32_t lastPongLines = halfLines % linesPerLoop;
+
+    int32_t pingPongTimes = halfLines / linesPerLoop * 2;
+    int32_t pingPongTailLines = 0;
+
+    if (lastPingLines > 0) {
+        pingPongTimes += 2;
+    }
+
+    if (curCoreProcessLines % 2 > 0) {
+        // 奇数时，ping-pong不等长 （在pong上加一个会更好）；=0表示刚好用完，所以不能增加额外的一行
+        if (lastPongLines < linesPerLoop && lastPongLines != 0) {
+            // 没到最长，pong时多处理一行， 此时不需要在多一次循环
+            lastPongLines += 1;
+        } else {
+            // 否则当做尾行处理
+            pingPongTailLines = 1;
+            pingPongTimes += 1;
+        }
+    }
+
+    int32_t lastPingPong = pingPongTimes - 2 - (pingPongTailLines > 0 ? 1 : 0);
+
+    lastPingLines = (lastPingLines == 0 ? linesPerLoop : lastPingLines);
+    lastPongLines = (lastPongLines == 0 ? linesPerLoop : lastPongLines);
+
+    int32_t pingPongFlag = 0;
+
+    // 需要计算的偏移地址
+    int32_t pvResultOffset = curCoreOffsetLines * HEAD_DIM;
+    int32_t rowsumOffset = curCoreOffsetLines;
+
+    int32_t pingPvResultOffset = pvResultOffset;
+    int32_t pongPvResultOffset = pvResultOffset + halfLines * HEAD_DIM;
+
+    int32_t pingRowsumOffsetOffset = rowsumOffset;
+    int32_t pongRowsumOffsetOffset = rowsumOffset + halfLines;
+
+    int *curPvResultOffset = nullptr;
+    int *curRowsumOffset = nullptr;
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+    for (int procTimes = 0; procTimes < pingPongTimes; procTimes++) {
+        curPvResultOffset = (pingPongFlag == 0 ? &pingPvResultOffset : &pongPvResultOffset);
+        curRowsumOffset = (pingPongFlag == 0 ? &pingRowsumOffsetOffset : &pongRowsumOffsetOffset);
+
+        if (pingPongTailLines > 0 && procTimes == pingPongTimes - 1) {
+            // 只有1行，pongPvResultOffset：需要计算偏移地址
+            ProcessForNormalize(1, pongPvResultOffset, pongRowsumOffsetOffset, 1, UbNorm, gmCCube2, gm_cube2_out,
+                                rowsumGm);
+            continue;
+        }
+
+        auto curLines = linesPerLoop;
+        if (procTimes >= lastPingPong) {
+            if (pingPongFlag == 0) {
+                curLines = lastPingLines;
+            } else {
+                curLines = lastPongLines;
+            }
+        }
+
+        ProcessForNormalize(curLines, *curPvResultOffset, *curRowsumOffset, pingPongFlag, UbNorm, gmCCube2,
+                            gm_cube2_out, rowsumGm);
+
+        *curPvResultOffset += curLines * HEAD_DIM;
+        *curRowsumOffset += curLines;
+
+        pingPongFlag = 1 - pingPongFlag;
+    }
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+}
+
+/**
+ * 短序列一次拷贝
+ * qkTriangle: 是否倒三角
+ * triMatrixMaskOffset: 128*128的三角阵中取第n行
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AttentionScoreShortDoubleLineOne(
+    bool qkTriangle, int32_t sectionOneBlockNums, int32_t sectionTwoBlockNums, int32_t triMatrixMaskOffset,
+    int32_t eachVectorProcLineNum, int32_t localSectionOneStartLineOffset, int32_t localSectionTwoStartLineOffset,
+    __gm__ WORKSPACE_T *__restrict__ attnScoreGm, __gm__ INPUT_T *__restrict__ attnMaskGm,
+    UB_FOR_SHORT_LEN_ATTN_SCORE &UbAttn)
+{
+    // 一定是分两段计算rowmax，否则没有必要； 只能支持8192以下的序列，否则没法一次拷贝一个完整行
+    PARAM_SHORT_SEQ_MAX ParamPingPong[2] = {0};
+
+    ParamPingPong[0].sectionStartLineOffset = localSectionOneStartLineOffset;
+    ParamPingPong[0].sectionOneBlockNum = sectionOneBlockNums;
+    // 非三角阵为0
+    ParamPingPong[0].sectionTwoBlockNum = sectionTwoBlockNums;
+    ParamPingPong[0].recordRowmaxOffset = 0;
+
+    // pong和ping相差一行或两行
+    ParamPingPong[1].sectionStartLineOffset = localSectionOneStartLineOffset + BASE_BLOCK_SIDE_LEN * rowPerTime;
+    ParamPingPong[1].sectionOneBlockNum = sectionOneBlockNums;
+    // 非三角阵为0
+    ParamPingPong[1].sectionTwoBlockNum = sectionTwoBlockNums;
+    ParamPingPong[1].recordRowmaxOffset = 0;
+
+    GetPaddingInfoForRowMax(sectionOneBlockNums, &ParamPingPong[0].sectionOnePaddingBlockNum);
+    ParamPingPong[1].sectionOnePaddingBlockNum = ParamPingPong[0].sectionOnePaddingBlockNum;
+    if (qkTriangle == true) {
+        ParamPingPong[0].sectionMaskOffset = triMatrixMaskOffset;
+        ParamPingPong[1].sectionMaskOffset = triMatrixMaskOffset + maskSeqLength * rowPerTime;
+        GetPaddingInfoForRowMax(sectionTwoBlockNums, &ParamPingPong[0].sectionTwoPaddingBlockNum);
+        ParamPingPong[1].sectionTwoPaddingBlockNum = ParamPingPong[0].sectionTwoPaddingBlockNum;
+    }
+
+    int32_t pingPongFlag = 0;
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+
+    for (int lines = 0; lines < eachVectorProcLineNum / PING_PONG_NUM / rowPerTime; lines++) {
+        // 一定循环是2
+        for (int oneLineProc = 0; oneLineProc < 2; oneLineProc++) {
+            int32_t offset = lines * BASE_BLOCK_SIDE_LEN * PING_PONG_NUM * rowPerTime +
+                             pingPongFlag * BASE_BLOCK_SIDE_LEN * rowPerTime;
+            ProcessLinePhaseOneForShortSeqMax(qkTriangle, ParamPingPong[pingPongFlag], pingPongFlag, lines == 0,
+                                              attnScoreGm, attnMaskGm, UbAttn);
+
+            ProcessLinePhaseOneForShortSeqExp(qkTriangle, ParamPingPong[pingPongFlag], pingPongFlag, attnScoreGm,
+                                              attnMaskGm, UbAttn, offset);
+            pingPongFlag = 1 - pingPongFlag;
+        }
+
+        ParamPingPong[0].sectionStartLineOffset += BASE_BLOCK_SIZE_DOUBLE * rowPerTime;
+        ParamPingPong[1].sectionStartLineOffset += BASE_BLOCK_SIZE_DOUBLE * rowPerTime;
+
+        ParamPingPong[0].sectionMaskOffset += maskSeqLength * 2 * rowPerTime;
+        ParamPingPong[1].sectionMaskOffset += maskSeqLength * 2 * rowPerTime;
+
+        ParamPingPong[0].recordRowmaxOffset += 16;
+        ParamPingPong[1].recordRowmaxOffset += 16;
+    }
+
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+}
+
+/**
+ * blockNumPerFullLine
+ * subSeqLengthPerProc: 一次处理处理的长度
+ * *pingBlockOffsetNum: ping起始块
+ * *pongBlockOffsetNum: pong起始块
+ * *tailBlockOffsetNum: tail起始块
+ * *tailBlockNum:       tail的块数
+ * *pingPongTimes:      pingpang的循环次数
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::GetUniRowmaxSeqInfoPerProc(
+    int32_t blockNumPerFullLine, int32_t subSeqLengthPerProc, int32_t *pingBlockOffsetNum, int32_t *pongBlockOffsetNum,
+    int32_t *tailBlockOffsetNum, int32_t *tailBlockNum, int32_t *pingPongTimes)
+{
+    // 完整行块的剩余尾块：pingpong需要偶数
+    *tailBlockNum = blockNumPerFullLine % 2;
+    *tailBlockOffsetNum = blockNumPerFullLine - *tailBlockNum;
+    *pingBlockOffsetNum = 0;
+    *pongBlockOffsetNum = (*tailBlockOffsetNum) / 2;
+
+    auto totalSize = *pongBlockOffsetNum * BASE_BLOCK_SIDE_LEN;
+
+    // ping和pong各算一次循环
+    *pingPongTimes = totalSize / subSeqLengthPerProc * 2;
+
+    // 分2个Section处理时，不是2的幂，未必能整除
+    if (totalSize % subSeqLengthPerProc > 0) {
+        // 并非是尾块  (尾块只有一个block )
+        *pingPongTimes += 2;
+    }
+}
+
+/**
+ * 中序列
+ * 处理完整行的阶段一，计算出max值
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessLinePhaseOneForMax(
+    PARAM_MEDIUM_SEQ_EXP Param, int32_t pingPongFlag, bool firstProc, __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+    __gm__ INPUT_T *__restrict__ attnMaskGm, UB_FOR_MDDIUM_SEQ_ATTN_SCORE &UbAttn, int32_t lines)
+{
+    auto eventId = (pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1);
+    bool firstFrag = Param.curFrag == 0;
+    bool lastFrag = Param.curFrag == Param.totalFragNum - 1;
+
+    // ping-pong， 确定ub上的地址
+    constexpr uint32_t maxLength = MAX_LENG_PER_UB_PROC > 4096 ? 4096 : MAX_LENG_PER_UB_PROC;
+    LocalTensor<float> curTensorForAttnScore =
+        pingPongFlag == 0 ? UbAttn.tensorForLoadAttnScoreFp32 : UbAttn.tensorForLoadAttnScoreFp32[maxLength];
+    LocalTensor<INPUT_T> fp16CurTensorForAttnScore =
+        curTensorForAttnScore[16 * BASE_BLOCK_SIDE_LEN].template ReinterpretCast<INPUT_T>();
+
+    LocalTensor<float> curTensorForRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclRowmaxFp32 : UbAttn.tensorForCaclRowmaxFp32[maxLength / 2];
+    LocalTensor<float> curTensorForMask = pingPongFlag == 0 ?
+                                              UbAttn.tensorForLoadOneBlockTriMaskFp32 :
+                                              UbAttn.tensorForLoadOneBlockTriMaskFp32[BASE_BLOCK_SIDE_LEN];
+    LocalTensor<INPUT_T> curTensorForMaskFp16 = curTensorForMask[64].template ReinterpretCast<INPUT_T>();
+    LocalTensor<float> curPpTensorForAttenScore = curTensorForAttnScore;
+    LocalTensor<float> curTensorForFinalRowmax = UbAttn.tensorForCaclFinalRowmaxFp32;
+
+    auto curBlockNum = 1;
+    if (Param.tailBlock == false) {
+        curBlockNum = Param.blockNumPerStep;
+        if (lastFrag == true) {
+            curBlockNum = Param.blockNumForLast;
+        }
+    }
+
+    WaitFlag<HardEvent::MTE3_MTE2>(eventId);
+    WaitFlag<HardEvent::V_MTE2>(eventId);
+
+    // copy: gm -> ub
+    GlobalTensor<float> tmpScoreGmTensor;
+    tmpScoreGmTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(attnScoreGm + Param.sectionStartLineOffset));
+    DataCopy(curTensorForAttnScore, tmpScoreGmTensor,
+             DataCopyParams(curBlockNum, BASE_BLOCK_SIDE_LEN / 8, BASIC_GAP / 8, 0));
+
+    // 尾块 && 三角阵，一定需要mask
+    if (Param.applyTriMask && Param.triMatrixNum > 0) {
+        gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(curTensorForMaskFp16, attnMaskGmTensor[Param.sectionMaskOffset], 0, 1,
+                                                 BASE_BLOCK_SIDE_LEN / 16, 0, 0);
+    }
+
+    SetFlag<HardEvent::MTE2_V>(eventId);
+    WaitFlag<HardEvent::MTE2_V>(eventId);
+
+    auto paddingBlock = 0;
+
+    if (lastFrag && Param.lastPaddingBlockNum > 0) {
+        paddingBlock = Param.lastPaddingBlockNum;
+        PaddingForRowMaxTensor(Param.blockNumForLast, Param.lastPaddingBlockNum, curPpTensorForAttenScore,
+                               PADDING_FOR_MAX);
+    }
+
+    // 乘scale
+    muls_v<ArchType::ASCEND_V220, float>(UbAttn.tensorForStoreOneLineAttnScoreFp32[Param.bufOffset],
+                                         curTensorForAttnScore, (float)scale, (curBlockNum + paddingBlock) * 2, 1, 1, 8,
+                                         8);
+    PipeBarrier<PIPE_V>();
+
+    if (Param.applyTriMask && Param.triMatrixNum > 0) {
+        conv_v<ArchType::ASCEND_V220, INPUT_T, float>(curTensorForMask, curTensorForMaskFp16, 2, 1, 1, 8, 4);
+        PipeBarrier<PIPE_V>();
+
+        // 128个fp32 --> 2 repeats
+        muls_v<ArchType::ASCEND_V220, float>(curTensorForMask, curTensorForMask, (float)PADDING_FOR_MAX, 2, 1, 1, 8, 8);
+        PipeBarrier<PIPE_V>();
+
+        // fp32, 2-repeat
+        add_v<ArchType::ASCEND_V220, float>(
+            UbAttn.tensorForStoreOneLineAttnScoreFp32[Param.bufOffset + (curBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+            UbAttn.tensorForStoreOneLineAttnScoreFp32[Param.bufOffset + (curBlockNum - 1) * BASE_BLOCK_SIDE_LEN],
+            curTensorForMask, 2, 1, 1, 1, 8, 8, 8);
+        PipeBarrier<PIPE_V>();
+    }
+
+    // ping-pong一行，所以第一次ping才是第一段
+    ProcessCaclMaxTensor(curBlockNum, paddingBlock, firstFrag && firstProc,
+                         UbAttn.tensorForStoreOneLineAttnScoreFp32[Param.bufOffset], curTensorForRowmax,
+                         curTensorForFinalRowmax);
+
+    // 非最后一次，需要MTE2
+    if (lastFrag == false && lines == 0) {
+        SetFlag<HardEvent::V_MTE3>(eventId);
+        WaitFlag<HardEvent::V_MTE3>(eventId);
+
+        SetFlag<HardEvent::MTE3_MTE2>(eventId);
+    }
+    SetFlag<HardEvent::V_MTE2>(eventId);
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::ProcessLinePhaseOneForExp(
+    PARAM_MEDIUM_SEQ_EXP Param, int32_t pingPongFlag, bool firstProc, __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+    __gm__ INPUT_T *__restrict__ attnMaskGm, UB_FOR_MDDIUM_SEQ_ATTN_SCORE &UbAttn, int32_t fp32Offset)
+{
+    constexpr uint32_t maxLength = MAX_LENG_PER_UB_PROC > 4096 ? 4096 : MAX_LENG_PER_UB_PROC;
+    LocalTensor<float> curTensorForRowmax =
+        pingPongFlag == 0 ? UbAttn.tensorForCaclRowmaxFp32 : UbAttn.tensorForCaclRowmaxFp32[maxLength / 2];
+    auto eventId = pingPongFlag == 0 ? EVENT_ID0 : EVENT_ID1;
+
+    bool lastFrag = Param.curFrag == Param.totalFragNum - 1;
+    bool firstFrag = Param.curFrag == 0;
+
+    auto curBlockNum = 1;
+    auto paddingBlock = 0;
+    if (Param.tailBlock == false) {
+        curBlockNum = Param.blockNumPerStep;
+        if (lastFrag == true) {
+            curBlockNum = Param.blockNumForLast;
+        }
+    }
+
+    // 当前处理的数据
+    LocalTensor<float> tensorForAttnScore = UbAttn.tensorForStoreOneLineAttnScoreFp32[Param.bufOffset];
+
+    sub_v<ArchType::ASCEND_V220, float>(tensorForAttnScore, tensorForAttnScore, UbAttn.tensorForVbrcbRowmaxFp32,
+                                        curBlockNum * 2, 1, 1, 0, 8, 8, 0);
+    PipeBarrier<PIPE_V>();
+
+    // 原地覆盖
+    exp_v<ArchType::ASCEND_V220, float>(tensorForAttnScore, tensorForAttnScore, curBlockNum * 2, 1, 1, 8, 8);
+    PipeBarrier<PIPE_V>();
+
+    if (lastFrag && Param.lastPaddingBlockNum > 0) {
+        paddingBlock = Param.lastPaddingBlockNum;
+        auto tailNum = Param.blockNumForLast % MAX_BLOCK_PER_ONE_PROC;
+        AscendC::Duplicate(tensorForAttnScore[tailNum * BASE_BLOCK_SIDE_LEN], (float)0, 64,
+                           Param.lastPaddingBlockNum * 2, 1, 8);
+        PipeBarrier<PIPE_V>();
+    }
+    ProcessCaclSumTensor(curBlockNum, paddingBlock, firstFrag && firstProc, tensorForAttnScore, curTensorForRowmax,
+                         UbAttn.tensorForCaclFinalRowsumFp32);
+
+    LocalTensor<INPUT_T> fp16TensorForAttnScoreRes = tensorForAttnScore.template ReinterpretCast<INPUT_T>();
+    if constexpr (IF_BF16) {
+        convr_v<ArchType::ASCEND_V220, float, INPUT_T>(fp16TensorForAttnScoreRes, tensorForAttnScore, curBlockNum * 2,
+                                                       1, 1, 4, 8);
+    } else {
+        conv_v<ArchType::ASCEND_V220, float, INPUT_T>(fp16TensorForAttnScoreRes, tensorForAttnScore, curBlockNum * 2, 1,
+                                                      1, 4, 8);
+    }
+    PipeBarrier<PIPE_V>();
+
+    SetFlag<HardEvent::V_MTE3>(eventId);
+    WaitFlag<HardEvent::V_MTE3>(eventId);
+
+    int32_t totalOffsetFp32 = Param.sectionStartLineOffset - fp32Offset + fp32Offset / 2;
+    GlobalTensor<INPUT_T> tmpScoreGmTensorOut;
+    tmpScoreGmTensorOut.SetGlobalBuffer(
+        reinterpret_cast<__gm__ INPUT_T *>((__gm__ INPUT_T *)(attnScoreGm + totalOffsetFp32)));
+    DataCopy(tmpScoreGmTensorOut, fp16TensorForAttnScoreRes,
+             DataCopyParams(curBlockNum, BASE_BLOCK_SIDE_LEN / 16, 0, (BASIC_GAP + BASE_BLOCK_DATA_NUM) / 16));
+
+    SetFlag<HardEvent::MTE3_MTE2>(eventId);
+}
+
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AttentionScoreSingleLine(
+    bool qkTriangle, int32_t sectionLoopTimes, int32_t sectionOneBlockNums, int32_t sectionTwoBlockNums,
+    int32_t triMatrixMaskOffset, int32_t eachVectorProcLineNum, int32_t localSectionOneStartLineOffset,
+    int32_t localSectionTwoStartLineOffset, int32_t triMatrixNum, __gm__ WORKSPACE_T *__restrict__ attnScoreGm,
+    __gm__ INPUT_T *__restrict__ attnMaskGm, UB_FOR_MDDIUM_SEQ_ATTN_SCORE &UbAttn)
+{
+    int32_t *curSectionBlockNums = &sectionOneBlockNums;
+
+    // ping-pong每次处理得长度
+    int32_t subSeqLengthPerProc;
+
+    int32_t pingBlockOffsetNum = 0;
+    int32_t pongBlockOffsetNum = 0;
+    int32_t tailBlockOffsetNum = 0;
+    int32_t tailBlockNum = 0;
+    int32_t tailOffset = 0;
+    int32_t pingPongTimes = 0;
+    int32_t pingPongOffsetInLoop[2] = {0};
+
+    bool lastTimePingOrPong = false;
+
+    int32_t paddingBlockNumForSepSection = 0;
+    int32_t curTriMatrixMaskOffset = 0;
+
+    int32_t *curLocalSectionStartLineOffset = &localSectionOneStartLineOffset;
+
+    int32_t pingPongFlag = 0;
+
+    // ping-pong参数
+    PARAM_MEDIUM_SEQ_EXP ParamPingPong[2] = {0};
+    PARAM_MEDIUM_SEQ_EXP ParamTail = {0};
+
+    // 尾块是固定的
+    ParamTail.curFrag = 0;
+    ParamTail.totalFragNum = 1;
+    ParamTail.blockNumPerStep = 1;
+    ParamTail.blockNumForLast = 1;
+    ParamTail.tailBlock = true;
+    // 补成2
+    ParamTail.lastPaddingBlockNum = 1;
+
+    auto maxBlock = MAX_BLOCK_PER_ONE_PROC / 2;
+    auto maxLength = MAX_LENG_PER_UB_PROC / 2;
+
+    ParamPingPong[0].blockNumPerStep = maxBlock;
+    ParamPingPong[1].blockNumPerStep = maxBlock;
+
+    ParamPingPong[0].tailBlock = false;
+    ParamPingPong[1].tailBlock = false;
+
+    // ping不会遭遇三角块
+    ParamPingPong[0].applyTriMask = false;
+    // 若有尾块（1块），一定是尾块
+    ParamTail.applyTriMask = true;
+
+    int32_t recordRowmaxOffset = 0;
+
+    for (int sectionLoop = 0; sectionLoop < sectionLoopTimes; sectionLoop++) {
+        auto sectionBlockNum = *curSectionBlockNums;
+        auto sectionSeqLen = sectionBlockNum * BASE_BLOCK_SIDE_LEN;
+
+        // 每个Section长度非2的幂，折半求max时，需要padding值
+        if (sectionBlockNum > 1 && sectionBlockNum % 2 > 0) {
+            // 保证偶数，为了能ping-pong （非2的幂）
+            sectionSeqLen -= BASE_BLOCK_SIDE_LEN;
+        }
+
+        // 确定ping-pong的处理长度
+        subSeqLengthPerProc = sectionSeqLen / 2;
+        subSeqLengthPerProc = subSeqLengthPerProc > maxLength ? maxLength : subSeqLengthPerProc;
+
+        GetUniRowmaxSeqInfoPerProc(sectionBlockNum, subSeqLengthPerProc, &pingBlockOffsetNum, &pongBlockOffsetNum,
+                                   &tailBlockOffsetNum, &tailBlockNum, &pingPongTimes);
+
+        // ping的padding数量
+        GetPaddingInfoForRowMax(pongBlockOffsetNum, &ParamPingPong[0].lastPaddingBlockNum);
+
+        ParamPingPong[1].lastPaddingBlockNum = ParamPingPong[0].lastPaddingBlockNum;
+
+        ParamPingPong[0].blockNumForLast = pongBlockOffsetNum % maxBlock;
+        if (ParamPingPong[0].blockNumForLast == 0) {
+            // 最后一次时，会使用该参数
+            ParamPingPong[0].blockNumForLast = maxBlock;
+        }
+
+        ParamPingPong[1].blockNumForLast = ParamPingPong[0].blockNumForLast;
+
+        if (tailBlockNum > 0) {
+            if (ParamPingPong[1].blockNumForLast < maxBlock) {
+                // 等于0或者等于max_block， 说明没有额外空间，尽量减少ping-pong的次数，补充到pong
+                ParamPingPong[1].blockNumForLast += 1;
+                GetPaddingInfoForRowMax(pongBlockOffsetNum + 1, &ParamPingPong[1].lastPaddingBlockNum);
+            } else {
+                pingPongTimes += 1;
+            }
+        }
+
+        curTriMatrixMaskOffset = triMatrixMaskOffset;
+
+        ParamTail.sectionStartLineOffset = (sectionBlockNum - 1) * BASE_BLOCK_DATA_NUM;
+
+        // tail加载ping上
+        ParamPingPong[0].totalFragNum = pingPongTimes / 2 + pingPongTimes % 2;
+        ParamPingPong[1].totalFragNum = pingPongTimes / 2;
+
+        ParamPingPong[0].triMatrixNum = triMatrixNum;
+        ParamPingPong[1].triMatrixNum = triMatrixNum;
+        ParamTail.triMatrixNum = triMatrixNum;
+
+        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+
+        SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+        // 流水变更
+        SetFlag<HardEvent::V_MTE2>(EVENT_ID1);
+
+        // 一个完整行循环
+        for (int lines = 0; lines < eachVectorProcLineNum; lines++) {
+            pingPongFlag = 0;
+
+            // ping从行首开始，pong从行中间的某一块开始 （section loop = 1时，ping从第二段开始）
+            ParamPingPong[0].sectionStartLineOffset = *curLocalSectionStartLineOffset;
+            ParamPingPong[1].sectionStartLineOffset =
+                *curLocalSectionStartLineOffset + pongBlockOffsetNum * BASE_BLOCK_DATA_NUM;
+            ParamTail.sectionStartLineOffset =
+                *curLocalSectionStartLineOffset + tailBlockOffsetNum * BASE_BLOCK_DATA_NUM;
+
+            ParamPingPong[0].bufOffset = 0;
+            ParamPingPong[1].bufOffset = MDDIUM_SEQ_THRESHOLD / 2;
+            // 始终是最后一块
+            ParamTail.bufOffset = MDDIUM_SEQ_THRESHOLD / 2 + (sectionBlockNum / 2) * BASE_BLOCK_SIDE_LEN;
+
+            ParamPingPong[1].applyTriMask = false;
+
+            // 一定循环是2
+            for (int oneLineProc = 0; oneLineProc < pingPongTimes; oneLineProc++) {
+                ParamPingPong[pingPongFlag].curFrag = oneLineProc / 2;
+
+                // 尾块
+                if (pingPongTimes % 2 == 1 && oneLineProc == pingPongTimes - 1) {
+                    ParamTail.sectionMaskOffset = curTriMatrixMaskOffset;
+                    ProcessLinePhaseOneForMax(ParamTail, pingPongFlag, oneLineProc == 0, attnScoreGm, attnMaskGm,
+                                              UbAttn, lines);
+                    continue;
+                }
+
+                // 最后一次ping-pong
+                lastTimePingOrPong = oneLineProc >= pingPongTimes / 2 * 2 - 2;
+
+                // 没有尾块
+                if (lastTimePingOrPong && pingPongTimes % 2 == 0 && oneLineProc == pingPongTimes - 1) {
+                    ParamPingPong[1].applyTriMask = true;
+                }
+
+                ParamPingPong[pingPongFlag].sectionMaskOffset = curTriMatrixMaskOffset;
+
+                ProcessLinePhaseOneForMax(ParamPingPong[pingPongFlag], pingPongFlag, oneLineProc == 0, attnScoreGm,
+                                          attnMaskGm, UbAttn, lines);
+
+                ParamPingPong[pingPongFlag].bufOffset +=
+                    (ParamPingPong[pingPongFlag].blockNumPerStep * BASE_BLOCK_SIDE_LEN);
+                ParamPingPong[pingPongFlag].sectionStartLineOffset +=
+                    (ParamPingPong[pingPongFlag].blockNumPerStep * BASE_BLOCK_DATA_NUM);
+
+                pingPongFlag = 1 - pingPongFlag;
+            }
+
+            // 接着计算max (只有一个max)，有2个最大值(两段)
+            cmax_v<ArchType::ASCEND_V220, float, ReduceOrder::ORDER_ONLY_VALUE>(
+                UbAttn.tensorForRecordRowmaxFp32[recordRowmaxOffset], UbAttn.tensorForCaclFinalRowmaxFp32, 1, 1, 1, 8);
+            PipeBarrier<PIPE_V>();
+
+            brcb_v<ArchType::ASCEND_V220, uint32_t>(
+                UbAttn.tensorForVbrcbRowmaxFp32.template ReinterpretCast<uint32_t>(),
+                UbAttn.tensorForRecordRowmaxFp32[recordRowmaxOffset].template ReinterpretCast<uint32_t>(), 1, 8, 1);
+            PipeBarrier<PIPE_V>();
+
+            pingPongFlag = 0;
+
+            ParamPingPong[0].sectionStartLineOffset = *curLocalSectionStartLineOffset;
+            ParamPingPong[1].sectionStartLineOffset =
+                *curLocalSectionStartLineOffset + pongBlockOffsetNum * BASE_BLOCK_DATA_NUM;
+
+            ParamPingPong[0].bufOffset = 0;
+            ParamPingPong[1].bufOffset = MDDIUM_SEQ_THRESHOLD / 2;
+
+            // 计算exp
+            for (int oneLineProc = 0; oneLineProc < pingPongTimes; oneLineProc++) {
+                // 找到buf的偏移地址；拷贝数据的地址
+                ParamPingPong[pingPongFlag].curFrag = oneLineProc / 2;
+                int32_t fp32Offset = lines * BASE_BLOCK_SIDE_LEN;
+                if (pingPongTimes % 2 == 1 && oneLineProc == 0) {
+                    ProcessLinePhaseOneForExp(ParamTail, pingPongFlag, oneLineProc == 0, attnScoreGm, attnMaskGm,
+                                              UbAttn, fp32Offset);
+
+                    // 尾块
+                    continue;
+                }
+                ProcessLinePhaseOneForExp(ParamPingPong[pingPongFlag], pingPongFlag, oneLineProc == 0, attnScoreGm,
+                                          attnMaskGm, UbAttn, fp32Offset);
+
+                ParamPingPong[pingPongFlag].sectionStartLineOffset +=
+                    (ParamPingPong[pingPongFlag].blockNumPerStep * BASE_BLOCK_DATA_NUM);
+                ParamPingPong[pingPongFlag].bufOffset +=
+                    (ParamPingPong[pingPongFlag].blockNumPerStep * BASE_BLOCK_SIDE_LEN);
+
+                pingPongFlag = 1 - pingPongFlag;
+            }
+
+            AscendC::RepeatReduceSum<float, false>(UbAttn.tensorForRecordRowsumFp32[recordRowmaxOffset],
+                                                   UbAttn.tensorForCaclFinalRowsumFp32, 1, 0, 0, 1, 1, 8);
+            PipeBarrier<PIPE_V>();
+
+            recordRowmaxOffset += 8;
+
+            *curLocalSectionStartLineOffset += BASE_BLOCK_SIDE_LEN;
+            curTriMatrixMaskOffset += maskSeqLength;
+        }
+
+        // 流水变更
+        int32_t flag = 0;
+        for (int32_t i = 0; i < pingPongTimes; i++) {
+            auto eventId = (flag == 0 ? EVENT_ID0 : EVENT_ID1);
+            WaitFlag<HardEvent::MTE3_MTE2>(eventId);
+            flag = 1 - flag;
+        }
+
+        WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+        WaitFlag<HardEvent::V_MTE2>(EVENT_ID1);
+
+        curSectionBlockNums = &sectionTwoBlockNums;
+        curLocalSectionStartLineOffset = &localSectionTwoStartLineOffset;
+    }
+}
+
+/**
+ * 单行ping-pong计算
+ */
+template <typename INPUT_T, bool IF_BF16, typename WORKSPACE_T>
+__aicore__ __inline__ void VectorForward<INPUT_T, IF_BF16, WORKSPACE_T>::AttentionScoreDoubleLine(
+    bool qkTriangle, int32_t sectionLoopTimes, int32_t sectionOneBlockNums, int32_t sectionTwoBlockNums,
+    int32_t triMatrixMaskOffset, int32_t eachVectorProcLineNum, int32_t localSectionOneStartLineOffset,
+    int32_t localSectionTwoStartLineOffset, int32_t triMatrixNum, int32_t triMatrixOffset[],
+    __gm__ WORKSPACE_T *__restrict__ attnScoreGm, __gm__ INPUT_T *__restrict__ attnMaskGm,
+    UB_FOR_LONG_SEQ_ATTN_SCORE &UbAttn, bool sparseFlag)
+{
+    // section one 需要处理的块
+    int32_t *curSectionBlockNums = &sectionOneBlockNums;
+    // section的起始地址
+    int32_t *curLocalSectionStartLineOffset = &localSectionOneStartLineOffset;
+
+    int32_t pingPongFlag = 0;
+    int32_t pingPongTimes = 0;
+
+    int32_t tailBlockNum = 0;
+    int32_t sectionPaddingBlockNum = 0;
+
+    PARAM_LONG_SEQ_EXP ParamPingPong[2] = {0};
+
+    int32_t rowmaxOffset = 0;
+    int32_t rowsumOffset = 0;
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+    for (int sectionLoop = 0; sectionLoop < sectionLoopTimes; sectionLoop++) {
+        LocalTensor curTensorForRecordRowmaxFp32 =
+            sectionLoop == 0 ? UbAttn.tensorForRecordRowmaxFp32 : UbAttn.tensorForRecordRowmaxFp32[256];
+        rowmaxOffset = 0;
+        rowsumOffset = 0;
+
+        pingPongTimes = (*curSectionBlockNums) / MAX_BLOCK_PER_ONE_PROC;
+        tailBlockNum = (*curSectionBlockNums) % MAX_BLOCK_PER_ONE_PROC;
+
+        // tail指的是，相对于64个block，剩余的部分
+        if (tailBlockNum > 0) {
+            pingPongTimes += 1;
+            // 非三角阵不需要
+            GetPaddingInfoForRowMax(*curSectionBlockNums, &sectionPaddingBlockNum);
+        } else {
+            sectionPaddingBlockNum = 0;
+        }
+
+        if (tailBlockNum == 0) {
+            tailBlockNum = MAX_BLOCK_PER_ONE_PROC;
+        }
+
+        ParamPingPong[0].totalFragNum = pingPongTimes;
+        ParamPingPong[1].totalFragNum = pingPongTimes;
+
+        pingPongTimes *= 2; // ping一行，pong一行
+
+        ParamPingPong[0].sectionMaskOffset = triMatrixMaskOffset;
+        ParamPingPong[1].sectionMaskOffset = triMatrixMaskOffset + maskSeqLength;
+
+        ParamPingPong[0].triMatrixNum = triMatrixNum;
+        ParamPingPong[1].triMatrixNum = triMatrixNum;
+
+        // ping一行，pong一行；eachVectorProcLineNum一定是偶数
+        for (int lines = 0; lines < eachVectorProcLineNum / 2; lines++) {
+            // 第一行
+            ParamPingPong[0].sectionStartLineOffset = *curLocalSectionStartLineOffset;
+            // 第二行
+            ParamPingPong[1].sectionStartLineOffset = *curLocalSectionStartLineOffset + BASE_BLOCK_SIDE_LEN;
+
+            ParamPingPong[0].sectionBlockNum = MAX_BLOCK_PER_ONE_PROC;
+            ParamPingPong[1].sectionBlockNum = MAX_BLOCK_PER_ONE_PROC;
+            ParamPingPong[0].sectionPaddingBlockNum = 0;
+            ParamPingPong[1].sectionPaddingBlockNum = 0;
+
+            ParamPingPong[0].applyTriMask = TRI_MATRIX_NONE;
+            ParamPingPong[1].applyTriMask = TRI_MATRIX_NONE;
+
+            pingPongFlag = 0;
+
+            for (int oneLineProc = 0; oneLineProc < pingPongTimes; oneLineProc++) {
+                ParamPingPong[pingPongFlag].curFrag = oneLineProc / 2;
+
+                // 最后一段
+                if (oneLineProc >= pingPongTimes - 2) {
+                    ParamPingPong[pingPongFlag].sectionBlockNum = tailBlockNum;
+                    ParamPingPong[pingPongFlag].sectionPaddingBlockNum = sectionPaddingBlockNum;
+                    if (qkTriangle) {
+                        ParamPingPong[pingPongFlag].applyTriMask = TRI_MATRIX_TAIL;
+                    }
+                    if (sparseFlag) {
+                        ParamPingPong[pingPongFlag].sectionPaddingBlockNum = sectionPaddingBlockNum;
+                    }
+                }
+
+                if (sparseFlag) {
+                    if (pingPongTimes == 2) {
+                        ParamPingPong[pingPongFlag].applyTriMask = TRI_MATRIX_HEAD_AND_TAIL;
+                    } else {
+                        if (oneLineProc <= 1) {
+                            ParamPingPong[pingPongFlag].applyTriMask = TRI_MATRIX_HEAD;
+                        }
+                        if (oneLineProc >= pingPongTimes - 2) {
+                            ParamPingPong[pingPongFlag].applyTriMask = TRI_MATRIX_TAIL;
+                        }
+                    }
+                }
+
+                ProcessLinePhaseOneForMax(ParamPingPong[pingPongFlag], pingPongFlag, attnScoreGm, attnMaskGm, UbAttn,
+                                          sparseFlag);
+                // -- 需要加tri-matrix （0-非三角阵; 1-三角阵, 非unirow; 2-三角阵, unirow）
+
+                // 恢复mask
+                ParamPingPong[pingPongFlag].applyTriMask = TRI_MATRIX_NONE;
+
+                ParamPingPong[pingPongFlag].sectionStartLineOffset += BASE_BLOCK_DATA_NUM * MAX_BLOCK_PER_ONE_PROC;
+
+                pingPongFlag = 1 - pingPongFlag;
+            }
+
+            // 计算两个max值 (ppBufForCaclFinalRowmaxFp32)
+            cmax_v<ArchType::ASCEND_V220, float, ReduceOrder::ORDER_ONLY_VALUE>(
+                curTensorForRecordRowmaxFp32[rowmaxOffset], UbAttn.tensorForCaclFinalRowmaxFp32, 2, 1, 1, 8);
+            PipeBarrier<PIPE_V>();
+
+            brcb_v<ArchType::ASCEND_V220, uint32_t>(
+                UbAttn.tensorForVbrcbRowmaxFp32.template ReinterpretCast<uint32_t>(),
+                curTensorForRecordRowmaxFp32[rowmaxOffset].template ReinterpretCast<uint32_t>(), 1, 8, 1);
+            PipeBarrier<PIPE_V>();
+
+            rowmaxOffset += 8;
+
+            SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+            SetFlag<HardEvent::V_MTE3>(EVENT_ID1);
+
+            WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+            WaitFlag<HardEvent::V_MTE3>(EVENT_ID1);
+
+            // EXP
+            pingPongFlag = 0;
+
+            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+
+            for (int oneLineProc = pingPongTimes - 1; oneLineProc > -1; oneLineProc--) {
+                ParamPingPong[pingPongFlag].curFrag = oneLineProc / 2;
+
+                ParamPingPong[0].sectionBlockNum = MAX_BLOCK_PER_ONE_PROC;
+                ParamPingPong[1].sectionBlockNum = MAX_BLOCK_PER_ONE_PROC;
+                ParamPingPong[0].sectionPaddingBlockNum = 0;
+                ParamPingPong[1].sectionPaddingBlockNum = 0;
+
+                // 最后一段
+                if (oneLineProc >= pingPongTimes - 2) {
+                    ParamPingPong[pingPongFlag].sectionBlockNum = tailBlockNum;
+                    ParamPingPong[pingPongFlag].sectionPaddingBlockNum = sectionPaddingBlockNum;
+                }
+
+                ParamPingPong[pingPongFlag].sectionStartLineOffset -= BASE_BLOCK_DATA_NUM * MAX_BLOCK_PER_ONE_PROC;
+
+                int64_t tmp = lines * BASE_BLOCK_SIDE_LEN * 2 + pingPongFlag * BASE_BLOCK_SIDE_LEN;
+                ProcessLinePhaseOneForExp(ParamPingPong[pingPongFlag], pingPongFlag, attnScoreGm, attnMaskGm, UbAttn,
+                                          tmp, sparseFlag);
+
+                pingPongFlag = 1 - pingPongFlag;
+            }
+
+            LocalTensor<float> curTensorForRecordRowsumFp32 =
+                sectionLoop == 0 ? UbAttn.tensorForRecordRowsumFp32 : UbAttn.tensorForRecordRowsumFp32[256];
+            AscendC::RepeatReduceSum<float, false>(curTensorForRecordRowsumFp32[rowsumOffset],
+                                                   UbAttn.tensorForCaclFinalRowsumFp32, 2, 0, 0, 1, 1, 8);
+            PipeBarrier<PIPE_V>();
+
+            rowsumOffset += 8;
+            *curLocalSectionStartLineOffset += BASE_BLOCK_SIZE_DOUBLE;
+
+            ParamPingPong[0].sectionMaskOffset += maskSeqLength * PING_PONG_NUM;
+            ParamPingPong[1].sectionMaskOffset += maskSeqLength * PING_PONG_NUM;
+        }
+
+        // 切换section two
+        curLocalSectionStartLineOffset = &localSectionTwoStartLineOffset;
+        curSectionBlockNums = &sectionTwoBlockNums;
+    }
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
+}
+
+#endif
+
+#endif // __VECTORFORWARD_H__
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/address_const_192.h b/src/kernels/mixkernels/laser_attention/op_kernel/address_const_192.h
new file mode 100644
index 00000000..4a9c15ee
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/address_const_192.h
@@ -0,0 +1,99 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef __ADDRESS_CONST_192_H__
+#define __ADDRESS_CONST_192_H__
+
+namespace Address_192 {
+    // 定义常量
+    const int SIZE_128 = 128;
+    const int SIZE_192 = 192;
+    const int SIZE_256 = 256;
+    const int DIVISOR_2 = 2;
+
+    // 基本块的length
+    const int BASE_BLOCK_LENGTH = SIZE_128;
+
+    // 读取query,key,value的block size
+    const int QUERY_BLOCK_SIZE = SIZE_192 * BASE_BLOCK_LENGTH;
+    const int KEY_BLOCK_SIZE = SIZE_256 * BASE_BLOCK_LENGTH;
+    const int VALUE_BLOCK_SIZE = SIZE_128 * BASE_BLOCK_LENGTH;
+    const int ROWSUM_BLOCK_SIZE = SIZE_128;
+    const int ATTENTION_SCORE_BLOCK_SIZE = BASE_BLOCK_LENGTH * BASE_BLOCK_LENGTH;
+    const int OUTPUT_BLOCK_SIZE = SIZE_128 * BASE_BLOCK_LENGTH;
+
+    // vector的全局信息
+    struct GLOBAL_INFO {
+        int64_t cubeNum = 0;              // cube数量
+        int64_t blockNumPerCube = 0;      // 每个cube处理block的数量
+        int64_t headNum = 0;              // head数量
+        int64_t batchNum = 0;             // batch数量
+        int64_t seqLenQ = 0;              // query的序列长度
+        int64_t seqLenK = 0;              // key、value的序列长度
+        int64_t headDim = 0;              // headDim = 192;
+        bool triMatix = false;            // 是否是三角阵
+        int64_t blockRows;                // 行数
+        int64_t blockCols;                // 列数
+        int64_t blockNumPerRow = 0;    // 一行有几个block（三角阵和非三角阵不同）
+        int64_t blockNumPerCol = 0;    // 一列有几个block（也就是一个head有几个行）
+        int64_t blockNumPerLoop = 0;   // 一个loop处理block的数量
+        int64_t blockNumPerHead = 0;   // 一个head包含的block数量
+        int64_t blockNumPerBatch = 0;  // 一个batch包含的block数量
+        int64_t loopTimes = 0;           // 大循环次数
+        int64_t tailBlockNum = 0;       // 最后一次循环处理的block数量
+        int64_t isSparse = 0;            // 是否为sparse 魔道e
+        int64_t windowLength = 0;        // 滑动窗口的length
+        int64_t windowsBlockNum = 0;    // 滑窗大小, sparse使用
+    };
+
+    // vector的当前信息
+    struct LOCAL_INFO {
+        int64_t cubeIndex = 0;                // 所属的cube分组（第几个cube很重要）
+        int64_t vectorIdx = 0;                // 当前vector在cube内的编号（0或1）
+        int64_t startLineInBaseBlock = 0;  // 处理基本块的起始行（都是2个vector处理一个基本块）
+        bool procTail = false;                // 是否参与尾块处理
+        int64_t procTailBlockNum = 0;       // 处理尾块中的block数量
+    };
+
+    // vector的分段信息
+    const int MAX_SWITCH_TIME = 16;
+    struct SECTION_INFO {
+        int64_t sectionNum = 0;                               // 当前处理的block条，包含几个块
+        int64_t sectionStartBlock[MAX_SWITCH_TIME] = {0};    // 起始block编号
+        int64_t headSkipBlock[MAX_SWITCH_TIME] = {0};          // 当前section的头尾是否要跳过计算
+        int64_t tailSkipBlock[MAX_SWITCH_TIME] = {0};          // 当前section的头尾是否要跳过计算
+        int64_t globalLinesInHeads[MAX_SWITCH_TIME] = {0};  // 在所有heads中的起始行
+        int64_t len[MAX_SWITCH_TIME] = {0};                    // 每一段的长度
+        bool headApplyMask[MAX_SWITCH_TIME] = {false};
+        bool tailApplyMask[MAX_SWITCH_TIME] = {false};
+        int64_t dOutputAndOutputOffset[MAX_SWITCH_TIME] = {0};  // 矩阵O dO的偏移量
+        int64_t dSoftmaxScoreAndScoreOffset[MAX_SWITCH_TIME] = {0};  // 矩阵S dP的偏移量
+        int64_t processLines = {0};                 // 处理的行数
+    };
+
+    // 前向cube1寻址的结构体
+    template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    struct PHY_ADDR_FORWARD_CUBE1 {
+        __gm__ T_LEFT *left;                      // 左矩阵的起始地址，基本块为 128 * 192
+        __gm__ T_RIGHT *right;                    // 右矩阵的起始地址， 基本块为 128 * 192
+        __gm__ T_OUTPUT *out;                     // 输出的起始地址， 基本块为 128 * 128
+        int32_t k = 0;                            // 连续计算的基本块数量
+    };
+
+    // 前向cube2寻址的结构体
+    template<typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    struct PHY_ADDR_FORWARD_ROWSUM_CUBE2 {
+        __gm__ T_LEFT *left;                      // 左矩阵的起始地址，基本块为 128 * 128
+        __gm__ T_RIGHT *right;                    // 右矩阵的起始地址， 基本块为 128 * 192
+        __gm__ T_OUTPUT *out;                     // cube2输出的起始地址， 基本块为 128 * 192
+        __gm__ T_OUTPUT *rowsumOut;                  // row_sum输出的起始地址， 基本块为 128 * 1
+        int32_t k = 0;                            // 连续计算的基本块数量
+    };
+}
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/laser_attention.cpp b/src/kernels/mixkernels/laser_attention/op_kernel/laser_attention.cpp
new file mode 100644
index 00000000..5e653632
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/laser_attention.cpp
@@ -0,0 +1,164 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#include "mixkernels/laser_attention/tiling/tiling_data.h"
+#include "mixkernels/utils/common/kernel/kernel_utils.h"
+#include "kernel_operator.h"
+#include "lib/matmul_intf.h"
+#include "CubeForward.h"
+#include "CubeForward_192.h"
+#include "VectorForward.h"
+#include "TransposeCustom.h"
+#include "TransposeWithDtype.h"
+
+using namespace AscendC;
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, AtbOps::LaserAttentionTilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->batchSize = (*(const __gm__ int32_t *)(p_tilingdata + 0));
+    tilingdata->headNum = (*(const __gm__ int32_t *)(p_tilingdata + 4));
+    tilingdata->seqSize = (*(const __gm__ int32_t *)(p_tilingdata + 8));
+    tilingdata->headDim = (*(const __gm__ int32_t *)(p_tilingdata + 12));
+    tilingdata->coreNumPerGroup = (*(const __gm__ int32_t *)(p_tilingdata + 16));
+    tilingdata->coreGroupNum = (*(const __gm__ int32_t *)(p_tilingdata + 20));
+    tilingdata->qSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 24));
+    tilingdata->kSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 28));
+    tilingdata->vSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 32));
+    tilingdata->maskSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 36));
+    tilingdata->scale = (*(const __gm__ float *)(p_tilingdata + 40));
+    tilingdata->keepProb = (*(const __gm__ float *)(p_tilingdata + 44));
+    tilingdata->preTokens = (*(const __gm__ int32_t *)(p_tilingdata + 48));
+    tilingdata->nextTokens = (*(const __gm__ int32_t *)(p_tilingdata + 52));
+    tilingdata->attenType = (*(const __gm__ int32_t *)(p_tilingdata + 56));
+    tilingdata->sparseMode = (*(const __gm__ int32_t *)(p_tilingdata + 60));
+    tilingdata->headGroupSize = (*(const __gm__ int32_t *)(p_tilingdata + 64));
+    tilingdata->windowLen = (*(const __gm__ int32_t *)(p_tilingdata + 68));
+    tilingdata->isTriangle = (*(const __gm__ int32_t *)(p_tilingdata + 72));
+    tilingdata->isHighPrecision = (*(const __gm__ int32_t *)(p_tilingdata + 76));
+    tilingdata->inputLayout = (*(const __gm__ int32_t *)(p_tilingdata + 80));
+#else
+    __ubuf__ uint8_t *tilingdata_in_ub = (__ubuf__ uint8_t *)get_imm(0);
+    int32_t tilingBlockNum = sizeof(AtbOps::LaserAttentionTilingData) / 32 + 1;
+    copy_gm_to_ubuf(((__ubuf__ uint8_t *)tilingdata_in_ub), p_tilingdata, 0, 1, tilingBlockNum, 0, 0);
+    pipe_barrier(PIPE_ALL);
+    tilingdata->batchSize = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 0));
+    pipe_barrier(PIPE_ALL);
+#endif
+}
+
+extern "C" __global__ __aicore__ void
+laser_attention(__gm__ uint8_t *__restrict__ ffts_addr, __gm__ uint8_t *__restrict__ q_gm,
+                __gm__ uint8_t *__restrict__ k_gm, __gm__ uint8_t *__restrict__ v_gm,
+                __gm__ uint8_t *__restrict__ alibi_mask_gm, __gm__ uint8_t *__restrict__ drop_mask_gm,
+                __gm__ uint8_t *__restrict__ atten_mask_gm, __gm__ uint8_t *__restrict__ softmax_max_gm,
+                __gm__ uint8_t *__restrict__ softmax_sum_gm, __gm__ uint8_t *__restrict__ attention_out_gm,
+                __gm__ uint8_t *__restrict__ workspace, __gm__ uint8_t *__restrict__ tiling_para_gm)
+{
+    AtbOps::LaserAttentionTilingData tilingData;
+    InitTilingData(tiling_para_gm, &(tilingData));
+    const AtbOps::LaserAttentionTilingData *__restrict tiling_data = &tilingData;
+    SetSysWorkspaceForce(workspace);
+    __gm__ uint8_t *user = GetUserWorkspace(workspace);
+
+    set_ffts_base_addr((uint64_t)ffts_addr);
+
+    int32_t batch = tiling_data->batchSize;
+    int32_t headNum = tiling_data->headNum;
+    int32_t coreNumPerGroup = tiling_data->coreNumPerGroup;
+    int32_t coreGroupNum = tiling_data->coreGroupNum;
+    int32_t baseS1 = tiling_data->qSeqLength;
+    int32_t baseS2 = tiling_data->kSeqLength;
+    int32_t headDim = tiling_data->headDim;
+    int32_t headGroupSize = tiling_data->headGroupSize;
+    int32_t isTriangle = tiling_data->isTriangle;
+    int32_t sparseMode = tiling_data->sparseMode;
+    int32_t windowLen = tiling_data->windowLen;
+    int32_t maskSeqLength = tiling_data->maskSeqLength;
+    int32_t isHighPrecision = 1;
+    float scale = tiling_data->scale;
+    int32_t inputLayout = tiling_data->inputLayout;
+
+    __gm__ float *__restrict__ c_cube2 = (__gm__ float *__restrict__)attention_out_gm;
+    __gm__ float *__restrict__ log_sum_gm = (__gm__ float *__restrict__)softmax_max_gm;
+    __gm__ uint8_t *__restrict__ const_ones_gm = (__gm__ uint8_t *__restrict__)user;
+    __gm__ float *__restrict__ const_zero_gm = (__gm__ float *__restrict__)(const_ones_gm + 128 * 128 * 2);
+    __gm__ float *__restrict__ gm_rowsum = (__gm__ float *__restrict__)(softmax_sum_gm);
+    __gm__ float *__restrict__ c_cube2_workspace =
+        (__gm__ float *__restrict__)(const_zero_gm + batch * headNum * baseS1);
+    __gm__ uint8_t *__restrict__ score_gm =
+        (__gm__ uint8_t *__restrict__)(c_cube2_workspace + batch * headNum * baseS1 * 128);
+
+    uint32_t col_size = sparseMode ? windowLen : baseS2;
+    uint32_t score_size = coreGroupNum * (col_size + 128) * 128 * 2 * 4;
+    uint32_t N2 = (headNum + headGroupSize - 1) / headGroupSize;
+    uint32_t q_size = batch * headNum * baseS1 * headDim * 2;
+    uint32_t kv_size = batch * N2 * baseS2 * 256 * 2;
+
+    __gm__ uint8_t *__restrict__ gmq_out = (__gm__ uint8_t *__restrict__)(score_gm + score_size);
+    __gm__ uint8_t *__restrict__ gmk_out = (__gm__ uint8_t *__restrict__)(gmq_out + q_size);
+    __gm__ uint8_t *__restrict__ gmv_out = (__gm__ uint8_t *__restrict__)(gmk_out + kv_size);
+
+#ifdef __DAV_C220_CUBE__
+    if (headDim == 128) {
+        CubeForward<__bf16, true, float> op;
+        op.Init(q_gm, k_gm, v_gm, score_gm, c_cube2_workspace, const_ones_gm, gm_rowsum, coreNumPerGroup, coreGroupNum,
+                batch, headNum, baseS1, baseS2, headDim, headGroupSize, isTriangle, sparseMode, windowLen);
+        op.SetHighPrecision(isHighPrecision);
+        op.Run();
+    } else {
+        CubeForward_192<__bf16, true, float> op;
+        if (inputLayout != 3) {
+            wait_flag_dev(AIV2AICFLAGID);
+            op.Init(gmq_out, gmk_out, gmv_out, score_gm, c_cube2_workspace, const_ones_gm, gm_rowsum, coreNumPerGroup,
+                    coreGroupNum, batch, headNum, baseS1, baseS2, headDim, headGroupSize, isTriangle, sparseMode,
+                    windowLen);
+        } else {
+            op.Init(q_gm, k_gm, v_gm, score_gm, c_cube2_workspace, const_ones_gm, gm_rowsum, coreNumPerGroup,
+                    coreGroupNum, batch, headNum, baseS1, baseS2, headDim, headGroupSize, isTriangle, sparseMode,
+                    windowLen);
+        }
+        op.SetHighPrecision(isHighPrecision);
+        op.Run();
+    }
+#elif __DAV_C220_VEC__
+    VectorForward<__bf16, true, float> op;
+    if (inputLayout != 3) {
+        TransposeCustom<__bf16, true> op_in;
+        op_in.Init(q_gm, gmq_out, batch, headNum, baseS1, 192, inputLayout);
+        op_in.Run();
+        op_in.Init(k_gm, gmk_out, batch, N2, baseS2, 256, inputLayout);
+        op_in.Run();
+        op_in.Init(v_gm, gmv_out, batch, N2, baseS2, 128, inputLayout);
+        op_in.Run();
+
+        uint64_t mode = 0;
+        uint64_t config = 1 | (mode << 4) | (AIVFLAGID << 8);
+        ffts_cross_core_sync(PIPE_MTE3, config);
+        wait_flag_dev(AIVFLAGID);
+        mode = 2;
+        config = 1 | (mode << 4) | (AIV2AICFLAGID << 8);
+        ffts_cross_core_sync(PIPE_MTE3, config);
+    }
+    op.Init(q_gm, k_gm, v_gm, atten_mask_gm, const_ones_gm, const_zero_gm, score_gm, c_cube2_workspace,
+            attention_out_gm, log_sum_gm, gm_rowsum, baseS1, baseS2, headNum, batch, coreNumPerGroup, isTriangle,
+            windowLen / BASE_BLOCK_SIDE_LEN, maskSeqLength, scale, inputLayout);
+    op.SetHighPrecision(isHighPrecision);
+    op.Run();
+#endif
+
+    if (inputLayout != 3) {
+        __gm__ __bf16 *__restrict__ gm_O_out = (__gm__ __bf16 *__restrict__)attention_out_gm;
+#ifdef __DAV_C220_VEC__
+        TransposeWithDtype<__bf16, true> op_transpose;
+        op_transpose.Init(c_cube2_workspace, gm_rowsum, gm_O_out, batch, headNum, baseS1, 128, inputLayout, 1);
+        op_transpose.Run();
+#endif
+    }
+}
diff --git a/src/kernels/mixkernels/laser_attention/op_kernel/ppmatmul_const.h b/src/kernels/mixkernels/laser_attention/op_kernel/ppmatmul_const.h
new file mode 100644
index 00000000..778374af
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention/op_kernel/ppmatmul_const.h
@@ -0,0 +1,75 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef __PPMATMUL_CONST_BF16_H__
+#define __PPMATMUL_CONST_BF16_H__
+
+constexpr int AICFLAGID = 0;
+constexpr int AIVFLAGID = 1;
+constexpr int AIC2AIVFLAGID = 2;
+constexpr int AIV2AICFLAGID = 3;
+
+using T_OUTPUT = float;
+
+constexpr int32_t PING_PONG_NUM = 2;
+constexpr int32_t L0AB_PINGPONG_BUFFER_LEN = 16384; // 32 KB
+constexpr int32_t BLOCK_SIZE = 16;
+constexpr int32_t CUBE_MATRIX_SIZE = 256;          // 16 * 16
+constexpr int64_t L1_PINGPONG_BUFFER_LEN = 16384;  // 32 KB
+constexpr int64_t L0C_PINGPONG_BUFFER_LEN = 16384; // 64 KB
+
+constexpr int32_t BASE_BLOCK_SIZE = 16384;     // BASE_BLOCK shape ：[128 * 128]
+constexpr int32_t BASE_BLOCK_SIZE_192 = 24576; // BASE_BLOCK shape ：[128 * 192]
+constexpr int32_t BASE_BLOCK_SIZE_256 = 32768; // BASE_BLOCK shape ：[128 * 256]
+constexpr int32_t BASE_BLOCK_SIDE_LEN = 128;   // BASE_BLOCK  row adn column  size
+
+constexpr int32_t B16_SIZE = 2;
+constexpr int32_t B32_SIZE = 4;
+constexpr int32_t CUBE2_LENGTH_M = 128;
+constexpr int32_t CUBE2_LENGTH_K = 128;
+constexpr int32_t CUBE2_LENGTH_N = 128;
+constexpr int32_t MAX_SWITCH_TIME = 16; // 一个core最多处理16个基本块，因此最多只能有16段
+
+constexpr int32_t VEC_NUM_PER_CUBE = 2;
+constexpr int32_t BASE_BLOCK_LENGTH = 128;
+// 基本块是方阵，长和宽
+constexpr int BASE_BLOCK_SIZE_DOUBLE = BASE_BLOCK_SIDE_LEN * 2;
+constexpr int HEAD_DIM = 128;                                                  // head的维度
+constexpr int BASE_BLOCK_DATA_NUM = BASE_BLOCK_SIDE_LEN * BASE_BLOCK_SIDE_LEN; // 基本块含有数据量
+constexpr int MAX_LENG_PER_UB_PROC = 8192; // UB一次处理的最大长度 (单个ping)
+constexpr int MAX_BLOCK_PER_ONE_PROC = MAX_LENG_PER_UB_PROC / BASE_BLOCK_SIDE_LEN;
+constexpr int BLOCK_NUM_FOR_VMAX = 16; // 折半计算的基准block数量
+constexpr int SHORT_SEQ_THRESHOLD = 8192;
+constexpr int MDDIUM_SEQ_THRESHOLD = 32768;
+constexpr int BASIC_GAP = BASE_BLOCK_DATA_NUM - BASE_BLOCK_SIDE_LEN;
+
+constexpr float PADDING_FOR_MAX = -1e30; // 非2的幂长度，折半计算vmax时，需要padding
+
+constexpr int TRI_MATRIX_NONE = 0;
+constexpr int TRI_MATRIX_TAIL = 1;
+constexpr int TRI_MATRIX_HEAD = 2;
+constexpr int TRI_MATRIX_HEAD_AND_TAIL = 3;
+
+
+template <typename TYPE, typename WORKSPACE_TYPE> struct PhyAddrCube1 {
+    __gm__ TYPE *left;
+    __gm__ TYPE *right;
+    __gm__ WORKSPACE_TYPE *out;
+    int32_t k = 0;
+};
+
+template <typename TYPE, typename WORKSPACE_TYPE> struct PhyAddrCube2Rowsum {
+    __gm__ WORKSPACE_TYPE *left;
+    __gm__ TYPE *right;
+    __gm__ float *out;
+    __gm__ float *rowsumOut;
+    int32_t k = 0;
+};
+
+#endif
diff --git a/src/kernels/mixkernels/laser_attention_grad/CMakeLists.txt b/src/kernels/mixkernels/laser_attention_grad/CMakeLists.txt
index 54a997ea..73ab9fbf 100644
--- a/src/kernels/mixkernels/laser_attention_grad/CMakeLists.txt
+++ b/src/kernels/mixkernels/laser_attention_grad/CMakeLists.txt
@@ -14,3 +14,7 @@ set(laser_attention_grad_srcs
 )
 
 add_operation(LaserAttentionGradOperation "${laser_attention_grad_srcs}")
+
+add_kernel(laser_attention_grad ascend910b mix
+        op_kernel/laser_attention_grad.cpp
+        LaserAttentionGradKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeCustom.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeCustom.h
new file mode 100644
index 00000000..3df838f7
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeCustom.h
@@ -0,0 +1,257 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */                                                                                                                    \
+#ifndef __TRANSPOSE_CUSTOM_H__
+#define __TRANSPOSE_CUSTOM_H__
+
+#define USE_ASCENDC 1
+
+#include "ppmatmul_const_grad.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_VEC__
+
+template <typename TYPE, bool IF_BF16> class TransposeCustom {
+public:
+    __aicore__ inline TransposeCustom() {}
+    __aicore__ inline void Init(__gm__ uint8_t *__restrict__ gm_in_tensor,  // input
+                                __gm__ uint8_t *__restrict__ gm_out_tensor, // output
+                                int32_t batch_size, int32_t head_num, int32_t seq_size, int32_t head_dim,
+                                int32_t transpose_type);
+    __aicore__ inline void Run();
+    __aicore__ inline void transposeSBHToBNSD1(int32_t vector_num, int32_t cur_vector_idx, int S, int B, int N, int D);
+    __aicore__ inline void transposeSBHToBNSD2(int32_t vector_num, int32_t cur_vector_idx, int S, int B, int N, int D);
+
+private:
+    GlobalTensor<TYPE> gm_in_tensor;
+    GlobalTensor<TYPE> gm_out_tensor;
+    LocalTensor<TYPE> ub_tensor;
+    int32_t batch_size;     // B
+    int32_t head_num;       // N
+    int32_t seq_size;       // S
+    int32_t head_dim;       // D
+    int32_t transpose_type; // shape类型转换  0：SBH->BNSD
+    AsdopsBuffer<ArchType::ASCEND_V220> calBuf;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeCustom<TYPE, IF_BF16>::Init(__gm__ uint8_t *__restrict__ gm_in,  // input
+                                                            __gm__ uint8_t *__restrict__ gm_out, // output
+                                                            int32_t batch_size, int32_t head_num, int32_t seq_size,
+                                                            int32_t head_dim, int32_t transpose_type)
+{
+    gm_in_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ __bf16 *>(gm_in));
+    gm_out_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ __bf16 *>(gm_out));
+    ub_tensor = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(0);
+    this->batch_size = batch_size;         // B
+    this->head_num = head_num;             // N
+    this->seq_size = seq_size;             // S
+    this->head_dim = head_dim;             // D
+    this->transpose_type = transpose_type; // shape类型转换  0：SBH->BNSD  1:BSH->BNSD  2:BNSD->SBH 3:BNSD->BSH
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ inline void TransposeCustom<TYPE, IF_BF16>::Run()
+{
+    int32_t vector_num = get_subblockdim() * get_block_num();                        // vector的数量
+    int32_t cur_vector_idx = get_block_idx() * get_subblockdim() + get_subblockid(); // 当前编号
+    int S = seq_size;
+    int B = batch_size;
+    int N = head_num;
+    int D = head_dim;
+    int ub_max = 96 * 1024 / sizeof(TYPE); // 48 * 1024  一半拷贝进来，一半排序
+    int segmentation = ub_max / D;
+    // SBH  -> BNSD
+    if (transpose_type == 0) {
+        if (B * N > segmentation) {
+            transposeSBHToBNSD2(vector_num, cur_vector_idx, S, B, N, D);
+        } else {
+            transposeSBHToBNSD1(vector_num, cur_vector_idx, S, B, N, D);
+        }
+    }
+}
+
+// 典型场景测试  S<=8192且B*N <=192
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeCustom<TYPE, IF_BF16>::transposeSBHToBNSD1(int32_t vector_num, int32_t cur_vector_idx,
+                                                                           int S, int B, int N, int D)
+{
+    int H = N * D;
+    // UB 192K
+    // UB 192K   5_16_8192_128
+    int ub_size = 96 * 1024 / sizeof(TYPE); // 48 * 1024  一半拷贝进来，一半排序
+    int ub_max = 96 * 1024 / sizeof(TYPE);  // 48 * 1024  一半拷贝进来，一半排序
+    int length = ub_max / (B * H);          // 3 每次处理3行
+    if (length >= 1) {
+        ub_max = length * (B * H); // ub_max设置为B*H的整数倍
+    }
+    int seq_pre_core_length = S / (vector_num * length);
+    int seq_pre_core = seq_pre_core_length * length;                             // 168
+    int tail = S - seq_pre_core * vector_num;                                    // 8192 - 168*48 = 128
+    int tail_core = tail / length;                                               // 128/3=42余数为2
+    int inOffset = (seq_pre_core * cur_vector_idx + tail_core * length) * B * H; //
+    int outOffset = (seq_pre_core * cur_vector_idx + tail_core * length) * D;
+    if (cur_vector_idx < tail_core) {
+        seq_pre_core += length; // 168+3=171
+        inOffset = seq_pre_core * cur_vector_idx * B * H;
+        outOffset = seq_pre_core * cur_vector_idx * D;
+    }
+
+    int times = seq_pre_core / length; // 56或者57
+
+    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    for (int i = 0; i < times; i++) {
+        
+        WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+
+        AscendC::DataCopy(ub_tensor, gm_in_tensor[inOffset + ub_max * i],
+                          AscendC::DataCopyParams(1,
+                                                  ub_max / 16, // 一个block=32B N*2/32
+                                                  0, 0));
+        SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        
+        WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+        for (int j = 0; j < length; j++) {
+            AscendC::DataCopy(ub_tensor[ub_size + D * j], ub_tensor[B * H * j],
+                              AscendC::DataCopyParams(B * H / D, D / 16, 0, (length - 1) * D / 16));
+        }
+
+        SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+        SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+        WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+        AscendC::DataCopyPad(gm_out_tensor[outOffset + length * i * D], ub_tensor[ub_size],
+                             AscendC::DataCopyExtParams(B * H / D, D * length * 2, 0, (S - length) * D * 2, 0));
+        SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    }
+    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    // 处理不能整除的数据
+    int remain = tail - tail_core * length; // 128-42 * 3 =2
+    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    if (remain > 0 && cur_vector_idx == (vector_num - 1)) {
+        inOffset = (seq_pre_core * cur_vector_idx + tail_core * length + seq_pre_core) * B * H; // seq_pre_core=168
+        outOffset = (seq_pre_core * cur_vector_idx + tail_core * length + seq_pre_core) * D;
+
+        WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+
+        AscendC::DataCopy(ub_tensor, gm_in_tensor[inOffset], AscendC::DataCopyParams(1, remain * (B * H) / 16, 0, 0));
+
+        SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+        for (int j = 0; j < remain; j++) {
+            AscendC::DataCopy(ub_tensor[ub_size + D * j], ub_tensor[B * H * j],
+                              AscendC::DataCopyParams(B * H / D, D / 16, 0, (remain - 1) * D / 16));
+        }
+
+        SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+        SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+        WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+        AscendC::DataCopyPad(gm_out_tensor[outOffset], ub_tensor[ub_size],
+                             AscendC::DataCopyExtParams(B * H / D, D * remain * 2, 0, (S - remain) * D * 2, 0));
+        SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    }
+    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+}
+
+// 分段拷入，连续拷出  SBH  -> BNSD  应用场景  S>8192或B*N > 192
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeCustom<TYPE, IF_BF16>::transposeSBHToBNSD2(int32_t vector_num, int32_t cur_vector_idx,
+                                                                           int S, int B, int N, int D)
+{
+    int H = N * D;
+    // UB 192K  10_256_32_128
+    int ub_max = 96 * 1024 / sizeof(TYPE);    // 48 * 1024  一半拷贝进来，一半排序
+    int seq_pre_core = S / vector_num;        // 每个核平均处理个数5
+    int tail = S - seq_pre_core * vector_num; // 16
+    int inOffset = (seq_pre_core * cur_vector_idx + tail) * B * H;
+    int outOffset = (seq_pre_core * cur_vector_idx + tail) * D;
+    if (cur_vector_idx < tail) {
+        seq_pre_core += 1;
+        inOffset = seq_pre_core * cur_vector_idx * B * H;
+        outOffset = seq_pre_core * cur_vector_idx * D;
+    }
+    for (int batchSize = 0; batchSize < B; batchSize++) {
+        for (int headSize = 0; headSize < N; headSize++) {
+            int times = seq_pre_core * D / ub_max;
+            if (times > 0) {
+
+                SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+                for (int i = 0; i < times; i++) {
+                    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                    AscendC::DataCopy(
+                        ub_tensor, gm_in_tensor[inOffset + ub_max / D * i * B * H + headSize * D + batchSize * N * D],
+                        AscendC::DataCopyParams(ub_max / D, D / 16, (B * N - 1) * D / 16, 0));
+
+                    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+                    AscendC::DataCopy(ub_tensor[ub_max], ub_tensor, AscendC::DataCopyParams(1, ub_max / 16, 0, 0));
+
+                    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+                    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+                    AscendC::DataCopy(gm_out_tensor[outOffset + ub_max * i + headSize * S * D + batchSize * N * S * D],
+                                      ub_tensor[ub_max], AscendC::DataCopyParams(1, ub_max / 16, 0, 0));
+                    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                }
+                WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+            }
+            // 处理尾块
+            int tail_block = seq_pre_core * D - ub_max * times; // 5 * 128 = 640
+            SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+            SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+            if (tail_block > 0) {
+                WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                AscendC::DataCopy(
+                    ub_tensor, gm_in_tensor[inOffset + ub_max / D * B * H * times + headSize * D + batchSize * N * D],
+                    AscendC::DataCopyParams(tail_block / D, D / 16, (B * N - 1) * D / 16, 0));
+
+                SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+
+                AscendC::DataCopy(ub_tensor[ub_max], ub_tensor, AscendC::DataCopyParams(1, tail_block / 16, 0, 0));
+
+                SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+                SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+                WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+                AscendC::DataCopy(gm_out_tensor[outOffset + ub_max * times + headSize * S * D + batchSize * N * S * D],
+                                  ub_tensor[ub_max], AscendC::DataCopyParams(1, tail_block / 16, 0, 0));
+                SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+            }
+            WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+            WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+        }
+    }
+}
+
+#endif
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeGrad.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeGrad.h
new file mode 100644
index 00000000..703e8e29
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeGrad.h
@@ -0,0 +1,104 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef __TRANSPOSE_GRAD_H__
+#define __TRANSPOSE_GRAD_H__
+
+#include "ppmatmul_const_grad.h"
+#include "TransposeCustom.h"
+
+#ifdef __DAV_C220_VEC__
+
+template <typename TYPE, bool IF_BF16> class TransposeGrad {
+public:
+    __aicore__ TransposeGrad() {}
+    __aicore__ void Init(__gm__ TYPE *__restrict__ gm_Q, __gm__ TYPE *__restrict__ gm_K, __gm__ TYPE *__restrict__ gm_V,
+                         __gm__ TYPE *__restrict__ gm_dO, __gm__ TYPE *__restrict__ gm_O,
+                         __gm__ TYPE *__restrict__ gm_Q_trans, __gm__ TYPE *__restrict__ gm_K_trans,
+                         __gm__ TYPE *__restrict__ gm_V_trans, __gm__ TYPE *__restrict__ gm_dO_trans,
+                         __gm__ TYPE *__restrict__ gm_O_trans, int32_t batch_size, int32_t head_num, int32_t q_size,
+                         int32_t k_size, int32_t inputLayout, int32_t gqa_head);
+    __aicore__ void Run();
+
+private:
+    __gm__ TYPE *__restrict__ gm_Q{nullptr};
+    __gm__ TYPE *__restrict__ gm_K{nullptr};
+    __gm__ TYPE *__restrict__ gm_V{nullptr};
+    __gm__ TYPE *__restrict__ gm_dO{nullptr};
+    __gm__ TYPE *__restrict__ gm_O{nullptr};
+    __gm__ TYPE *__restrict__ gm_K_trans{nullptr};
+    __gm__ TYPE *__restrict__ gm_Q_trans{nullptr};
+    __gm__ TYPE *__restrict__ gm_V_trans{nullptr};
+    __gm__ TYPE *__restrict__ gm_dO_trans{nullptr};
+    __gm__ TYPE *__restrict__ gm_O_trans{nullptr};
+    int32_t batch_size;  // B
+    int32_t head_num;    // N
+    int32_t q_size;      // S
+    int32_t k_size;      // D
+    int32_t inputLayout; // shape类型转换  0：SBH->BNSD
+    int32_t gqa_head;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ void TransposeGrad<TYPE, IF_BF16>::Init(
+    __gm__ TYPE *__restrict__ gm_Q, __gm__ TYPE *__restrict__ gm_K, __gm__ TYPE *__restrict__ gm_V,
+    __gm__ TYPE *__restrict__ gm_dO, __gm__ TYPE *__restrict__ gm_O, __gm__ TYPE *__restrict__ gm_Q_trans,
+    __gm__ TYPE *__restrict__ gm_K_trans, __gm__ TYPE *__restrict__ gm_V_trans, __gm__ TYPE *__restrict__ gm_dO_trans,
+    __gm__ TYPE *__restrict__ gm_O_trans, int32_t batch_size, int32_t head_num, int32_t q_size, int32_t k_size,
+    int32_t inputLayout, int32_t gqa_head)
+{
+    this->gm_Q = gm_Q;
+    this->gm_K = gm_K;
+    this->gm_V = gm_V;
+    this->gm_dO = gm_dO;
+    this->gm_O = gm_O;
+    this->gm_Q_trans = gm_Q_trans;
+    this->gm_K_trans = gm_K_trans;
+    this->gm_V_trans = gm_V_trans;
+    this->gm_dO_trans = gm_dO_trans;
+    this->gm_O_trans = gm_O_trans;
+    this->batch_size = batch_size;   // B
+    this->head_num = head_num;       // N
+    this->q_size = q_size;           // S
+    this->k_size = k_size;           // D
+    this->inputLayout = inputLayout; // shape类型转换  0：SBH->BNSD  1:BSH->BNSD  2:BNSD->SBH 3:BNSD->BSH
+    this->gqa_head = gqa_head;
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ void TransposeGrad<TYPE, IF_BF16>::Run()
+{
+    if (inputLayout != 3) {
+        TransposeCustom<TYPE, true> op_in;
+        op_in.Init((__gm__ uint8_t *)gm_Q, (__gm__ uint8_t *)gm_Q_trans, batch_size, head_num, q_size, 192,
+                   inputLayout);
+        op_in.Run();
+        op_in.Init((__gm__ uint8_t *)gm_K, (__gm__ uint8_t *)gm_K_trans, batch_size, gqa_head, k_size, 256,
+                   inputLayout);
+        op_in.Run();
+        op_in.Init((__gm__ uint8_t *)gm_V, (__gm__ uint8_t *)gm_V_trans, batch_size, gqa_head, k_size, 128,
+                   inputLayout);
+        op_in.Run();
+        op_in.Init((__gm__ uint8_t *)gm_dO, (__gm__ uint8_t *)gm_dO_trans, batch_size, head_num, q_size, 128,
+                   inputLayout);
+        op_in.Run();
+        op_in.Init((__gm__ uint8_t *)gm_O, (__gm__ uint8_t *)gm_O_trans, batch_size, head_num, q_size, 128,
+                   inputLayout);
+        op_in.Run();
+        uint64_t mode = 0;
+        uint64_t config = 1 | (mode << 4) | (AIVFLAGID << 8);
+        ffts_cross_core_sync(PIPE_MTE3, config);
+        wait_flag_dev(AIVFLAGID);
+        mode = 2;
+        config = 1 | (mode << 4) | (AIV2AICFLAGID << 8);
+        ffts_cross_core_sync(PIPE_MTE3, config);
+    }
+}
+
+#endif
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeWithDtype.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeWithDtype.h
new file mode 100644
index 00000000..2489ab7d
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/TransposeWithDtype.h
@@ -0,0 +1,442 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifndef __TRANSPOSE_OUT_H__
+#define __TRANSPOSE_OUT_H__
+
+#define USE_ASCENDC 1
+#include "ppmatmul_const_grad.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/hardware.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_VEC__
+
+template <typename TYPE, bool IF_BF16> class TransposeWithDtype {
+public:
+    __aicore__ inline TransposeWithDtype() {}
+    __aicore__ inline void Init(__gm__ float *__restrict__ gm_in,     // input
+                                __gm__ float *__restrict__ gm_rowsum, // rowsum
+                                __gm__ TYPE *__restrict__ gm_out,     // output
+                                int32_t batch_size, int32_t head_num, int32_t seq_size, int32_t head_dim,
+                                int32_t transpose_type, int32_t is_forward, bool needMuls, float scale);
+    __aicore__ inline void Run();
+
+private:
+    __aicore__ __inline__ void SBHForward();
+    __aicore__ __inline__ void SBHBackward();
+    __aicore__ __inline__ void ComputeBackward(int32_t in_offset, int32_t out_offset, int32_t lenth, int32_t row);
+    __aicore__ __inline__ void ComputeForward(int32_t in_offset, int32_t rowsum_offset, int32_t out_offset,
+                                              int32_t lenth, int32_t row);
+
+private:
+    AsdopsBuffer<ArchType::ASCEND_V220> calBuf;
+    LocalTensor<TYPE> cur_tensor;
+    LocalTensor<float> cur_tensor_f32;
+    LocalTensor<float> cur_tensor_out;
+    LocalTensor<TYPE> tensor_transpose;
+    LocalTensor<float> tensor_transpose_f32;
+    LocalTensor<TYPE> tensor_rowsum_brcb;
+    LocalTensor<float> tensor_rowsum_brcb_f32;
+    LocalTensor<uint32_t> tensor_mask;
+    GlobalTensor<float> gm_in_tensor;
+    GlobalTensor<float> gm_rowsum_tensor;
+    GlobalTensor<TYPE> gm_out_tensor;
+    int32_t B;              // B
+    int32_t N;              // N
+    int32_t S;              // S
+    int32_t D;              // D
+    int32_t H;              // H
+    int32_t transpose_type; // shape类型转换
+    int32_t is_forward;     // 前向还是反向
+    int32_t vector_num;     // vector的数量
+    int32_t cur_vector_idx; // 当前编号
+    bool needMuls = true;
+    float scale;
+    int UB_MAX = 192 * 1024 / sizeof(float) / 3;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void TransposeWithDtype<TYPE, IF_BF16>::Init(__gm__ float *__restrict__ gm_in,     // input
+                                                               __gm__ float *__restrict__ gm_rowsum, // rowsum
+                                                               __gm__ TYPE *__restrict__ gm_out,     // output
+                                                               int32_t batch_size, int32_t head_num, int32_t seq_size,
+                                                               int32_t head_dim, int32_t transpose_type,
+                                                               int32_t is_forward, bool needMuls, float scale)
+{
+    gm_in_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_in));
+    gm_rowsum_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_rowsum));
+    gm_out_tensor.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_out));
+
+    cur_tensor = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(0);
+    cur_tensor_f32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    cur_tensor_out = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(0);
+    tensor_transpose = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(192 * 1024 / 3 * 2);
+    tensor_transpose_f32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(192 * 1024 / 3 * 2);
+    tensor_rowsum_brcb = calBuf.GetBuffer<BufferType::ASCEND_UB, TYPE>(192 * 1024 / 6 * 5);
+    tensor_rowsum_brcb_f32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(192 * 1024 / 6 * 5);
+    tensor_mask = calBuf.GetBuffer<BufferType::ASCEND_UB, uint32_t>(192 * 1024 / sizeof(TYPE) / 6 * 5);
+
+
+    this->B = batch_size;
+    this->N = head_num;
+    this->S = seq_size;
+    this->D = head_dim;
+    this->H = head_num * head_dim;
+    this->transpose_type = transpose_type;
+    this->is_forward = is_forward;
+    this->vector_num = get_block_num() * 2;
+    this->cur_vector_idx = get_block_idx() * 2 + get_subblockid(); // 当前编号
+    this->needMuls = needMuls;
+    this->scale = scale;
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ inline void TransposeWithDtype<TYPE, IF_BF16>::Run()
+{
+    if (transpose_type == 0) {
+        if (is_forward == 1) {
+            SBHForward();
+        } else {
+            SBHBackward();
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::SBHBackward()
+{
+    int length, times, remain, inOffset, outOffset = 0;
+    int length_per_core = S / vector_num / 8 * 8;
+    int tail_length = S - length_per_core * vector_num;
+    int tail = tail_length / 8;
+
+    if (cur_vector_idx < tail) {
+        length_per_core += 8;
+        inOffset = length_per_core * cur_vector_idx * D;
+        outOffset = length_per_core * cur_vector_idx * B * N * D;
+    } else {
+        inOffset = (length_per_core * cur_vector_idx + tail_length) * D;
+        outOffset = (length_per_core * cur_vector_idx + tail_length) * B * N * D;
+    }
+
+    if (UB_MAX >= B * N * D * length_per_core) {
+        length = length_per_core;
+        times = 1;
+        remain = 0;
+    } else {
+        if (UB_MAX / (B * N * D) < 8) {
+            // B * N > 16 走这个分支
+            length = 8;
+        } else {
+            length = UB_MAX / (B * N * D) / 8 * 8;
+        }
+        times = length_per_core / length;
+        remain = length_per_core % length;
+    }
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (UB_MAX / (B * N * D) < 8) {
+        auto row = (UB_MAX / D / length); // UB_MAX = 16384
+        auto loopBN = (B * N) / row;
+        auto remainBN = (B * N) % row; // 尾行
+        for (int j = 0; j < loopBN; j++) {
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeBackward(inOffset + i * length * D + j * S * D * row,
+                                outOffset + i * length * B * N * D + j * row * D,
+                                // outOffset + i * length * D + j * S * D * row, // no transpose copy
+                                length, row);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+        if (remainBN > 0) {
+            // 存在尾行时 要单独处理
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeBackward(inOffset + i * length * D + loopBN * S * D * row,
+                                outOffset + i * length * B * N * D + loopBN * row * D,
+                                // outOffset + i * length * D + j * S * D * remainBN, // no transpose copy
+                                length, remainBN);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+    } else {
+        for (int i = 0; i < times; i++) {
+            WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            ComputeBackward(inOffset + i * length * D, outOffset + i * length * B * N * D, length, 0);
+            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+        }
+    }
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (remain > 0) {
+        ComputeBackward(inOffset + times * length * D, outOffset + times * length * B * N * D, remain, 0);
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::ComputeBackward(int32_t in_offset, int32_t out_offset,
+                                                                              int32_t length, int32_t row)
+{
+    int32_t nBurst = 0;
+    if (transpose_type == 0) {
+        // SBH
+        nBurst = (UB_MAX / (B * N * D) < 8) ? row : B * N;
+    }
+
+    AscendC::DataCopyPad(cur_tensor_f32, gm_in_tensor[in_offset],
+                         AscendC::DataCopyExtParams(nBurst, D * length * 4, (S - length) * D * 4, 0, 0),
+                         AscendC::DataCopyPadExtParams<float>(false, 0, 0, 0));
+
+    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+
+    if (needMuls) {
+        Muls(cur_tensor_f32, cur_tensor_f32, this->scale, nBurst * D * length);
+        PIPE_BARRIER(V);
+    }
+
+    int repeatTimes =
+        nBurst * length * D / (256 / sizeof(float)); // B * N * length * D <= UB_MAX (16384)  即 repeatTimes <=256
+    if constexpr (IF_BF16) {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                        halfTimes, 1, 1, 4, 8);
+        }
+    } else {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                       halfTimes, 1, 1, 4, 8);
+        }
+    }
+
+    pipe_barrier(PIPE_V);
+    if (nBurst >= length) {
+        for (int j = 0; j < length; j++) {
+            AscendC::DataCopy(tensor_transpose[j * nBurst * D], cur_tensor[j * D],
+                              AscendC::DataCopyParams(nBurst, D / 16, (length - 1) * D / 16, 0));
+        }
+    } else {
+        for (int j = 0; j < nBurst; j++) {
+            AscendC::DataCopy(tensor_transpose[j * D], cur_tensor[j * D * length],
+                              AscendC::DataCopyParams(length, D / 16, 0, (nBurst - 1) * D / 16));
+        }
+    }
+    pipe_barrier(PIPE_V);
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    if (transpose_type == 0) {
+        if (UB_MAX / (B * N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (B * N - row) * D * 2, 0));
+        } else {
+            int ub_size = B * N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    } else if (transpose_type == 1) {
+        if (UB_MAX / (N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (N - row) * D * 2, 0));
+        } else {
+            int ub_size = N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16> __aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::SBHForward()
+{
+    int length, times, remain, inOffset, rowsumOffset, outOffset = 0;
+    int length_per_core = S / vector_num / 8 * 8;       // 16
+    int tail_length = S - length_per_core * vector_num; // 256 288
+    int tail = tail_length / 8;                         // 32
+
+    if (cur_vector_idx < tail) {
+        length_per_core += 8;
+        inOffset = length_per_core * cur_vector_idx * D;
+        rowsumOffset = length_per_core * cur_vector_idx;
+        outOffset = length_per_core * cur_vector_idx * B * N * D;
+    } else {
+        inOffset = (length_per_core * cur_vector_idx + tail_length) * D;
+        rowsumOffset = length_per_core * cur_vector_idx + tail_length;
+        outOffset = (length_per_core * cur_vector_idx + tail_length) * B * N * D;
+    }
+
+    if (UB_MAX >= B * N * D * length_per_core) {
+        length = length_per_core;
+        times = 1;
+        remain = 0;
+    } else {
+        if (UB_MAX / (B * N * D) < 8) {
+            length = 8;
+        } else {
+            length = UB_MAX / (B * N * D) / 8 * 8; // 16
+        }
+        times = length_per_core / length;  // 1
+        remain = length_per_core % length; // 8
+    }
+
+    SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (UB_MAX / (B * N * D) < 8) {
+        auto row = (UB_MAX / D / length); // UB_MAX = 16384
+        auto loopBN = (B * N) / row;
+        auto remainBN = (B * N) % row;
+        for (int j = 0; j < loopBN; j++) {
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeForward(inOffset + i * length * D + j * S * D * row, rowsumOffset + i * length + j * S * row,
+                               outOffset + i * length * B * N * D + j * row * D, length, row);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+        // 尾行逻辑
+        if (remainBN > 0) {
+            for (int i = 0; i < times; i++) {
+                WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+                ComputeForward(inOffset + i * length * D + loopBN * S * D * row,
+                               rowsumOffset + i * length + loopBN * S * row,
+                               outOffset + i * length * B * N * D + loopBN * row * D, length, remainBN);
+                SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            }
+        }
+    } else {
+        for (int i = 0; i < times; i++) {
+            WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+            ComputeForward(inOffset + i * length * D, rowsumOffset + i * length, outOffset + i * length * B * N * D,
+                           length, 0);
+            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+        }
+    }
+    WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
+    if (remain > 0) {
+        ComputeForward(inOffset + times * length * D, rowsumOffset + times * length,
+                       outOffset + times * length * B * N * D, remain, 0);
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void TransposeWithDtype<TYPE, IF_BF16>::ComputeForward(int32_t in_offset, int32_t rowsum_offset,
+                                                                             int32_t out_offset, int32_t length,
+                                                                             int32_t row)
+{
+    int32_t nBurst = 0;
+    if (transpose_type == 0) {
+        // SBH
+        nBurst = (UB_MAX / (B * N * D) < 8) ? row : B * N;
+    }
+    AscendC::DataCopyPad(cur_tensor_out, gm_in_tensor[in_offset],
+                         AscendC::DataCopyExtParams(nBurst, D * length * 4, (S - length) * D * 4, 0, 0),
+                         AscendC::DataCopyPadExtParams<float>(false, 0, 0, 0));
+
+    AscendC::DataCopyPad(
+        tensor_rowsum_brcb_f32, gm_rowsum_tensor[rowsum_offset],
+        AscendC::DataCopyExtParams(nBurst, length * sizeof(float), (S - length) * 1 * sizeof(float), 0, 0),
+        AscendC::DataCopyPadExtParams<float>(false, 0, 0, 0));
+
+    SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+
+    auto lines_per_loop = nBurst * length;
+    // 展开D * length个数据
+    brcb_v<ArchType::ASCEND_V220, uint32_t>(tensor_transpose.template ReinterpretCast<uint32_t>(),
+                                            tensor_rowsum_brcb.template ReinterpretCast<uint32_t>(), 1, 8,
+                                            lines_per_loop);
+    pipe_barrier(PIPE_V);
+    // D个FP32 需要两个repeat； 因此，需要先算前半段，再算后半段
+    AscendC::Div<float, false>(cur_tensor_out, cur_tensor_out, tensor_transpose_f32, (uint64_t)0, lines_per_loop,
+                               AscendC::BinaryRepeatParams(1, 1, 0, 16, 16, 1));
+    pipe_barrier(PIPE_V);
+    AscendC::Div<float, false>(cur_tensor_out[HEAD_DIM / 2], cur_tensor_out[HEAD_DIM / 2], tensor_transpose_f32,
+                               (uint64_t)0, lines_per_loop, AscendC::BinaryRepeatParams(1, 1, 0, 16, 16, 1));
+    pipe_barrier(PIPE_V);
+
+    int repeatTimes =
+        nBurst * length * D / (256 / sizeof(float)); // B * N * length * D <= UB_MAX (16384)  即 repeatTimes <=256
+    if constexpr (IF_BF16) {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            convr_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                        halfTimes, 1, 1, 4, 8);
+        }
+    } else {
+        if (repeatTimes < 256) {
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, repeatTimes, 1, 1, 4, 8);
+        } else {
+            int halfTimes = repeatTimes / 2;
+            LocalTensor<TYPE> cur_tensor_res = cur_tensor.template ReinterpretCast<TYPE>();
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res, cur_tensor_f32, halfTimes, 1, 1, 4, 8);
+            conv_v<ArchType::ASCEND_V220, float, TYPE>(cur_tensor_res[halfTimes * 64], cur_tensor_f32[halfTimes * 64],
+                                                       halfTimes, 1, 1, 4, 8);
+        }
+    }
+    pipe_barrier(PIPE_V);
+
+    if (nBurst >= length) {
+        for (int j = 0; j < length; j++) {
+            AscendC::DataCopy(tensor_transpose[j * nBurst * D], cur_tensor[j * D],
+                              AscendC::DataCopyParams(nBurst, D / 16, (length - 1) * D / 16, 0));
+        }
+    } else {
+        for (int j = 0; j < nBurst; j++) {
+            AscendC::DataCopy(tensor_transpose[j * D], cur_tensor[j * D * length],
+                              AscendC::DataCopyParams(length, D / 16, 0, (nBurst - 1) * D / 16));
+        }
+    }
+    pipe_barrier(PIPE_V);
+
+    SetFlag<HardEvent::V_MTE3>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE3>(EVENT_ID0);
+
+    if (transpose_type == 0) {
+        if (UB_MAX / (B * N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (B * N - row) * D * 2, 0));
+        } else {
+            int ub_size = B * N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    } else if (transpose_type == 1) {
+        if (UB_MAX / (N * D) < 8) {
+            AscendC::DataCopyPad(gm_out_tensor[out_offset], tensor_transpose,
+                                 AscendC::DataCopyExtParams(length, D * row * 2, 0, (N - row) * D * 2, 0));
+        } else {
+            int ub_size = N * D * length;
+            AscendC::DataCopy(gm_out_tensor[out_offset], tensor_transpose,
+                              AscendC::DataCopyParams(1, ub_size / 16, 0, 0));
+        }
+    }
+}
+
+#endif
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_const.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_const.h
new file mode 100644
index 00000000..b79abd61
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_const.h
@@ -0,0 +1,145 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+/*
+ * address_const头文件用于定义在寻址模块中使用到的常量
+ */
+
+#ifndef __ADDRESS_CONST_H__
+#define __ADDRESS_CONST_H__
+
+namespace Address {
+// 定义常量
+const int SIZE_128 = 128;
+const int SIZE_256 = 256;
+
+// 基本块的length
+const int BASE_BLOCK_LENGTH = SIZE_128;
+
+// 读取query,key,value的block size
+const int QUERY_BLOCK_SIZE = SIZE_128 * BASE_BLOCK_LENGTH;
+const int KEY_BLOCK_SIZE = SIZE_128 * BASE_BLOCK_LENGTH;
+const int VALUE_BLOCK_SIZE = SIZE_128 * BASE_BLOCK_LENGTH;
+const int ATTENTION_SCORE_BLOCK_SIZE = BASE_BLOCK_LENGTH * BASE_BLOCK_LENGTH;
+
+// 反向最长的分段数
+const int MAX_LENGTH = 16;
+
+// 反向寻址预处理的结构体
+struct BackWardAddr {
+    int32_t b;    // batch索引
+    int32_t n;    // head索引
+    int32_t iR;  // 行索引
+    int32_t iC;  // 列索引
+    int32_t kx;   // 行数
+    int32_t ky;   // 列数
+    int32_t k;    // 基本块的数量
+};
+
+// 反向cube1寻址模块
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+struct PhyAddrBackwardCube1 {
+    __gm__ T_LEFT *left;      // left起始位置
+    __gm__ T_RIGHT *right;    // right起始位置
+    __gm__ T_OUTPUT *out;     // out起始位置
+    int32_t kx = 0;           // x方向的长度
+    int32_t ky = 0;           // y方向的长度
+    int32_t k = 0;            // 总共的块数
+    int32_t lineStride = 0;  // 行和行的间隔
+    bool lowerLeft;          // 左下角是否不需要计算：true代表不需要计算，false代表需要计算
+    bool upperRight;         // 右上角是否不需要计算： true代表不需要计算，false代表需要计算
+};
+
+// 反向cube2寻址模块
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+struct PhyAddrBackwardCube2 {
+    __gm__ T_LEFT *left;      // left起始位置
+    __gm__ T_RIGHT *right;    // right起始位置
+    __gm__ T_OUTPUT *out;     // out起始位置
+    int32_t kx = 0;           // x方向的长度
+    int32_t ky = 0;           // y方向的长度
+    int32_t k = 0;            // 总共的块数
+    int32_t lineStride = 0;  // 行和行的间隔
+    bool lowerLeft;          // 左下角是否不需要计算：true代表不需要计算，false代表需要计算
+    bool upperRight;         // 右上角是否不需要计算： true代表不需要计算，false代表需要计算
+};
+
+// 反向cube3寻址模块
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+struct PhyAddrBackwardCube3 {
+    __gm__ T_LEFT *left;      // left起始位置
+    __gm__ T_RIGHT *right;    // right起始位置
+    __gm__ T_OUTPUT *out;     // out起始位置
+    int32_t kx = 0;           // x方向的长度
+    int32_t ky = 0;           // y方向的长度
+    int32_t k = 0;            // 总共的块数
+    int32_t lineStride = 0;  // 行和行的间隔
+    bool lowerLeft;          // 左下角是否不需要计算：true代表不需要计算，false代表需要计算
+    bool upperRight;         // 右上角是否不需要计算： true代表不需要计算，false代表需要计算
+};
+
+// 反向vector寻址预处理的结构体
+struct VectorAddr {
+    int32_t b = 0;    // batch索引
+    int32_t n = 0;    // head索引
+    int32_t iR = 0;  // 行索引
+    int32_t iC = 0;  // 列索引
+    int32_t kx = 0;   // 行数
+    int32_t ky = 0;   // 列数
+    int32_t k = 0;    // 基本块的数量
+};
+
+// vector的全局信息
+struct GLOBAL_INFO {
+    int64_t cubeNum = 0;           // cube数量
+    int64_t blockNumPerCube = 0;   // 每个cube处理block的数量
+    int64_t headNum = 0;           // head数量
+    int64_t batchNum = 0;          // batch数量
+    int64_t seqLenQ = 0;           // query的序列长度
+    int64_t seqLenK = 0;           // key、value的序列长度
+    int64_t headDim = 0;           // head-dim = 128;
+    bool triMatrix = false;        // 是否是三角阵
+    int32_t blockRows = 0;         // 负载均衡前的行数
+    int32_t blockCols = 0;         // 负载均衡前的列数
+    int64_t blockNumPerRow = 0;    // 一行有几个block（三角阵和非三角阵不同）
+    int64_t blockNumPerCol = 0;    // 一列有几个block（也就是一个head有几个行）
+    int64_t blockNumPerLoop = 0;   // 一个loop处理block的数量
+    int64_t blockNumPerHead = 0;   // 一个head包含的block数量
+    int64_t blockNumPerBatch = 0;  // 一个batch包含的block数量
+    int64_t tailBlockNum = 0;      // 最后一次循环处理的block数量
+    int64_t isSparse = 0;          // 是否为sparse 魔道e
+    int64_t windowLength = 0;      // 滑动窗口的length
+    int64_t windowsBlockNum = 0;   // 滑窗大小, sparse使用
+};
+
+// vector的当前信息
+struct LOCAL_INFO {
+    int64_t cubeIndex = 0;                // 所属的cube分组（第几个cube很重要）
+    int64_t vectorIdx = 0;                // 当前vector在cube内的编号（0或1）
+    int64_t startLineInBaseBlock = 0;  // 处理基本块的起始行（都是2个vector处理一个基本块）
+    bool procTail = false;                // 是否参与尾块处理
+    int64_t procTailBlockNum = 0;       // 处理尾块中的block数量
+};
+
+// vector的分段信息
+struct SECTION_INFO {
+    int64_t sectionNum = 0;                               // 当前处理的block条，包含几个块
+    int64_t section_start_block[MAX_SWITCH_TIME] = {0};    // 起始block编号
+    int64_t head_skip_block[MAX_SWITCH_TIME] = {0};        // 当前section的头尾是否要跳过计算
+    int64_t tail_skip_block[MAX_SWITCH_TIME] = {0};        // 当前section的头尾是否要跳过计算
+    int64_t global_lines_in_heads[MAX_SWITCH_TIME] = {0};  // 在所有heads中的起始行
+    int64_t len[MAX_SWITCH_TIME] = {0};                    // 每一段的长度
+    bool head_apply_mask[MAX_SWITCH_TIME] = {false};
+    bool tail_apply_mask[MAX_SWITCH_TIME] = {false};
+    int64_t O_dO_offset[MAX_SWITCH_TIME] = {0};  // 矩阵O dO的偏移量
+    int64_t S_dP_offset[MAX_SWITCH_TIME] = {0};  // 矩阵S dP的偏移量
+    int64_t processLines = {0};                 // 处理的行数
+};
+}  // namespace Address
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_mapping_cube.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_mapping_cube.h
new file mode 100644
index 00000000..ed8ec618
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_mapping_cube.h
@@ -0,0 +1,743 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#ifndef __ADDRESS_MAPPING_H__
+#define __ADDRESS_MAPPING_H__
+
+#include <cstdint>
+#include "address_const.h"
+
+namespace Address {
+template <typename TYPE>
+class AddressMappingCube {
+public:
+    __aicore__ __inline__ void BackWardAddrMapping_pre(BackWardAddr *addr, int64_t &addrLen, int64_t roundId)
+    {
+        // 当前核计算起始块的索引
+        int32_t curBlockId = roundId * this->backward_core_num_ * this->backward_block_num_per_core_ +
+                               this->backward_local_core_index_ * this->backward_block_num_per_core_;
+        int32_t rowNumPerRound = this->backward_block_num_ky_;
+        int32_t colNumPerRound = this->backward_block_num_per_core_ / this->backward_block_num_ky_;
+
+        // 最后轮次的尾块处理：
+        int32_t remain = this->backward_block_num_per_core_;
+        if (curBlockId + this->backward_block_num_per_core_ > this->backward_total_blocks_) {
+            remain = this->backward_total_blocks_ - curBlockId;
+        }
+
+        // 设置x，y方向的基本块数量
+        int32_t Ky = this->backward_block_num_ky_;
+        int32_t Kx = remain / Ky;
+
+        // 当前轮次的(b,n,ir_begin,ic_begin)
+        int32_t b = curBlockId / this->backward_block_num_per_batch_;
+        int32_t n = curBlockId % this->backward_block_num_per_batch_ / this->backward_block_num_per_head_;
+        int32_t blockRow =
+            curBlockId % this->backward_block_num_per_head_ / (rowNumPerRound * this->backward_block_num_per_row_);
+        int32_t ir = blockRow * rowNumPerRound;
+        int32_t ic = (roundId * this->backward_core_num_ * colNumPerRound +
+                         this->backward_local_core_index_ * colNumPerRound) %
+                     (this->backward_block_num_per_batch_) % this->backward_block_num_per_row_;
+
+        // 处理边界：
+        addr[0].b = b;
+        addr[0].n = n;
+        addr[0].iR = ir;
+        addr[0].iC = ic;
+        addr[0].kx = Kx;
+        addr[0].ky = Ky;
+        addr[0].k = remain;
+
+        int index = 0;
+        for (; remain > 0;) {
+            if (addr[index].iC + addr[index].kx > this->backward_block_num_per_row_) {  // 换行
+                addr[index].kx = this->backward_block_num_per_row_ - addr[index].iC;
+                addr[index].k = addr[index].kx * addr[index].ky;
+
+                addr[index + 1].b = addr[index].b;
+                addr[index + 1].n = addr[index].n;
+                addr[index + 1].iR = addr[index].iR + addr[index].ky;
+                addr[index + 1].iC = 0;
+                addr[index + 1].k = remain - addr[index].k;
+                addr[index + 1].ky = addr[index].ky;
+                addr[index + 1].kx = addr[index + 1].k / addr[index + 1].ky;
+                if (addr[index + 1].iR >= this->backward_block_num_per_column_) {  // 换head
+                    addr[index + 1].n = addr[index].n + 1;
+                    addr[index + 1].iR = addr[index + 1].iR % this->backward_block_num_per_column_;
+                    if (addr[index + 1].n >= this->head_num_) {  // 换batch
+                        addr[index + 1].b = addr[index].b + 1;
+                        addr[index + 1].n = 0;
+                    }
+                }
+            }
+
+            remain -= addr[index].k;
+            ++index;
+        }
+        addrLen = index;
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_rectangular_cube1(__gm__ T_LEFT *__restrict__ left,
+        __gm__ T_RIGHT *__restrict__ right, __gm__ T_OUTPUT *__restrict__ out, const BackWardAddr *addr,
+        PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t srcLen, int64_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 开启work space
+        auto outOffsetRoundEven =
+            out + this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+        auto outOffsetRoundOdd =
+            out + this->backward_core_num_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE +
+            this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+
+        for (int32_t i = 0; i < srcLen; ++i) {
+            auto b = addr[i].b;
+            auto n = addr[i].n;
+            auto ir = addr[i].iR;
+            auto ic = addr[i].iC;
+            auto Kx = addr[i].kx;
+            auto Ky = addr[i].ky;
+            auto k = addr[i].k;
+
+            // 设置偏移量
+            auto bnLeftOffset = left + (b * this->head_num_ + n) * this->query_sequence_len_ * dim_size_q;
+            auto bnRightOffset = right + (b * this->head_num_ + n) * this->key_value_sequence_len_ * dim_size_k;
+            int64_t gIndex = n / (this->head_num_ / this->gqa_group_num_);
+            auto bnRightOffsetGqa =
+                right + (b * this->gqa_group_num_ + gIndex) * this->key_value_sequence_len_ * dim_size_k;
+            src[i].left = bnLeftOffset + ir * SIZE_128 * dim_size_q;
+            src[i].right = bnRightOffsetGqa + ic * SIZE_128 * dim_size_k;
+            src[i].out = ((roundId + 1) % 2) ? outOffsetRoundEven : outOffsetRoundOdd;
+            src[i].kx = Kx;
+            src[i].ky = Ky;
+            src[i].k = k;
+            src[i].lineStride = Kx * ATTENTION_SCORE_BLOCK_SIZE;
+            src[i].lowerLeft = false;
+            src[i].upperRight = false;
+
+            // 多段时，workspoce的偏移量更新
+            outOffsetRoundEven += k * ATTENTION_SCORE_BLOCK_SIZE;
+            outOffsetRoundOdd += k * ATTENTION_SCORE_BLOCK_SIZE;
+        }
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_triangular_cube1(__gm__ T_LEFT *__restrict__ left,
+        __gm__ T_RIGHT *__restrict__ right, __gm__ T_OUTPUT *__restrict__ out, const BackWardAddr *addr,
+        PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t &srcLen, int64_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 开启work space
+
+        int64_t index = 0;
+        int64_t triBlockNumPerColumn = this->backward_block_num_per_column_ - 2 * this->is_odd_;
+        for (int32_t i = 0; i < srcLen; ++i) {
+            int64_t iR = addr[i].iR;
+            int64_t iC = addr[i].iC;
+            int64_t kx = addr[i].kx;
+            int64_t k = addr[i].k;
+
+            // 倒三角和非倒三角这三个变量不一样
+
+            int64_t switchIndex = triBlockNumPerColumn + iR + (iR + 1) % 2;
+            int64_t rowOffset = (iR + 1) % 2 == 1 ? -1 : 1;
+            int64_t rowIndexLeftSection = triBlockNumPerColumn + iR;  // 倒三角 ：非倒三角
+            int64_t rowIndexRightSection = triBlockNumPerColumn - 1 - iR + rowOffset;
+
+            int64_t colIndexLeftSection = iC;
+            int64_t colIndexRightSection = iC - switchIndex - 1;
+
+            // GQA设置：在cube1中K（右矩阵）进行修改
+            int64_t gIndex = addr[i].n / (this->head_num_ / this->gqa_group_num_);
+            int64_t bnOffsetGqaRightMatrix = (addr[i].b * this->gqa_group_num_ + gIndex) *
+                                                 this->key_value_sequence_len_ * dim_size_k;  // for gqa mode
+            int64_t bnOffsetRightMatrix =
+                (addr[i].b * this->head_num_ + addr[i].n) * this->key_value_sequence_len_ * dim_size_k;
+            int64_t bnOffsetLeftMatrix =
+                (addr[i].b * this->head_num_ + addr[i].n) * this->query_sequence_len_ * dim_size_q;
+
+            int64_t qLeftOffsetSection = rowIndexLeftSection * SIZE_128 * dim_size_q;
+            int64_t qRightOffsetSection = rowIndexRightSection * SIZE_128 * dim_size_q;
+            int64_t kLeftOffsetSection = colIndexLeftSection * SIZE_128 * dim_size_k;
+            int64_t kRightOffsetSection = colIndexRightSection * SIZE_128 * dim_size_k;
+
+            // sparse场景 isTri == true 以及 sparseMode == 1
+            bool sparseFlag = false;
+            int64_t windowBlockSize = this->window_length_ / 128;
+            if (this->is_triangle_ && this->sparse_mode_ == 1) {
+                sparseFlag = iR > ((windowBlockSize - 1) / 2) ? true : false;
+                switchIndex = (windowBlockSize / 2) + iR;
+                rowIndexLeftSection = (windowBlockSize / 2) + iR;
+                rowIndexRightSection = (windowBlockSize / 2) - 1 - iR;
+                colIndexLeftSection = iC;
+                colIndexRightSection = iC - switchIndex - 1;
+                qLeftOffsetSection = rowIndexLeftSection * ATTENTION_SCORE_BLOCK_SIZE;
+                qRightOffsetSection = rowIndexRightSection * ATTENTION_SCORE_BLOCK_SIZE;
+                kLeftOffsetSection = colIndexLeftSection * ATTENTION_SCORE_BLOCK_SIZE;
+                kRightOffsetSection = colIndexRightSection * ATTENTION_SCORE_BLOCK_SIZE;
+            }
+            int64_t rowIndexSparseSection = iR + (windowBlockSize / 2);
+            int64_t colIndexSparseSection = iR + iC - (windowBlockSize / 2);
+            int64_t qSparseOffsetSection = rowIndexSparseSection * ATTENTION_SCORE_BLOCK_SIZE;
+            int64_t kSparseOffsetSection = colIndexSparseSection * ATTENTION_SCORE_BLOCK_SIZE;
+
+            int64_t outOffset = ((addr[i].b * this->head_num_ + addr[i].n) * this->backward_block_num_per_row_ *
+                                         this->backward_block_num_per_column_ +
+                                     (iR * this->backward_block_num_per_row_)) *
+                                 ATTENTION_SCORE_BLOCK_SIZE;
+
+            int64_t dbOffset = (roundId % 2) * (this->backward_core_num_ * this->backward_block_num_per_core_ *
+                                                     ATTENTION_SCORE_BLOCK_SIZE);
+            if (index == 0) {
+                src[index].out =
+                    out + dbOffset +
+                    this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+            } else {
+                src[index].out = src[index - 1].out + src[index - 1].k * ATTENTION_SCORE_BLOCK_SIZE;
+            }
+
+            if (!sparseFlag && switchIndex < iC) {
+                src[index].left = left + bnOffsetLeftMatrix + qRightOffsetSection;
+                src[index].right = right + bnOffsetGqaRightMatrix + kRightOffsetSection;
+                // src[index].out = gm_c + outOffset + iC * BASE_BLOCK_SIZE; // 使用 workspace double buffer 后删除
+                src[index].kx = kx;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = k;
+                src[index].upperRight = iC + src[index].kx >= backward_block_num_per_row_ ? true : false;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index++;
+            } else if (!sparseFlag && iC <= switchIndex && iC + kx - 1 > switchIndex) {
+                src[index].left = left + bnOffsetLeftMatrix + qLeftOffsetSection;
+                src[index].right = right + bnOffsetGqaRightMatrix + kLeftOffsetSection;
+                // src[index].out = gm_c + outOffset + iC * BASE_BLOCK_SIZE;  // 使用 workspace double buffer 后删除
+                src[index].kx = switchIndex - iC + 1;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = src[index].kx * this->backward_block_num_ky_;
+                src[index].upperRight = true;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index + 1].left = left + bnOffsetLeftMatrix + qRightOffsetSection;
+                src[index + 1].right = right + bnOffsetGqaRightMatrix;
+                src[index + 1].out = src[index].out + src[index].k * ATTENTION_SCORE_BLOCK_SIZE;  // 保留
+                src[index + 1].kx = kx - src[index].kx;
+                src[index + 1].ky = this->backward_block_num_ky_;
+                src[index + 1].k = src[index + 1].kx * this->backward_block_num_ky_;
+                src[index + 1].upperRight =
+                    switchIndex + src[index + 1].kx >= backward_block_num_per_row_ - 1 ? true : false;
+                src[index + 1].lowerLeft = false;
+                src[index + 1].lineStride = src[index + 1].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index += 2;
+            } else if (!sparseFlag && iC <= switchIndex && iC + kx - 1 <= switchIndex) {
+                src[index].left = left + bnOffsetLeftMatrix + qLeftOffsetSection;
+                src[index].right = right + bnOffsetGqaRightMatrix + kLeftOffsetSection;
+                src[index].kx = kx;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = k;
+                src[index].upperRight = iC + src[index].kx - 1 >= switchIndex ? true : false;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index++;
+            } else {  // not finished
+                src[index].left = left + bnOffsetLeftMatrix + qSparseOffsetSection;
+                src[index].right = right + bnOffsetGqaRightMatrix + kSparseOffsetSection;
+                src[index].k = k;
+                index++;
+            }
+        }
+        srcLen = index;
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_rectangular_cube2(__gm__ T_LEFT *__restrict__ left,
+        __gm__ T_RIGHT *__restrict__ right, __gm__ T_OUTPUT *__restrict__ out, const BackWardAddr *addr,
+        PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t srcLen, int32_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 开启work space
+        auto leftOffsetRoundEven =
+            left + this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+        auto leftOffsetRoundOdd =
+            left + this->backward_core_num_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE +
+            this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+
+        for (int32_t i = 0; i < srcLen; ++i) {
+            auto b = addr[i].b;
+            auto n = addr[i].n;
+            auto ir = addr[i].iR;
+            auto ic = addr[i].iC;
+            auto Kx = addr[i].kx;
+            auto Ky = addr[i].ky;
+            auto k = addr[i].k;
+
+            // 设置偏移量
+            auto bnRightOffset = right + (b * this->head_num_ + n) * this->key_value_sequence_len_ * dim_size_k;
+            int64_t gIndex = n / (this->head_num_ / this->gqa_group_num_);
+            auto bnRightOffsetGqa =
+                right + (b * this->gqa_group_num_ + gIndex) * this->key_value_sequence_len_ * dim_size_k;
+            auto bnOutOffset = out + (b * this->head_num_ + n) * this->query_sequence_len_ * dim_size_q;
+
+            src[i].left = ((roundId + 1) % 2) ? leftOffsetRoundEven : leftOffsetRoundOdd;
+            src[i].right = bnRightOffsetGqa + ic * SIZE_128 * dim_size_k;
+            src[i].out = bnOutOffset + ir * SIZE_128 * dim_size_q;
+            src[i].kx = Kx;
+            src[i].ky = Ky;
+            src[i].k = k;
+            src[i].lineStride = Kx * ATTENTION_SCORE_BLOCK_SIZE;
+            src[i].lowerLeft = false;
+            src[i].upperRight = false;
+
+            // 多段时，work space偏移量更新
+            leftOffsetRoundEven += k * ATTENTION_SCORE_BLOCK_SIZE;
+            leftOffsetRoundOdd += k * ATTENTION_SCORE_BLOCK_SIZE;
+        }
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_triangular_cube2(__gm__ T_LEFT *__restrict__ left,
+        __gm__ T_RIGHT *__restrict__ right, __gm__ T_OUTPUT *__restrict__ out, const BackWardAddr *addr,
+        PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t &srcLen, int32_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 负载均衡的地址偏移
+        int64_t index = 0;
+        int64_t triBlocksPerColumn = this->backward_block_num_per_column_ - 2 * this->is_odd_;
+        for (int i = 0; i < srcLen; ++i) {
+            // left、right、out的地址配置
+            int64_t iR = addr[i].iR;
+            int64_t iC = addr[i].iC;
+            int64_t kx = addr[i].kx;
+            int64_t k = addr[i].k;
+            // 倒三角和非倒三角这三个变量不一样
+
+            int64_t switchIndex = triBlocksPerColumn + iR + (iR + 1) % 2;
+            int64_t rowIndexLeftSection = triBlocksPerColumn + iR;  // 倒三角 ：非倒三角
+            int64_t rowOffset = (iR + 1) % 2 == 1 ? -1 : 1;
+            int64_t rowIndexRightSection = triBlocksPerColumn - 1 - iR + rowOffset;
+            int64_t colIndexLeftSection = iC;
+            int64_t colIndexRightSection = iC - switchIndex - 1;
+            // GQA设置：在cube1中K（右矩阵）进行修改
+            int64_t gIndex = addr[i].n / (this->head_num_ / this->gqa_group_num_);
+            int64_t bnOffsetGqaRightMatrix =
+                (addr[i].b * this->gqa_group_num_ + gIndex) * this->key_value_sequence_len_ * dim_size_k;
+            int64_t bnOffsetLeftMatrix =
+                ((addr[i].b * this->head_num_ + addr[i].n) * this->backward_block_num_per_row_ *
+                    this->backward_block_num_per_column_) *
+                ATTENTION_SCORE_BLOCK_SIZE;  // 当前b,n下的偏移量
+            int64_t bnOffsetRightMatrix = (addr[i].b * this->head_num_ + addr[i].n) * this->key_value_sequence_len_ *
+                                             dim_size_k;  // 当前b,n下的偏移量
+            int64_t bn_offset_out = (addr[i].b * this->head_num_ + addr[i].n) * this->query_sequence_len_ *
+                                    dim_size_q;  // 当前b,n下的偏移量
+
+            // sparse场景：is_tri == true 以及 sparseMode == 1
+            bool sparseFlag = false;
+            int64_t windowBlockSize = this->window_length_ / 128;
+            if (this->is_triangle_ && this->sparse_mode_ == 1) {
+                sparseFlag = iR > ((windowBlockSize - 1) / 2) ? true : false;
+                switchIndex = (windowBlockSize / 2) + iR;
+                rowIndexLeftSection = (windowBlockSize / 2) + iR;
+                rowIndexRightSection = (windowBlockSize / 2) - 1 - iR;
+                colIndexLeftSection = iC;
+                colIndexRightSection = iC - switchIndex - 1;
+            }
+            int64_t rowIndexSparseSection = iR + (windowBlockSize / 2);
+            int64_t colIndexSparseSection = iR + iC - (windowBlockSize / 2);
+
+            // 开启double buffer
+            int64_t dbOffset = (roundId % 2) * (this->backward_core_num_ * this->backward_block_num_per_core_ *
+                                                     ATTENTION_SCORE_BLOCK_SIZE);
+            if (index == 0) {
+                src[index].left =
+                    left + dbOffset +
+                    this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+            } else {
+                src[index].left = src[index - 1].left + src[index - 1].k * ATTENTION_SCORE_BLOCK_SIZE;
+            }
+
+            if (!sparseFlag && switchIndex < iC) {
+                src[index].right =
+                    right + bnOffsetGqaRightMatrix + colIndexRightSection * SIZE_128 * dim_size_k;
+                src[index].out = out + bn_offset_out + rowIndexRightSection * SIZE_128 * dim_size_q;
+                src[index].kx = kx;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = k;
+                src[index].upperRight = iC + src[index].kx >= backward_block_num_per_row_ ? true : false;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                ++index;
+            } else if (!sparseFlag && iC <= switchIndex && iC + kx - 1 > switchIndex) {
+                // src[index].left = left + bnLeftOffset + iR * (H + 1) * BASE_BLOCK_SIZE + iC * BASE_BLOCK_SIZE;
+                src[index].right =
+                    right + bnOffsetGqaRightMatrix + colIndexLeftSection * SIZE_128 * dim_size_k;
+                src[index].out = out + bn_offset_out + rowIndexLeftSection * SIZE_128 * dim_size_q;
+                src[index].kx = switchIndex - iC + 1;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = src[index].kx * this->backward_block_num_ky_;
+                src[index].upperRight = true;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index + 1].left = src[index].left + src[index].k * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index + 1].right = right + bnOffsetGqaRightMatrix;
+                src[index + 1].out = out + bn_offset_out + rowIndexRightSection * SIZE_128 * dim_size_q;
+                src[index + 1].kx = kx - src[index].kx;
+                src[index + 1].ky = this->backward_block_num_ky_;
+                src[index + 1].k = src[index + 1].kx * this->backward_block_num_ky_;
+                src[index + 1].upperRight =
+                    switchIndex + src[index + 1].kx >= backward_block_num_per_row_ - 1 ? true : false;
+                src[index + 1].lowerLeft = false;
+                src[index + 1].lineStride = src[index + 1].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index += 2;
+            } else if (!sparseFlag && iC <= switchIndex && iC + kx - 1 <= switchIndex) {
+                src[index].right =
+                    right + bnOffsetGqaRightMatrix + colIndexLeftSection * SIZE_128 * dim_size_k;
+                src[index].out = out + bn_offset_out + rowIndexLeftSection * SIZE_128 * dim_size_q;
+                src[index].kx = kx;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = k;
+                src[index].upperRight = iC + src[index].kx - 1 >= switchIndex ? true : false;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index++;
+            } else {
+                src[index].right =
+                    right + bnOffsetGqaRightMatrix + colIndexSparseSection * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index].out = out + bn_offset_out + rowIndexSparseSection * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index].k = k;
+                ++index;
+            }
+        }
+        srcLen = index;
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_rectangular_cube3(__gm__ T_LEFT *__restrict__ left,
+        __gm__ T_RIGHT *__restrict__ right, __gm__ T_OUTPUT *__restrict__ out, const BackWardAddr *addr,
+        PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t &srcLen, int32_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 开启work space
+        auto leftOffsetRoundEven =
+            left + this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+        auto leftOffsetRoundOdd =
+            left + this->backward_core_num_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE +
+            this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+
+        for (int32_t i = 0; i < srcLen; ++i) {
+            auto b = addr[i].b;
+            auto n = addr[i].n;
+            auto ir = addr[i].iR;
+            auto ic = addr[i].iC;
+            auto Kx = addr[i].kx;
+            auto Ky = addr[i].ky;
+            auto k = addr[i].k;
+
+            // 设置偏移量
+            auto bnRightOffset = right + (b * this->head_num_ + n) * this->query_sequence_len_ * dim_size_q;
+            auto bnOutOffset = out + (b * this->head_num_ + n) * this->key_value_sequence_len_ * dim_size_k;
+            int64_t gIndex = n / (this->head_num_ / this->gqa_group_num_);
+            auto bn_out_offset_gqa =
+                out + (b * this->gqa_group_num_ + gIndex) * this->key_value_sequence_len_ * dim_size_k;
+            src[i].left = ((roundId + 1) % 2) ? leftOffsetRoundEven : leftOffsetRoundOdd;
+            src[i].right = bnRightOffset + ir * SIZE_128 * dim_size_q;
+            src[i].out = bn_out_offset_gqa + ic * SIZE_128 * dim_size_k;
+            src[i].kx = Kx;
+            src[i].ky = Ky;
+            src[i].k = k;
+            src[i].lineStride = Kx * ATTENTION_SCORE_BLOCK_SIZE;
+            src[i].lowerLeft = false;
+            src[i].upperRight = false;
+
+            // 多段时，work space偏移量更新
+            leftOffsetRoundEven += k * ATTENTION_SCORE_BLOCK_SIZE;
+            leftOffsetRoundOdd += k * ATTENTION_SCORE_BLOCK_SIZE;
+        }
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_triangular_cube3(__gm__ T_LEFT *__restrict__ left,
+        __gm__ T_RIGHT *__restrict__ right, __gm__ T_OUTPUT *__restrict__ out, const BackWardAddr *addr,
+        PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t &srcLen, int32_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 开启work space
+        auto leftOffsetRoundEven =
+            left + this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+        auto leftOffsetRoundOdd =
+            left + this->backward_core_num_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE +
+            this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+        int64_t triBlocksPerColumn = this->backward_block_num_per_column_ - 2 * this->is_odd_;
+        int64_t index = 0;
+        for (int32_t i = 0; i < srcLen; ++i) {
+            // left、right、out的地址配置
+            int64_t iR = addr[i].iR;
+            int64_t iC = addr[i].iC;
+            int64_t kx = addr[i].kx;
+            int64_t k = addr[i].k;
+
+            // 倒三角和非倒三角这三个变量不一样
+            int64_t switchIndex = triBlocksPerColumn + iR + (iR + 1) % 2;
+            int64_t rowIndexLeftSection = triBlocksPerColumn + iR;  // 倒三角 ：非倒三角
+            int64_t rowOffset = (iR + 1) % 2 == 1 ? -1 : 1;
+            int64_t rowIndexRightSection = triBlocksPerColumn - 1 - iR + rowOffset;
+            int64_t colIndexLeftSection = iC;
+            int64_t colIndexRightSection = iC - switchIndex - 1;
+
+            int64_t bnOffsetLeftMatrix =
+                ((addr[i].b * this->head_num_ + addr[i].n) * this->backward_block_num_per_row_ *
+                    this->backward_block_num_per_column_) *
+                ATTENTION_SCORE_BLOCK_SIZE;  // 当前b,n下的偏移量
+            int64_t bnOffsetRightMatrix = (addr[i].b * this->head_num_ + addr[i].n) * this->query_sequence_len_ *
+                                             dim_size_q;  // 当前b,n下的偏移量
+            int64_t bn_offset_out = (addr[i].b * this->head_num_ + addr[i].n) * this->key_value_sequence_len_ *
+                                    dim_size_k;  // 当前b,n下的偏移量
+            int64_t gIndex = addr[i].n / (this->head_num_ / this->gqa_group_num_);
+            int64_t bn_offset_gqa_out_matrix =
+                (addr[i].b * this->gqa_group_num_ + gIndex) * this->key_value_sequence_len_ * dim_size_k;
+            // sparse场景：is_tri == true 以及 sparseMode == 1
+            bool sparseFlag = false;
+            int64_t windowBlockSize = this->window_length_ / 128;
+            if (this->is_triangle_ && this->sparse_mode_ == 1) {
+                sparseFlag = iR > ((windowBlockSize - 1) / 2) ? true : false;
+                switchIndex = (windowBlockSize / 2) + iR;
+                rowIndexLeftSection = (windowBlockSize / 2) + iR;
+                rowIndexRightSection = (windowBlockSize / 2) - 1 - iR;
+                colIndexLeftSection = iC;
+                colIndexRightSection = iC - switchIndex - 1;
+            }
+            int64_t rowIndexSparseSection = iR + (windowBlockSize / 2);
+            int64_t colIndexSparseSection = iR + iC - (windowBlockSize / 2);
+
+            // 开启double buffer
+            int64_t dbOffset = (roundId % 2) * (this->backward_core_num_ * this->backward_block_num_per_core_ *
+                                                     ATTENTION_SCORE_BLOCK_SIZE);
+            if (index == 0) {
+                src[index].left =
+                    left + dbOffset +
+                    this->backward_local_core_index_ * this->backward_block_num_per_core_ * ATTENTION_SCORE_BLOCK_SIZE;
+            } else {
+                src[index].left = src[index - 1].left + src[index - 1].k * ATTENTION_SCORE_BLOCK_SIZE;
+            }
+
+            if (!sparseFlag && switchIndex < iC) {
+                // BASE_BLOCK_SIZE
+                src[index].right =
+                    right + bnOffsetRightMatrix + rowIndexRightSection * dim_size_q * SIZE_128;
+                src[index].out = out + bn_offset_gqa_out_matrix + colIndexRightSection * SIZE_128 * dim_size_k;
+                src[index].kx = kx;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = k;
+                src[index].upperRight = iC + src[index].kx >= backward_block_num_per_row_ ? true : false;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                ++index;
+            } else if (!sparseFlag && iC <= switchIndex && iC + kx - 1 > switchIndex) {
+                src[index].right = right + bnOffsetRightMatrix + rowIndexLeftSection * dim_size_q * SIZE_128;
+                src[index].out = out + bn_offset_gqa_out_matrix + colIndexLeftSection * SIZE_128 * dim_size_k;
+                src[index].kx = switchIndex - iC + 1;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = src[index].kx * this->backward_block_num_ky_;
+                src[index].upperRight = true;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+
+                src[index + 1].left = src[index].left + src[index].k * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index + 1].right =
+                    right + bnOffsetRightMatrix + rowIndexRightSection * dim_size_q * SIZE_128;
+                src[index + 1].out = out + bn_offset_gqa_out_matrix;
+                src[index + 1].kx = kx - src[index].kx;
+                src[index + 1].ky = this->backward_block_num_ky_;
+                src[index + 1].k = src[index + 1].kx * this->backward_block_num_ky_;
+                src[index + 1].upperRight =
+                    switchIndex + src[index + 1].kx >= backward_block_num_per_row_ - 1 ? true : false;
+                src[index + 1].lowerLeft = false;
+                src[index + 1].lineStride = src[index + 1].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index += 2;
+            } else if (!sparseFlag && iC <= switchIndex && iC + kx - 1 <= switchIndex) {
+                src[index].right = right + bnOffsetRightMatrix + rowIndexLeftSection * dim_size_q * SIZE_128;
+                src[index].out = out + bn_offset_gqa_out_matrix + colIndexLeftSection * SIZE_128 * dim_size_k;
+                src[index].kx = kx;
+                src[index].ky = this->backward_block_num_ky_;
+                src[index].k = k;
+                src[index].upperRight = iC + src[index].kx - 1 >= switchIndex ? true : false;
+                src[index].lowerLeft = false;
+                src[index].lineStride = src[index].kx * ATTENTION_SCORE_BLOCK_SIZE;
+                index++;
+            } else {
+                src[index].right =
+                    right + bnOffsetRightMatrix + rowIndexSparseSection * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index].out = out + bn_offset_gqa_out_matrix + colIndexSparseSection * ATTENTION_SCORE_BLOCK_SIZE;
+                src[index].k = k;
+                ++index;
+            }
+        }
+        srcLen = index;
+    }
+
+public:
+    __aicore__ __inline__ void init(int32_t batchSize, int32_t headNum, int32_t query_sequence_len,
+        int32_t key_value_sequence_len, int32_t headDim, int32_t gqa_group_num, bool isTriangle, int32_t sparseMode,
+        int32_t windowLength)
+    {
+        // B N S D初始化
+        this->batch_size_ = batchSize;
+        this->head_num_ = headNum;
+        this->query_sequence_len_ = query_sequence_len;
+        this->key_value_sequence_len_ = key_value_sequence_len;
+        this->head_dim_ = headDim;
+        this->gqa_group_num_ = gqa_group_num;
+        this->is_odd_ = this->query_sequence_len_ / BASE_BLOCK_LENGTH / TILING_DIVIDE % TILING_DIVIDE;
+        // 负责均衡前信息的初始化
+        this->blockRows = this->query_sequence_len_ / BASE_BLOCK_LENGTH;
+        this->blockCols = this->key_value_sequence_len_ / BASE_BLOCK_LENGTH;
+
+        // 初始化负载均衡的信息
+        this->is_triangle_ = isTriangle;
+
+        this->sparse_mode_ = sparseMode;
+        this->window_length_ = windowLength;
+        this->window_size_ = this->window_length_ / SIZE_128;
+    }
+
+    __aicore__ __inline__ int32_t get_total_rounds()
+    {
+        return this->backward_total_rounds_;
+    }
+
+    __aicore__ __inline__ bool is_running(int32_t roundId)
+    {
+        return (roundId * this->backward_block_num_per_core_ * this->backward_core_num_ +
+                   this->backward_local_core_index_ * this->backward_block_num_per_core_) <
+                this->backward_total_blocks_;
+    }
+
+    __aicore__ __inline__ void set_backward_tiling(int32_t backward_core_num, int32_t backward_local_core_index,
+        int32_t backward_block_num_per_core, int32_t backward_block_num_ky)
+    {
+        // 初始化
+        this->backward_core_num_ = backward_core_num;
+        this->backward_local_core_index_ = backward_local_core_index;
+        this->backward_block_num_per_core_ = backward_block_num_per_core;
+        this->backward_block_num_ky_ = backward_block_num_ky;
+
+        // 根据负载均衡信息来计算基本块
+        this->backward_block_num_per_column_ = this->blockRows;
+        this->backward_block_num_per_row_ = this->blockCols;
+        if (this->is_triangle_) {
+            this->backward_block_num_per_column_ = this->blockRows / TILING_DIVIDE + this->is_odd_;
+            this->backward_block_num_per_row_ = this->blockCols + TILING_DIVIDE * (1 - this->is_odd_);
+            if (this->sparse_mode_ == 1) {
+                this->backward_block_num_per_column_ = this->blockRows - this->window_size_ / TILING_DIVIDE;
+                this->backward_block_num_per_row_ = this->window_size_ + 1;
+            }
+        }
+
+        // 初始化基本块数量
+        this->backward_block_num_per_head_ = this->backward_block_num_per_column_ * this->backward_block_num_per_row_;
+        this->backward_block_num_per_batch_ = this->head_num_ * this->backward_block_num_per_head_;
+        this->backward_total_blocks_ = this->batch_size_ * this->backward_block_num_per_batch_;
+
+        // 轮次的计算
+        this->backward_total_rounds_ =
+            (this->backward_total_blocks_ + this->backward_block_num_per_core_ * this->backward_core_num_ - 1) /
+            (this->backward_block_num_per_core_ * this->backward_core_num_);
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_cube1(__gm__ T_LEFT *__restrict__ left, __gm__ T_RIGHT *__restrict__ right,
+        __gm__ T_OUTPUT *__restrict__ out, PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t &srcLen,
+        int64_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 寻址的预处理
+        BackWardAddr BackWardAddr[MAX_SWITCH_TIME];
+        BackWardAddrMapping_pre(BackWardAddr, srcLen, roundId);
+
+        // cube1地址偏移的计算
+        if (this->is_triangle_) {
+            addrMapping_triangular_cube1(left, right, out, BackWardAddr, src, srcLen, roundId, dim_size_q, dim_size_k);
+        } else {  // no-mask
+            addrMapping_rectangular_cube1(left, right, out, BackWardAddr, src, srcLen, roundId, dim_size_q, dim_size_k);
+        }
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_cube2(__gm__ T_LEFT *__restrict__ left, __gm__ T_RIGHT *__restrict__ right,
+        __gm__ T_OUTPUT *__restrict__ out, PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t &srcLen,
+        int64_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 寻址的预处理
+        BackWardAddr BackWardAddr[MAX_SWITCH_TIME];
+        BackWardAddrMapping_pre(BackWardAddr, srcLen, roundId);
+
+        // cube1地址偏移的计算
+        if (this->is_triangle_) {
+            addrMapping_triangular_cube2(left, right, out, BackWardAddr, src, srcLen, roundId, dim_size_q, dim_size_k);
+        } else {
+            addrMapping_rectangular_cube2(left, right, out, BackWardAddr, src, srcLen, roundId, dim_size_q, dim_size_k);
+        }
+    }
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void addrMapping_cube3(__gm__ T_LEFT *__restrict__ left, __gm__ T_RIGHT *__restrict__ right,
+        __gm__ T_OUTPUT *__restrict__ out, PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t &srcLen,
+        int64_t roundId, int64_t dim_size_q, int64_t dim_size_k)
+    {
+        // 寻址的预处理
+        BackWardAddr BackWardAddr[MAX_SWITCH_TIME];
+        BackWardAddrMapping_pre(BackWardAddr, srcLen, roundId);
+
+        // cube1地址偏移的计算
+        if (this->is_triangle_) {
+            addrMapping_triangular_cube3(left, right, out, BackWardAddr, src, srcLen, roundId, dim_size_q, dim_size_k);
+        } else {
+            addrMapping_rectangular_cube3(left, right, out, BackWardAddr, src, srcLen, roundId, dim_size_q, dim_size_k);
+        }
+    }
+
+private:
+    // B N S D的格式
+    int32_t batch_size_;
+    int32_t head_num_;
+    int32_t query_sequence_len_;
+    int32_t key_value_sequence_len_;
+    int32_t head_dim_;
+    int32_t gqa_group_num_;
+
+    int32_t is_odd_;
+    // 负载均衡前的信息
+    int32_t blockRows;  // 行数
+    int32_t blockCols;  // 列数
+
+    // 负责均衡的信息
+    bool is_triangle_;       // 三角形的标志
+    int32_t sparse_mode_;    // sparse模式
+    int32_t window_length_;  // 滑动窗口
+    int32_t window_size_;    // 滑动窗口
+
+    // 前向的信息
+    int32_t forward_core_num_;            // 前向的核数
+    int32_t forward_group_num_;           // 前向核心的组
+    int32_t forward_core_num_per_group_;  // 每个组的核心数
+    int32_t forward_local_core_index_;    // 当前核心的index
+    int32_t forward_total_rounds_;        // 前向的总轮次
+
+    // 反向的信息
+    int32_t backward_core_num_;              // 反向的核数
+    int32_t backward_local_core_index_;      // 反向当前核的index
+    int32_t backward_block_num_per_core_;    // 反向每个核计算的基本块数量
+    int32_t backward_block_num_ky_;          // 反向按y方向变量的基本块数量
+    int32_t backward_block_num_per_row_;     // 反向每行的基本块数量
+    int32_t backward_block_num_per_column_;  // 反向每列的基本块数量
+    int32_t backward_block_num_per_head_;    // 反向每个head的基本块数量
+    int32_t backward_block_num_per_batch_;   // 反向每个batch的基本块数量
+    int32_t backward_total_blocks_;          // 反向计算的总块数
+    int32_t backward_total_rounds_;          // 反向总共的轮次
+};
+}  // namespace Address
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_mapping_vector.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_mapping_vector.h
new file mode 100644
index 00000000..c26dc59b
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/address_mapping_vector.h
@@ -0,0 +1,419 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+/**
+ * AddressMapping_Vector头文件，用于Vector的寻址
+ */
+#ifndef ADDRESS_MODULE_ADDRESSMAPPING_VECTOR_H
+#define ADDRESS_MODULE_ADDRESSMAPPING_VECTOR_H
+
+#include <cstdint>
+#include "address_const.h"
+
+namespace Address {
+// 定义常量
+const int MODULUS_TWO = 2;
+const int CONSTANT_FACTOR_TWO = 2;
+const int VECTOR_NUM_PER_CUBE = 2;
+const int WINDOW_LEN_ADJUSTOR = 2;
+
+class AddressMappingVector {
+public:
+    __aicore__ __inline__ void addrMapping_pre(int64_t round_id, VectorAddr *vector_addr, int64_t &addr_len)
+    {
+        // 起始块的索引id
+        int64_t cur_block_id =
+            round_id * this->global_.blockNumPerLoop + this->local_.cubeIndex * this->global_.blockNumPerCube;
+        int64_t row_num_per_round = this->ky_;
+        int64_t col_num_per_round = this->global_.blockNumPerCube / this->ky_;
+
+        // 最后轮次的尾块处理
+        int64_t remain = this->global_.blockNumPerCube;
+        if (cur_block_id + this->global_.blockNumPerCube > this->mTotalBlocks) {
+            remain = this->mTotalBlocks - cur_block_id;
+        }
+
+        // 设置x,y方向的基本块数量
+        int64_t Ky = this->ky_;
+        int64_t Kx = remain / Ky;
+
+        // 计算起始的B N S D
+        int64_t b = cur_block_id / this->global_.blockNumPerBatch;
+        int64_t n = cur_block_id % this->global_.blockNumPerBatch / this->global_.blockNumPerHead;
+        int64_t block_row =
+            cur_block_id % this->global_.blockNumPerHead / (row_num_per_round * this->global_.blockNumPerRow);
+        int64_t ir = block_row * row_num_per_round;
+        int64_t ic = (round_id * this->global_.cubeNum * col_num_per_round + local_.cubeIndex * col_num_per_round) %
+                     (this->global_.blockNumPerHead) % this->global_.blockNumPerRow;
+
+        // 处理边界：
+        vector_addr[0].b = b;
+        vector_addr[0].n = n;
+        vector_addr[0].iR = ir;
+        vector_addr[0].iC = ic;
+        vector_addr[0].kx = Kx;
+        vector_addr[0].ky = Ky;
+        vector_addr[0].k = remain;
+
+        int64_t index = 0;
+        for (; remain > 0;) {
+            // 跳变点的位置
+            int64_t switch_index = vector_addr[index].iR + this->mBalanceRow +
+                                   (vector_addr[index].iR + 1) % MODULUS_TWO;
+            if (this->global_.triMatrix && (vector_addr[index].iC <= switch_index) &&
+                (vector_addr[index].iC + vector_addr[index].kx - 1 > switch_index)) {  // 沿着跳变点切分
+                vector_addr[index].kx = switch_index - vector_addr[index].iC + 1;
+                vector_addr[index].k = vector_addr[index].kx * vector_addr[index].ky;
+
+                vector_addr[index + 1].b = vector_addr[index].b;
+                vector_addr[index + 1].n = vector_addr[index].n;
+                vector_addr[index + 1].iR = vector_addr[index].iR;
+                vector_addr[index + 1].iC = switch_index + 1;
+                vector_addr[index + 1].k = remain - vector_addr[index].k;
+                vector_addr[index + 1].ky = vector_addr[index].ky;
+                vector_addr[index + 1].kx = vector_addr[index + 1].k / vector_addr[index + 1].ky;
+            }
+            if (vector_addr[index].iC + vector_addr[index].kx > this->global_.blockNumPerRow) {  // 换行
+                vector_addr[index].kx = this->global_.blockNumPerRow - vector_addr[index].iC;
+                vector_addr[index].k = vector_addr[index].kx * vector_addr[index].ky;
+
+                vector_addr[index + 1].b = vector_addr[index].b;
+                vector_addr[index + 1].n = vector_addr[index].n;
+                vector_addr[index + 1].iR = vector_addr[index].iR + vector_addr[index].ky;
+                vector_addr[index + 1].iC = 0;
+                vector_addr[index + 1].k = remain - vector_addr[index].k;
+                vector_addr[index + 1].ky = vector_addr[index].ky;
+                vector_addr[index + 1].kx = vector_addr[index + 1].k / vector_addr[index + 1].ky;
+                if (vector_addr[index + 1].iR >= this->global_.blockNumPerCol) {  // 换head
+                    vector_addr[index + 1].n = vector_addr[index].n + 1;
+                    vector_addr[index + 1].iR = vector_addr[index + 1].iR % this->global_.blockNumPerCol;
+                    if (vector_addr[index + 1].n >= this->global_.headNum) {  // 换batch
+                        vector_addr[index + 1].b = vector_addr[index].b + 1;
+                        vector_addr[index + 1].n = 0;
+                    }
+                }
+            }
+
+            remain -= vector_addr[index].k;
+            ++index;
+        }
+
+        // 处理空的段
+        int64_t pos = 0;
+        for (size_t i = 0; i < index; ++i) {
+            if (vector_addr[i].k == 0) {
+                continue;
+            }
+            vector_addr[pos++] = vector_addr[i];
+        }
+        addr_len = pos;
+    }
+
+    __aicore__ __inline__ void addrMapping_rectangle_nomark(
+        int64_t round_id, VectorAddr *vector_addr, int64_t &addr_len)
+    {
+        // workspace:
+        int64_t work_space_offset_odd =
+            this->local_.cubeIndex * this->global_.blockNumPerCube * ATTENTION_SCORE_BLOCK_SIZE;
+        int64_t work_space_offset_even =
+            this->global_.cubeNum * this->global_.blockNumPerCube * ATTENTION_SCORE_BLOCK_SIZE +
+            this->local_.cubeIndex * this->global_.blockNumPerCube * ATTENTION_SCORE_BLOCK_SIZE;
+        int64_t work_space_offset = ((round_id + 1) % MODULUS_TWO) ? work_space_offset_odd : work_space_offset_even;
+
+        // 分段的偏移量
+        for (size_t i = 0; i < addr_len; ++i) {
+            for (size_t j = 0; j < this->ky_; ++j) {
+                int64_t b = vector_addr[i].b;
+                int64_t n = vector_addr[i].n;
+                int64_t ir = vector_addr[i].iR + j;
+                int64_t ic = vector_addr[i].iC;
+                int64_t kx = vector_addr[i].kx;
+
+                // 更新section信息
+                int64_t index = i * this->ky_ + j;
+                this->section_.section_start_block[index] =
+                    ((i == 0) && (j == 0))
+                        ? 0
+                        : (this->section_.section_start_block[index - 1] + this->section_.len[index - 1]);
+                this->section_.len[index] = kx;
+                this->section_.global_lines_in_heads[index] =
+                    (b * this->global_.headNum + n) * this->global_.blockRows * BASE_BLOCK_LENGTH +
+                    ir * BASE_BLOCK_LENGTH + this->local_.startLineInBaseBlock;
+                this->section_.O_dO_offset[index] =
+                    this->section_.global_lines_in_heads[index] * this->global_.headDim;
+                this->section_.S_dP_offset[index] =
+                    work_space_offset + this->section_.section_start_block[index] * ATTENTION_SCORE_BLOCK_SIZE;
+
+                // mask
+                this->section_.head_apply_mask[index] = false;
+                this->section_.tail_apply_mask[index] = false;
+            }
+        }
+        this->section_.processLines = BASE_BLOCK_LENGTH / VECTOR_NUM_PER_CUBE;
+        this->section_.sectionNum = addr_len * this->ky_;
+    }
+
+    __aicore__ __inline__ void addrMapping_rectangle_mark(int64_t round_id, VectorAddr *vector_addr, int64_t &addr_len)
+    {
+        // workspace:
+        int64_t work_space_offset_odd =
+            this->local_.cubeIndex * this->global_.blockNumPerCube * ATTENTION_SCORE_BLOCK_SIZE;
+        int64_t work_space_offset_even =
+            this->global_.cubeNum * this->global_.blockNumPerCube * ATTENTION_SCORE_BLOCK_SIZE +
+            this->local_.cubeIndex * this->global_.blockNumPerCube * ATTENTION_SCORE_BLOCK_SIZE;
+        int64_t work_space_offset = ((round_id + 1) % MODULUS_TWO) ? work_space_offset_odd : work_space_offset_even;
+
+        // 分段的偏移量
+        int32_t index = 0;
+        for (size_t i = 0; i < addr_len; ++i) {
+            for (size_t j = 0; j < vector_addr[i].ky; ++j) {
+                int64_t b = vector_addr[i].b;
+                int64_t n = vector_addr[i].n;
+                int64_t ir = vector_addr[i].iR + j;
+                int64_t ic = vector_addr[i].iC;
+                int64_t kx = vector_addr[i].kx;
+
+                int64_t left_ir = ir + this->mBalanceRow;
+                int64_t row_offset = (ir + 1) % MODULUS_TWO == 1 ? -1 : 1;
+                int64_t right_ir = this->mBalanceRow - 1 - ir + row_offset;
+                int64_t left_global_line_offset =
+                    (b * this->global_.headNum + n) * this->global_.blockRows * BASE_BLOCK_LENGTH +
+                    left_ir * BASE_BLOCK_LENGTH + this->local_.startLineInBaseBlock;
+                int64_t right_global_line_offset =
+                    (b * this->global_.headNum + n) * this->global_.blockRows * BASE_BLOCK_LENGTH +
+                    right_ir * BASE_BLOCK_LENGTH + this->local_.startLineInBaseBlock;
+
+                // 跳变点的索引
+                int64_t switch_index = ir + this->mBalanceRow + (ir + 1) % MODULUS_TWO;
+                if (ir >= this->mBalanceRow) {  // 256奇数块的最后两行
+                    this->section_.head_skip_block[index] = 0;
+                    this->section_.tail_skip_block[index] =
+                        (ir + 1) % MODULUS_TWO && ic + kx == this->global_.blockNumPerRow ? 1 : 0;
+                    this->section_.len[index] =
+                        (ir + 1) % MODULUS_TWO && ic + kx == this->global_.blockNumPerRow ? kx - 1 : kx;
+                    this->section_.section_start_block[index] =
+                        (ir + 1) % MODULUS_TWO && index == 0
+                            ? 0
+                            : (!((ir + 1) % MODULUS_TWO) && index == 0
+                                      ? kx
+                                      : this->section_.section_start_block[index - 1] + this->section_.len[index - 1] +
+                                            this->section_.head_skip_block[index - 1] +
+                                            this->section_.tail_skip_block[index - 1]);
+                    this->section_.global_lines_in_heads[index] = left_global_line_offset;
+                    this->section_.O_dO_offset[index] =
+                        this->section_.global_lines_in_heads[index] * this->global_.headDim;
+                    this->section_.S_dP_offset[index] =
+                        work_space_offset + this->section_.section_start_block[index] * ATTENTION_SCORE_BLOCK_SIZE;
+                    // mask
+                    this->section_.head_apply_mask[index] = false;
+                    this->section_.tail_apply_mask[index] =
+                        (ic + kx >= this->global_.blockNumPerRow - 1 && (ir + 1) % MODULUS_TWO) ||
+                        (ic + kx == this->global_.blockNumPerRow && !((ir + 1) % MODULUS_TWO));
+                    index = this->section_.len[index] != 0 ? index + 1 : index;
+                } else if (ir < this->mBalanceRow && switch_index < ic) {
+                    this->section_.head_skip_block[index] = 0;
+                    this->section_.tail_skip_block[index] =
+                        (ir + 1) % MODULUS_TWO && ic + kx == this->global_.blockNumPerRow ? 1 : 0;
+                    this->section_.len[index] =
+                        (ir + 1) % MODULUS_TWO && ic + kx == this->global_.blockNumPerRow ? kx - 1 : kx;
+                    this->section_.section_start_block[index] =
+                        (ir + 1) % MODULUS_TWO && index == 0
+                            ? 0
+                            : (!((ir + 1) % MODULUS_TWO) && index == 0
+                                      ? kx
+                                      : this->section_.section_start_block[index - 1] + this->section_.len[index - 1] +
+                                            this->section_.head_skip_block[index - 1] +
+                                            this->section_.tail_skip_block[index - 1]);
+                    this->section_.global_lines_in_heads[index] = right_global_line_offset;
+                    this->section_.O_dO_offset[index] =
+                        this->section_.global_lines_in_heads[index] * this->global_.headDim;
+                    this->section_.S_dP_offset[index] =
+                        work_space_offset + this->section_.section_start_block[index] * ATTENTION_SCORE_BLOCK_SIZE;
+                    // mask
+                    this->section_.head_apply_mask[index] = false;
+                    this->section_.tail_apply_mask[index] =
+                        (ic + kx >= this->global_.blockNumPerRow - 1 && (ir + 1) % MODULUS_TWO) ||
+                        (ic + kx == this->global_.blockNumPerRow && !((ir + 1) % MODULUS_TWO));
+                    index = this->section_.len[index] != 0 ? index + 1 : index;
+                } else {
+                    this->section_.head_skip_block[index] = 0;
+                    this->section_.tail_skip_block[index] = (ir + 1) % MODULUS_TWO && ic + kx - 1 == switch_index ?
+                                                            1 : 0;
+                    // len和workspace位置
+                    this->section_.len[index] = (ir + 1) % MODULUS_TWO && ic + kx - 1 == switch_index ? kx - 1 : kx;
+                    this->section_.section_start_block[index] =
+                        (ir + 1) % MODULUS_TWO && index == 0
+                            ? 0
+                            : (!((ir + 1) % MODULUS_TWO) && index == 0
+                                      ? kx
+                                      : this->section_.section_start_block[index - 1] + this->section_.len[index - 1] +
+                                            this->section_.head_skip_block[index - 1] +
+                                            this->section_.tail_skip_block[index - 1]);
+                    // line、O 、S的偏移量
+                    this->section_.global_lines_in_heads[index] = left_global_line_offset;
+                    this->section_.O_dO_offset[index] =
+                        this->section_.global_lines_in_heads[index] * this->global_.headDim;
+                    this->section_.S_dP_offset[index] =
+                        work_space_offset + this->section_.section_start_block[index] * ATTENTION_SCORE_BLOCK_SIZE;
+                    // mask
+                    this->section_.head_apply_mask[index] = false;
+                    this->section_.tail_apply_mask[index] =
+                        (ic + kx >= switch_index && (ir + 1) % MODULUS_TWO) ||
+                        (ic + kx - 1 == switch_index && !((ir + 1) % MODULUS_TWO));
+                    index = this->section_.len[index] != 0 ? index + 1 : index;
+                }
+            }
+        }
+        this->section_.processLines = BASE_BLOCK_LENGTH / VECTOR_NUM_PER_CUBE;
+        this->section_.sectionNum = index;
+    }
+
+public:
+    __aicore__ __inline__ void init_global_info(int64_t batchSize, int64_t headNum, int64_t query_sequence_len,
+        int64_t key_value_sequence_len, int64_t headDim, bool isTriangle, int64_t sparseMode, int64_t windowLength,
+        int64_t cubeNum, int64_t blockNumPerCube, int64_t ky)
+    {
+        // 初始化
+        this->global_.batchNum = batchSize;
+        this->global_.headNum = headNum;
+        this->global_.seqLenQ = query_sequence_len;
+        this->global_.seqLenK = key_value_sequence_len;
+        this->global_.headDim = headDim;
+        this->global_.triMatrix = isTriangle;
+        this->global_.isSparse = sparseMode;
+        this->global_.windowLength = windowLength;
+        this->global_.windowsBlockNum = windowLength / BASE_BLOCK_LENGTH;
+        this->global_.cubeNum = cubeNum;
+        this->global_.blockNumPerCube = blockNumPerCube;
+        this->ky_ = ky;
+        this->mOddFlag = this->global_.seqLenQ / SIZE_256 % MODULUS_TWO;
+
+        // tiling策略
+        this->global_.blockRows = this->global_.seqLenQ / BASE_BLOCK_LENGTH;
+        this->global_.blockCols = this->global_.seqLenK / BASE_BLOCK_LENGTH;
+        this->global_.blockNumPerCol = this->global_.blockRows;
+        this->global_.blockNumPerRow = this->global_.blockCols;
+        if (this->global_.triMatrix) {  // 三角标志
+            if (this->mOddFlag) {
+                this->mBalanceRow = (this->global_.seqLenQ - SIZE_256) / SIZE_128 / CONSTANT_FACTOR_TWO;
+                this->global_.blockNumPerCol = this->global_.blockRows / CONSTANT_FACTOR_TWO + 1;
+                this->global_.blockNumPerRow = this->global_.blockCols;
+            } else {
+                this->mBalanceRow = this->global_.seqLenQ / SIZE_128 / CONSTANT_FACTOR_TWO;
+                this->global_.blockNumPerCol = this->global_.blockRows / CONSTANT_FACTOR_TWO;
+                this->global_.blockNumPerRow = this->global_.blockCols + CONSTANT_FACTOR_TWO;
+            }
+            if (this->global_.isSparse == 1) {  // sparse mode
+                this->global_.blockNumPerCol = this->global_.windowsBlockNum + WINDOW_LEN_ADJUSTOR;
+                this->global_.blockNumPerRow = this->global_.blockNumPerCol -
+                                               this->global_.windowsBlockNum / CONSTANT_FACTOR_TWO;
+            }
+        }
+
+        // 计算块数、轮次等
+        this->global_.blockNumPerLoop = this->global_.cubeNum * this->global_.blockNumPerCube;
+        this->global_.blockNumPerHead = this->global_.blockNumPerRow * this->global_.blockNumPerCol;
+        this->global_.blockNumPerBatch = this->global_.headNum * this->global_.blockNumPerHead;
+        this->mTotalBlocks = this->global_.batchNum * this->global_.headNum * this->global_.blockNumPerHead;
+        this->mTotalRounds =
+            (this->mTotalBlocks + this->global_.blockNumPerLoop - 1) / this->global_.blockNumPerLoop;
+        this->global_.tailBlockNum = this->mTotalBlocks % this->global_.blockNumPerLoop;
+    }
+
+    __aicore__ __inline__ void init_local_info(int64_t local_cube_index, int64_t local_vector_index)
+    {
+        // 初始化
+        this->local_.cubeIndex = local_cube_index;
+        this->local_.vectorIdx = local_vector_index;
+
+        // 计算
+        this->local_.startLineInBaseBlock = BASE_BLOCK_LENGTH / VEC_NUM_PER_CUBE * this->local_.vectorIdx;
+        this->local_.procTail = false;
+        if (this->global_.tailBlockNum > 0) {
+            int64_t cube_used = this->global_.tailBlockNum / this->global_.blockNumPerCube;
+            int64_t tail_block = this->global_.tailBlockNum % this->global_.blockNumPerCube;
+
+            if (this->local_.cubeIndex < cube_used) {
+                this->local_.procTail = true;
+                this->local_.procTailBlockNum = this->global_.blockNumPerCube;
+            }
+            if (tail_block > 0) {
+                this->local_.procTail = true;
+                this->local_.procTailBlockNum = tail_block;
+            }
+        }
+    }
+
+    __aicore__ __inline__ int64_t
+
+        get_total_rounds()
+    {
+        return this->mTotalRounds;
+    }
+
+    __aicore__ __inline__ const SECTION_INFO &get_section_info(int64_t round_id)
+    {
+        clear_section();
+        VectorAddr vector_addr[MAX_SWITCH_TIME];
+        int64_t addr_len = 0;
+        addrMapping_pre(round_id, vector_addr, addr_len);
+
+        if (this->global_.triMatrix) {
+            addrMapping_rectangle_mark(round_id, vector_addr, addr_len);
+        } else {
+            addrMapping_rectangle_nomark(round_id, vector_addr, addr_len);
+        }
+        return this->section_;
+    }
+
+    __aicore__ __inline__ bool is_running(int64_t round_id)
+    {
+        // 起始块的索引id
+        int64_t cur_block_id =
+            round_id * this->global_.blockNumPerLoop + this->local_.cubeIndex * this->global_.blockNumPerCube;
+
+        return cur_block_id < this->mTotalBlocks;
+    }
+
+protected:
+    __aicore__ __inline__ void clear_section()
+    {
+        this->section_.sectionNum = 0;
+        this->section_.section_start_block[MAX_SWITCH_TIME] = {0};
+        this->section_.head_skip_block[MAX_SWITCH_TIME] = {0};
+        this->section_.tail_skip_block[MAX_SWITCH_TIME] = {0};
+        this->section_.global_lines_in_heads[MAX_SWITCH_TIME] = {0};
+        this->section_.len[MAX_SWITCH_TIME] = {0};
+        this->section_.head_apply_mask[MAX_SWITCH_TIME] = {false};
+        this->section_.tail_apply_mask[MAX_SWITCH_TIME] = {false};
+        this->section_.O_dO_offset[MAX_SWITCH_TIME] = {0};
+        this->section_.S_dP_offset[MAX_SWITCH_TIME] = {0};
+        this->section_.processLines = 0;
+    }
+
+private:
+    // vector全局信息
+    GLOBAL_INFO global_;
+    // vector当前信息
+    LOCAL_INFO local_;
+    // vector分段信息
+    SECTION_INFO section_;
+    // 总块数
+    int64_t mTotalBlocks;
+    // 轮次
+    int64_t mTotalRounds;
+    // ky方向
+    int64_t ky_;
+    // 序列长度是否为256倍数的奇数倍：true表示是
+    bool mOddFlag;
+    // 负载均衡矩阵的行数
+    int64_t mBalanceRow;
+};
+}  // namespace Address
+#endif
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward.h
new file mode 100644
index 00000000..dd8acd83
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward.h
@@ -0,0 +1,1347 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __CUBEBACKWARD_FP32_OP_H__
+#define __CUBEBACKWARD_FP32_OP_H__
+
+#define SET_FLAG(trigger, waiter, e) AscendC::SetFlag<AscendC::HardEvent::trigger##_##waiter>((e))
+#define WAIT_FLAG(trigger, waiter, e) AscendC::WaitFlag<AscendC::HardEvent::trigger##_##waiter>((e))
+
+#include "ppmatmul_const_grad.h"
+#include "kernels/utils/kernel/utils.h"
+#include "kernels/utils/kernel/iterator.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_CUBE__
+namespace CUBE_BACKWARD_FP32_OP {
+
+using WORKSPACE_DTYPE = float;
+using WORKSPACE_DTYPE_DP = float;
+const int CONTINUED_BLOCKS_NUM_S = 2;
+const int CONTINUED_BLOCKS_NUM_DP = 2;
+constexpr int32_t SIZE_128 = 128;
+constexpr int32_t SIZE_256 = 256;
+constexpr int32_t SIZE_384 = 384;
+constexpr int32_t SIZE_ONE_K = 1024;
+constexpr int64_t BASE_LEN = 128;
+constexpr int64_t BASE_BLOCK = 128 * 128;
+constexpr int64_t REPEAT_TIME_8 = 8;
+constexpr int64_t REPEAT_TIME_64 = 64;
+constexpr int64_t SRC_STRIDE_1 = 1;
+constexpr int64_t SRC_STRIDE_8 = 8;
+
+template<typename TYPE = half, typename WORKSPACE_TYPE = float>
+struct PhyAddrCube1 {
+    __gm__ TYPE* left;
+    __gm__ TYPE* right;
+    __gm__ WORKSPACE_TYPE* out;
+    int64_t k = 0;
+};
+
+template<typename TYPE = half, typename WORKSPACE_TYPE = float>
+struct PhyAddrCube2 {
+    __gm__ WORKSPACE_TYPE* left;
+    __gm__ TYPE* right;
+    __gm__ float* out;
+    int64_t k = 0;
+};
+
+template<typename TYPE = half, typename WORKSPACE_TYPE = float>
+struct PhyAddrCube3 {
+    __gm__ WORKSPACE_TYPE* left;
+    __gm__ TYPE* right;
+    __gm__ T_OUTPUT* out;
+    int64_t k = 0;
+};
+
+template <typename TYPE, bool IF_BF16>
+class CubeBackward {
+public:
+    __aicore__ inline CubeBackward() {}
+
+    __aicore__ inline void Init(
+        __gm__ TYPE * __restrict__ gmQ, __gm__ TYPE * __restrict__ gmK,
+        __gm__ TYPE * __restrict__ gmV, __gm__ TYPE * __restrict__ gmDO,
+        __gm__ WORKSPACE_DTYPE_DP * __restrict__ gm_dP, __gm__ WORKSPACE_DTYPE * __restrict__ gmS,
+        __gm__ float * __restrict__ gmDQ, __gm__ float * __restrict__ gmDK, __gm__ float * __restrict__ gmDV,
+        bool isTri, int64_t qSize, int64_t kSize, int64_t batch, int64_t head, int64_t groupSize,
+        int64_t baseLength, int64_t sparseMode, int64_t windowLength, int64_t blockNumPerCore);
+
+    __aicore__ inline void Run() ;
+    __aicore__ inline void Run_mix();
+
+private:
+    __aicore__ __inline__ void addrMappingPre(Addr *addr, int64_t& curAddrLen, int64_t roundId);
+    __aicore__ __inline__ void addrMapping_cube1(int64_t roundId, PhyAddrCube1<TYPE, WORKSPACE_DTYPE> *src1,
+                                                 PhyAddrCube1<TYPE, WORKSPACE_DTYPE_DP> *src2, int *srcLength);
+    __aicore__ __inline__ void cube1(int64_t roundId);
+    __aicore__ __inline__ void base_block_mad(PhyAddrCube1<TYPE, WORKSPACE_DTYPE> addr_1,
+                                              PhyAddrCube1<TYPE, WORKSPACE_DTYPE_DP> addr_2,
+                                              int64_t l0a_offset_remain = -1);
+    __aicore__ inline void Mat_mix_cube2_cube3(int64_t roundId);
+    template <typename T> __aicore__ __inline__ void AddrToPhyAddr(Addr *addr, int length, __gm__ TYPE * gm_a,
+                                                                   __gm__ TYPE * gm_b, __gm__ T * gm_c,
+                                                                   PhyAddrCube1<TYPE, T> *src, int *srcLength,
+                                                                   int64_t roundId);
+    template <typename T> __aicore__ __inline__ void addrMapping_cube2(__gm__ T *__restrict__ left,
+                                                                        __gm__ TYPE *__restrict__ right,
+                                                                        __gm__ T_OUTPUT *__restrict__ out,
+                                                                        int64_t roundId, PhyAddrCube2<TYPE, T> *src,
+                                                                        int64_t &len);
+    template <typename T> __aicore__ __inline__ void addrMapping_cube3(__gm__ T *__restrict__ left,
+                                                                        __gm__ TYPE *__restrict__ right,
+                                                                        __gm__ T_OUTPUT *__restrict__ out,
+                                                                        int64_t roundId, PhyAddrCube3<TYPE, T> *src,
+                                                                        int64_t& switchIndex);
+    template <typename T> __aicore__ __inline__ void cube3_matmul(const PhyAddrCube3<TYPE, T> *src, int64_t srcLen,
+                                                                    int64_t enventIdA, int64_t enventIdB,
+                                                                    int64_t vcoreNumPerHead);
+    template <typename T> __aicore__ __inline__ void cube2_cube3_mix(const PhyAddrCube2<TYPE, T> *cube2_addr,
+                                                                        int64_t cube2_length,
+                                                                        const PhyAddrCube3<TYPE, T> *cube3_addr,
+                                                                        int64_t cube3_length,
+                                                                        int64_t vcoreNumPerHead);
+
+    __gm__ TYPE* __restrict__ a_cube1{nullptr};
+    __gm__ TYPE* __restrict__ b_cube1{nullptr};
+    __gm__ TYPE* __restrict__ b_cube2{nullptr};
+    __gm__ TYPE* __restrict__ c_cube1{nullptr};
+    __gm__ float* __restrict__ c_cube2{nullptr};
+
+    __gm__ TYPE* __restrict__ gmQ{nullptr};
+    __gm__ TYPE* __restrict__ gmK{nullptr};
+    __gm__ TYPE* __restrict__ gmV{nullptr};
+    __gm__ TYPE* __restrict__ gmDO{nullptr};
+
+    __gm__ WORKSPACE_DTYPE_DP* __restrict__ gm_dP{nullptr};
+    __gm__ WORKSPACE_DTYPE* __restrict__ gmS{nullptr};
+    __gm__ float* __restrict__ gmDQ{nullptr};
+    __gm__ float* __restrict__ gmDK{nullptr};
+    __gm__ float* __restrict__ gmDV{nullptr};
+    __gm__ uint8_t* __restrict__ ffts_addr{nullptr};
+    __gm__ uint8_t* __restrict__ tiling_para_gm{nullptr};
+
+    AscendC::Nd2NzParams commonNd2NzParams {
+        1,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        1,
+        0
+    };
+
+    AscendC::LoadData2dParams commonLoadData2dParamsTranspose {
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        0,
+        true,
+        0
+    };
+
+    AscendC::LoadData2dParams commonLoadData2dParamsNoTranspose {
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        0,
+        false,
+        0
+    };
+
+    AscendC::MmadParams commonMadParams {
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        3,
+        false,
+        true
+    };
+
+    AscendC::FixpipeParamsV220 commonFixpipeParamsV220 {
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        false
+    };
+
+    AsdopsBuffer<ArchType::ASCEND_V220> asdopsBuf;
+
+    LocalTensor<TYPE> l1_base_a_cube1_tensor;
+    LocalTensor<TYPE> l1_base_b_cube1_tensor;
+    LocalTensor<TYPE> l1_base_a_cube2_tensor;
+    LocalTensor<TYPE> l1_base_b_cube2_tensor;
+
+    LocalTensor<TYPE> l0_a_base_tensor;
+    LocalTensor<TYPE> l0_b_base_tensor;
+    LocalTensor<float> l0_c_base_tensor;
+    LocalTensor<float> l0_c_buf_tensor;
+
+    GlobalTensor<TYPE> temp_tensor_bf16;
+    GlobalTensor<float> temp_tensor_fp32;
+
+    int64_t coreNum;
+    int64_t curCoreIndex;
+    int64_t blockNumPerCore;
+    int64_t batchSize;
+    int64_t headNum;
+    int64_t gqaGroupNum;
+    int64_t headDim;
+    int64_t seqLenQ;
+    int64_t seqLenK;
+    int64_t blocksPerColumn;
+    int64_t blocksPerRow;
+    bool isTri;
+    int64_t sparseMode;
+    int64_t windowLength;
+
+    bool isCube1;
+    bool isCube2;
+    bool isCube3;
+    bool isSyn;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackward<TYPE, IF_BF16>::Init(
+    __gm__ TYPE * __restrict__ gmQ, __gm__ TYPE * __restrict__ gmK,
+    __gm__ TYPE * __restrict__ gmV, __gm__ TYPE * __restrict__ gmDO,
+    __gm__ WORKSPACE_DTYPE_DP * __restrict__ gm_dP, __gm__ WORKSPACE_DTYPE * __restrict__ gmS,
+    __gm__ float * __restrict__ gmDQ, __gm__ float * __restrict__ gmDK, __gm__ float * __restrict__ gmDV,
+    bool isTri, int64_t qSize, int64_t kSize, int64_t batch, int64_t head, int64_t groupSize, int64_t baseLength,
+    int64_t sparseMode, int64_t windowLength, int64_t blockNumPerCore) {
+    this->gmQ = gmQ;
+    this->gmK = gmK;
+    this->gmV = gmV;
+    this->gmDO = gmDO;
+
+    this->gm_dP = gm_dP;
+    this->gmS = gmS;
+    this->gmDQ = gmDQ;
+    this->gmDK = gmDK;
+    this->gmDV = gmDV;
+
+    this->seqLenQ = qSize;
+    this->seqLenK = kSize;
+    this->batchSize = batch;
+    this->headNum = head;
+    this->headDim = baseLength;
+    this->gqaGroupNum = (head + groupSize - 1) / groupSize;
+    this->blockNumPerCore = blockNumPerCore;
+    this->isTri = isTri;
+    this->sparseMode = sparseMode;
+    this->windowLength = windowLength;
+
+    this->coreNum =  get_block_num();
+    this->curCoreIndex = get_block_idx();
+
+    this->blocksPerColumn = this->isTri ? ((this->seqLenQ) / BASE_LEN / 2)  : ((this->seqLenQ) / BASE_LEN);
+    this->blocksPerRow =  this->isTri ? ((this->seqLenK) / BASE_LEN + 1) : ((this->seqLenK) / BASE_LEN);
+
+    if (this->isTri && this->sparseMode == 1) {
+        this->blocksPerColumn = (this->seqLenQ / BASE_LEN) - (this->windowLength / BASE_LEN / 2);
+        this->blocksPerRow = (this->windowLength / BASE_LEN) + 1;
+    }
+
+    isCube1 = true;
+    isCube2 = true;
+    isCube3 = true;
+    isSyn = true;
+
+    temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(0));
+    temp_tensor_fp32.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(0));
+
+    // init L1 tensor
+    l1_base_a_cube1_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(0);
+    l1_base_b_cube1_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_128 * SIZE_ONE_K);
+    l1_base_a_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_256 * SIZE_ONE_K);
+    l1_base_b_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_384 * SIZE_ONE_K);
+
+    l0_a_base_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(0);
+    l0_b_base_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(0);
+    l0_c_base_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+    l0_c_buf_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+
+    commonFixpipeParamsV220.quantPre = QuantMode_t::NoQuant;
+    commonFixpipeParamsV220.unitFlag = 3;
+
+    commonMadParams.m = BASE_LEN;
+    commonMadParams.n = BASE_LEN;
+    commonMadParams.k = BASE_LEN;
+}
+
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackward<TYPE, IF_BF16>::cube1(int64_t roundId) {
+    PhyAddrCube1<TYPE, WORKSPACE_DTYPE> src1[MAX_SWITCH_TIME];
+    PhyAddrCube1<TYPE, WORKSPACE_DTYPE_DP> src2[MAX_SWITCH_TIME];
+    int srcLength = 0;
+    addrMapping_cube1(roundId, src1, src2, &srcLength);
+
+    SET_FLAG(M, MTE1, EVENT_ID6);
+    SET_FLAG(M, MTE1, EVENT_ID7);
+    SET_FLAG(MTE1, MTE2, EVENT_ID3);
+    SET_FLAG(MTE1, MTE2, EVENT_ID4);
+    SET_FLAG(MTE1, MTE2, EVENT_ID1);
+    SET_FLAG(M, MTE1, EVENT_ID2);
+
+    for (int i = 0; i < srcLength; i++) {
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        commonNd2NzParams.nValue = BASE_LEN;
+        commonNd2NzParams.dValue = BASE_LEN;
+        commonNd2NzParams.srcDValue = BASE_LEN;
+        commonNd2NzParams.srcNdMatrixStride = 0;
+        commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+        commonNd2NzParams.dstNzMatrixStride = 0;
+        temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(src1[i].left));
+        AscendC::DataCopy(
+            l1_base_a_cube1_tensor,
+            temp_tensor_bf16,
+            commonNd2NzParams
+        );
+
+        temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(src2[i].left));
+        AscendC::DataCopy(
+            l1_base_a_cube1_tensor[BASE_BLOCK_SIZE],
+            temp_tensor_bf16,
+            commonNd2NzParams
+        );
+
+        SET_FLAG(MTE2, MTE1, EVENT_ID3);
+        WAIT_FLAG(MTE2, MTE1, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID2);
+
+        for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+            commonLoadData2dParamsNoTranspose.repeatTimes = REPEAT_TIME_8;
+            commonLoadData2dParamsNoTranspose.srcStride = SRC_STRIDE_8;
+            AscendC::LoadData(
+                l0_a_base_tensor[i * BASE_LEN * BLOCK_SIZE],
+                l1_base_a_cube1_tensor[i * CUBE_MATRIX_SIZE],
+                commonLoadData2dParamsNoTranspose
+            );
+        }
+
+        for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+            AscendC::LoadData(
+                l0_a_base_tensor[BASE_LEN * BASE_LEN + i * BASE_LEN * BLOCK_SIZE],
+                l1_base_a_cube1_tensor[BASE_LEN * BASE_LEN + i * CUBE_MATRIX_SIZE],
+                AscendC::LoadData2dParams(
+                    0,                // startIndex
+                    REPEAT_TIME_8,    // repeatTimes
+                    SRC_STRIDE_8,     // srcStride
+                    0,                // sid
+                    0,                // dstGap
+                    false,            // ifTranspose
+                    0                 // addrMode
+                )
+            );
+        }
+
+        base_block_mad(src1[i], src2[i]);
+
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(M, MTE1, EVENT_ID2);
+    }
+
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+    WAIT_FLAG(M, MTE1, EVENT_ID2);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
+    WAIT_FLAG(M, MTE1, EVENT_ID6);
+    WAIT_FLAG(M, MTE1, EVENT_ID7);
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::base_block_mad(
+    PhyAddrCube1<TYPE, WORKSPACE_DTYPE> addr_1,
+    PhyAddrCube1<TYPE, WORKSPACE_DTYPE_DP> addr_2, int64_t l0a_offset_remain)
+{
+    int64_t ping_flag_l1_cube1 = 1;
+    int64_t ping_flag_l0b_cube1 = 1;
+    int64_t ping_flag_l0c_cube1 = 1;
+
+    auto gm_k = addr_1.right;
+    auto gm_c = addr_1.out;
+    auto gm_c2 = addr_2.out;
+    bool is_cal_s = true;
+
+
+    for (int64_t j = 0; j < addr_1.k + addr_2.k; j++) {
+        auto event_id = ping_flag_l1_cube1 ? EVENT_ID3 : EVENT_ID4;
+        if (j == addr_1.k) {
+            gm_k = addr_2.right;
+            gm_c = addr_2.out;
+        }
+
+        WAIT_FLAG(MTE1, MTE2, event_id);
+
+        auto l1_buf_b_tensor = ping_flag_l1_cube1 ? l1_base_b_cube1_tensor : l1_base_b_cube1_tensor[BASE_BLOCK_SIZE];
+        commonNd2NzParams.nValue = BASE_LEN;
+        commonNd2NzParams.dValue = BASE_LEN;
+        commonNd2NzParams.srcDValue = BASE_LEN;
+        commonNd2NzParams.srcNdMatrixStride = 0;
+        commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+        commonNd2NzParams.dstNzMatrixStride = 0;
+        temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_k));
+            AscendC::DataCopy(
+                l1_buf_b_tensor,
+                temp_tensor_bf16,
+                commonNd2NzParams
+            );
+
+        SET_FLAG(MTE2, MTE1, event_id);
+        ping_flag_l1_cube1 = 1 - ping_flag_l1_cube1;
+
+        auto event_id_2 = ping_flag_l0b_cube1 ? EVENT_ID6 : EVENT_ID7;
+        int64_t offset_l0b_cube1 = ping_flag_l0b_cube1 ? 0 : BASE_BLOCK_SIZE;
+
+        WAIT_FLAG(MTE2, MTE1, event_id);
+        WAIT_FLAG(M, MTE1, event_id_2);
+        commonLoadData2dParamsNoTranspose.repeatTimes = REPEAT_TIME_64;
+        commonLoadData2dParamsNoTranspose.srcStride = SRC_STRIDE_1;
+        AscendC::LoadData(
+                l0_b_base_tensor[offset_l0b_cube1],
+                l1_buf_b_tensor,
+                commonLoadData2dParamsNoTranspose
+            );
+
+        SET_FLAG(MTE1, MTE2, event_id);
+        SET_FLAG(MTE1, M, event_id_2);
+        WAIT_FLAG(MTE1, M, event_id_2);
+
+        int64_t offset_l0c_offset_cube1 = ping_flag_l0c_cube1 ? 0 : BASE_BLOCK_SIZE;
+        int64_t l0a_offset = 0;
+
+        if (j >= addr_1.k) {
+            l0a_offset = BASE_BLOCK_SIZE;
+        }
+
+        int unit_flag = 0b11;
+        commonMadParams.unitFlag = unit_flag;
+        commonMadParams.cmatrixInitVal = true;
+        AscendC::Mmad(
+            l0_c_buf_tensor[offset_l0c_offset_cube1],
+            l0_a_base_tensor[l0a_offset],
+            l0_b_base_tensor[offset_l0b_cube1],
+            commonMadParams
+        );
+
+        SET_FLAG(M, MTE1, event_id_2);
+        ping_flag_l0b_cube1 = 1 - ping_flag_l0b_cube1;
+
+        auto intriParams = AscendC::FixpipeParamsV220(
+            BASE_LEN,
+            BASE_LEN,
+            BASE_LEN,
+            BASE_LEN,
+            false
+        );
+        intriParams.quantPre = QuantMode_t::NoQuant;
+        intriParams.unitFlag = unit_flag;
+
+        temp_tensor_fp32.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_c));
+        AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+            temp_tensor_fp32,
+            l0_c_buf_tensor[offset_l0c_offset_cube1],
+            intriParams
+        );
+
+        gm_c += BASE_BLOCK_SIZE;
+        ping_flag_l0c_cube1 = 1 - ping_flag_l0c_cube1;
+        gm_k += BASE_BLOCK_SIZE;
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::cube3_matmul(const PhyAddrCube3<TYPE, T> *src,
+                            int64_t  srcLen, int64_t enventIdA, int64_t enventIdB, int64_t vcoreNumPerHead) {
+    if (srcLen == 0) return;
+
+    int64_t switchIndexList[MAX_SWITCH_TIME] = {0};
+    int64_t totalLen = 0;
+    for (int64_t srcIndex = 0; srcIndex < srcLen; srcIndex ++) {
+        switchIndexList[srcIndex] = totalLen;
+        totalLen += src[srcIndex].k;
+    }
+
+    SET_FLAG(MTE1, MTE2, enventIdA);
+    SET_FLAG(MTE1, MTE2, enventIdB);
+
+    commonNd2NzParams.nValue = BASE_LEN;
+    commonNd2NzParams.dValue = BASE_LEN;
+    commonNd2NzParams.srcDValue = BASE_LEN;
+    commonNd2NzParams.srcNdMatrixStride = 0;
+    commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+    commonNd2NzParams.dstNzMatrixStride = 0;
+
+    temp_tensor_bf16.SetGlobalBuffer(
+        reinterpret_cast<__gm__ TYPE *>(src[0].right));
+    AscendC::DataCopy(
+        l1_base_b_cube2_tensor,
+        temp_tensor_bf16,
+        commonNd2NzParams
+    );
+
+    SET_FLAG(MTE2, MTE1, enventIdA);
+    WAIT_FLAG(MTE2, MTE1, enventIdA);
+
+    SET_FLAG(MTE2, MTE1, enventIdB);
+    WAIT_FLAG(MTE2, MTE1, enventIdB);
+
+
+    for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+        commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+        commonLoadData2dParamsTranspose.srcStride = BASE_LEN / BLOCK_SIZE;
+        AscendC::LoadData(
+            l0_b_base_tensor[i * BASE_LEN * BLOCK_SIZE],
+            l1_base_b_cube2_tensor[i * CUBE_MATRIX_SIZE],
+            commonLoadData2dParamsTranspose
+        );
+    }
+
+    int64_t FlagL1BPingPong  = 1;
+    int64_t FlagL1APingPong  = 1;
+    int64_t FlagL0APingPong = 1;
+    int64_t FlagL0BPingPong = 1;
+    int64_t FlagL0CPingPong = 1;
+    int64_t mte1MadPingFlag = 1;
+
+    SET_FLAG(M, MTE1, enventIdA);
+    SET_FLAG(M, MTE1, enventIdB);
+
+    int64_t curSwitchIndex = 0;
+    int64_t curSwitch  = switchIndexList[curSwitchIndex];
+    auto last_event_id = EVENT_ID3;
+    for (int64_t kIdx = 0; kIdx < totalLen; kIdx++) {
+        if ((srcLen >= 2 && kIdx == switchIndexList[curSwitchIndex+1])) {
+            curSwitchIndex ++;
+            curSwitch = switchIndexList[curSwitchIndex];
+        }
+        int64_t actualKIdx =  kIdx - curSwitch;
+        int64_t iC = actualKIdx;
+
+        auto mte1MadEventId = FlagL0APingPong ? enventIdA : enventIdB;
+        auto eventIdCube3 = FlagL1APingPong ? enventIdA : enventIdB;
+
+        auto l0a_buf_tensor = FlagL0APingPong ? l0_a_base_tensor : l0_a_base_tensor[L0AB_PINGPONG_BUFFER_LEN];
+        auto l1b_buf_tensor = FlagL0APingPong ? l1_base_a_cube2_tensor : l1_base_a_cube2_tensor[L1_PINGPONG_BUFFER_LEN];
+
+        if (srcLen >= 2 && curSwitchIndex >= 1 && kIdx == curSwitch) {
+            commonNd2NzParams.nValue = BASE_LEN;
+            commonNd2NzParams.dValue = BASE_LEN;
+            commonNd2NzParams.srcDValue = BASE_LEN;
+            commonNd2NzParams.srcNdMatrixStride = 0;
+            commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+            commonNd2NzParams.dstNzMatrixStride = 0;
+            temp_tensor_bf16.SetGlobalBuffer(
+                reinterpret_cast<__gm__ TYPE *>(src[curSwitchIndex].right));
+            AscendC::DataCopy(
+                l1_base_b_cube2_tensor[(curSwitchIndex % 2) * BASE_LEN * BASE_LEN],
+                temp_tensor_bf16,
+                commonNd2NzParams
+            );
+
+            SET_FLAG(MTE2, MTE1, EVENT_ID6);
+            WAIT_FLAG(MTE2, MTE1, EVENT_ID6);
+
+            if (src[curSwitchIndex-1].k == 1) {
+                SET_FLAG(M, MTE1, EVENT_ID6);
+                WAIT_FLAG(M, MTE1, EVENT_ID6);
+            }
+
+            for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+                commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = BASE_LEN / BLOCK_SIZE;
+                AscendC::LoadData(
+                    l0_b_base_tensor[(curSwitchIndex % 2) * BASE_LEN * BASE_LEN + i * BASE_LEN * BLOCK_SIZE],
+                    l1_base_b_cube2_tensor[(curSwitchIndex % 2) * BASE_LEN * BASE_LEN +  i * CUBE_MATRIX_SIZE],
+                    commonLoadData2dParamsTranspose
+                );
+            }
+        }
+
+        WAIT_FLAG(MTE1, MTE2, eventIdCube3);
+
+        int64_t localOffsetA =  actualKIdx * BASE_LEN * BASE_LEN;
+        temp_tensor_bf16.SetGlobalBuffer(
+                reinterpret_cast<__gm__ TYPE *>(src[curSwitchIndex].left + localOffsetA));
+        AscendC::DataCopy(
+            l1b_buf_tensor,
+            temp_tensor_bf16,
+            AscendC::Nd2NzParams(
+                vcoreNumPerHead,
+                BASE_LEN / vcoreNumPerHead,
+                BASE_LEN,
+                BASE_LEN / vcoreNumPerHead * BASE_LEN * 2,
+                BASE_LEN,
+                BASE_LEN,
+                1,
+                BASE_LEN * BLOCK_SIZE / vcoreNumPerHead
+            )
+        );
+
+        SET_FLAG(MTE2, MTE1, eventIdCube3);
+        WAIT_FLAG(MTE2, MTE1, eventIdCube3);
+        WAIT_FLAG(M, MTE1, mte1MadEventId);
+
+        for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+            commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+            commonLoadData2dParamsTranspose.srcStride = SRC_STRIDE_1;
+            AscendC::LoadData(
+                l0a_buf_tensor[i * BASE_LEN * BLOCK_SIZE],
+                l1b_buf_tensor[i * BASE_LEN * BLOCK_SIZE],
+                commonLoadData2dParamsTranspose
+            );
+        }
+
+        SET_FLAG(MTE1, MTE2, eventIdCube3);
+        SET_FLAG(MTE1, M, mte1MadEventId);   // mte1MadEventId
+        WAIT_FLAG(MTE1, M, mte1MadEventId);  // mte1MadEventId
+
+        int unit_flag = 0b11;                     // allow reading from L0C when finish mad
+        auto l0b_buf_tensor = l0_b_base_tensor[(curSwitchIndex % 2) * BASE_LEN * BASE_LEN];
+        auto l0c_buf_tensor = FlagL0CPingPong ? l0_c_base_tensor : l0_c_base_tensor[L0C_PINGPONG_BUFFER_LEN];
+        commonMadParams.unitFlag = unit_flag;
+        commonMadParams.cmatrixInitVal = true;
+        AscendC::Mmad(
+            l0c_buf_tensor,
+            l0a_buf_tensor,
+            l0b_buf_tensor,
+            commonMadParams
+        );
+
+        SET_FLAG(M, MTE1, mte1MadEventId); // mte1MadEventId
+
+        AscendC::SetAtomicAdd<float>();
+        AscendC::SetAtomicType<float>();
+        int64_t localOffsetGmC = actualKIdx  * BASE_LEN * BASE_LEN;
+        commonFixpipeParamsV220.unitFlag = unit_flag;
+        temp_tensor_fp32.SetGlobalBuffer(
+            reinterpret_cast<__gm__ float *>(src[curSwitchIndex].out  + localOffsetGmC));
+        AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+            temp_tensor_fp32,
+            l0c_buf_tensor,
+            commonFixpipeParamsV220
+        );
+        AscendC::SetAtomicNone();
+
+        FlagL0APingPong = 1 - FlagL0APingPong;
+        FlagL1APingPong = 1 - FlagL1APingPong;
+        FlagL0CPingPong = 1 - FlagL0CPingPong ;
+    }
+
+    WAIT_FLAG(M, MTE1, enventIdA);
+    WAIT_FLAG(M, MTE1, enventIdB);
+    WAIT_FLAG(MTE1, MTE2, enventIdA);
+    WAIT_FLAG(MTE1, MTE2, enventIdB);
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::addrMappingPre(
+                                Addr *addr, int64_t& curAddrLen, int64_t roundId) {
+    int64_t blocksPerBatch = this->headNum * blocksPerRow * blocksPerColumn;
+    int64_t allBlocks = this->batchSize * blocksPerBatch;
+    int64_t curBlockId = this->blockNumPerCore * this->coreNum * roundId + curCoreIndex * this->blockNumPerCore;
+
+    int64_t remain = this->blockNumPerCore;
+    if (curBlockId + this->blockNumPerCore  > allBlocks) {
+        remain = allBlocks - curBlockId;
+    }
+
+    int64_t b = curBlockId / blocksPerBatch;
+
+    int64_t n = (curBlockId % blocksPerBatch) / (blocksPerRow * blocksPerColumn);
+
+    int64_t ir = (curBlockId % blocksPerBatch) % (blocksPerRow * blocksPerColumn) / (blocksPerRow);
+
+    int64_t ic = (curBlockId % blocksPerBatch) % (blocksPerRow * blocksPerColumn) % (blocksPerRow);
+    int64_t rows = (ic + remain + blocksPerRow - 1) / blocksPerRow;
+
+    addr[0].b = b;
+    addr[0].n = n;
+    addr[0].iR = ir;
+    addr[0].iC = ic;
+    addr[0].k = remain;
+
+    for (int i = 0; i < rows && remain > 0; i++) {
+        if (addr[curAddrLen].iC + remain > blocksPerRow) {
+            addr[curAddrLen].k = blocksPerRow - addr[curAddrLen].iC;
+            addr[curAddrLen + 1].k = remain - addr[curAddrLen].k;
+            addr[curAddrLen + 1].b = addr[curAddrLen].b;
+            addr[curAddrLen + 1].n = addr[curAddrLen].n;
+            addr[curAddrLen + 1].iR = addr[curAddrLen].iR + 1;
+            addr[curAddrLen + 1].iC = 0;
+            if (addr[curAddrLen + 1].iR >= blocksPerColumn) {
+                addr[curAddrLen + 1].n = addr[curAddrLen].n + 1;
+                addr[curAddrLen + 1].iR = 0;
+                if (addr[curAddrLen + 1].n >= this->headNum) {
+                    addr[curAddrLen + 1].b = addr[curAddrLen].b + 1;
+                    addr[curAddrLen + 1].n = 0;
+                }
+            }
+        }
+        remain -= addr[curAddrLen].k;
+        ++curAddrLen;
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::addrMapping_cube1(
+    int64_t roundId, PhyAddrCube1<TYPE, WORKSPACE_DTYPE> *src1,
+    PhyAddrCube1<TYPE, WORKSPACE_DTYPE_DP> *src2, int *srcLength) {
+
+    int64_t curAddrLen = 0;
+    Addr addr[MAX_SWITCH_TIME];
+    addrMappingPre(addr, curAddrLen, roundId);
+
+    AddrToPhyAddr(addr, curAddrLen, gmQ, gmK, gmS, src1, srcLength, roundId);
+    AddrToPhyAddr(addr, curAddrLen, gmDO, gmV, gm_dP, src2, srcLength, roundId);
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::AddrToPhyAddr(
+    Addr *addr, int length, __gm__ TYPE * gm_a, __gm__ TYPE * gm_b, __gm__ T * gm_c,
+    PhyAddrCube1<TYPE, T> *src, int *srcLength, int64_t roundId)
+{
+    int index = 0;
+    for (int i = 0 ; i < length; i++) {
+        int64_t iR = addr[i].iR;
+        int64_t iC = addr[i].iC;
+        int64_t k = addr[i].k;
+        int64_t switchIndex = this->isTri ? blocksPerColumn + iR : blocksPerRow;
+        int64_t rowIndexLeftSection = this->isTri ? blocksPerColumn + iR :iR;
+        int64_t rowIndexRightSection = this->isTri ? blocksPerColumn - 1 - iR : iR;
+
+        int64_t colIndexLeftSection = iC;
+        int64_t colIndexRightSection =  this->isTri ?  (iC - switchIndex - 1) : iC;
+
+        int64_t gIndex = addr[i].n / (this->headNum / this->gqaGroupNum);
+        int64_t bnOffsetGqaRightMatrix = (addr[i].b * this->gqaGroupNum + gIndex) * this->seqLenK *
+                                         this->headDim; // for gqa mode
+        int64_t bnOffsetRightMatrix = (addr[i].b * headNum  + addr[i].n  ) * this->seqLenK * this->headDim;
+        int64_t bnOffsetLeftMatrix =  (addr[i].b * headNum  + addr[i].n  ) * this->seqLenQ * this->headDim;
+
+        int64_t qLeftOffsetSection =   rowIndexLeftSection * BASE_BLOCK_SIZE;
+        int64_t qRightOffsetSection =  rowIndexRightSection * BASE_BLOCK_SIZE;
+        int64_t kLeftOffsetSection =  colIndexLeftSection * BASE_BLOCK_SIZE;
+        int64_t kRightOffsetSection =  colIndexRightSection * BASE_BLOCK_SIZE;
+
+        // sparse场景：isTri == true 以及 sparseMode == 1
+        bool sparseFlag = false;
+        int64_t windowBlockSize = this->windowLength / BASE_LEN;
+        if (this->isTri && this->sparseMode == 1) {
+            sparseFlag = iR > ((windowBlockSize -1) / 2) ? true: false;
+            switchIndex = (windowBlockSize / 2)+ iR;
+            rowIndexLeftSection = (windowBlockSize / 2) + iR;
+            rowIndexRightSection = (windowBlockSize / 2) - 1 - iR;
+            colIndexLeftSection = iC;
+            colIndexRightSection = iC - switchIndex - 1;
+            qLeftOffsetSection =   rowIndexLeftSection * BASE_BLOCK_SIZE;
+            qRightOffsetSection =  rowIndexRightSection * BASE_BLOCK_SIZE;
+            kLeftOffsetSection =  colIndexLeftSection * BASE_BLOCK_SIZE;
+            kRightOffsetSection =  colIndexRightSection * BASE_BLOCK_SIZE;
+        }
+        int64_t rowIndexSparseSection = iR + (windowBlockSize / 2);
+        int64_t colIndexSparseSection = iR + iC - (windowBlockSize / 2);
+        int64_t qSparseOffsetSection = rowIndexSparseSection * BASE_BLOCK_SIZE;
+        int64_t kSparseOffsetSection = colIndexSparseSection * BASE_BLOCK_SIZE;
+
+        int64_t outOffset = ((addr[i].b * headNum + addr[i].n) * blocksPerRow * blocksPerColumn +
+                            (iR * blocksPerRow)) * BASE_BLOCK_SIZE;
+
+        int64_t dbOffset = (roundId % 2) * (coreNum * blockNumPerCore * BASE_BLOCK_SIZE);
+        if (index == 0) {
+            src[index].out = gm_c + dbOffset + curCoreIndex * blockNumPerCore * BASE_BLOCK_SIZE;
+        } else {
+            src[index].out = src[index - 1].out + src[index - 1].k * BASE_BLOCK_SIZE;
+        }
+
+        if (!sparseFlag && switchIndex < iC) {
+            src[index].left = gm_a + bnOffsetLeftMatrix  + qRightOffsetSection;
+            src[index].right = gm_b + bnOffsetGqaRightMatrix + kRightOffsetSection;
+            src[index].k = k;
+            index++;
+        } else if (!sparseFlag && iC <= switchIndex && iC + k - 1 > switchIndex) {
+            src[index].left = gm_a + bnOffsetLeftMatrix + qLeftOffsetSection;
+            src[index].right = gm_b + bnOffsetGqaRightMatrix + kLeftOffsetSection;
+            src[index].k = switchIndex - iC + 1;
+
+            src[index + 1].left = gm_a + bnOffsetLeftMatrix + qRightOffsetSection;
+            src[index + 1].right = gm_b + bnOffsetGqaRightMatrix;
+            src[index + 1].out = src[index].out + src[index].k * BASE_BLOCK_SIZE;
+            src[index + 1].k = k - src[index].k;
+            index += 2;
+        } else if (!sparseFlag && iC <= switchIndex && iC + k - 1 <= switchIndex) {
+            src[index].left = gm_a + bnOffsetLeftMatrix + qLeftOffsetSection;
+            src[index].right = gm_b + bnOffsetGqaRightMatrix + kLeftOffsetSection;
+            src[index].k = k;
+            index++;
+        } else {
+            src[index].left = gm_a + bnOffsetLeftMatrix + qSparseOffsetSection;
+            src[index].right = gm_b + bnOffsetGqaRightMatrix + kSparseOffsetSection;
+            src[index].k = k;
+            index++;
+        }
+    }
+
+    *srcLength = index;
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T> __aicore__ __inline__ void  CubeBackward<TYPE, IF_BF16>::addrMapping_cube2(
+        __gm__ T *__restrict__ left,
+        __gm__ TYPE *__restrict__ right,
+        __gm__ float *__restrict__ out,
+        int64_t roundId,
+        PhyAddrCube2<TYPE, T> *src,
+        int64_t &len) {
+    int64_t curAddrLen = 0;
+    Addr addr[MAX_SWITCH_TIME];
+    addrMappingPre(addr, curAddrLen, roundId);
+    auto BASE_BLOCK_SIZE = this->headDim * this->headDim;
+
+    int64_t index = 0;
+    for (int i = 0; i < curAddrLen; ++i) {
+        int64_t iR = addr[i].iR;
+        int64_t iC = addr[i].iC;
+        int64_t k = addr[i].k;
+        int64_t switchIndex = this->isTri ? blocksPerColumn + iR : blocksPerRow;
+        int64_t rowIndexLeftSection = this->isTri ? blocksPerColumn + iR :iR;
+        int64_t rowIndexRightSection = this->isTri ? blocksPerColumn - 1 - iR : iR;
+        int64_t colIndexLeftSection = iC;
+        int64_t colIndexRightSection =  this->isTri ?  (iC - switchIndex - 1) : iC;
+        int64_t gIndex = addr[i].n / (this->headNum / this->gqaGroupNum);
+        int64_t bnOffsetGqaRightMatrix = (addr[i].b * this->gqaGroupNum + gIndex) * this->seqLenK * this->headDim;
+        int64_t bnOffsetLeftMatrix = ((addr[i].b * this->headNum + addr[i].n) * blocksPerRow * blocksPerColumn) *
+                                     BASE_BLOCK_SIZE;
+        int64_t bnOffsetRightMatrix = (addr[i].b * this->headNum + addr[i].n) * this->seqLenK * this->headDim;
+        int64_t bn_offset_out = (addr[i].b * this->headNum + addr[i].n) * this->seqLenQ * this->headDim;
+
+        bool sparseFlag = false;
+        int64_t windowBlockSize = this->windowLength / BASE_LEN;
+        if (this->isTri && this->sparseMode == 1) {
+            sparseFlag = iR > ((windowBlockSize -1) / 2) ? true: false;
+            switchIndex = (windowBlockSize / 2)+ iR;
+            rowIndexLeftSection = (windowBlockSize / 2) + iR;
+            rowIndexRightSection = (windowBlockSize / 2) - 1 - iR;
+            colIndexLeftSection = iC;
+            colIndexRightSection = iC - switchIndex - 1;
+        }
+        int64_t rowIndexSparseSection = iR + (windowBlockSize / 2);
+        int64_t colIndexSparseSection = iR + iC - (windowBlockSize / 2);
+        int64_t dbOffset = (roundId % 2) * (coreNum * blockNumPerCore * BASE_BLOCK_SIZE);
+        if (index == 0) {
+            src[index].left = left + dbOffset + curCoreIndex * blockNumPerCore * BASE_BLOCK_SIZE;
+        } else {
+            src[index].left = src[index - 1].left + src[index - 1].k * BASE_BLOCK_SIZE;
+        }
+
+        if (!sparseFlag && switchIndex < iC) {
+            src[index].right = right + bnOffsetGqaRightMatrix + colIndexRightSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_out + rowIndexRightSection * BASE_BLOCK_SIZE;
+            src[index].k = k;
+            ++index;
+        } else if (!sparseFlag && iC <= switchIndex && iC + k -1 > switchIndex) {
+            src[index].right = right + bnOffsetGqaRightMatrix + colIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_out + rowIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].k = switchIndex - iC + 1;
+
+            src[index + 1].left = src[index].left + src[index].k * BASE_BLOCK_SIZE;
+            src[index + 1].right = right + bnOffsetGqaRightMatrix;
+            src[index + 1].out = out + bn_offset_out + rowIndexRightSection * BASE_BLOCK_SIZE;
+            src[index + 1].k = k - src[index].k;
+            index += 2;
+        } else if (!sparseFlag && iC <= switchIndex && iC + k -1 <= switchIndex) {
+            src[index].right = right + bnOffsetGqaRightMatrix + colIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_out + rowIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].k = k;
+            index++;
+        } else {
+            src[index].right = right + bnOffsetGqaRightMatrix + colIndexSparseSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_out + rowIndexSparseSection * BASE_BLOCK_SIZE;
+            src[index].k = k;
+            ++index;
+        }
+    }
+    len = index;
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::addrMapping_cube3(
+    __gm__ T * __restrict__ left, __gm__ TYPE * __restrict__ right, __gm__ T_OUTPUT * __restrict__ out, int64_t roundId,
+    PhyAddrCube3<TYPE, T>* src, int64_t& len) {
+
+    int64_t curAddrLen = 0;
+    Addr addr[MAX_SWITCH_TIME];
+    addrMappingPre(addr, curAddrLen, roundId);
+    auto BASE_BLOCK_SIZE =  this->headDim * this->headDim;
+
+    int64_t index = 0;
+    for (int i = 0; i < curAddrLen; ++i) {
+        int64_t iR = addr[i].iR;
+        int64_t iC = addr[i].iC;
+        int64_t k = addr[i].k;
+
+        int64_t switchIndex = this->isTri ? blocksPerColumn + iR : blocksPerRow;
+        int64_t rowIndexLeftSection = this->isTri ? blocksPerColumn + iR :iR;
+        int64_t rowIndexRightSection = this->isTri ? blocksPerColumn - 1 - iR : iR;
+        int64_t colIndexLeftSection = iC;
+        int64_t colIndexRightSection =  this->isTri ?  (iC - switchIndex - 1) : iC;
+
+        int64_t bnOffsetLeftMatrix = ((addr[i].b * this->headNum + addr[i].n) * blocksPerRow * blocksPerColumn) *
+                                     BASE_BLOCK_SIZE;  // 当前b,n下的偏移量
+        int64_t bnOffsetRightMatrix = (addr[i].b * this->headNum + addr[i].n) * this->seqLenQ *
+                                      this->headDim;   // 当前b,n下的偏移量
+        int64_t bn_offset_out = (addr[i].b * this->headNum + addr[i].n) * this->seqLenK *
+                                this->headDim;         // 当前b,n下的偏移量
+        int64_t gIndex = addr[i].n / (this->headNum / this->gqaGroupNum);
+        int64_t bn_offset_gqa_out_matrix = (addr[i].b * this->gqaGroupNum + gIndex) * this->seqLenK * this->headDim;
+        bool sparseFlag = false;
+        int64_t windowBlockSize = this->windowLength / BASE_LEN;
+        if (this->isTri && this->sparseMode == 1) {
+            sparseFlag = iR > ((windowBlockSize -1) / 2) ? true: false;
+            switchIndex = (windowBlockSize / 2)+ iR;
+            rowIndexLeftSection = (windowBlockSize / 2) + iR;
+            rowIndexRightSection = (windowBlockSize / 2) - 1 - iR;
+            colIndexLeftSection = iC;
+            colIndexRightSection = iC - switchIndex - 1;
+        }
+        int64_t rowIndexSparseSection = iR + (windowBlockSize / 2);
+        int64_t colIndexSparseSection = iR + iC - (windowBlockSize / 2);
+        int64_t dbOffset = (roundId % 2) * (coreNum * blockNumPerCore * BASE_BLOCK_SIZE);
+
+        if (index == 0) {
+            src[index].left = left + dbOffset + curCoreIndex * blockNumPerCore * BASE_BLOCK_SIZE;
+        } else {
+            src[index].left = src[index - 1].left + src[index - 1].k * BASE_BLOCK_SIZE;
+        }
+
+        if (!sparseFlag && switchIndex < iC) {
+            src[index].right = right + bnOffsetRightMatrix + rowIndexRightSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_gqa_out_matrix + colIndexRightSection * BASE_BLOCK_SIZE;
+            src[index].k = k;
+            ++index;
+        } else if (!sparseFlag && iC <= switchIndex && iC + k -1 > switchIndex) {
+            src[index].right = right + bnOffsetRightMatrix + rowIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_gqa_out_matrix + colIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].k = switchIndex - iC + 1;
+
+            src[index + 1].left = src[index].left + src[index].k * BASE_BLOCK_SIZE;
+            src[index + 1].right = right + bnOffsetRightMatrix + rowIndexRightSection * BASE_BLOCK_SIZE;
+            src[index + 1].out = out + bn_offset_gqa_out_matrix;
+            src[index + 1].k = k - src[index].k;
+            index += 2;
+        } else if (!sparseFlag && iC <= switchIndex && iC + k -1 <= switchIndex) {
+            src[index].right = right + bnOffsetRightMatrix + rowIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_gqa_out_matrix + colIndexLeftSection * BASE_BLOCK_SIZE;
+            src[index].k = k;
+            index++;
+        } else {
+            src[index].right = right + bnOffsetRightMatrix + rowIndexSparseSection * BASE_BLOCK_SIZE;
+            src[index].out = out + bn_offset_gqa_out_matrix + colIndexSparseSection * BASE_BLOCK_SIZE;
+            src[index].k = k;
+            ++index;
+        }
+    }
+    len = index;
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackward<TYPE, IF_BF16>::Mat_mix_cube2_cube3(int64_t roundId) {
+
+    PhyAddrCube3<TYPE, WORKSPACE_DTYPE_DP> src_dK[MAX_SWITCH_TIME];
+    int64_t switchIndex = 0;
+    addrMapping_cube3(this->gm_dP, this->gmQ, this->gmDK, roundId, src_dK, switchIndex);
+
+    PhyAddrCube2<TYPE, WORKSPACE_DTYPE_DP> src_cube2[MAX_SWITCH_TIME];
+    int64_t src_length_cube2 = 0;
+    addrMapping_cube2(this->gm_dP, this->gmK, this->gmDQ, roundId, src_cube2, src_length_cube2);
+
+    cube2_cube3_mix(src_cube2, src_length_cube2, src_dK, switchIndex, 2);
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackward<TYPE, IF_BF16>::Run_mix() {
+
+    SetPadding(0);
+    uint64_t config = 0x1;
+    AscendC::SetNdParaImpl(config);
+    int64_t continuedBlocksNum = 2;
+
+    int64_t blocksPerBatch = headNum * blocksPerRow * blocksPerColumn;
+    int64_t allBlocks = batchSize * blocksPerBatch ;
+    int64_t Z = (allBlocks + blockNumPerCore * coreNum - 1) / (blockNumPerCore * coreNum);
+
+    int64_t switchIndex = 0;
+    int64_t src_length_cube2 = 0;
+    int64_t remain_length_cube2 = 0;
+    uint64_t mode;
+    uint64_t sync_config;
+
+    if (Z == 1) {
+        if (curCoreIndex * blockNumPerCore < allBlocks) {
+            cube1(0);
+        }
+        FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+
+        WaitFlagDev(AIV2AICFLAGID);
+
+        if (curCoreIndex * blockNumPerCore < allBlocks) {
+            PhyAddrCube3<TYPE, WORKSPACE_DTYPE> srcdV[MAX_SWITCH_TIME];
+            switchIndex = 0;
+            addrMapping_cube3(this->gmS, this->gmDO, this->gmDV, 0, srcdV, switchIndex);
+            cube3_matmul(srcdV, switchIndex, EVENT_ID3, EVENT_ID4, continuedBlocksNum);
+            Mat_mix_cube2_cube3(0);
+        }
+    } else {
+        for (int64_t roundId = 0; roundId < 2; roundId++) {
+            if (roundId * blockNumPerCore * coreNum + curCoreIndex * blockNumPerCore < allBlocks && isCube1) {
+                cube1(roundId);
+            }
+            if (isSyn) {
+                if (roundId == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            }
+        }
+
+        for (int64_t roundId =2; roundId < Z; roundId++) {
+            if (isSyn) {
+                WaitFlagDev(AIV2AICFLAGID);
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            }
+            if ((roundId - 2) * blockNumPerCore * coreNum + curCoreIndex * blockNumPerCore < allBlocks) {
+                if (isCube3) {
+                    PhyAddrCube3<TYPE, WORKSPACE_DTYPE> srcdV[MAX_SWITCH_TIME];
+                    switchIndex = 0;
+                    addrMapping_cube3(this->gmS, this->gmDO, this->gmDV, roundId - 2, srcdV, switchIndex);
+                    cube3_matmul(srcdV, switchIndex, EVENT_ID3, EVENT_ID4, continuedBlocksNum);
+                    Mat_mix_cube2_cube3(roundId - 2);
+                }
+            }
+            if (roundId * blockNumPerCore * coreNum + curCoreIndex * blockNumPerCore < allBlocks && isCube1) {
+                cube1(roundId);
+            }
+        }
+
+        for (int64_t roundId = 0; roundId < 2; roundId++) {
+            if (isSyn) {
+                WaitFlagDev(AIV2AICFLAGID);
+                if (roundId == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            }
+            if ((roundId + Z - 2) * blockNumPerCore * coreNum + curCoreIndex * blockNumPerCore < allBlocks) {
+                if (isCube3) {
+                    PhyAddrCube3<TYPE, WORKSPACE_DTYPE> srcdV[MAX_SWITCH_TIME];
+                    switchIndex = 0;
+                    addrMapping_cube3(this->gmS, this->gmDO, this->gmDV, roundId+Z-2, srcdV, switchIndex);
+                    cube3_matmul(srcdV, switchIndex, EVENT_ID3, EVENT_ID4, continuedBlocksNum);
+                    Mat_mix_cube2_cube3(roundId + Z - 2);
+                }
+            }
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T>
+__aicore__ __inline__ void CubeBackward<TYPE, IF_BF16>::cube2_cube3_mix(
+    const PhyAddrCube2<TYPE, T> *cube2_addr, int64_t cube2_length, const PhyAddrCube3<TYPE, T> *cube3_addr,
+    int64_t cube3_length, int64_t vcoreNumPerHead) {
+
+    if (cube2_length == 0 || cube3_length == 0) return;
+
+    auto l0a_buf_cube3 = l0_a_base_tensor;
+    auto l0b_buf_cube3 = l0_b_base_tensor;
+    auto l0c_buf_cube3 = l0_c_base_tensor;
+
+    auto l0a_buf_cube2 = l0_a_base_tensor[BASE_BLOCK];
+    auto l0b_buf_cube2 = l0_b_base_tensor[BASE_BLOCK];
+    auto l0c_buf_cube2 = l0_c_base_tensor[BASE_BLOCK];
+
+    int64_t switchIndexList[MAX_SWITCH_TIME] = {0};
+    int64_t totalLen = 0;
+
+    for (int64_t srcIndex = 0; srcIndex < cube2_length; srcIndex ++) {
+        switchIndexList[srcIndex] = totalLen;
+        totalLen += cube2_addr[srcIndex].k;
+    }
+
+    SET_FLAG(MTE1, MTE2, EVENT_ID0);
+    SET_FLAG(MTE1, MTE2, EVENT_ID1);
+    SET_FLAG(MTE1, MTE2, EVENT_ID2);
+    SET_FLAG(MTE1, MTE2, EVENT_ID3);
+    SET_FLAG(MTE1, MTE2, EVENT_ID4);
+    SET_FLAG(MTE1, MTE2, EVENT_ID5);
+    SET_FLAG(MTE1, MTE2, EVENT_ID6);
+    SET_FLAG(MTE1, MTE2, EVENT_ID7);
+
+    commonNd2NzParams.nValue = BASE_LEN;
+    commonNd2NzParams.dValue = BASE_LEN;
+    commonNd2NzParams.srcDValue = BASE_LEN;
+    commonNd2NzParams.srcNdMatrixStride = 0;
+    commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+    commonNd2NzParams.dstNzMatrixStride = 0;
+    temp_tensor_bf16.SetGlobalBuffer(
+        reinterpret_cast<__gm__ TYPE *>(cube3_addr[0].right));
+    AscendC::DataCopy(
+        l1_base_b_cube1_tensor,
+        temp_tensor_bf16,
+        commonNd2NzParams
+    );
+
+    SET_FLAG(MTE2, MTE1, EVENT_ID0);
+    WAIT_FLAG(MTE2, MTE1, EVENT_ID0);
+
+    for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+        commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+        commonLoadData2dParamsTranspose.srcStride = BASE_LEN / BLOCK_SIZE;
+        AscendC::LoadData(
+            l0b_buf_cube3[i * BASE_LEN * BLOCK_SIZE],
+            l1_base_b_cube1_tensor[i * CUBE_MATRIX_SIZE],
+            commonLoadData2dParamsTranspose
+        );
+    }
+
+    int64_t pingFlagCube2 = 1;
+    int64_t pingFlagCube3 = 1;
+    int64_t mte1MadPingFlag = 1;
+
+    SET_FLAG(M, MTE1, EVENT_ID0);
+    SET_FLAG(M, MTE1, EVENT_ID1);
+    SET_FLAG(M, MTE1, EVENT_ID2);
+    SET_FLAG(M, MTE1, EVENT_ID3);
+    SET_FLAG(M, MTE1, EVENT_ID4);
+    SET_FLAG(M, MTE1, EVENT_ID5);
+    SET_FLAG(M, MTE1, EVENT_ID6);
+    SET_FLAG(M, MTE1, EVENT_ID7);
+
+    int64_t curSwitchIndex = 0;
+    int64_t curSwitch  = switchIndexList[curSwitchIndex];
+    int unit_flag;
+    for (int64_t kIdx = 0; kIdx < totalLen; kIdx ++) {
+
+        if ((cube2_length >= 2 && kIdx == switchIndexList[curSwitchIndex+1])) {
+            curSwitchIndex ++;
+            curSwitch = switchIndexList[curSwitchIndex];
+        }
+
+        int64_t actualKIdx = kIdx - curSwitch;
+        int64_t iC = actualKIdx;
+
+        pingFlagCube2 = 1 - pingFlagCube2;
+        pingFlagCube3 = 1 - pingFlagCube3;
+        mte1MadPingFlag = 1 - mte1MadPingFlag;
+        auto l1BufACube2 = pingFlagCube2 ? l1_base_a_cube2_tensor : l1_base_a_cube2_tensor[L1_PINGPONG_BUFFER_LEN];
+        auto l1BufBCube2 = pingFlagCube2 ? l1_base_b_cube2_tensor : l1_base_b_cube2_tensor[L1_PINGPONG_BUFFER_LEN];
+
+        auto eventIdCube2 = pingFlagCube2 ? EVENT_ID4 : EVENT_ID7;
+        auto eventIdCube3 = pingFlagCube3 ? EVENT_ID3 : EVENT_ID6;
+
+        auto mte1Cube2MadEventId = mte1MadPingFlag ? EVENT_ID4 : EVENT_ID3;
+        auto mte1Cube3MadEventId = mte1MadPingFlag ? EVENT_ID3 : EVENT_ID4;
+
+        WAIT_FLAG(MTE1, MTE2, eventIdCube3);
+        int64_t localOffsetA =  actualKIdx * BASE_LEN * BASE_LEN;
+        temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(
+            cube3_addr[curSwitchIndex].left + localOffsetA));
+        AscendC::DataCopy(
+            l1BufACube2,
+            temp_tensor_bf16,
+            AscendC::Nd2NzParams(
+                vcoreNumPerHead,
+                BASE_LEN / vcoreNumPerHead,
+                BASE_LEN,
+                BASE_LEN / vcoreNumPerHead * SIZE_128 * 2,
+                BASE_LEN,
+                BASE_LEN,
+                1,
+                BASE_LEN * BLOCK_SIZE / vcoreNumPerHead
+            )
+        );
+
+        SET_FLAG(MTE2, MTE1, eventIdCube3);
+        WAIT_FLAG(MTE2, MTE1, eventIdCube3);
+        WAIT_FLAG(M, MTE1, mte1Cube3MadEventId);
+
+        for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+            commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+            commonLoadData2dParamsTranspose.srcStride = SRC_STRIDE_1;
+            AscendC::LoadData(
+                l0a_buf_cube3[i * BASE_LEN * BLOCK_SIZE],
+                l1BufACube2[i * BASE_LEN * BLOCK_SIZE],
+                commonLoadData2dParamsTranspose
+            );
+        }
+
+        commonNd2NzParams.nValue = BASE_LEN;
+        commonNd2NzParams.dValue = BASE_LEN;
+        commonNd2NzParams.srcDValue = BASE_LEN;
+        commonNd2NzParams.srcNdMatrixStride = 0;
+        commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+        commonNd2NzParams.dstNzMatrixStride = 0;
+        if (cube2_length >= 2 && curSwitchIndex >= 1 && kIdx == curSwitch) {
+            temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(cube3_addr[curSwitchIndex].right));
+            AscendC::DataCopy(
+                l1_base_b_cube1_tensor[(curSwitchIndex % 2) * BASE_LEN * BASE_LEN],
+                temp_tensor_bf16,
+                commonNd2NzParams
+            );
+
+            SET_FLAG(MTE2, MTE1, eventIdCube3);
+            WAIT_FLAG(MTE2, MTE1, eventIdCube3);
+
+            for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+                commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = BASE_LEN / BLOCK_SIZE;
+                AscendC::LoadData(
+                    l0b_buf_cube3[i * BASE_LEN * BLOCK_SIZE],
+                    l1_base_b_cube1_tensor[(curSwitchIndex % 2) * BASE_BLOCK +  i * CUBE_MATRIX_SIZE],
+                    commonLoadData2dParamsTranspose
+                );
+            }
+        }
+
+        SET_FLAG(MTE1, M, mte1Cube3MadEventId);  // mte1MadEventId
+        WAIT_FLAG(MTE1, M, mte1Cube3MadEventId); // mte1MadEventId
+
+        unit_flag = 0b11;
+        commonMadParams.unitFlag = unit_flag;
+        commonMadParams.cmatrixInitVal = true;
+        AscendC::Mmad(
+            l0c_buf_cube3,
+            l0a_buf_cube3,
+            l0b_buf_cube3,
+            commonMadParams
+        );
+
+        SET_FLAG(M, MTE1, mte1Cube3MadEventId); // mte1MadEventId
+
+        AscendC::SetAtomicAdd<float>();
+        AscendC::SetAtomicType<float>();
+        int64_t localOffsetGmC = actualKIdx * BASE_LEN * BASE_LEN;
+        commonFixpipeParamsV220.unitFlag = unit_flag;
+        temp_tensor_fp32.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(
+            cube3_addr[curSwitchIndex].out  + localOffsetGmC));
+        AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+            temp_tensor_fp32,
+            l0c_buf_cube3,
+            commonFixpipeParamsV220
+        );
+        AscendC::SetAtomicNone();
+
+        // 计算cube2
+        if (true) {
+            for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+                commonLoadData2dParamsNoTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+                commonLoadData2dParamsNoTranspose.srcStride = BASE_LEN / BLOCK_SIZE;
+                AscendC::LoadData(
+                    l0a_buf_cube2[i * BASE_LEN * BLOCK_SIZE],
+                    l1BufACube2[i * CUBE_MATRIX_SIZE],
+                    commonLoadData2dParamsNoTranspose
+                );
+            }
+
+            SET_FLAG(MTE1, MTE2, eventIdCube3);
+            WAIT_FLAG(MTE1, MTE2, eventIdCube2);
+
+            commonNd2NzParams.nValue = BASE_LEN;
+            commonNd2NzParams.dValue = BASE_LEN;
+            commonNd2NzParams.srcDValue = BASE_LEN;
+            commonNd2NzParams.srcNdMatrixStride = BASE_LEN * BASE_LEN;
+            commonNd2NzParams.dstNzMatrixStride = BASE_LEN * BASE_LEN;
+            commonNd2NzParams.dstNzC0Stride = BASE_LEN;
+            temp_tensor_bf16.SetGlobalBuffer(
+            reinterpret_cast<__gm__ TYPE *>(cube2_addr[curSwitchIndex].right + actualKIdx * BASE_LEN * BASE_LEN));
+            AscendC::DataCopy(
+                l1BufBCube2,
+                temp_tensor_bf16,
+                commonNd2NzParams
+            );
+
+            SET_FLAG(MTE2, MTE1, eventIdCube2);
+            WAIT_FLAG(M, MTE1, mte1Cube2MadEventId);
+            WAIT_FLAG(MTE2, MTE1, eventIdCube2);
+
+            for (int64_t i = 0; i < BASE_LEN / BLOCK_SIZE; i++) {
+                commonLoadData2dParamsTranspose.repeatTimes = BASE_LEN / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = BASE_LEN / BLOCK_SIZE;
+                AscendC::LoadData(
+                    l0b_buf_cube2[i * BASE_LEN * BLOCK_SIZE],
+                    l1BufBCube2[i * CUBE_MATRIX_SIZE],
+                    commonLoadData2dParamsTranspose
+                );
+            }
+
+            SET_FLAG(MTE1, MTE2, eventIdCube2);
+            SET_FLAG(MTE1, M, mte1Cube2MadEventId);
+            WAIT_FLAG(MTE1, M, mte1Cube2MadEventId);
+
+            bool init_c = (actualKIdx == 0);
+            bool out_c = (actualKIdx == cube2_addr[curSwitchIndex].k - 1);
+            unit_flag = out_c ? 0b11 : 0b10;
+            commonMadParams.unitFlag = unit_flag;
+            commonMadParams.cmatrixInitVal = init_c;
+            AscendC::Mmad(
+                l0c_buf_cube2,
+                l0a_buf_cube2,
+                l0b_buf_cube2,
+                commonMadParams
+            );
+
+            SET_FLAG(M, MTE1, mte1Cube2MadEventId);
+
+            if (out_c) {
+                AscendC::SetAtomicAdd<float>();
+                AscendC::SetAtomicType<float>();
+                commonFixpipeParamsV220.unitFlag = unit_flag;
+                temp_tensor_fp32.SetGlobalBuffer(
+                    reinterpret_cast<__gm__ float *>(cube2_addr[curSwitchIndex].out));
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    temp_tensor_fp32,
+                    l0c_buf_cube2,
+                    commonFixpipeParamsV220
+                );
+                AscendC::SetAtomicNone();
+            }
+        }
+    }
+
+    WAIT_FLAG(M, MTE1, EVENT_ID0);
+    WAIT_FLAG(M, MTE1, EVENT_ID1);
+    WAIT_FLAG(M, MTE1, EVENT_ID2);
+    WAIT_FLAG(M, MTE1, EVENT_ID3);
+    WAIT_FLAG(M, MTE1, EVENT_ID4);
+    WAIT_FLAG(M, MTE1, EVENT_ID5);
+    WAIT_FLAG(M, MTE1, EVENT_ID6);
+    WAIT_FLAG(M, MTE1, EVENT_ID7);
+
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID6);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID7);
+}
+
+}
+
+#endif // __DAV_C220_CUBE__
+
+#endif // __CUBEBACKWARD_H__
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward_band_op.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward_band_op.h
new file mode 100644
index 00000000..74d759c2
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward_band_op.h
@@ -0,0 +1,1031 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __CUBEBACKWARD_BAND_OP_H__
+#define __CUBEBACKWARD_BAND_OP_H__
+
+#define SET_FLAG(trigger, waiter, e) AscendC::SetFlag<AscendC::HardEvent::trigger##_##waiter>((e))
+#define WAIT_FLAG(trigger, waiter, e) AscendC::WaitFlag<AscendC::HardEvent::trigger##_##waiter>((e))
+
+#include "address_mapping_cube.h"
+#include "ppmatmul_const_grad.h"
+#include "kernels/utils/kernel/utils.h"
+#include "kernels/utils/kernel/iterator.h"
+
+using namespace AscendC;
+
+#ifdef __DAV_C220_CUBE__
+
+
+namespace CUBE_BACK_BAND_OP {
+
+constexpr int32_t SIZE_16 = 16;
+constexpr int32_t SIZE_32 = 32;
+constexpr int32_t SIZE_64 = 64;
+constexpr int32_t SIZE_128 = 128;
+constexpr int32_t SIZE_256 = 256;
+constexpr int32_t SIZE_384 = 384;
+constexpr int32_t SIZE_ONE_K = 1024;
+constexpr int32_t SIZE_LONG_BLOCK = 16384;
+
+using WORKSPACE_DTYPE = float;
+using WORKSPACE_DTYPE_DP = float;
+
+template <typename TYPE, bool IF_BF16>
+class CubeBackwardBandOp {
+public:
+    __aicore__ inline CubeBackwardBandOp() {}
+
+    __aicore__ inline void Init(__gm__ TYPE *__restrict__ gm_Q,
+        __gm__ TYPE *__restrict__ gm_K,
+        __gm__ TYPE *__restrict__ gm_V,
+        __gm__ TYPE *__restrict__ gm_dO,
+        __gm__ float *__restrict__ gm_dP,
+        __gm__ float *__restrict__ gm_S,
+        __gm__ float *__restrict__ gm_dQ,
+        __gm__ float *__restrict__ gm_dK,
+        __gm__ float *__restrict__ gm_dV,
+        bool isTri, int64_t qSize, int64_t kSize, int64_t batch, int64_t head, int64_t group_size,
+        int64_t base_length, int64_t sparseMode, int64_t windowLength, int64_t blockNumPerCore);
+    
+    __aicore__ inline void Run();
+    
+    __aicore__ inline void Run_mix();
+    
+    __aicore__ __inline__ void Run_op();
+
+protected:
+    __aicore__ __inline__ void clear_flag();
+
+    __aicore__ __inline__ void preset_flag();
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ inline __attribute__((always_inline)) void cube3_matmul_op(
+        const Address::PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len,
+        int64_t vcore_num_per_head, int64_t copy_matrix_stride, int64_t right_matrix_stride, int64_t out_matrix_stride);
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ inline __attribute__((always_inline)) void cube2_matmul_op(
+        const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len,
+        int64_t vcore_num_per_head, uint64_t n_offset, uint64_t n_step, uint64_t n_size, uint64_t n_size_k);
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ inline __attribute__((always_inline)) void cube1_matmul_op(
+        const Address::PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len);
+
+    __aicore__ __inline__ void cube1_base_matmul(LocalTensor<TYPE> *l1_a_tensor, LocalTensor<TYPE>* l1_b_tensor,
+        __gm__ float *gm_out, int32_t ky, int32_t out_put_matrix_line_strid, bool upper_right_flag);
+
+protected:
+    AsdopsBuffer<ArchType::ASCEND_V220> asdopsBuf;
+
+    __gm__ TYPE *__restrict__ gm_Q{nullptr};
+    __gm__ TYPE *__restrict__ gm_K{nullptr};
+    __gm__ TYPE *__restrict__ gm_V{nullptr};
+    __gm__ TYPE *__restrict__ gm_dO{nullptr};
+
+    __gm__ float *__restrict__ gm_dP{nullptr};
+    __gm__ float *__restrict__ gm_S{nullptr};
+    __gm__ float *__restrict__ gm_dQ{nullptr};
+    __gm__ float *__restrict__ gm_dK{nullptr};
+    __gm__ float *__restrict__ gm_dV{nullptr};
+
+    __gm__ uint8_t *__restrict__ ffts_addr{nullptr};
+    __gm__ uint8_t *__restrict__ tiling_para_gm{nullptr};
+
+    AscendC::Nd2NzParams commonNd2NzParams {
+        1,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        1,
+        0
+    };
+
+    AscendC::Nd2NzParams commonNd2NzParamsFp32 {
+        2,
+        64,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH * BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        1,
+        BASE_BLOCK_LENGTH * BLOCK_SIZE / 2
+    };
+
+    AscendC::LoadData2dParams commonLoadData2dParamsTranspose {
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        0,
+        true,
+        0
+    };
+
+    AscendC::LoadData2dParams commonLoadData2dParamsNoTranspose {
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        0,
+        false,
+        0
+    };
+
+    AscendC::MmadParams commonMadParams {
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        3,
+        false,
+        true
+    };
+
+    AscendC::FixpipeParamsV220 commonFixpipeParamsV220 {
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        false
+    };
+
+    LocalTensor<TYPE> l1_base_a_cube1_tensor;
+    LocalTensor<TYPE> l1_base_b_cube1_tensor;
+    LocalTensor<TYPE> l1_base_a_cube2_tensor;
+    LocalTensor<TYPE> l1_base_b_cube2_tensor;
+
+    LocalTensor<TYPE> l1_a_ping_tensor;
+    LocalTensor<TYPE> l1_a_pong_tensor;
+    LocalTensor<TYPE> l1_b_ping_tensor;
+    LocalTensor<TYPE> l1_b_pong_tensor;
+    
+    LocalTensor<TYPE> l1_a_ping_double_tensor;
+    LocalTensor<TYPE> l1_a_pong_double_tensor;
+    LocalTensor<TYPE> l1_b_ping_double_tensor;
+    LocalTensor<TYPE> l1_b_pong_double_tensor;
+
+    GlobalTensor<TYPE> temp_tensor_bf16;
+    GlobalTensor<float> temp_tensor_fp32;
+
+    uint32_t ping_pong_flag_l1_a_ = 0;
+    uint32_t ping_pong_flag_l1_b_ = 0;
+
+    // L0A L0B
+    LocalTensor<TYPE> l0_a_ping_tensor;
+    LocalTensor<TYPE> l0_a_pong_tensor;
+    LocalTensor<TYPE> l0_b_ping_tensor;
+    LocalTensor<TYPE> l0_b_pong_tensor;
+
+    // L0C
+    LocalTensor<float> l0_c_buf_tensor;
+    LocalTensor<float> l0_c_ping_tensor;
+    LocalTensor<float> l0_c_pong_tensor;
+
+    uint32_t ping_pong_flag_l0_a_ = 0;
+    uint32_t ping_pong_flag_l0_b_ = 0;
+    uint32_t ping_pong_flag_l0_c_ = 0;
+
+    int64_t core_num;
+    int64_t cur_core_index;
+    int64_t blockNumPerCore;
+    int64_t batchSize;
+    int64_t headNum;
+    int64_t gqa_group_num;
+    int64_t headDim;
+    int64_t seqLenQ;
+    int64_t seqLenK;
+    int64_t blocks_per_column;
+    int64_t blocks_per_row;
+    bool isTri;
+    int64_t sparseMode;
+    int64_t windowLength;
+
+    bool is_cube1;
+    bool is_cube2;
+    bool is_cube3;
+    bool is_syn;
+
+    Address::AddressMappingCube<TYPE> address;
+};
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackwardBandOp<TYPE, IF_BF16>::preset_flag()
+{
+    SET_FLAG(MTE1, MTE2, EVENT_ID0);
+    SET_FLAG(MTE1, MTE2, EVENT_ID1);
+    SET_FLAG(MTE1, MTE2, EVENT_ID2);
+    SET_FLAG(MTE1, MTE2, EVENT_ID3);
+    SET_FLAG(MTE1, MTE2, EVENT_ID4);
+    SET_FLAG(MTE1, MTE2, EVENT_ID5);
+    SET_FLAG(M, MTE1, EVENT_ID0);
+    SET_FLAG(M, MTE1, EVENT_ID1);
+    SET_FLAG(M, MTE1, EVENT_ID2);
+    SET_FLAG(M, MTE1, EVENT_ID3);
+    SET_FLAG(M, MTE1, EVENT_ID4);
+    SET_FLAG(M, MTE1, EVENT_ID5);
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackwardBandOp<TYPE, IF_BF16>::clear_flag()
+{
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID4);
+    WAIT_FLAG(MTE1, MTE2, EVENT_ID5);
+
+    WAIT_FLAG(M, MTE1, EVENT_ID0);
+    WAIT_FLAG(M, MTE1, EVENT_ID1);
+    WAIT_FLAG(M, MTE1, EVENT_ID2);
+    WAIT_FLAG(M, MTE1, EVENT_ID3);
+    WAIT_FLAG(M, MTE1, EVENT_ID4);
+    WAIT_FLAG(M, MTE1, EVENT_ID5);
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackwardBandOp<TYPE, IF_BF16>::cube1_base_matmul(
+    LocalTensor<TYPE> *l1_a_tensor, LocalTensor<TYPE> *l1_b_tensor,
+    __gm__ float *gm_out, int32_t ky, int32_t out_put_matrix_line_strid, bool upper_right_flag)
+{
+    auto l1_n_size_ = 128;
+    auto l1_m_size_ = ky * 128;
+    auto k0_ = 128;
+    auto n0_ = 128;
+    auto m0_ = 128;
+    auto n_ = 128;
+
+    for (int n_offset = 0; n_offset < l1_n_size_; n_offset += SIZE_128) {
+        LocalTensor<TYPE>* l0_b_tensor = ping_pong_flag_l0_b_ ? &l0_b_pong_tensor : &l0_b_ping_tensor;
+
+        WAIT_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+        if (l1_n_size_ == SIZE_128) {
+            commonLoadData2dParamsNoTranspose.repeatTimes = k0_ * n0_ / SIZE_256;
+            commonLoadData2dParamsNoTranspose.srcStride = 1;
+            AscendC::LoadData(
+                *l0_b_tensor,
+                *l1_b_tensor,
+                commonLoadData2dParamsNoTranspose
+            );
+        } else {
+            commonLoadData2dParamsNoTranspose.repeatTimes = n0_ / SIZE_16;
+            commonLoadData2dParamsNoTranspose.srcStride = 1;
+            for (int i = 0; i < k0_ / SIZE_16; i++) {
+                AscendC::LoadData(
+                    (*l0_b_tensor)[i * n0_ * SIZE_16],
+                    (*l1_b_tensor)[i * l1_n_size_ * SIZE_16 + n_offset * SIZE_16],
+                    commonLoadData2dParamsNoTranspose
+                );
+            }
+        }
+
+        SET_FLAG(MTE1, M, ping_pong_flag_l0_b_ + 2);
+        WAIT_FLAG(MTE1, M, ping_pong_flag_l0_b_ + 2);
+
+        for (int m_offset = 0; m_offset < l1_m_size_; m_offset += SIZE_128) {
+            bool l0_skip_flag = (upper_right_flag && m_offset == 0);
+            LocalTensor<TYPE>* l0_a_tensor = ping_pong_flag_l0_a_ ? &l0_a_pong_tensor : &l0_a_ping_tensor;
+            LocalTensor<float>* l0_c_tensor = ping_pong_flag_l0_c_ ? &l0_c_pong_tensor : &l0_c_ping_tensor;
+
+            WAIT_FLAG(M, MTE1, ping_pong_flag_l0_a_);
+            if (!l0_skip_flag) {
+                commonLoadData2dParamsNoTranspose.repeatTimes = k0_ / SIZE_16;
+                commonLoadData2dParamsNoTranspose.srcStride = l1_m_size_ / SIZE_16;
+                for (int32_t i = 0; i < m0_ / SIZE_16; i++) {
+                    AscendC::LoadData(
+                        (*l0_a_tensor)[i * k0_ * SIZE_16],
+                        (*l1_a_tensor)[m_offset * SIZE_16 + i * SIZE_256],
+                        commonLoadData2dParamsNoTranspose
+                    );
+                }
+            }
+            SET_FLAG(MTE1, M, ping_pong_flag_l0_a_);
+
+            WAIT_FLAG(MTE1, M, ping_pong_flag_l0_a_);
+            if (!l0_skip_flag) {
+                commonMadParams.unitFlag = 3;
+                commonMadParams.cmatrixInitVal = true;
+                AscendC::Mmad(
+                    *l0_c_tensor,
+                    *l0_a_tensor,
+                    *l0_b_tensor,
+                    commonMadParams
+                );
+            }
+            SET_FLAG(M, MTE1, ping_pong_flag_l0_a_);
+            auto out_offset = (m_offset / 128) * out_put_matrix_line_strid + n_offset;
+
+            if (!l0_skip_flag) {
+                commonFixpipeParamsV220.dstStride = n_;
+                temp_tensor_fp32.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_out + out_offset));
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    temp_tensor_fp32,
+                    *l0_c_tensor,
+                    commonFixpipeParamsV220
+                );
+            }
+
+            ping_pong_flag_l0_c_ = 1 - ping_pong_flag_l0_c_;
+            ping_pong_flag_l0_a_ = 1 - ping_pong_flag_l0_a_;
+        }
+
+        SET_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+        ping_pong_flag_l0_b_ = 1 - ping_pong_flag_l0_b_;
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ inline __attribute__((always_inline)) void CubeBackwardBandOp<TYPE, IF_BF16>::cube3_matmul_op(
+    const Address::PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len, int64_t vcore_num_per_head,
+    int64_t copy_matrix_stride, int64_t right_matrix_stride, int64_t out_matrix_stride)
+{
+    LocalTensor<TYPE>* l1_base_b_cube3_tensor = ping_pong_flag_l1_b_ ? &l1_b_pong_tensor : &l1_b_ping_tensor;
+
+    auto lineStride = src[0].lineStride;
+    auto ky = src[0].ky;
+    int64_t base_len = 128;
+    int64_t switch_index_list[MAX_SWITCH_TIME] = {0};
+    int64_t total_len = 0;
+    for (int64_t src_index = 0; src_index < src_len; src_index++) {
+        switch_index_list[src_index] = total_len;
+        total_len += src[src_index].kx;
+    }
+
+    int64_t Flag_l1b_PingPong = 1;
+    int64_t Flag_l1a_PingPong = 1;
+    int64_t Flag_L0a_PingPong = 1;
+    int64_t Flag_L0b_PingPong = 1;
+    int64_t Flag_L0c_PingPong = 1;
+    int64_t mte1_mad_ping_flag = 1;
+
+    int32_t cur_switch_index = 0;
+    int32_t cur_switch = switch_index_list[cur_switch_index];
+
+    WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+
+    for (auto idx = 0; idx < ky; idx++) {
+        commonNd2NzParams.nValue = base_len;
+        commonNd2NzParams.dValue = copy_matrix_stride;
+        commonNd2NzParams.srcDValue = right_matrix_stride;
+        commonNd2NzParams.dstNzC0Stride = base_len;
+        temp_tensor_bf16.SetGlobalBuffer(
+            reinterpret_cast<__gm__ TYPE *>(src[0].right + idx * 128 * right_matrix_stride));
+        AscendC::DataCopy(
+            (*l1_base_b_cube3_tensor)[idx * 128 * 128],
+            temp_tensor_bf16,
+            commonNd2NzParams
+        );
+    }
+
+    SET_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+    WAIT_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+
+    WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_a_ + 2);
+
+    for (int32_t k_idx = 0; k_idx < total_len; k_idx++) {
+        if (k_idx != 0 && (src_len >= 2 && k_idx == switch_index_list[cur_switch_index + 1])) {
+            cur_switch = switch_index_list[cur_switch_index + 1];
+            lineStride = src[cur_switch_index + 1].lineStride;
+            ky = src[cur_switch_index + 1].ky;
+            SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+            WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+            for (auto idx = 0; idx < ky; idx++) {
+                commonNd2NzParams.nValue = base_len;
+                commonNd2NzParams.dValue = copy_matrix_stride;
+                commonNd2NzParams.srcDValue = right_matrix_stride;
+                commonNd2NzParams.dstNzC0Stride = base_len;
+                temp_tensor_bf16.SetGlobalBuffer(
+                    reinterpret_cast<__gm__ TYPE *>(src[cur_switch_index + 1].right + idx * 128 * right_matrix_stride));
+                AscendC::DataCopy(
+                    (*l1_base_b_cube3_tensor)[idx * 128 * 128],
+                    temp_tensor_bf16,
+                    commonNd2NzParams
+                );
+            }
+            SET_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+            WAIT_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+        }
+        int32_t actual_k_idx = k_idx - cur_switch;
+        int32_t iC = actual_k_idx;
+        bool is_skip_up = false;
+        for (int32_t m_idx = 0; m_idx < ky; m_idx++) {
+            LocalTensor<TYPE>* l0a_buf_tensor = ping_pong_flag_l0_a_ ? &l0_a_pong_tensor : &l0_a_ping_tensor;
+            LocalTensor<TYPE>* l1_buf_a_cube3_tensor = Flag_l1a_PingPong ?
+                                (ping_pong_flag_l1_a_ ? &l1_a_pong_tensor : &l1_a_ping_tensor)
+                                : (ping_pong_flag_l1_a_ ? &l1_a_pong_double_tensor : &l1_a_ping_double_tensor);
+
+            auto event_id_cube3 = Flag_l1a_PingPong ? EVENT_ID4 : EVENT_ID5;
+
+            LocalTensor<TYPE>* l0b_buf_tensor = ping_pong_flag_l0_b_ ? &l0_b_pong_tensor : &l0_b_ping_tensor;
+
+            if (k_idx == 0 || ((src_len >= 2 && k_idx == switch_index_list[cur_switch_index + 1]))) {
+                if (k_idx != 0) {
+                    SET_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+                }
+                WAIT_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+                commonLoadData2dParamsTranspose.repeatTimes = base_len / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = base_len / BLOCK_SIZE;
+                for (int32_t i = 0; i < base_len / BLOCK_SIZE; i++) {
+                    AscendC::LoadData(
+                        (*l0b_buf_tensor)[i * base_len * BLOCK_SIZE],
+                        (*l1_base_b_cube3_tensor)[m_idx * 128 * 128 + i * CUBE_MATRIX_SIZE],
+                        commonLoadData2dParamsTranspose
+                    );
+                }
+            }
+
+            WAIT_FLAG(MTE1, MTE2, event_id_cube3);
+            int32_t tmp_left_idx = cur_switch_index;
+            if (src_len >= 2 && k_idx == switch_index_list[cur_switch_index + 1]) {
+                tmp_left_idx = cur_switch_index + 1;
+            }
+            auto gm_left = src[tmp_left_idx].left + lineStride * m_idx;
+            bool is_skip = false;
+            if (iC == 0 && m_idx == ky - 1 && src[tmp_left_idx].lowerLeft) {
+                is_skip = true;
+            }
+            if (iC == src[tmp_left_idx].kx - 1 && m_idx == 0 && src[tmp_left_idx].upperRight) {
+                is_skip = true;
+            }
+            int32_t local_offset_a = actual_k_idx * base_len * base_len;
+
+            if (!is_skip) {
+                temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_left + local_offset_a));
+                AscendC::DataCopy(
+                    *l1_buf_a_cube3_tensor,
+                    temp_tensor_bf16,
+                    commonNd2NzParamsFp32
+                );
+            }
+
+            SET_FLAG(MTE2, MTE1, event_id_cube3);
+            WAIT_FLAG(MTE2, MTE1, event_id_cube3);
+            WAIT_FLAG(M, MTE1, ping_pong_flag_l0_a_);
+
+            if (!is_skip) {
+                commonLoadData2dParamsTranspose.repeatTimes = 128 / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = 1;
+                for (int32_t i = 0; i < 128 / BLOCK_SIZE; i++) {
+                    AscendC::LoadData(
+                        (*l0a_buf_tensor)[i * 128 * BLOCK_SIZE],
+                        (*l1_buf_a_cube3_tensor)[i * 128 * BLOCK_SIZE],
+                        commonLoadData2dParamsTranspose
+                    );
+                }
+            }
+            SET_FLAG(MTE1, MTE2, event_id_cube3);
+
+            SET_FLAG(MTE1, M, ping_pong_flag_l0_a_);
+            WAIT_FLAG(MTE1, M, ping_pong_flag_l0_a_);
+
+            bool init_c = (m_idx == 0);
+            if (is_skip_up && m_idx == 1) {
+                init_c = true;
+            }
+
+            bool out_c = (m_idx == (ky - 1));
+            int unit_flag = 0b10;
+            if (out_c) {
+                unit_flag = 0b11;
+            }
+            LocalTensor<float> *l0c_buf_tensor = ping_pong_flag_l0_c_ ? &l0_c_pong_tensor : &l0_c_ping_tensor;
+
+            if (!is_skip) {
+                commonMadParams.unitFlag = unit_flag;
+                commonMadParams.cmatrixInitVal = init_c;
+                AscendC::Mmad(
+                    *l0c_buf_tensor,
+                    *l0a_buf_tensor,
+                    *l0b_buf_tensor,
+                    commonMadParams
+                );
+            }
+
+            if (k_idx == total_len - 1) {
+                SET_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+            }
+
+            SET_FLAG(M, MTE1, ping_pong_flag_l0_a_);
+            if (out_c) {
+                AscendC::SetAtomicAdd<float>();
+                AscendC::SetAtomicType<float>();
+                int32_t local_offset_gm_c = actual_k_idx * base_len * out_matrix_stride;
+                int32_t src_out_idx = cur_switch_index;
+                if ((src_len >= 2 && k_idx == switch_index_list[cur_switch_index + 1])) {
+                    src_out_idx += 1;
+                }
+                commonFixpipeParamsV220.dstStride = out_matrix_stride;
+                temp_tensor_fp32.SetGlobalBuffer(
+                    reinterpret_cast<__gm__ float *>(src[src_out_idx].out + local_offset_gm_c));
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    temp_tensor_fp32,
+                    *l0c_buf_tensor,
+                    commonFixpipeParamsV220
+                );
+                AscendC::SetAtomicNone();
+                ping_pong_flag_l0_c_ = 1 - ping_pong_flag_l0_c_;
+            }
+            is_skip_up = is_skip;
+            ping_pong_flag_l0_b_ = 1 - ping_pong_flag_l0_b_;
+            ping_pong_flag_l0_a_ = 1 - ping_pong_flag_l0_a_;
+            Flag_l1a_PingPong = 1 - Flag_l1a_PingPong;
+        }
+
+        if ((src_len >= 2 && k_idx == switch_index_list[cur_switch_index + 1])) {
+            cur_switch_index++;
+        }
+    }
+    SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+    SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_a_ + 2);
+
+    ping_pong_flag_l1_b_ = 1 - ping_pong_flag_l1_b_;
+    ping_pong_flag_l1_a_ = 1 - ping_pong_flag_l1_a_;
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ inline __attribute__((always_inline)) void CubeBackwardBandOp<TYPE, IF_BF16>::cube2_matmul_op(
+    const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len,
+    int64_t vcore_num_per_head, uint64_t n_offset, uint64_t n_step, uint64_t n_size, uint64_t n_size_k)
+{
+    int64_t l1_m = CUBE2_LENGTH_M;               // 128
+    int64_t l1_k = CUBE2_LENGTH_K;               // 128
+    int64_t l1_n = n_step;                       // 128
+    int64_t l0_k = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_m = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_n = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_k_block_num = l0_k / BLOCK_SIZE;  // 16
+    int64_t l0_n_block_num = l0_n / BLOCK_SIZE;  // 16
+
+    int64_t SIZE_256 = 256;
+    int64_t SIZE_128 = 128;
+    int64_t SIZE_16 = 16;
+    auto n_ = n_size;
+    auto n0_ = n_step;
+    auto n_k = n_size_k;
+
+    auto m0_ = 128;
+    auto k0_ = 128;
+
+    for (int32_t idx = 0; idx < src_len; ++idx) {
+        auto left_start_addr = src[idx].left;
+        auto right_start_addr = src[idx].right;
+        auto result_gm = src[idx].out;
+        int32_t Ky = src[idx].ky;
+        int32_t Kx = src[idx].kx;
+        int32_t lineStride = src[idx].lineStride;
+        bool upperRight = src[idx].upperRight;
+        bool lowerLeft = src[idx].lowerLeft;
+
+        for (int32_t i = 0; i < Kx; i++) {
+            bool l1_skip_flag = (upperRight && i == Kx - 1);
+            bool last_k = (i >= Kx - 1);
+            bool l0_c_init_flag = (i == 0);
+
+            LocalTensor<TYPE>* l1_a_buf_tensor = ping_pong_flag_l1_a_ ? &l1_a_pong_tensor : &l1_a_ping_tensor;
+            LocalTensor<TYPE>* l1_b_buf_tensor = ping_pong_flag_l1_b_ ? &l1_b_pong_tensor : &l1_b_ping_tensor;
+
+            WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+            WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_a_ + 2);
+
+            if (!l1_skip_flag) {
+                temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(left_start_addr + i * l1_m * l1_k));
+                AscendC::DataCopy(
+                    *l1_a_buf_tensor,
+                    temp_tensor_bf16,
+                    commonNd2NzParamsFp32
+                );
+            }
+            temp_tensor_bf16.SetGlobalBuffer(
+                reinterpret_cast<__gm__ TYPE *>(left_start_addr + i * l1_k * l1_m + lineStride));
+            AscendC::DataCopy(
+                (*l1_a_buf_tensor)[SIZE_128 * SIZE_128],
+                temp_tensor_bf16,
+                commonNd2NzParamsFp32
+            );
+            
+            commonNd2NzParams.nValue = l1_k;
+            commonNd2NzParams.dValue = l1_n;
+            commonNd2NzParams.srcDValue = n_k;
+            commonNd2NzParams.dstNzC0Stride = l1_k;
+            temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(right_start_addr + i * l1_k * n_k));
+            AscendC::DataCopy(
+                *l1_b_buf_tensor,
+                temp_tensor_bf16,
+                commonNd2NzParams
+            );
+            
+            SET_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+            WAIT_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+            SET_FLAG(MTE2, MTE1, ping_pong_flag_l1_a_ + 2);
+            WAIT_FLAG(MTE2, MTE1, ping_pong_flag_l1_a_ + 2);
+
+            for (int32_t n_offset = 0; n_offset < l1_n; n_offset += SIZE_128) {
+                LocalTensor<TYPE>* l0_b_buf_tensor = ping_pong_flag_l0_b_ ? &l0_b_pong_tensor : &l0_b_ping_tensor;
+                WAIT_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+                commonLoadData2dParamsTranspose.repeatTimes = n0_ / SIZE_16;
+                commonLoadData2dParamsTranspose.srcStride = l1_k / SIZE_16;
+                for (int nn = 0; nn < k0_ / SIZE_16; nn++) {
+                    AscendC::LoadData(
+                        (*l0_b_buf_tensor)[nn * n0_ * SIZE_16],
+                        (*l1_b_buf_tensor)[n_offset * l1_k + nn * SIZE_256],
+                        commonLoadData2dParamsTranspose
+                    );
+                }
+                SET_FLAG(MTE1, M, ping_pong_flag_l0_b_ + 2);
+                WAIT_FLAG(MTE1, M, ping_pong_flag_l0_b_ + 2);
+
+                for (int32_t j = 0; j < Ky; j++) {
+                    bool l0_skip_flag = (l1_skip_flag && j == 0);
+                    bool last_k = (i == Kx - 1 && j != 0) || (i == Kx - 1 && j == 0 && !upperRight) ||
+                                  (upperRight && i == Kx - 2 && j == 0);
+                    LocalTensor<TYPE>* l0_a_buf_tensor = ping_pong_flag_l0_a_ ? &l0_a_pong_tensor : &l0_a_ping_tensor;
+                    LocalTensor<float>* l0_c_buf_tensor = ping_pong_flag_l0_c_ ? &l0_c_pong_tensor : &l0_c_ping_tensor;
+                    WAIT_FLAG(M, MTE1, ping_pong_flag_l0_a_);
+                    if (!l0_skip_flag) {
+                        commonLoadData2dParamsNoTranspose.repeatTimes = k0_ / SIZE_16;
+                        commonLoadData2dParamsNoTranspose.srcStride = l1_m / SIZE_16;
+                        for (int32_t jj = 0; jj < m0_ / SIZE_16; jj++) {
+                            AscendC::LoadData(
+                                (*l0_a_buf_tensor)[jj * k0_ * SIZE_16],
+                                (*l1_a_buf_tensor)[j * SIZE_128 * SIZE_128 + jj * SIZE_256],
+                                commonLoadData2dParamsNoTranspose
+                            );
+                        }
+                    }
+                    SET_FLAG(MTE1, M, ping_pong_flag_l0_a_);
+                    WAIT_FLAG(MTE1, M, ping_pong_flag_l0_a_);
+
+                    commonMadParams.unitFlag = last_k ? 3 : 2;
+                    commonMadParams.cmatrixInitVal = l0_c_init_flag;
+                    AscendC::Mmad(
+                        *l0_c_buf_tensor,
+                        *l0_a_buf_tensor,
+                        *l0_b_buf_tensor,
+                        commonMadParams
+                    );
+
+                    if (last_k) {
+                        AscendC::SetAtomicType<float>();
+                        commonFixpipeParamsV220.dstStride = n_;
+                        temp_tensor_fp32.SetGlobalBuffer(
+                            reinterpret_cast<__gm__ float *>(result_gm + j * SIZE_128 * n_ + n_offset));
+                        AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                            temp_tensor_fp32,
+                            *l0_c_buf_tensor,
+                            commonFixpipeParamsV220
+                        );
+                        AscendC::SetAtomicNone();
+                    }
+
+                    SET_FLAG(M, MTE1, ping_pong_flag_l0_a_);
+                    ping_pong_flag_l0_a_ = 1 - ping_pong_flag_l0_a_;
+                    ping_pong_flag_l0_c_ = 1 - ping_pong_flag_l0_c_;
+                }
+                SET_FLAG(M, MTE1, ping_pong_flag_l0_b_ + 2);
+                ping_pong_flag_l0_b_ = 1 - ping_pong_flag_l0_b_;
+            }
+            SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+            SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_a_ + 2);
+
+            ping_pong_flag_l1_a_ = 1 - ping_pong_flag_l1_a_;
+            ping_pong_flag_l1_b_ = 1 - ping_pong_flag_l1_b_;
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ inline __attribute__((always_inline)) void CubeBackwardBandOp<TYPE, IF_BF16>::cube1_matmul_op(
+    const Address::PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len)
+{
+    for (int32_t idx = 0; idx < src_len; ++idx) {
+        auto kx = src[idx].kx;
+        auto ky = src[idx].ky;
+        auto lineStride = src[idx].lineStride;
+
+        int32_t m_loop = 1;
+        int32_t n_loop = kx;
+        auto l1_m_size_ = ky * 128;
+        auto l1_k_size_ = 128;
+        auto l1_n_size_ = 128;
+        auto k_ = 128;
+        auto n_ = 128;
+
+        auto gm_a = src[idx].left;
+        auto gm_b = src[idx].right;
+        auto gm_c = src[idx].out;
+        bool upperRight = src[idx].upperRight;
+        bool lowerLeft = src[idx].lowerLeft;
+
+        commonNd2NzParams.nValue = l1_m_size_;
+        commonNd2NzParams.dValue = l1_k_size_;
+        commonNd2NzParams.srcDValue = l1_k_size_;
+        commonNd2NzParams.dstNzC0Stride = l1_m_size_;
+        for (int m_index = 0; m_index < m_loop; m_index++) {
+            LocalTensor<TYPE>* l1_a_tensor = ping_pong_flag_l1_a_ ? &l1_a_pong_tensor : &l1_a_ping_tensor;
+            WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_a_ + 2);
+            temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_a));
+            AscendC::DataCopy(
+                *l1_a_tensor,
+                temp_tensor_bf16,
+                commonNd2NzParams
+            );
+
+            SET_FLAG(MTE2, MTE1, ping_pong_flag_l1_a_ + 2);
+            WAIT_FLAG(MTE2, MTE1, ping_pong_flag_l1_a_ + 2);
+            
+            commonNd2NzParams.nValue = l1_k_size_;
+            commonNd2NzParams.dValue = l1_n_size_;
+            commonNd2NzParams.srcDValue = l1_n_size_;
+            commonNd2NzParams.dstNzC0Stride = l1_k_size_;
+            for (int n_index = 0; n_index < n_loop; n_index++) {
+                bool upper_right_flag = (upperRight && n_index == n_loop - 1);
+                LocalTensor<TYPE>* l1_b_tensor = ping_pong_flag_l1_b_ ? &l1_b_pong_tensor : &l1_b_ping_tensor;
+                WAIT_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+                temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_b + n_index * l1_n_size_ * k_));
+                AscendC::DataCopy(
+                    *l1_b_tensor,
+                    temp_tensor_bf16,
+                    commonNd2NzParams
+                );
+
+                SET_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+                WAIT_FLAG(MTE2, MTE1, ping_pong_flag_l1_b_);
+
+                cube1_base_matmul(l1_a_tensor, l1_b_tensor, gm_c, ky, lineStride, upper_right_flag);
+
+                SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_b_);
+                ping_pong_flag_l1_b_ = 1 - ping_pong_flag_l1_b_;
+                gm_c += SIZE_128 * SIZE_128;
+            }
+
+            SET_FLAG(MTE1, MTE2, ping_pong_flag_l1_a_ + 2);
+            ping_pong_flag_l1_a_ = 1 - ping_pong_flag_l1_a_;
+        }
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackwardBandOp<TYPE, IF_BF16>::Run_op()
+{
+    AscendC::SetLoadDataPaddingValue<uint64_t>(0);
+    uint64_t config = 0x1;
+    AscendC::SetNdParaImpl(config);
+
+    preset_flag();
+    int64_t Z = address.get_total_rounds();
+
+    if (Z == 1) {
+        for (int64_t roundId = 0; roundId < Z; roundId++) {
+            if (address.is_running(roundId) && is_cube1) {
+                // cube1
+                int64_t len_S = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_S[MAX_SWITCH_TIME];
+                address.addrMapping_cube1(gm_Q, gm_K, gm_S, addr_S, len_S, roundId, this->headDim, this->headDim);
+                cube1_matmul_op(addr_S, len_S);
+
+                int64_t len_dP = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_dP[MAX_SWITCH_TIME];
+                address.addrMapping_cube1(gm_dO, gm_V, gm_dP, addr_dP, len_dP, roundId, this->headDim, this->headDim);
+                cube1_matmul_op(addr_dP, len_dP);
+            }
+
+            if (is_syn) {
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                WaitFlagDev(AIV2AICFLAGID);
+            }
+
+            if (address.is_running(roundId)) {
+                // cube3
+                int64_t len_dV = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dV[MAX_SWITCH_TIME];
+                address.addrMapping_cube3(gm_S, gm_dO, gm_dV, addr_dV, len_dV, roundId, this->headDim, this->headDim);
+                cube3_matmul_op(addr_dV, len_dV, 2, 128, 128, 128);
+
+                int64_t len_dK = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dK[MAX_SWITCH_TIME];
+                address.addrMapping_cube3(gm_dP, gm_Q, gm_dK, addr_dK, len_dK, roundId, this->headDim, this->headDim);
+                cube3_matmul_op(addr_dK, len_dK, 2, 128, 128, 128);
+
+                // cube2
+                int64_t len_dQ = 0;
+                Address::PhyAddrBackwardCube2<float, TYPE, float> addr_dQ[MAX_SWITCH_TIME];
+                address.addrMapping_cube2(gm_dP, gm_K, gm_dQ, addr_dQ, len_dQ, roundId, this->headDim, this->headDim);
+                cube2_matmul_op(addr_dQ, len_dQ, 2, 0, 128, 128, 128);
+            }
+        }
+    } else {
+        for (int64_t roundId = 0; roundId < 2; roundId++) {
+            if (address.is_running(roundId) && is_cube1) {
+                int64_t len_S = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_S[MAX_SWITCH_TIME];
+                address.addrMapping_cube1(gm_Q, gm_K, gm_S, addr_S, len_S, roundId, this->headDim, this->headDim);
+                cube1_matmul_op(addr_S, len_S);
+
+                int64_t len_dP = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_dP[MAX_SWITCH_TIME];
+                address.addrMapping_cube1(gm_dO, gm_V, gm_dP, addr_dP, len_dP, roundId, this->headDim, this->headDim);
+                cube1_matmul_op(addr_dP, len_dP);
+            }
+
+            if (is_syn) {
+                if (roundId == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            }
+        }
+        // cube2 + cube3 + cube1
+        for (int64_t roundId = 2; roundId < Z; roundId++) {
+            if (is_syn) {
+                WaitFlagDev(AIV2AICFLAGID);
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            }
+
+            if (address.is_running(roundId - 2)) {
+                // cube3
+                int64_t len_dV = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dV[MAX_SWITCH_TIME];
+                address.addrMapping_cube3(gm_S, gm_dO, gm_dV, addr_dV, len_dV,
+                    roundId - 2, this->headDim, this->headDim);
+                cube3_matmul_op(addr_dV, len_dV, 2, 128, 128, 128);
+
+                int64_t len_dK = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dK[MAX_SWITCH_TIME];
+                address.addrMapping_cube3(gm_dP, gm_Q, gm_dK, addr_dK, len_dK,
+                    roundId - 2, this->headDim, this->headDim);
+                cube3_matmul_op(addr_dK, len_dK, 2, 128, 128, 128);
+
+                // cube2
+                int64_t len_dQ = 0;
+                Address::PhyAddrBackwardCube2<float, TYPE, float> addr_dQ[MAX_SWITCH_TIME];
+                address.addrMapping_cube2(gm_dP, gm_K, gm_dQ, addr_dQ, len_dQ,
+                    roundId - 2, this->headDim, this->headDim);
+                cube2_matmul_op(addr_dQ, len_dQ, 2, 0, 128, 128, 128);
+            }
+
+            if (address.is_running(roundId) && is_cube1) {
+                // cube1
+                int64_t len_S = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_S[MAX_SWITCH_TIME];
+                address.addrMapping_cube1(gm_Q, gm_K, gm_S, addr_S, len_S, roundId, this->headDim, this->headDim);
+                cube1_matmul_op(addr_S, len_S);
+
+                int64_t len_dP = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_dP[MAX_SWITCH_TIME];
+                address.addrMapping_cube1(gm_dO, gm_V, gm_dP, addr_dP, len_dP, roundId, this->headDim, this->headDim);
+                cube1_matmul_op(addr_dP, len_dP);
+            }
+        }
+
+        /**** (cube2 + cube3) * 2 ****/
+        for (int64_t roundId = 0; roundId < 2; roundId++) {
+            if (is_syn) {
+                WaitFlagDev(AIV2AICFLAGID);
+                if (roundId == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            }
+
+            if (address.is_running(roundId + Z - 2)) {
+                // cube3
+                int64_t len_dV = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dV[MAX_SWITCH_TIME];
+                address.addrMapping_cube3(gm_S, gm_dO, gm_dV, addr_dV, len_dV,
+                        roundId + Z - 2, this->headDim, this->headDim);
+                cube3_matmul_op(addr_dV, len_dV, 2, 128, 128, 128);
+
+                int64_t len_dK = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dK[MAX_SWITCH_TIME];
+                address.addrMapping_cube3(gm_dP, gm_Q, gm_dK, addr_dK, len_dK,
+                        roundId + Z - 2, this->headDim, this->headDim);
+                cube3_matmul_op(addr_dK, len_dK, 2, 128, 128, 128);
+
+                // cube2
+                int64_t len_dQ = 0;
+                Address::PhyAddrBackwardCube2<float, TYPE, float> addr_dQ[MAX_SWITCH_TIME];
+                address.addrMapping_cube2(gm_dP, gm_K, gm_dQ, addr_dQ,
+                        len_dQ, roundId + Z - 2, this->headDim, this->headDim);
+                cube2_matmul_op(addr_dQ, len_dQ, 2, 0, 128, 128, 128);
+            }
+        }
+    }
+    clear_flag();
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackwardBandOp<TYPE, IF_BF16>::Init(
+    __gm__ TYPE *__restrict__ gm_Q,
+    __gm__ TYPE *__restrict__ gm_K,
+    __gm__ TYPE *__restrict__ gm_V,
+    __gm__ TYPE *__restrict__ gm_dO,
+    __gm__ float *__restrict__ gm_dP,
+    __gm__ float *__restrict__ gm_S,
+    __gm__ float *__restrict__ gm_dQ,
+    __gm__ float *__restrict__ gm_dK,
+    __gm__ float *__restrict__ gm_dV,
+    bool isTri, int64_t qSize, int64_t kSize, int64_t batch, int64_t head, int64_t group_size, int64_t base_length,
+    int64_t sparseMode, int64_t windowLength, int64_t blockNumPerCore)
+{
+    this->gm_Q = gm_Q;
+    this->gm_K = gm_K;
+    this->gm_V = gm_V;
+    this->gm_dO = gm_dO;
+    this->gm_dP = gm_dP;
+    this->gm_S = gm_S;
+
+    this->gm_dQ = gm_dQ;
+    this->gm_dK = gm_dK;
+    this->gm_dV = gm_dV;
+
+    this->seqLenQ = qSize;
+    this->seqLenK = kSize;
+    this->batchSize = batch;
+    this->headNum = head;
+    this->headDim = base_length;
+    this->gqa_group_num = (head + group_size - 1) / group_size;
+    this->blockNumPerCore = blockNumPerCore;
+    this->isTri = isTri;
+    this->sparseMode = sparseMode;
+    this->windowLength = windowLength;
+
+    this->core_num = get_block_num();
+    this->cur_core_index = get_block_idx();
+
+    this->blocks_per_column = this->isTri ? ((this->seqLenQ) / 128 / 2) : ((this->seqLenQ) / 128);
+    this->blocks_per_row = this->isTri ? ((this->seqLenK) / 128 + 1) : ((this->seqLenK) / 128);
+
+    if (this->isTri && this->sparseMode == 1) {
+        this->blocks_per_column = (this->seqLenQ / 128) - (this->windowLength / 128 / 2);
+        this->blocks_per_row = (this->windowLength / 128) + 1;
+    }
+
+    is_cube1 = true;
+    is_cube2 = true;
+    is_cube3 = true;
+    is_syn = true;
+
+    address.init(
+        this->batchSize,
+        this->headNum,
+        this->seqLenQ,
+        this->seqLenK,
+        this->headDim,
+        this->gqa_group_num,
+        this->isTri,
+        this->sparseMode,
+        this->windowLength
+    );
+
+    address.set_backward_tiling(this->core_num, this->cur_core_index, this->blockNumPerCore, 2);
+
+    temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(0));
+    temp_tensor_fp32.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(0));
+
+    // init L1 tensor
+    l1_base_a_cube1_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(0);
+    l1_base_b_cube1_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_128 * SIZE_ONE_K);
+    l1_base_a_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_256 * SIZE_ONE_K);
+    l1_base_b_cube2_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_384 * SIZE_ONE_K);
+
+    l1_a_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(0);
+    l1_a_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_128 * SIZE_ONE_K);
+    l1_b_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_256 * SIZE_ONE_K);
+    l1_b_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_384 * SIZE_ONE_K);
+
+    // workspace ping pong addr shift
+    l1_a_ping_double_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_LONG_BLOCK * 2);
+    l1_a_pong_double_tensor =
+        asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_128 * SIZE_ONE_K + SIZE_LONG_BLOCK * 2);
+    l1_b_ping_double_tensor =
+        asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_256 * SIZE_ONE_K + SIZE_LONG_BLOCK * 2);
+    l1_b_pong_double_tensor =
+        asdopsBuf.GetBuffer<BufferType::ASCEND_CB, TYPE>(SIZE_384 * SIZE_ONE_K + SIZE_LONG_BLOCK * 2);
+
+    // init L0A/L0B/L0C tensor
+    l0_a_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(0);
+    l0_a_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0A, TYPE>(SIZE_32 * SIZE_ONE_K);
+    l0_b_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(0);
+    l0_b_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0B, TYPE>(SIZE_32 * SIZE_ONE_K);
+    l0_c_buf_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+    l0_c_ping_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+    l0_c_pong_tensor = asdopsBuf.GetBuffer<BufferType::ASCEND_L0C, float>(SIZE_64 * SIZE_ONE_K);
+
+    commonFixpipeParamsV220.quantPre = QuantMode_t::NoQuant;
+    commonFixpipeParamsV220.unitFlag = 3;
+}
+
+}  // namespace CUBE_BACK_BAND_OP
+#endif  // __DAV_C220_CUBE__
+
+#endif  // __CUBEFORWARD_H__
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward_band_op_192.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward_band_op_192.h
new file mode 100644
index 00000000..0df012b1
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/cube_backward_band_op_192.h
@@ -0,0 +1,850 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __CUBEBACKWARD_BAND_OP_192_H__
+#define __CUBEBACKWARD_BAND_OP_192_H__
+#ifdef __DAV_C220_CUBE__
+
+#include "address_mapping_cube.h"
+#include "ppmatmul_const_grad.h"
+#include "kernel_operator.h"
+#include "cube_backward_band_op.h"
+
+namespace CUBE_BACK_BAND_OP_192 {
+
+constexpr int32_t src_stride_K = 256;
+constexpr int32_t src_stride_Q = 192;
+constexpr int32_t SIZE_16 = 16;
+constexpr int32_t SIZE_32 = 32;
+constexpr int32_t SIZE_256 = 256;
+constexpr int32_t SIZE_128 = 128;
+constexpr int32_t SIZE_192 = 192;
+
+template <typename TYPE, bool IF_BF16>
+class CubeBackwardBandOp192 : public CUBE_BACK_BAND_OP::CubeBackwardBandOp<TYPE, IF_BF16> {
+public:
+    __aicore__ inline CubeBackwardBandOp192() {}
+
+    __aicore__ __inline__ void Run_op();
+
+private:
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ inline __attribute__((always_inline)) void cube1_matmul_op_192(
+        const Address::PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len);
+
+    __aicore__ __inline__ void cube1_base_matmul_headDim192(
+        LocalTensor<TYPE> *l1_a_tensor, LocalTensor<TYPE> *l1_b_tensor,
+        __gm__ float *gm_out, int32_t ky, int32_t out_put_matrix_line_strid, bool upper_right_flag);
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void cube2_matmul_op_192_base(
+        const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t src_len, int64_t vcore_num_per_head, uint64_t n_offset, uint64_t n_step, uint64_t n_size);
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void cube2_matmul_op_192_left(
+        const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t src_len, int64_t vcore_num_per_head);
+
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    __aicore__ __inline__ void cube3_matmul_op_192_left(
+        const Address::PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src,
+        int64_t src_len, int64_t vcore_num_per_head);
+    
+    __aicore__ __inline__ void Mat_mix_cube2_cube3_op_192_right(int64_t roundId);
+    template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+    
+    __aicore__ inline __attribute__((always_inline)) void cube2_cube3_mix_op_192_right(
+        const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src_cube2, int64_t src_length,
+        const Address::PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src_cube3, int64_t vcore_num_per_head);
+
+    __aicore__ __inline__ void cube2_base_matmul(LocalTensor<TYPE> *l1_a_buf_tensor, LocalTensor<TYPE> &l1_b_buf_tensor,
+        __gm__ float *result_gm, int64_t Kx_start, int64_t Kx_end, int64_t Ky_start, int64_t Ky_end,
+        int64_t Ky, int64_t Kx, uint64_t n_offset, uint64_t n_step, uint64_t n_size, bool upperRight, bool is_skip);
+
+    __aicore__ __inline__ void cube3_base_matmul(LocalTensor<TYPE> *l1_a_buf_tensor, LocalTensor<TYPE> *l1_b_buf_tensor,
+        __gm__ float *result_gm, int64_t Kx_start, int64_t Kx_end, int64_t Ky_start, int64_t Ky_end,
+        int64_t Ky, int64_t Kx, uint64_t n_offset, uint64_t n_step, uint64_t n_size, bool is_skip, bool is_skip_up);
+
+private:
+    AscendC::LoadData2dParams commonLoadData2dParamsTranspose {
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        0,
+        true,
+        0
+    };
+
+    AscendC::LoadData2dParams commonLoadData2dParamsNoTranspose {
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        0,
+        false,
+        0
+    };
+
+    AscendC::Nd2NzParams commonNd2NzParams {
+        1,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        0,
+        BASE_BLOCK_LENGTH,
+        BASE_BLOCK_LENGTH,
+        1,
+        0
+    };
+};
+
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ inline __attribute__((always_inline)) void CubeBackwardBandOp192<TYPE, IF_BF16>::cube1_matmul_op_192(
+    const Address::PhyAddrBackwardCube1<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len)
+{
+    for (int32_t idx = 0; idx < src_len; ++idx) {
+        auto kx = src[idx].kx;
+        auto ky = src[idx].ky;
+
+        auto lineStride = src[idx].lineStride;
+        int32_t m_loop = 1;
+        int32_t n_loop = kx;
+        auto l1_m_size_ = ky * 128;
+
+        auto l1_k_size_ = 192;
+        auto l1_n_size_ = 192;
+        auto k_ = 128;
+        auto n_ = 128;
+
+        auto gm_a = src[idx].left;
+        auto gm_b = src[idx].right;
+        auto gm_c = src[idx].out;
+        bool upperRight = src[idx].upperRight;
+        bool lowerLeft = src[idx].lowerLeft;
+        LocalTensor<TYPE> *l1_a_tensor = this->ping_pong_flag_l1_a_ ? &this->l1_a_pong_tensor : &this->l1_a_ping_tensor;
+
+        WAIT_FLAG(MTE1, MTE2, this->ping_pong_flag_l1_a_ + 2);
+        this->temp_tensor_bf16.SetGlobalBuffer(reinterpret_cast<__gm__ TYPE *>(gm_a));
+        commonNd2NzParams.nValue = l1_m_size_;
+        commonNd2NzParams.dValue = l1_k_size_;
+        commonNd2NzParams.srcDValue = l1_k_size_;
+        commonNd2NzParams.dstNzC0Stride = SIZE_128 * 2;
+        AscendC::DataCopy(
+            *l1_a_tensor,
+            this->temp_tensor_bf16,
+            commonNd2NzParams
+        );
+        SET_FLAG(MTE2, MTE1, this->ping_pong_flag_l1_a_ + 2);
+        WAIT_FLAG(MTE2, MTE1, this->ping_pong_flag_l1_a_ + 2);
+
+        commonNd2NzParams.nValue = SIZE_128;
+        commonNd2NzParams.dValue = src_stride_K;
+        commonNd2NzParams.srcDValue = src_stride_K;
+        commonNd2NzParams.dstNzC0Stride = SIZE_128;
+        
+        for (int n_index = 0; n_index < n_loop; n_index++) {
+            bool upper_right_flag = (upperRight && n_index == n_loop - 1);
+            LocalTensor<TYPE>* l1_b_tensor = this->ping_pong_flag_l1_b_ ?
+                    &this->l1_b_pong_tensor : &this->l1_b_ping_tensor;
+
+            WAIT_FLAG(MTE1, MTE2, this->ping_pong_flag_l1_b_);
+            this->temp_tensor_bf16.SetGlobalBuffer(
+                reinterpret_cast<__gm__ TYPE *>(gm_b + n_index * src_stride_K * 128));
+            AscendC::DataCopy(
+                *l1_b_tensor,
+                this->temp_tensor_bf16,
+                commonNd2NzParams
+            );
+
+            SET_FLAG(MTE2, MTE1, this->ping_pong_flag_l1_b_);
+            WAIT_FLAG(MTE2, MTE1, this->ping_pong_flag_l1_b_);
+
+            cube1_base_matmul_headDim192(l1_a_tensor, l1_b_tensor, gm_c, ky, lineStride, upper_right_flag);
+
+            SET_FLAG(MTE1, MTE2, this->ping_pong_flag_l1_b_);
+            this->ping_pong_flag_l1_b_ = 1 - this->ping_pong_flag_l1_b_;
+
+            gm_c += SIZE_128 * SIZE_128;
+        }
+        SET_FLAG(MTE1, MTE2, this->ping_pong_flag_l1_a_ + 2);
+        this->ping_pong_flag_l1_a_ = 1 - this->ping_pong_flag_l1_a_;
+    }
+}
+
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackwardBandOp192<TYPE, IF_BF16>::cube1_base_matmul_headDim192(
+    LocalTensor<TYPE> *l1_a_tensor, LocalTensor<TYPE> *l1_b_tensor,
+    __gm__ float *gm_out, int32_t ky, int32_t out_put_matrix_line_strid, bool upper_right_flag)
+{
+    auto l1_n_size_ = 192;
+    auto l1_m_size_ = ky * 128;
+    auto k0_ = 192;
+    auto n0_ = 128;
+    auto m0_ = 128;
+    auto n_ = 128;
+    int32_t head_dim = 192;
+    int32_t head_dim_half = 192 / 2;
+
+    for (int idx = 0; idx < 2; idx += 1) {
+        int m_offset = idx * 128;
+        bool l0_skip_flag = (upper_right_flag && m_offset == 0);
+        LocalTensor<float>* l0_c_buf_tensor = this->ping_pong_flag_l0_c_ ?
+                &this->l0_c_pong_tensor : &this->l0_c_ping_tensor;
+
+        for (int32_t m = 0; m < 2; m++) {
+            LocalTensor<TYPE>* l0_a_buf_tensor = this->ping_pong_flag_l0_a_ ?
+                    &this->l0_a_pong_tensor : &this->l0_a_ping_tensor;
+
+            WAIT_FLAG(M, MTE1, this->ping_pong_flag_l0_a_);
+            if (!l0_skip_flag) {
+                int32_t l0a_oneline_offset = 128;
+                if (m == 1) {
+                    l0a_oneline_offset = head_dim - 128;
+                }
+                int32_t repeat_times = l0a_oneline_offset / SIZE_16;
+                commonLoadData2dParamsNoTranspose.repeatTimes = l0a_oneline_offset / SIZE_16;
+                commonLoadData2dParamsNoTranspose.srcStride = 16;
+                for (int32_t i = 0; i < m0_ / SIZE_16; i++) {
+                    AscendC::LoadData(
+                        (*l0_a_buf_tensor)[i * l0a_oneline_offset * SIZE_16],
+                        (*l1_a_tensor)[idx * 128 * 16 + m * 256 * 128 + i * SIZE_256],
+                        commonLoadData2dParamsNoTranspose
+                    );
+                }
+            }
+
+            LocalTensor<TYPE>* l0_b_buf_tensor = this->ping_pong_flag_l0_b_ ?
+                    &this->l0_b_pong_tensor : &this->l0_b_ping_tensor;
+
+            WAIT_FLAG(M, MTE1, this->ping_pong_flag_l0_b_ + 2);
+            if (!l0_skip_flag) {
+                int32_t l0b_col_num = 128;
+                if (m == 1) {
+                    l0b_col_num = head_dim - 128;
+                }
+                commonLoadData2dParamsNoTranspose.repeatTimes = l0b_col_num * n0_ / SIZE_256;
+                commonLoadData2dParamsNoTranspose.srcStride = 1;
+                AscendC::LoadData(
+                    *l0_b_buf_tensor,
+                    (*l1_b_tensor)[m * 128 * 128],
+                    commonLoadData2dParamsNoTranspose
+                );
+            }
+            SET_FLAG(MTE1, M, this->ping_pong_flag_l0_b_ + 2);
+            WAIT_FLAG(MTE1, M, this->ping_pong_flag_l0_b_ + 2);
+
+            // 搬出等于0b11，累加等于0b10
+            int unit_flag = 0b10;
+            if (m == 1) {
+                unit_flag = 0b11;
+            }
+            bool init_c = (m == 0);
+            int32_t l0c_col_num = 128;
+            if (m == 1) {
+                l0c_col_num = head_dim - 128;
+            }
+            if (!l0_skip_flag) {
+                AscendC::Mmad(
+                    *l0_c_buf_tensor,
+                    *l0_a_buf_tensor,
+                    *l0_b_buf_tensor,
+                    AscendC::MmadParams(
+                        m0_,
+                        n0_,
+                        l0c_col_num,
+                        unit_flag,
+                        false,
+                        init_c
+                    )
+                );
+            }
+            SET_FLAG(M, MTE1, this->ping_pong_flag_l0_b_ + 2);
+            SET_FLAG(M, MTE1, this->ping_pong_flag_l0_a_);
+            this->ping_pong_flag_l0_b_ = 1 - this->ping_pong_flag_l0_b_;
+            this->ping_pong_flag_l0_a_ = 1 - this->ping_pong_flag_l0_a_;
+        }
+
+        auto out_offset = (m_offset / 128) * out_put_matrix_line_strid;
+        
+        if (!l0_skip_flag) {
+            auto intriParams = AscendC::FixpipeParamsV220(
+                n0_,
+                m0_,
+                m0_,
+                n_,
+                false
+            );
+            intriParams.quantPre = QuantMode_t::NoQuant;
+            intriParams.unitFlag = 3;
+
+            this->temp_tensor_fp32.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gm_out + out_offset));
+            AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                this->temp_tensor_fp32,
+                *l0_c_buf_tensor,
+                intriParams
+            );
+        }
+        this->ping_pong_flag_l0_c_ = 1 - this->ping_pong_flag_l0_c_;
+    }
+}
+
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackwardBandOp192<TYPE, IF_BF16>::cube2_base_matmul(
+    LocalTensor<TYPE> *l1_a_buf_tensor, LocalTensor<TYPE> &l1_b_buf_tensor, __gm__ float *result_gm,
+    int64_t Kx_start, int64_t Kx_end, int64_t Ky_start, int64_t Ky_end, int64_t Ky, int64_t Kx,
+    uint64_t n_offset, uint64_t n_step, uint64_t n_size, bool upperRight, bool is_skip)
+{
+    n_offset = 128;
+    n_step = 64;
+    n_size = 192;
+    int64_t l1_m = CUBE2_LENGTH_M;               // 128
+    int64_t l1_k = CUBE2_LENGTH_K;               // 128
+    int64_t l1_n = n_step;                       // 128
+    int64_t l0_k = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_m = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_n = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_k_block_num = l0_k / BLOCK_SIZE;  // 16
+    int64_t l0_n_block_num = l0_n / BLOCK_SIZE;  // 16
+
+    int64_t SIZE_256 = 256;
+    int64_t SIZE_128 = 128;
+    int64_t SIZE_16 = 16;
+    auto n_ = n_size;
+    auto n0_ = n_step;
+    auto m0_ = 128;
+    auto k0_ = 128;
+
+    for (int32_t i = Kx_start; i < Kx_end; i++) {
+        bool l1_skip_flag = (upperRight && i == Kx - 1);
+        bool last_k = (i >= Kx - 1);
+        bool l0_c_init_flag = (i == 0);
+        LocalTensor<TYPE>* l0_b_buf_tensor = this->ping_pong_flag_l0_b_ ?
+            &this->l0_b_pong_tensor : &this->l0_b_ping_tensor;
+        WAIT_FLAG(M, MTE1, this->ping_pong_flag_l0_b_ + 2);
+        if (!is_skip) {
+            commonLoadData2dParamsTranspose.repeatTimes = n0_ / SIZE_16;
+            commonLoadData2dParamsTranspose.srcStride = l1_k / SIZE_16;
+            for (int nn = 0; nn < k0_ / SIZE_16; nn++) {
+                AscendC::LoadData(
+                    (*l0_b_buf_tensor)[nn * n0_ * SIZE_16],
+                    l1_b_buf_tensor[nn * SIZE_256],
+                    commonLoadData2dParamsTranspose
+                );
+            }
+        }
+        SET_FLAG(MTE1, M, this->ping_pong_flag_l0_b_ + 2);
+        WAIT_FLAG(MTE1, M, this->ping_pong_flag_l0_b_ + 2);
+
+        for (int32_t j = Ky_start; j < Ky_end; j++) {
+            bool l0_skip_flag = (l1_skip_flag && j == 0);
+            bool last_k = (i == Kx - 1 && j != 0) || (i == Kx - 1 && j == 0 && !upperRight) ||
+                            (upperRight && i == Kx - 2 && j == 0);
+            LocalTensor<TYPE>* l0_a_buf_tensor = this->ping_pong_flag_l0_a_ ?
+                &this->l0_a_pong_tensor : &this->l0_a_ping_tensor;
+            LocalTensor<float> l0_c_buf_tensor = (j == 0) ? this->l0_c_pong_tensor : this->l0_c_pong_tensor[128 * 64];
+
+            WAIT_FLAG(M, MTE1, this->ping_pong_flag_l0_a_);
+            if (!l0_skip_flag) {
+                commonLoadData2dParamsNoTranspose.repeatTimes = k0_ / SIZE_16;
+                commonLoadData2dParamsNoTranspose.srcStride = l1_m / SIZE_16;
+                for (int32_t jj = 0; jj < m0_ / SIZE_16; jj++) {
+                    AscendC::LoadData(
+                        (*l0_a_buf_tensor)[jj * k0_ * SIZE_16],
+                        (*l1_a_buf_tensor)[jj * SIZE_256],
+                        commonLoadData2dParamsNoTranspose
+                    );
+                }
+            }
+            SET_FLAG(MTE1, M, this->ping_pong_flag_l0_a_);
+            WAIT_FLAG(MTE1, M, this->ping_pong_flag_l0_a_);
+
+            if (!l0_skip_flag) {
+                AscendC::Mmad(
+                    l0_c_buf_tensor,
+                    *l0_a_buf_tensor,
+                    *l0_b_buf_tensor,
+                    AscendC::MmadParams(
+                        m0_,
+                        n0_,
+                        k0_,
+                        last_k ? 3 : 2,
+                        false,
+                        l0_c_init_flag
+                    )
+                );
+            }
+
+            if (last_k) {
+                AscendC::SetAtomicType<float>();
+                auto intriParams = AscendC::FixpipeParamsV220(
+                    n0_,
+                    m0_,
+                    m0_,
+                    n_,
+                    false
+                );
+                intriParams.quantPre = QuantMode_t::NoQuant;
+                intriParams.unitFlag = 3;
+                this->temp_tensor_fp32.SetGlobalBuffer(
+                    reinterpret_cast<__gm__ float *>(result_gm + j * SIZE_128 * n_ + n_offset));
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    this->temp_tensor_fp32,
+                    l0_c_buf_tensor,
+                    intriParams
+                );
+                AscendC::SetAtomicNone();
+            }
+
+            SET_FLAG(M, MTE1, this->ping_pong_flag_l0_a_);
+            this->ping_pong_flag_l0_a_ = 1 - this->ping_pong_flag_l0_a_;
+        }
+        SET_FLAG(M, MTE1, this->ping_pong_flag_l0_b_ + 2);
+        this->ping_pong_flag_l0_b_ = 1 - this->ping_pong_flag_l0_b_;
+    }
+}
+
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ __inline__ void CubeBackwardBandOp192<TYPE, IF_BF16>::cube3_base_matmul(
+    LocalTensor<TYPE> *l1_a_buf_tensor, LocalTensor<TYPE> *l1_b_buf_tensor,
+    __gm__ float *result_gm, int64_t Kx_start, int64_t Kx_end, int64_t Ky_start, int64_t Ky_end, int64_t Ky,
+    int64_t Kx, uint64_t n_offset, uint64_t n_step, uint64_t n_size, bool is_skip,
+    bool is_skip_up)
+{
+    auto base_len = 128;
+    auto k_stride = 64;
+    int64_t out_matrix_cols = src_stride_K;
+
+    auto n = 192;
+    auto n1 = 64;
+    auto n0 = 64;
+    for (int64_t k_idx = Kx_start; k_idx < Kx_end; k_idx++) {
+        for (int64_t m_idx = Ky_start; m_idx < Ky_end; m_idx ++) {
+            LocalTensor<TYPE>* l0a_buf_tensor = this->ping_pong_flag_l0_a_ ?
+                    &this->l0_a_pong_tensor : &this->l0_a_ping_tensor;
+            LocalTensor<TYPE>* l0b_buf_tensor = this->ping_pong_flag_l0_b_ ?
+                    &this->l0_b_pong_tensor : &this->l0_b_ping_tensor;
+
+            WAIT_FLAG(M, MTE1, this->ping_pong_flag_l0_b_ + 2);
+            if (!is_skip) {
+                commonLoadData2dParamsTranspose.repeatTimes = n1 / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = base_len / BLOCK_SIZE;
+                for (int32_t i = 0; i < base_len / BLOCK_SIZE; i++) {
+                    AscendC::LoadData(
+                        (*l0b_buf_tensor)[i * 64 * BLOCK_SIZE],
+                        (*l1_b_buf_tensor)[m_idx * 128 * 64 + i * CUBE_MATRIX_SIZE],
+                        commonLoadData2dParamsTranspose
+                    );
+                }
+            }
+
+            WAIT_FLAG(M, MTE1, this->ping_pong_flag_l0_a_);
+            if (!is_skip) {
+                commonLoadData2dParamsTranspose.repeatTimes = 128 / BLOCK_SIZE;
+                commonLoadData2dParamsTranspose.srcStride = 1;
+                for (int32_t i = 0; i < 128 / BLOCK_SIZE; i++) {
+                    AscendC::LoadData(
+                        (*l0a_buf_tensor)[i * 128 * BLOCK_SIZE],
+                        (*l1_a_buf_tensor)[i * 128 * BLOCK_SIZE],
+                        commonLoadData2dParamsTranspose
+                    );
+                }
+            }
+
+            SET_FLAG(MTE1, M, this->ping_pong_flag_l0_a_);
+            WAIT_FLAG(MTE1, M, this->ping_pong_flag_l0_a_);
+
+            bool init_c = (m_idx == 0);
+            if (is_skip_up && m_idx == 1) {
+                init_c = true;
+            }
+            bool out_c = (m_idx == (Ky - 1));
+            int unit_flag = 0b10;
+            if (out_c) {
+                unit_flag = 0b11;
+            }
+
+            auto l0c_buf_tensor = this->ping_pong_flag_l0_c_ ?
+                this->l0_c_ping_tensor : this->l0_c_ping_tensor[128 * 64];
+
+            if (!is_skip) {
+                AscendC::Mmad(
+                    l0c_buf_tensor,
+                    *l0a_buf_tensor,
+                    *l0b_buf_tensor,
+                    AscendC::MmadParams(
+                        128,
+                        n0,
+                        128,
+                        unit_flag,
+                        false,
+                        (init_c == 1) ? true : false
+                    )
+                );
+            }
+            SET_FLAG(M, MTE1, this->ping_pong_flag_l0_b_ + 2);
+            SET_FLAG(M, MTE1, this->ping_pong_flag_l0_a_);
+            if (out_c) {
+                AscendC::SetAtomicAdd<float>();
+                AscendC::SetAtomicType<float>();
+                int32_t local_offset_gm_c = k_idx * base_len * out_matrix_cols + 128;
+                auto intriParams = AscendC::FixpipeParamsV220(
+                    n0,
+                    128,
+                    128,
+                    out_matrix_cols,
+                    false
+                );
+                intriParams.quantPre = QuantMode_t::NoQuant;
+                intriParams.unitFlag = unit_flag;
+                this->temp_tensor_fp32.SetGlobalBuffer(
+                    reinterpret_cast<__gm__ float *>(result_gm + local_offset_gm_c));
+                AscendC::Fixpipe<float, float, AscendC::CFG_ROW_MAJOR>(
+                    this->temp_tensor_fp32,
+                    l0c_buf_tensor,
+                    intriParams
+                );
+                AscendC::SetAtomicNone();
+                this->ping_pong_flag_l0_c_ = 1 - this->ping_pong_flag_l0_c_;
+            }
+
+            this->ping_pong_flag_l0_b_ = 1 - this->ping_pong_flag_l0_b_;
+            this->ping_pong_flag_l0_a_ = 1 - this->ping_pong_flag_l0_a_;
+        }
+    }
+}
+
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ inline __attribute__((always_inline))
+void CubeBackwardBandOp192<TYPE, IF_BF16>::cube2_cube3_mix_op_192_right(
+    const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src_cube2, int64_t src_length,
+    const Address::PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src_cube3, int64_t vcore_num_per_head) {
+    
+    int64_t l1_m = CUBE2_LENGTH_M;               // 128
+    int64_t l1_k = CUBE2_LENGTH_K;               // 128
+    int64_t l1_n = 64;                           // 64
+    int64_t l0_k = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_m = BASE_BLOCK_LENGTH;            // 128
+    int64_t l0_n = 64;                           // 64
+    int64_t l0_k_block_num = l0_k / BLOCK_SIZE;  // 128/16
+    int64_t l0_n_block_num = l0_n / BLOCK_SIZE;  // 128/16
+
+    if (src_length == 0) {
+        return;
+    }
+
+    int64_t Flag_l1b_PingPong = 1;
+    int64_t Flag_l1a_PingPong = 1;
+    int64_t Flag_L0a_PingPong = 1;
+    int64_t Flag_L0b_PingPong = 1;
+    int64_t Flag_L0c_PingPong = 1;
+    int64_t mte1_mad_ping_flag = 1;
+    int64_t Flag_l1b_PingPong_cube2 = 1;
+
+    for (int64_t section_idx = 0; section_idx < src_length; section_idx ++) {
+        auto Kx = src_cube2[section_idx].kx;
+        auto Ky = src_cube2[section_idx].ky;
+        auto lineStride = src_cube2[section_idx].lineStride;
+        
+        auto gm_left_cube2 = src_cube2[section_idx].left;    // dP  4
+        auto gm_right_cube2 = src_cube2[section_idx].right;  // K   1
+        auto gm_out_cube2 = src_cube2[section_idx].out;      // dQ  6
+
+        auto gm_left_cube3 = src_cube3[section_idx].left;    // dP  4
+        auto gm_right_cube3 = src_cube3[section_idx].right;  // Q   0
+        auto gm_out_cube3 = src_cube3[section_idx].out;      // dK  7
+
+        auto lowerLeft = src_cube3[section_idx].lowerLeft;
+        auto upperRight = src_cube3[section_idx].upperRight;
+        LocalTensor<TYPE>* l1_b_buf_cube3_tensor = this->ping_pong_flag_l1_b_ ?
+                &this->l1_b_pong_tensor : &this->l1_b_ping_tensor;
+
+        WAIT_FLAG(MTE1, MTE2, this->ping_pong_flag_l1_b_);
+        commonNd2NzParams.nValue = 128;
+        commonNd2NzParams.dValue = 64;
+        commonNd2NzParams.srcDValue = 192;
+        commonNd2NzParams.dstNzC0Stride = 128;
+        for (auto idx = 0; idx < Ky; idx++) {
+            this->temp_tensor_bf16.SetGlobalBuffer(
+                reinterpret_cast<__gm__ TYPE *>(gm_right_cube3 + idx * 128 * 192 + 128));
+            AscendC::DataCopy(
+                (*l1_b_buf_cube3_tensor)[idx * 128 * 64],
+                this->temp_tensor_bf16,
+                commonNd2NzParams
+            );
+        }
+        SET_FLAG(MTE2, MTE1, this->ping_pong_flag_l1_b_);
+        WAIT_FLAG(MTE2, MTE1, this->ping_pong_flag_l1_b_);
+
+        for (int64_t k_idx = 0 ; k_idx < Kx; k_idx ++) {
+            bool is_skip_up = false;
+            for (int64_t m_idx = 0; m_idx < Ky; m_idx ++) {
+                LocalTensor<TYPE>* l1_buf_a_cube3_tensor =
+                    Flag_l1a_PingPong ?
+                    (this->ping_pong_flag_l1_a_ ? &this->l1_a_pong_tensor : &this->l1_a_ping_tensor)
+                    : (this->ping_pong_flag_l1_a_ ? &this->l1_a_pong_double_tensor : &this->l1_a_ping_double_tensor);
+                auto event_id_cube3 = Flag_l1a_PingPong ? EVENT_ID4 : EVENT_ID5;
+                bool is_skip = false;
+                if (k_idx == 0 && m_idx == Ky - 1 && lowerLeft) {
+                    is_skip = true;
+                }
+                if (k_idx == Kx - 1 && m_idx == 0 && upperRight) {
+                    is_skip = true;
+                }
+
+                WAIT_FLAG(MTE1, MTE2, event_id_cube3);
+                if (!is_skip) {
+                    this->temp_tensor_bf16.SetGlobalBuffer(
+                        reinterpret_cast<__gm__ TYPE *>(gm_left_cube3 + lineStride * m_idx + k_idx * 128 * 128));
+                    AscendC::DataCopy(
+                        *l1_buf_a_cube3_tensor,
+                        this->temp_tensor_bf16,
+                        AscendC::Nd2NzParams(
+                            vcore_num_per_head,
+                            128 / vcore_num_per_head,
+                            128,
+                            128 / vcore_num_per_head * 128 * 2,
+                            128,
+                            128,
+                            1,
+                            128 * BLOCK_SIZE / vcore_num_per_head
+                        )
+                    );
+                }
+                SET_FLAG(MTE2, MTE1, event_id_cube3);
+                WAIT_FLAG(MTE2, MTE1, event_id_cube3);
+
+                cube3_base_matmul(l1_buf_a_cube3_tensor, l1_b_buf_cube3_tensor, gm_out_cube3,
+                    k_idx, k_idx+1, m_idx, m_idx +1, Ky, Kx, 0, 0, 0, is_skip, is_skip_up);
+                SET_FLAG(MTE1, MTE2, event_id_cube3);
+
+                auto event_id_cube2 = Flag_l1b_PingPong ?  EVENT_ID3 : EVENT_ID4;
+                auto l1_b_buf_cube2_tensor = Flag_l1b_PingPong ?
+                        (*l1_b_buf_cube3_tensor)[128 * 64 * 2] : (*l1_b_buf_cube3_tensor)[128 * 64 * 3];
+
+                WAIT_FLAG(MTE1, MTE2, event_id_cube2);
+                if (!is_skip) {
+                    commonNd2NzParams.nValue = 128;
+                    commonNd2NzParams.dValue = 64;
+                    commonNd2NzParams.srcDValue = src_stride_K;
+                    commonNd2NzParams.dstNzC0Stride = 128;
+                    this->temp_tensor_bf16.SetGlobalBuffer(
+                        reinterpret_cast<__gm__ TYPE *>(gm_right_cube2 + k_idx * 128 * src_stride_K + 128));
+                    AscendC::DataCopy(
+                        l1_b_buf_cube2_tensor,
+                        this->temp_tensor_bf16,
+                        commonNd2NzParams
+                    );
+                }
+
+                SET_FLAG(MTE2, MTE1, event_id_cube2);
+                WAIT_FLAG(MTE2, MTE1, event_id_cube2);
+
+                cube2_base_matmul(l1_buf_a_cube3_tensor, l1_b_buf_cube2_tensor, gm_out_cube2, k_idx, k_idx + 1,
+                    m_idx, m_idx+1, Ky, Kx, 0, 0, 0, upperRight, is_skip);
+
+                SET_FLAG(MTE1, MTE2, event_id_cube2);
+
+                Flag_l1a_PingPong = 1 - Flag_l1a_PingPong;
+                Flag_l1b_PingPong = 1 - Flag_l1b_PingPong;
+                is_skip_up = is_skip;
+            }
+
+            this->ping_pong_flag_l1_a_ = 1 - this->ping_pong_flag_l1_a_;
+        }
+        SET_FLAG(MTE1, MTE2, this->ping_pong_flag_l1_b_);
+        this->ping_pong_flag_l1_b_ = 1 - this->ping_pong_flag_l1_b_;
+    }
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ __inline__ void CubeBackwardBandOp192<TYPE, IF_BF16>::cube2_matmul_op_192_left(
+    const Address::PhyAddrBackwardCube2<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len, int64_t vcore_num_per_head)
+{
+    this->cube2_matmul_op(src, src_len, vcore_num_per_head, 0, 128, 192, 256);
+}
+
+template <typename TYPE, bool IF_BF16>
+template <typename T_LEFT, typename T_RIGHT, typename T_OUTPUT>
+__aicore__ __inline__ void CubeBackwardBandOp192<TYPE, IF_BF16>::cube3_matmul_op_192_left(
+    const Address::PhyAddrBackwardCube3<T_LEFT, T_RIGHT, T_OUTPUT> *src, int64_t src_len, int64_t vcore_num_per_head)
+{
+    this->cube3_matmul_op(src, src_len, vcore_num_per_head, 128, 192, 256);
+}
+
+template <typename TYPE, bool IF_BF16>
+__aicore__ inline void CubeBackwardBandOp192<TYPE, IF_BF16>::Run_op()
+{
+    AscendC::SetLoadDataPaddingValue<uint64_t>(0);
+    uint64_t config = 0x1;
+    AscendC::SetNdParaImpl(config);
+
+    this->preset_flag();
+    int64_t Z = this->address.get_total_rounds();
+
+    if (Z == 1) {
+        for (int64_t roundId = 0; roundId < Z; roundId++) {
+            if (this->address.is_running(roundId)) {
+                // cube1
+                int64_t len_S = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_S[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube1(this->gm_Q, this->gm_K, this->gm_S, addr_S,
+                    len_S, roundId, 192, src_stride_K);
+                cube1_matmul_op_192(addr_S, len_S);
+
+                int64_t len_dP = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_dP[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube1(this->gm_dO, this->gm_V, this->gm_dP,
+                    addr_dP, len_dP, roundId, 128, 128);
+                this->cube1_matmul_op(addr_dP, len_dP);
+            }
+
+            if (this->is_syn) {
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                WaitFlagDev(AIV2AICFLAGID);
+            }
+
+            if (this->address.is_running(roundId) && this->is_cube2) {
+                // cube3
+                int64_t len_dV = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dV[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube3(this->gm_S, this->gm_dO, this->gm_dV, addr_dV,
+                    len_dV, roundId, 128, 128);
+                this->cube3_matmul_op(addr_dV, len_dV, 2, 128, 128, 128);
+
+                int64_t len_dK = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dK[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube3(this->gm_dP, this->gm_Q, this->gm_dK, addr_dK,
+                    len_dK, roundId, 192, 256);
+                cube3_matmul_op_192_left(addr_dK, len_dK, 2);
+
+                // cube2
+                int64_t len_dQ = 0;
+                Address::PhyAddrBackwardCube2<float, TYPE, float> addr_dQ[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube2(this->gm_dP, this->gm_K, this->gm_dQ, addr_dQ,
+                    len_dQ, roundId, 192, 256);
+                cube2_matmul_op_192_left(addr_dQ, len_dQ, 2);
+                cube2_cube3_mix_op_192_right(addr_dQ, len_dQ, addr_dK, 2);
+            }
+        }
+    } else {
+        for (int64_t roundId = 0; roundId < 2; roundId++) {
+            if (this->address.is_running(roundId)) {
+                // cube1
+                int64_t len_S = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_S[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube1(this->gm_Q, this->gm_K, this->gm_S, addr_S, len_S, roundId, 192, 256);
+                cube1_matmul_op_192(addr_S, len_S);
+
+                int64_t len_dP = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_dP[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube1(this->gm_dO, this->gm_V, this->gm_dP, addr_dP, len_dP,
+                    roundId, 128, 128);
+                this->cube1_matmul_op(addr_dP, len_dP);
+            }
+
+            if (this->is_syn) {
+                if (roundId == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            }
+        }
+        // cube2 + cube3 + cube1
+        for (int64_t roundId = 2; roundId < Z; roundId++) {
+            if (this->is_syn) {
+                WaitFlagDev(AIV2AICFLAGID);
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            }
+
+            if (this->address.is_running(roundId - 2)) {
+                int64_t len_dV = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dV[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube3(this->gm_S, this->gm_dO, this->gm_dV, addr_dV,
+                    len_dV, roundId - 2, 128, 128);
+                this->cube3_matmul_op(addr_dV, len_dV, 2, 128, 128, 128);
+
+                int64_t len_dK = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dK[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube3(this->gm_dP, this->gm_Q, this->gm_dK, addr_dK,
+                    len_dK, roundId - 2, 192, 256);
+                cube3_matmul_op_192_left(addr_dK, len_dK, 2);
+
+                int64_t len_dQ = 0;
+                Address::PhyAddrBackwardCube2<float, TYPE, float> addr_dQ[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube2(this->gm_dP, this->gm_K, this->gm_dQ, addr_dQ,
+                    len_dQ, roundId - 2, 192, 256);
+                cube2_matmul_op_192_left(addr_dQ, len_dQ, 2);
+
+                cube2_cube3_mix_op_192_right(addr_dQ, len_dQ, addr_dK, 2);
+            }
+
+            if (this->address.is_running(roundId) && this->is_cube1) {
+                // cube1
+                int64_t len_S = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_S[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube1(this->gm_Q, this->gm_K, this->gm_S, addr_S, len_S, roundId, 192, 256);
+                cube1_matmul_op_192(addr_S, len_S);
+
+                int64_t len_dP = 0;
+                Address::PhyAddrBackwardCube1<TYPE, TYPE, float> addr_dP[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube1(this->gm_dO, this->gm_V, this->gm_dP, addr_dP,
+                    len_dP, roundId, 128, 128);
+                this->cube1_matmul_op(addr_dP, len_dP);
+            }
+        }
+
+        // (cube2 + cube3) * 2
+        for (int64_t roundId = 0; roundId < 2; roundId++) {
+            if (this->is_syn) {
+                WaitFlagDev(AIV2AICFLAGID);
+                if (roundId == 0) {
+                    FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                }
+            }
+            if (this->address.is_running(roundId + Z - 2)) {
+                // cube3
+                int64_t len_dV = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dV[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube3(this->gm_S, this->gm_dO, this-> gm_dV, addr_dV,
+                    len_dV, roundId + Z - 2, 128, 128);
+                this->cube3_matmul_op(addr_dV, len_dV, 2, 128, 128, 128);
+
+                int64_t len_dK = 0;
+                Address::PhyAddrBackwardCube3<float, TYPE, float> addr_dK[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube3(this->gm_dP, this->gm_Q, this->gm_dK, addr_dK,
+                    len_dK, roundId + Z - 2, 192, 256);
+                cube3_matmul_op_192_left(addr_dK, len_dK, 2);
+            
+                // cube2
+                int64_t len_dQ = 0;
+                Address::PhyAddrBackwardCube2<float, TYPE, float> addr_dQ[MAX_SWITCH_TIME];
+                this->address.addrMapping_cube2(this->gm_dP, this->gm_K, this->gm_dQ, addr_dQ,
+                    len_dQ, roundId + Z - 2, 192, 256);
+                cube2_matmul_op_192_left(addr_dQ, len_dQ, 2);
+                cube2_cube3_mix_op_192_right(addr_dQ, len_dQ, addr_dK, 2);
+            }
+        }
+    }
+    FftsCrossCoreSync<PIPE_FIX, 0>(AICFLAGID);
+    WaitFlagDev(AICFLAGID);
+    this->clear_flag();
+}
+
+}  // namespace CUBE_BACK_BAND_OP
+#endif  // __DAV_C220_CUBE__
+
+#endif  // __CUBEFORWARD_H__
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/lag_post.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/lag_post.h
new file mode 100644
index 00000000..2b7b5487
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/lag_post.h
@@ -0,0 +1,239 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __LAG_POST_H__
+#define __LAG_POST_H__
+
+#ifdef __DAV_C220_VEC__
+
+#include "ppmatmul_const_grad.h"
+#include "kernel_operator.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/utils.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/simd.h"
+
+namespace LAG_POST {
+
+using namespace AscendC;
+
+constexpr int32_t BUFFER_NUM = 2;
+
+template<typename INPUT_T, bool IF_BF16>
+class LagPost {
+public:
+    __aicore__ inline LagPost() {}
+    __aicore__ inline void Init(__gm__ float * __restrict__ dqWorkspace,
+                                __gm__ float * __restrict__ dkWorkspace,
+                                __gm__ float * __restrict__ dvWorkspace,
+                                __gm__ INPUT_T * __restrict__ dq,
+                                __gm__ INPUT_T * __restrict__ dk,
+                                __gm__ INPUT_T * __restrict__ dv,
+                                int64_t postBaseNum,
+                                int64_t postDqFrontCoreNum,
+                                int64_t postDqFrontDataNum,
+                                int64_t postDqTailDataNum,
+                                int64_t postDkFrontCoreNum,
+                                int64_t postDkFrontDataNum,
+                                int64_t postDkTailDataNum,
+                                int64_t postDvFrontCoreNum,
+                                int64_t postDvFrontDataNum,
+                                int64_t postDvTailDataNum,
+                                float scale);
+    __aicore__ inline void Process();
+
+private:
+    __aicore__ inline void SetAllFlags();
+    __aicore__ inline void WaitAllFlags();
+    __aicore__ inline void BeforeCompute();
+    __aicore__ inline void AfterCompute();
+    __aicore__ inline void Compute(GlobalTensor<INPUT_T> gm, GlobalTensor<float> workspace, uint64_t start,
+                                   uint64_t actualSize, bool needMuls);
+
+private:
+    TPipe pipe;
+    TBuf<QuePosition::VECCALC> fp32Buf;
+    TBuf<QuePosition::VECCALC> bf16Buf;
+
+    LocalTensor<float> dqUbFp32, dkUbFp32, dvUbFp32;
+    LocalTensor<INPUT_T> dqUbBf16, dkUbBf16, dvUbBf16;
+
+    GlobalTensor<float> dqWorkspaceFp32, dkWorkspaceFp32, dvWorkspaceFp32;
+    GlobalTensor<INPUT_T> dqGm, dkGm, dvGm;
+
+    DataCopyExtParams copyExtParams = {1, 0, 0, 0, 0};
+    DataCopyPadExtParams<float> copyPadExtParams = {false, 0, 0, 0};
+
+    float scale;
+    int32_t thisCoreIdx;
+    int64_t baseNum;
+    int64_t dqDataNum, dqDataStart, dkDataNum, dkDataStart, dvDataNum, dvDataStart;
+    event_t eventIdVMte2 = EVENT_ID0;
+    event_t eventIdVMte3 = EVENT_ID0;
+
+    int64_t pingPongFlag = 0;
+};
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::SetAllFlags()
+{
+    SetFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    SetFlag<HardEvent::V_MTE2>(EVENT_ID1);
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    SetFlag<HardEvent::MTE3_V>(EVENT_ID1);
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::WaitAllFlags()
+{
+    WaitFlag<HardEvent::V_MTE2>(EVENT_ID0);
+    WaitFlag<HardEvent::V_MTE2>(EVENT_ID1);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE3_V>(EVENT_ID1);
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::BeforeCompute()
+{
+    this->eventIdVMte2 = pingPongFlag ? EVENT_ID0 : EVENT_ID1;
+    this->eventIdVMte3 = pingPongFlag ? EVENT_ID0 : EVENT_ID1;
+    WaitFlag<HardEvent::V_MTE2>(this->eventIdVMte2);
+    WaitFlag<HardEvent::MTE3_V>(this->eventIdVMte3);
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::AfterCompute()
+{
+    SetFlag<HardEvent::V_MTE2>(this->eventIdVMte2);
+    SetFlag<HardEvent::MTE3_V>(this->eventIdVMte3);
+    this->pingPongFlag = 1 - this->pingPongFlag;
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::Init(
+                                __gm__ float * __restrict__ dqWorkspace,
+                                __gm__ float * __restrict__ dkWorkspace,
+                                __gm__ float * __restrict__ dvWorkspace,
+                                __gm__ INPUT_T * __restrict__ dq,
+                                __gm__ INPUT_T * __restrict__ dk,
+                                __gm__ INPUT_T * __restrict__ dv,
+                                int64_t postBaseNum,
+                                int64_t postDqFrontCoreNum,
+                                int64_t postDqFrontDataNum,
+                                int64_t postDqTailDataNum,
+                                int64_t postDkFrontCoreNum,
+                                int64_t postDkFrontDataNum,
+                                int64_t postDkTailDataNum,
+                                int64_t postDvFrontCoreNum,
+                                int64_t postDvFrontDataNum,
+                                int64_t postDvTailDataNum,
+                                float scale)
+{
+    this->thisCoreIdx = GetBlockIdx();
+    this->scale = scale;
+
+    this->baseNum = postBaseNum;
+    this->dqDataNum = this->thisCoreIdx < postDqFrontCoreNum ? postDqFrontDataNum : postDqTailDataNum;
+    this->dqDataStart = this->thisCoreIdx < postDqFrontCoreNum ?
+                        postDqFrontDataNum * this->thisCoreIdx :
+                        postDqFrontDataNum * postDqFrontCoreNum +
+                        postDqTailDataNum * (this->thisCoreIdx - postDqFrontCoreNum);
+    
+    this->dkDataNum = this->thisCoreIdx < postDkFrontCoreNum ? postDkFrontDataNum : postDkTailDataNum;
+    this->dkDataStart = this->thisCoreIdx < postDkFrontCoreNum ?
+                        postDkFrontDataNum * this->thisCoreIdx :
+                        postDkFrontDataNum * postDkFrontCoreNum +
+                        postDkTailDataNum * (this->thisCoreIdx - postDkFrontCoreNum);
+
+    this->dvDataNum = this->thisCoreIdx < postDvFrontCoreNum ? postDvFrontDataNum : postDvTailDataNum;
+    this->dvDataStart = this->thisCoreIdx < postDvFrontCoreNum ?
+                        postDvFrontDataNum * this->thisCoreIdx :
+                        postDvFrontDataNum * postDvFrontCoreNum +
+                        postDvTailDataNum * (this->thisCoreIdx - postDvFrontCoreNum);
+
+    dqGm.SetGlobalBuffer(dq);
+    dkGm.SetGlobalBuffer(dk);
+    dvGm.SetGlobalBuffer(dv);
+    dqWorkspaceFp32.SetGlobalBuffer(dqWorkspace);
+    dkWorkspaceFp32.SetGlobalBuffer(dkWorkspace);
+    dvWorkspaceFp32.SetGlobalBuffer(dvWorkspace);
+
+    pipe.InitBuffer(fp32Buf, BUFFER_NUM * baseNum * sizeof(float)); // ping: 前baseNum个；pong: 后baseNum个
+    pipe.InitBuffer(bf16Buf, BUFFER_NUM * baseNum * sizeof(INPUT_T)); // ping: 前baseNum个；pong: 后baseNum个
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::Process()
+{
+    // compute dq
+    SetAllFlags();
+    for (uint64_t i = this->dqDataStart; i < this->dqDataStart + this->dqDataNum; i = i + this->baseNum) {
+        BeforeCompute();
+        uint64_t actualSize = i + this->baseNum < this->dqDataStart + this->dqDataNum ?
+                              this->baseNum : this->dqDataStart + this->dqDataNum - i;
+        Compute(dqGm, dqWorkspaceFp32, i, actualSize, true);
+        AfterCompute();
+    }
+    WaitAllFlags();
+
+    // compute dk
+    SetAllFlags();
+    for (uint64_t i = this->dkDataStart; i < this->dkDataStart + this->dkDataNum; i = i + this->baseNum) {
+        BeforeCompute();
+        uint64_t actualSize = i + this->baseNum < this->dkDataStart + this->dkDataNum ?
+                              this->baseNum : this->dkDataStart + this->dkDataNum - i;
+        Compute(dkGm, dkWorkspaceFp32, i, actualSize, true);
+        AfterCompute();
+    }
+    WaitAllFlags();
+
+    // compute dv
+    SetAllFlags();
+    for (uint64_t i = this->dvDataStart; i < this->dvDataStart + this->dvDataNum; i = i + this->baseNum) {
+        BeforeCompute();
+        uint64_t actualSize = i + this->baseNum < this->dvDataStart + this->dvDataNum ?
+                              this->baseNum : this->dvDataStart + this->dvDataNum - i;
+        Compute(dvGm, dvWorkspaceFp32, i, actualSize, false);
+        AfterCompute();
+    }
+    WaitAllFlags();
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void LagPost<INPUT_T, IF_BF16>::Compute(
+    GlobalTensor<INPUT_T> gm, GlobalTensor<float> workspace, uint64_t start,
+    uint64_t actualSize, bool needMuls)
+{
+    int32_t ubOffset = pingPongFlag ? 0 : this->baseNum;
+    LocalTensor<float> fp32Ub = this->fp32Buf.template Get<float>();
+    LocalTensor<INPUT_T> bf16Ub = this->bf16Buf.template Get<INPUT_T>();
+
+    // CopyIn
+    this->copyExtParams.blockLen = static_cast<uint32_t>(actualSize * sizeof(float));
+    DataCopyPad(fp32Ub[ubOffset], workspace[start], this->copyExtParams, this->copyPadExtParams);
+    SetFlag<HardEvent::MTE2_V>(this->eventIdVMte2);
+
+    // Compute
+    WaitFlag<HardEvent::MTE2_V>(this->eventIdVMte2);
+    if (needMuls) {
+        Muls(fp32Ub[ubOffset], fp32Ub[ubOffset], this->scale, actualSize);
+        pipe_barrier(PIPE_V);
+    }
+    Cast(bf16Ub[ubOffset], fp32Ub[ubOffset], RoundMode::CAST_RINT, actualSize);
+    SetFlag<HardEvent::V_MTE3>(this->eventIdVMte3);
+
+    // CopyOut
+    WaitFlag<HardEvent::V_MTE3>(this->eventIdVMte3);
+    this->copyExtParams.blockLen = static_cast<uint32_t>(actualSize * sizeof(INPUT_T));
+    DataCopyPad(gm[start], bf16Ub[ubOffset], this->copyExtParams);
+}
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/laser_attention_grad.cpp b/src/kernels/mixkernels/laser_attention_grad/op_kernel/laser_attention_grad.cpp
new file mode 100644
index 00000000..8d5d6836
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/laser_attention_grad.cpp
@@ -0,0 +1,268 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#include "mixkernels/laser_attention_grad/tiling/tiling_data.h"
+#include "mixkernels/utils/common/kernel/kernel_utils.h"
+#include "kernel_operator.h"
+#include "lib/matmul_intf.h"
+#include "cube_backward.h"
+#include "vector_backward.h"
+#include "cube_backward_band_op.h"
+#include "cube_backward_band_op_192.h"
+#include "vector_backward_band_op.h"
+#include "lag_post.h"
+#include "TransposeCustom.h"
+#include "TransposeWithDtype.h"
+#include "TransposeGrad.h"
+
+using namespace AscendC;
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata,
+                                      AtbOps::LaserAttentionGradTilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->batchSize = (*(const __gm__ int32_t *)(p_tilingdata + 0));
+    tilingdata->headNum = (*(const __gm__ int32_t *)(p_tilingdata + 4));
+    tilingdata->seqSize = (*(const __gm__ int32_t *)(p_tilingdata + 8));
+    tilingdata->headDim = (*(const __gm__ int32_t *)(p_tilingdata + 12));
+    tilingdata->blockDim = (*(const __gm__ int32_t *)(p_tilingdata + 16));
+    tilingdata->blockNumPerCore = (*(const __gm__ int32_t *)(p_tilingdata + 20));
+    tilingdata->isTriangle = (*(const __gm__ int32_t *)(p_tilingdata + 24));
+    tilingdata->attenType = (*(const __gm__ int32_t *)(p_tilingdata + 28));
+    tilingdata->sparseMode = (*(const __gm__ int32_t *)(p_tilingdata + 32));
+    tilingdata->headGroupSize = (*(const __gm__ int32_t *)(p_tilingdata + 36));
+    tilingdata->windowLen = (*(const __gm__ int32_t *)(p_tilingdata + 40));
+    tilingdata->qSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 44));
+    tilingdata->kSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 48));
+    tilingdata->vSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 52));
+    tilingdata->isHighPrecision = (*(const __gm__ int32_t *)(p_tilingdata + 56));
+    tilingdata->scale = (*(const __gm__ float *)(p_tilingdata + 60));
+    tilingdata->keepProb = (*(const __gm__ float *)(p_tilingdata + 64));
+    tilingdata->preTokens = (*(const __gm__ int32_t *)(p_tilingdata + 68));
+    tilingdata->nextTokens = (*(const __gm__ int32_t *)(p_tilingdata + 72));
+    tilingdata->maskSeqLength = (*(const __gm__ int32_t *)(p_tilingdata + 76));
+    tilingdata->queryShape = (*(const __gm__ int64_t *)(p_tilingdata + 80));
+    tilingdata->keyShape = (*(const __gm__ int64_t *)(p_tilingdata + 88));
+    tilingdata->valueShape = (*(const __gm__ int64_t *)(p_tilingdata + 96));
+    tilingdata->postBaseNum = (*(const __gm__ int64_t *)(p_tilingdata + 104));
+    tilingdata->postDqFrontCoreNum = (*(const __gm__ int64_t *)(p_tilingdata + 112));
+    tilingdata->postDqTailCoreNum = (*(const __gm__ int64_t *)(p_tilingdata + 120));
+    tilingdata->postDqFrontDataNum = (*(const __gm__ int64_t *)(p_tilingdata + 128));
+    tilingdata->postDqTailDataNum = (*(const __gm__ int64_t *)(p_tilingdata + 136));
+    tilingdata->postDkFrontCoreNum = (*(const __gm__ int64_t *)(p_tilingdata + 144));
+    tilingdata->postDkTailCoreNum = (*(const __gm__ int64_t *)(p_tilingdata + 152));
+    tilingdata->postDkFrontDataNum = (*(const __gm__ int64_t *)(p_tilingdata + 160));
+    tilingdata->postDkTailDataNum = (*(const __gm__ int64_t *)(p_tilingdata + 168));
+    tilingdata->postDvFrontCoreNum = (*(const __gm__ int64_t *)(p_tilingdata + 176));
+    tilingdata->postDvTailCoreNum = (*(const __gm__ int64_t *)(p_tilingdata + 184));
+    tilingdata->postDvFrontDataNum = (*(const __gm__ int64_t *)(p_tilingdata + 192));
+    tilingdata->postDvTailDataNum = (*(const __gm__ int64_t *)(p_tilingdata + 200));
+    tilingdata->outputWorkspaceOffset = (*(const __gm__ int64_t *)(p_tilingdata + 208));
+    tilingdata->inputLayout = (*(const __gm__ int32_t *)(p_tilingdata + 216));
+
+#else
+    __ubuf__ uint8_t *tilingdata_in_ub = (__ubuf__ uint8_t *)get_imm(0);
+    int32_t tilingBlockNum = sizeof(AtbOps::LaserAttentionGradTilingData) / 32 + 1;
+    copy_gm_to_ubuf(((__ubuf__ uint8_t *)tilingdata_in_ub), p_tilingdata, 0, 1, tilingBlockNum, 0, 0);
+    pipe_barrier(PIPE_ALL);
+    tilingdata->batchSize = (*(__ubuf__ uint32_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 0));
+    pipe_barrier(PIPE_ALL);
+#endif
+}
+
+extern "C" __global__ __aicore__ void
+laser_attention_grad(__gm__ uint8_t *__restrict__ ffts_addr, __gm__ uint8_t *__restrict__ query_gm,
+                     __gm__ uint8_t *__restrict__ key_gm, __gm__ uint8_t *__restrict__ value_gm,
+                     __gm__ uint8_t *__restrict__ attention_out_grad_gm, __gm__ uint8_t *__restrict__ alibi_mask_gm,
+                     __gm__ uint8_t *__restrict__ drop_mask_gm, __gm__ uint8_t *__restrict__ atten_mask_gm,
+                     __gm__ uint8_t *__restrict__ softmax_max_gm, __gm__ uint8_t *__restrict__ softmax_sum_gm,
+                     __gm__ uint8_t *__restrict__ attention_in_gm, __gm__ uint8_t *__restrict__ query_grad_gm,
+                     __gm__ uint8_t *__restrict__ key_grad_gm, __gm__ uint8_t *__restrict__ value_grad_gm,
+                     __gm__ uint8_t *__restrict__ workspace, __gm__ uint8_t *__restrict__ tiling_para_gm)
+
+{
+    AtbOps::LaserAttentionGradTilingData tilingData;
+    InitTilingData(tiling_para_gm, &(tilingData));
+    const AtbOps::LaserAttentionGradTilingData *__restrict tiling_data = &tilingData;
+    SetSysWorkspaceForce(workspace);
+    __gm__ uint8_t *user = GetUserWorkspace(workspace);
+
+    set_ffts_base_addr((uint64_t)ffts_addr);
+
+    int32_t batchSize = tiling_data->batchSize;
+    int32_t headNum = tiling_data->headNum;
+    int32_t seqSize = tiling_data->seqSize;
+    int32_t qSize = tiling_data->qSeqLength;
+    int32_t kSize = tiling_data->kSeqLength;
+    int32_t isTriangle = tiling_data->isTriangle;
+    int32_t headDim = tiling_data->headDim;
+    int32_t gqaGroupSize = tiling_data->headGroupSize;
+    int32_t sparseMode = tiling_data->sparseMode;
+    int32_t windowLength = tiling_data->windowLen;
+
+    int32_t postBaseNum = tiling_data->postBaseNum;
+    int32_t postDqFrontCoreNum = tiling_data->postBaseNum;
+    int32_t postDqFrontDataNum = tiling_data->postDqFrontDataNum;
+    int32_t postDqTailDataNum = tiling_data->postDqTailDataNum;
+    int32_t postDkFrontCoreNum = tiling_data->postDkFrontCoreNum;
+    int32_t postDkFrontDataNum = tiling_data->postDkFrontDataNum;
+    int32_t postDkTailDataNum = tiling_data->postDkTailDataNum;
+    int32_t postDvFrontCoreNum = tiling_data->postDvFrontCoreNum;
+    int32_t postDvFrontDataNum = tiling_data->postDvFrontDataNum;
+    int32_t postDvTailDataNum = tiling_data->postDvTailDataNum;
+
+    int32_t blockDim = tiling_data->blockDim;
+    int32_t blockNumPerCore = tiling_data->blockNumPerCore;
+    int32_t maskSeqLength = tiling_data->maskSeqLength;
+    int32_t dpOffset = blockDim * blockNumPerCore * 128 * 128 * 2;
+    float scale = tiling_data->scale;
+    int32_t isTri = isTriangle;
+    int32_t inputLayout = tiling_data->inputLayout;
+    int32_t gqaHead = (headNum + gqaGroupSize - 1) / gqaGroupSize;
+
+    int64_t dkOffset = tiling_data->queryShape;
+    int64_t dvOffset = dkOffset + tiling_data->keyShape;
+    int64_t sOffset = dvOffset + tiling_data->valueShape;
+
+    if (TILING_KEY_IS(2)) {
+        using TYPE = __bf16;
+        __gm__ TYPE *__restrict__ gm_Q = (__gm__ TYPE *__restrict__)query_gm;
+        __gm__ TYPE *__restrict__ gm_K = (__gm__ TYPE *__restrict__)key_gm;
+        __gm__ TYPE *__restrict__ gm_V = (__gm__ TYPE *__restrict__)value_gm;
+        __gm__ TYPE *__restrict__ gm_dO = (__gm__ TYPE *__restrict__)attention_out_grad_gm;
+        __gm__ TYPE *__restrict__ gm_dQ = (__gm__ TYPE *__restrict__)query_grad_gm;
+        __gm__ TYPE *__restrict__ gm_dK = (__gm__ TYPE *__restrict__)key_grad_gm;
+        __gm__ TYPE *__restrict__ gm_dV = (__gm__ TYPE *__restrict__)value_grad_gm;
+        __gm__ TYPE *__restrict__ gm_O = (__gm__ TYPE *__restrict__)attention_in_gm;
+        __gm__ float *__restrict__ gm_rowmax = (__gm__ float *__restrict__)softmax_max_gm;
+        __gm__ float *__restrict__ gm_rowsum = (__gm__ float *__restrict__)softmax_sum_gm;
+        __gm__ TYPE *__restrict__ gm_mask = (__gm__ TYPE *__restrict__)atten_mask_gm;
+        __gm__ float *__restrict__ workspace_dQ = (__gm__ float *__restrict__)user;
+        __gm__ float *__restrict__ workspace_dK = (__gm__ float *__restrict__)(workspace_dQ + dkOffset);
+        __gm__ float *__restrict__ workspace_dV = (__gm__ float *__restrict__)(workspace_dQ + dvOffset);
+        __gm__ float *__restrict__ gm_S = (__gm__ float *__restrict__)(workspace_dQ + sOffset);
+        __gm__ float *__restrict__ gm_dP = (__gm__ float *__restrict__)(gm_S + dpOffset);
+
+#ifdef __DAV_C220_CUBE__
+        CUBE_BACKWARD_FP32_OP::CubeBackward<TYPE, true> cube;
+        cube.Init(gm_Q, gm_K, gm_V, gm_dO, gm_dP, gm_S, workspace_dQ, workspace_dK, workspace_dV, isTri, qSize, kSize,
+                  batchSize, headNum, gqaGroupSize, headDim, sparseMode, windowLength, blockNumPerCore);
+        cube.Run_mix();
+        SyncAll<false>();
+#elif __DAV_C220_VEC__
+        VEC_BACKWARD_FP32_OP::VectorBackward<TYPE, true> Vector;
+        Vector.Init(gm_dO, gm_O, gm_S, gm_rowmax, gm_rowsum, gm_dP, gm_mask, isTri, qSize, kSize, batchSize, headNum,
+                    headDim, blockNumPerCore, sparseMode, windowLength, maskSeqLength, scale);
+        Vector.Run();
+
+        LAG_POST::LagPost<TYPE, true> post;
+        post.Init(workspace_dQ, workspace_dK, workspace_dV, gm_dQ, gm_dK, gm_dV, postBaseNum, postDqFrontCoreNum,
+                  postDqFrontDataNum, postDqTailDataNum, postDkFrontCoreNum, postDkFrontDataNum, postDkTailDataNum,
+                  postDvFrontCoreNum, postDvFrontDataNum, postDvTailDataNum, scale);
+        SyncAll<false>();
+        post.Process();
+#endif
+    } else if (TILING_KEY_IS(3)) {
+        using TYPE = __bf16;
+        __gm__ TYPE *__restrict__ gm_Q = (__gm__ TYPE *__restrict__)query_gm;
+        __gm__ TYPE *__restrict__ gm_K = (__gm__ TYPE *__restrict__)key_gm;
+        __gm__ TYPE *__restrict__ gm_V = (__gm__ TYPE *__restrict__)value_gm;
+        __gm__ TYPE *__restrict__ gm_dO = (__gm__ TYPE *__restrict__)attention_out_grad_gm;
+        __gm__ TYPE *__restrict__ gm_dQ = (__gm__ TYPE *__restrict__)query_grad_gm;
+        __gm__ TYPE *__restrict__ gm_dK = (__gm__ TYPE *__restrict__)key_grad_gm;
+        __gm__ TYPE *__restrict__ gm_dV = (__gm__ TYPE *__restrict__)value_grad_gm;
+        __gm__ TYPE *__restrict__ gm_O = (__gm__ TYPE *__restrict__)attention_in_gm;
+        __gm__ float *__restrict__ gm_rowmax = (__gm__ float *__restrict__)softmax_max_gm;
+        __gm__ float *__restrict__ gm_rowsum = (__gm__ float *__restrict__)softmax_sum_gm;
+        __gm__ TYPE *__restrict__ gm_mask = (__gm__ TYPE *__restrict__)atten_mask_gm;
+        __gm__ float *__restrict__ workspace_dQ = (__gm__ float *__restrict__)user;
+        __gm__ float *__restrict__ workspace_dK = (__gm__ float *__restrict__)(workspace_dQ + dkOffset);
+        __gm__ float *__restrict__ workspace_dV = (__gm__ float *__restrict__)(workspace_dQ + dvOffset);
+        __gm__ float *__restrict__ gm_S = (__gm__ float *__restrict__)(workspace_dQ + sOffset);
+        __gm__ float *__restrict__ gm_dP = (__gm__ float *__restrict__)(gm_S + dpOffset);
+
+        uint32_t dpSize = blockDim * blockNumPerCore * 128 * 128 * 2;
+        uint32_t gmQSize = batchSize * 192 * headNum * qSize;
+        uint32_t gmKSize = batchSize * 256 * gqaHead * kSize;
+        uint32_t gmVSize = batchSize * 128 * gqaHead * kSize;
+        uint32_t gmOSize = batchSize * 128 * headNum * qSize;
+
+        __gm__ TYPE *__restrict__ gm_Q_trans =
+            (__gm__ TYPE *__restrict__)(gm_dP + blockDim * blockNumPerCore * 128 * 128 * 2);
+        __gm__ TYPE *__restrict__ gm_K_trans = (__gm__ TYPE *__restrict__)(gm_Q_trans + gmQSize);
+        __gm__ TYPE *__restrict__ gm_V_trans = (__gm__ TYPE *__restrict__)(gm_K_trans + gmKSize);
+        __gm__ TYPE *__restrict__ gm_O_trans = (__gm__ TYPE *__restrict__)(gm_V_trans + gmVSize);
+        __gm__ TYPE *__restrict__ gm_dO_trans = (__gm__ TYPE *__restrict__)(gm_O_trans + gmOSize);
+
+#ifdef __DAV_C220_CUBE__
+        if (headDim == 128) {
+            CUBE_BACK_BAND_OP::CubeBackwardBandOp<TYPE, true> cube;
+            cube.Init(gm_Q, gm_K, gm_V, gm_dO, gm_dP, gm_S, workspace_dQ, workspace_dK, workspace_dV, isTri, qSize,
+                      kSize, batchSize, headNum, gqaGroupSize, headDim, sparseMode, windowLength, blockNumPerCore);
+            cube.Run_op();
+            SyncAll<false>();
+        } else {
+            CUBE_BACK_BAND_OP_192::CubeBackwardBandOp192<TYPE, true> cube;
+            if (inputLayout != 3) {
+                wait_flag_dev(AIV2AICFLAGID);
+                cube.Init(gm_Q_trans, gm_K_trans, gm_V_trans, gm_dO_trans, gm_dP, gm_S, workspace_dQ, workspace_dK,
+                          workspace_dV, isTri, qSize, kSize, batchSize, headNum, gqaGroupSize, headDim, sparseMode,
+                          windowLength, blockNumPerCore);
+            } else {
+                cube.Init(gm_Q, gm_K, gm_V, gm_dO, gm_dP, gm_S, workspace_dQ, workspace_dK, workspace_dV, isTri, qSize,
+                          kSize, batchSize, headNum, gqaGroupSize, headDim, sparseMode, windowLength, blockNumPerCore);
+            }
+            cube.Run_op();
+            SyncAll<false>();
+        }
+#elif __DAV_C220_VEC__
+        VEC_BACKWARD_BAND_OP::VectorBackwardBandOp<TYPE, true> Vector;
+
+        TransposeGrad<TYPE, true> transpose_grad;
+        transpose_grad.Init(gm_Q, gm_K, gm_V, gm_dO, gm_O, gm_Q_trans, gm_K_trans, gm_V_trans, gm_dO_trans, gm_O_trans,
+                            batchSize, headNum, qSize, kSize, inputLayout, gqaHead);
+        transpose_grad.Run();
+
+        if (inputLayout != 3) {
+            Vector.Init(gm_dO_trans, gm_O_trans, gm_S, gm_rowmax, gm_rowsum, gm_dP, gm_mask, isTri, qSize, kSize,
+                        batchSize, headNum, headDim, blockNumPerCore, sparseMode, windowLength, maskSeqLength, scale);
+        } else {
+            Vector.Init(gm_dO, gm_O, gm_S, gm_rowmax, gm_rowsum, gm_dP, gm_mask, isTri, qSize, kSize, batchSize,
+                        headNum, headDim, blockNumPerCore, sparseMode, windowLength, maskSeqLength, scale);
+        }
+
+        Vector.Run_op();
+
+        if (inputLayout == 3) {
+            LAG_POST::LagPost<TYPE, true> post;
+            post.Init(workspace_dQ, workspace_dK, workspace_dV, gm_dQ, gm_dK, gm_dV, postBaseNum, postDqFrontCoreNum,
+                      postDqFrontDataNum, postDqTailDataNum, postDkFrontCoreNum, postDkFrontDataNum, postDkTailDataNum,
+                      postDvFrontCoreNum, postDvFrontDataNum, postDvTailDataNum, scale);
+            SyncAll<false>();
+            post.Process();
+        } else {
+            TransposeWithDtype<TYPE, true> op_transpose;
+            SyncAll<false>();
+            op_transpose.Init(workspace_dQ, nullptr, gm_dQ, batchSize, headNum, qSize, 192, inputLayout, 0, true,
+                              scale);
+            op_transpose.Run();
+
+            op_transpose.Init(workspace_dK, nullptr, gm_dK, batchSize, gqaHead, kSize, 256, inputLayout, 0, true,
+                              scale);
+            op_transpose.Run();
+
+            op_transpose.Init(workspace_dV, nullptr, gm_dV, batchSize, gqaHead, kSize, 128, inputLayout, 0, false,
+                              scale);
+            op_transpose.Run();
+            // SyncAll<false>();
+        }
+
+#endif
+    }
+}
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/ppmatmul_const_grad.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/ppmatmul_const_grad.h
new file mode 100644
index 00000000..4b1ab281
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/ppmatmul_const_grad.h
@@ -0,0 +1,82 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __PPMATMUL_CONST_BF16_H__
+#define __PPMATMUL_CONST_BF16_H__
+
+constexpr int AICFLAGID = 0;
+constexpr int AIVFLAGID = 1;
+constexpr int AIC2AIVFLAGID = 2;
+constexpr int AIV2AICFLAGID = 3;
+
+using T_OUTPUT = float;
+
+constexpr int32_t PING_PONG_NUM = 2;
+constexpr int32_t L0AB_PINGPONG_BUFFER_LEN = 16384;     // 32 KB
+constexpr int32_t BLOCK_SIZE = 16;
+constexpr int32_t CUBE_MATRIX_SIZE = 256;               // 16 * 16
+constexpr int64_t L1_PINGPONG_BUFFER_LEN = 16384;       // 32 KB
+constexpr int64_t L0C_PINGPONG_BUFFER_LEN = 16384;      // 64 KB
+
+constexpr int32_t BASE_BLOCK_SIZE = 16384;      // BASE_BLOCK shape ：[128 * 128]
+constexpr int32_t BASE_BLOCK_SIDE_LEN = 128;    // BASE_BLOCK  row adn column  size
+
+constexpr int32_t B16_SIZE = 2;
+constexpr int32_t B32_SIZE = 4;
+constexpr int32_t CUBE2_LENGTH_M = 128;
+constexpr int32_t CUBE2_LENGTH_K = 128;
+constexpr int32_t CUBE2_LENGTH_N = 128;
+constexpr int32_t MAX_SWITCH_TIME = 16; // 一个core最多处理16个基本块，因此最多只能有16段
+
+constexpr int32_t VEC_NUM_PER_CUBE = 2;
+constexpr int32_t TILING_DIVIDE = 2;
+constexpr int32_t BASE_BLOCK_LENGTH = 128;
+// 基本块是方阵，长和宽
+constexpr int BASE_BLOCK_SIZE_DOUBLE = BASE_BLOCK_SIDE_LEN * 2;
+// head的维度
+constexpr int HEAD_DIM = 128;
+// 基本块含有数据量
+constexpr int BASE_BLOCK_DATA_NUM = BASE_BLOCK_SIDE_LEN * BASE_BLOCK_SIDE_LEN;
+// UB一次处理的最大长度 (单个ping)
+constexpr int MAX_LENG_PER_UB_PROC = 8192;
+constexpr int MAX_BLOCK_PER_ONE_PROC = MAX_LENG_PER_UB_PROC / BASE_BLOCK_SIDE_LEN;
+// 折半计算的基准block数量
+constexpr int BLOCK_NUM_FOR_VMAX = 16;
+constexpr int SHORT_SEQ_THRESHOLD = 8192;
+constexpr int MDDIUM_SEQ_THRESHOLD = 32768;
+
+// backward vector
+// 基本块是方阵，边长是head的维度
+constexpr int BASE_BLOCK_SIZE_LEN_BACKWARD = 128;
+// 基本块含有数据量
+constexpr int BASE_BLOCK_DATA_NUM_BACKWARD = BASE_BLOCK_SIZE_LEN_BACKWARD * BASE_BLOCK_SIZE_LEN_BACKWARD;
+// UB一次处理的最大长度 (单个ping)
+constexpr int MAX_LENG_PER_UB_PROC_BACKWARD = 4096;
+// 行数
+constexpr int MAX_BLOCK_PER_ONE_PROC_BACKWARD = MAX_LENG_PER_UB_PROC_BACKWARD / BASE_BLOCK_SIZE_LEN_BACKWARD;
+constexpr int BASIC_GAP_BACKWARD = BASE_BLOCK_DATA_NUM_BACKWARD - BASE_BLOCK_SIZE_LEN_BACKWARD;
+
+constexpr int BASIC_GAP = BASE_BLOCK_DATA_NUM - BASE_BLOCK_SIDE_LEN;
+// 非2的幂长度，折半计算vmax时，需要padding
+constexpr float PADDING_FOR_MAX = -1e30;
+
+constexpr int TRI_MATRIX_NONE = 0;
+constexpr int TRI_MATRIX_TAIL = 1;
+constexpr int TRI_MATRIX_HEAD = 2;
+constexpr int TRI_MATRIX_HEAD_AND_TAIL = 3;
+
+struct Addr {
+    int32_t b;
+    int32_t n;
+    int32_t iR;
+    int32_t iC;
+    int32_t k;
+};
+
+#endif
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/vector_backward.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/vector_backward.h
new file mode 100644
index 00000000..aab90374
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/vector_backward.h
@@ -0,0 +1,1185 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __VECTORBACKWARD_FP32_OP_H__
+#define __VECTORBACKWARD_FP32_OP_H__
+#define USE_ASCENDC 1
+
+#ifdef __DAV_C220_VEC__
+
+#include "ppmatmul_const_grad.h"
+#include "kernel_operator.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/utils.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/simd.h"
+namespace VEC_BACKWARD_FP32_OP {
+using namespace AscendC;
+using WORKSPACE_DTYPE = float;
+using WORKSPACE_DTYPE_DP = float;
+constexpr int32_t REPEAT_TIME_FP32 = 4;
+
+/*
+* 存放分段信息
+*/
+struct SectionInfo {
+    // 当前处理的block条，包含block数量
+    int32_t sectionNum{0};
+
+    // 在当前workspace里的偏移，与对应的cube_id有关
+    int32_t curBlockOffset{0};
+    // 当前处理的block的总数量
+    int32_t curBlockNum{0};
+
+    // 起始block编号
+    int32_t sectionStartBlock[MAX_SWITCH_TIME] = {0};
+    // 结束block编号
+    int32_t sectionEndBlock[MAX_SWITCH_TIME] = {0};
+
+    // 在所有heads中的起始行
+    int32_t globalLinesInHead[MAX_SWITCH_TIME] = {0};
+
+    // 是否需要mask（如果需要，总在section end）
+    bool apply_mask[MAX_SWITCH_TIME] = {false};
+
+    // 每一段的长度
+    int32_t len[MAX_SWITCH_TIME] = {0};
+
+    // 头尾block是否需要mask
+    bool headApplyMask[MAX_SWITCH_TIME] = {false};
+    bool tailApplyMask[MAX_SWITCH_TIME] = {false};
+};
+
+/**
+* 全局相关的信息
+*/
+struct GlobalInfo {
+    // cube数量
+    int32_t cubeNum{0};
+    // 每个cube处理block的数量
+    int32_t blockNumPerCube{0};
+    int32_t headNum{0};
+    int32_t batchNum{0};
+    int32_t seq_len{0};
+    int32_t qSeqLen{0};
+    int32_t kSeqLen{0};
+
+    // 是否是三角阵
+    bool isTriMatrix{false};
+
+    // 一行有几个block（三角阵和非三角阵不同）
+    int32_t blockNumPerRow{0};
+    // 一列有几个block（也就是一个head有几个行）
+    int32_t blockNumPerCol{0};
+
+    // 一个loop处理block的数量
+    int32_t blockNumPerLoop{0};
+    // 一个head包含的block数量
+    int32_t blockNumPerHead{0};
+
+    // 大循环次数
+    int32_t loopTimes{0};
+    // 最后一次循环处理的block数量
+    int32_t tailBlockNum{0};
+
+    int32_t isSparse{0};
+    // 滑窗大小, sparse使用
+    int32_t windowsBlockNum{0};
+};
+
+/**
+* 当前vector的信息
+*/
+struct LocalInfo {
+    // 所属的cube分组（第几个cube很重要）
+    int32_t cubeIndex{0};
+    // 当前vector在cube内的编号（0或1）
+    int32_t vectorIndex{0};
+
+    // 处理基本块的起始行（都是2个vector处理一个基本块）
+    int32_t startLineInBaseBlock{0};
+
+    // 是否参与尾块处理
+    bool processTail{false};
+    // 处理尾块中的block数量
+    int32_t tailProcessedBlockNum{0};
+};
+
+template<typename INPUT_T, bool IF_BF16>
+class VectorBackward {
+public:
+    __aicore__ inline VectorBackward() {}
+    __aicore__ inline void Init(__gm__ INPUT_T * __restrict__ doGm,
+                                __gm__ INPUT_T* __restrict__ oGm,
+                                __gm__ WORKSPACE_DTYPE * __restrict__ sGm,
+                                __gm__ float * __restrict__ rowmaxGm,
+                                __gm__ float * __restrict__ rowsumGm,
+                                __gm__ WORKSPACE_DTYPE_DP * __restrict__ dpGm,
+                                __gm__ INPUT_T * __restrict__ maskGm,
+                                bool isTriangle,
+                                int32_t qSize,
+                                int32_t kSize,
+                                int32_t batch,
+                                int32_t head,
+                                int32_t baseLength,
+                                int32_t blockNumPerCore,
+                                int32_t isSparse,
+                                int32_t windowLength,
+                                int32_t maskSeqLength,
+                                float scale);
+    __aicore__ inline void Run();
+
+private:
+    __aicore__ inline void GetGlobalInfo(GlobalInfo *globalInfo);
+    __aicore__ inline void GetLocalInfo(GlobalInfo *globalInfo,
+                                        LocalInfo *localInfo,
+                                        SectionInfo *sectionInfo);
+    __aicore__ inline void AllocateUbufForNorm ();
+    __aicore__ inline void UpdateLoopInfo(int32_t curLoop,
+                                            GlobalInfo *globalInfo,
+                                            LocalInfo *localInfo,
+                                            SectionInfo *sectionInfo);
+    // curProcessLines：总处理64行（2个vector处理一个基本块）BASE_BLOCK_SIZE_LEN_BACKWARD /2
+    // oTotalOffset：O,dO的总偏移量，global_lines_in_heads*BASE_BLOCK_SIZE_LEN_BACKWARD，L是global_lines_in_heads
+    // blockNumber：S,dP一次处理几块  endBlock - startBlock
+    // sTotalOffset：S,dP的总偏移量start_block * BASE_BLOCK_DATA_NUM_BACKWARD 128*128
+    // gradAttnOutputGm：dO
+    // attnOutputGm：O
+    // attnScoreGm：S
+    // attnRowmaxGm：rowmax
+    // attnRowsumGm：rowsum
+    __aicore__ inline void ProcessBackwardFp32(int32_t curProcessLines,
+                                                int32_t oTotalOffset,
+                                                int32_t blockNumber,
+                                                int32_t sTotalOffset,
+                                                bool headApplyMask,
+                                                bool tailApplyMask,
+                                                __gm__ INPUT_T * __restrict__ gradAttnOutputGm,
+                                                __gm__ INPUT_T * __restrict__ attnOutputGm,
+                                                __gm__ WORKSPACE_DTYPE * __restrict__ attnScoreGm,
+                                                __gm__ float *__restrict__ attnRowmaxGm,
+                                                __gm__ float *__restrict__ attnRowsumGm,
+                                                __gm__ WORKSPACE_DTYPE_DP * __restrict__ gradPGm,
+                                                __gm__ INPUT_T * __restrict__ attnMaskGm);
+
+private:
+    // dO
+    __gm__ INPUT_T *__restrict__ doGm{nullptr};
+    // O
+    __gm__ INPUT_T *__restrict__ oGm{nullptr};
+    // S
+    __gm__ WORKSPACE_DTYPE *__restrict__ sGm{nullptr};
+    __gm__ float *__restrict__ rowmaxGm;
+    __gm__ float *__restrict__ rowsumGm;
+    __gm__ WORKSPACE_DTYPE_DP *__restrict__ dpGm{nullptr};
+    __gm__ INPUT_T *__restrict__ maskGm{nullptr};
+    __gm__ uint8_t *__restrict__ tilingParaGm{nullptr};
+    __gm__ uint8_t *__restrict__ fftsAddr{nullptr};
+
+    GlobalInfo globalInfo;
+    LocalInfo localInfo;
+    SectionInfo sectionInfo;
+    bool isVector;
+    bool isSync;
+    int32_t H;
+    int32_t maskSeqLength;
+    float SCALE;
+    AscendC::LocalTensor<float> temp0BufFp32;
+    AscendC::LocalTensor<float> temp1BufFp32;
+    AscendC::LocalTensor<INPUT_T> temp0BufFp16;
+    AscendC::LocalTensor<INPUT_T> temp1BufFp16;
+    AscendC::LocalTensor<float> bufForCalcRowsumFp32Buf;
+    AscendC::LocalTensor<float> bufForBrcbRowsumFp32Buf;
+    AscendC::LocalTensor<float> bufForLoadLFp32Buf;
+    AscendC::LocalTensor<float> bufForBrcbLFp32Buf;
+    AscendC::LocalTensor<float> bufForRowmaxFp32Buf;
+    AscendC::LocalTensor<float> bufForRowsumFp32Buf;
+
+    AscendC::LocalTensor<INPUT_T> fp16_temp_2_ub_buf;
+    AscendC::LocalTensor<INPUT_T> bufForMaskFp16Buf;
+    AscendC::LocalTensor<float> bufForMaskFp32Buf;
+
+    int32_t doubleBufferAddr = MAX_LENG_PER_UB_PROC_BACKWARD / sizeof(float);
+    int32_t doubleBufferAddrInputT = MAX_LENG_PER_UB_PROC_BACKWARD / sizeof(INPUT_T);
+    AscendC::LocalTensor<float> temp0BufFp32Ping;
+    AscendC::LocalTensor<float> temp1BufFp32Ping;
+    AscendC::LocalTensor<INPUT_T> temp0BufFp16Ping;
+    AscendC::LocalTensor<INPUT_T> temp1BufFp16Ping;
+    AscendC::LocalTensor<float> bufForLoadLFp32BufPing;
+    AscendC::LocalTensor<float> bufForRowmaxFp32BufPing;
+    AscendC::LocalTensor<float> bufForRowsumFp32BufPing;
+    AscendC::LocalTensor<INPUT_T> bufForMaskFp16BufPing;
+    AscendC::LocalTensor<float> bufForMaskFp32BufPing;
+    AscendC::GlobalTensor<INPUT_T> doGmGlobalTensor;
+    AscendC::GlobalTensor<INPUT_T> oGmGlobalTensor;
+    AscendC::GlobalTensor<WORKSPACE_DTYPE> sGmGlobalTensor;
+    AscendC::GlobalTensor<float> rowmaxGmGlobalTensor;
+    AscendC::GlobalTensor<float> rowsumGmGlobalTensor;
+    AscendC::GlobalTensor<WORKSPACE_DTYPE_DP> dpGmGlobalTensor;
+    AscendC::GlobalTensor<INPUT_T> maskGmGlobalTensor;
+};
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackward<INPUT_T, IF_BF16>::Init(
+                                __gm__ INPUT_T * __restrict__ doGm,
+                                __gm__ INPUT_T * __restrict__ oGm,
+                                __gm__ WORKSPACE_DTYPE * __restrict__ sGm,
+                                __gm__ float * __restrict__ rowmaxGm,
+                                __gm__ float * __restrict__ rowsumGm,
+                                __gm__ WORKSPACE_DTYPE_DP * __restrict__ dpGm,
+                                __gm__ INPUT_T * __restrict__ maskGm,
+                                bool isTriangle,
+                                int32_t qSize,
+                                int32_t kSize,
+                                int32_t batch,
+                                int32_t head,
+                                int32_t baseLength,
+                                int32_t blockNumPerCore,
+                                int32_t isSparse,
+                                int32_t windowLength,
+                                int32_t maskSeqLength,
+                                float scale)
+{
+    this->maskSeqLength = maskSeqLength;
+    this->doGm = doGm;
+    this->oGm = oGm;
+    this->sGm = sGm;
+    this->rowmaxGm = rowmaxGm;
+    this->rowsumGm = rowsumGm;
+    this->dpGm = dpGm;
+    this->maskGm = maskGm;
+    this->SCALE = scale;
+
+    isVector = true;
+    isSync = true;
+
+    // tiling_para[10]
+    this->globalInfo.qSeqLen = qSize;
+    this->globalInfo.kSeqLen = kSize;
+    // tiling_para[12]
+    this->globalInfo.headNum = head;
+    // tiling_para[0]
+    this->globalInfo.batchNum = batch;
+    // tiling_para[11]
+    this->globalInfo.blockNumPerCube = blockNumPerCore;
+    this->globalInfo.isTriMatrix = isTriangle;
+
+    this->globalInfo.isSparse = isSparse;
+    this->globalInfo.windowsBlockNum = windowLength / BASE_BLOCK_SIZE_LEN_BACKWARD;
+
+    H = qSize / 128;
+    // 获取全局信息
+    GetGlobalInfo(&this->globalInfo);
+    GetLocalInfo(&this->globalInfo, &this->localInfo, &this->sectionInfo);
+    AllocateUbufForNorm();
+    doGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(doGm));
+    oGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(oGm));
+    sGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ WORKSPACE_DTYPE *>(sGm));
+    rowmaxGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(rowmaxGm));
+    rowsumGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(rowsumGm));
+    dpGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ WORKSPACE_DTYPE_DP *>(dpGm));
+    maskGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(maskGm));
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackward<INPUT_T, IF_BF16>::Run() {
+    AscendC::SetAtomicNone();
+    AscendC::SetMaskNorm();
+    AscendC::SetVectorMask<int8_t>((uint64_t)-1, (uint64_t)-1);
+
+    // attention score共有的block数量
+    int32_t totalBlockNum = globalInfo.blockNumPerRow * globalInfo.blockNumPerCol *
+                            globalInfo.headNum * globalInfo.batchNum;
+
+    for (int32_t loopTimes = 0; loopTimes < globalInfo.loopTimes; loopTimes++) {
+        // cube核ID*每个cube核16个block*（128*128）+ 24个cube核*每个cube核16个block*（128*128）*（0或1）
+        // 每个核上的元素偏移，每两个loop是一对，因为两个vector核对应一个cube
+        int32_t totalOffsetEachCore = get_block_idx() * globalInfo.blockNumPerCube *
+                                        BASE_BLOCK_DATA_NUM_BACKWARD +
+                                        get_block_num() * globalInfo.blockNumPerCube *
+                                        BASE_BLOCK_DATA_NUM_BACKWARD * (loopTimes % 2);
+        int32_t curBlockInGlobal = globalInfo.blockNumPerLoop * loopTimes + sectionInfo.curBlockOffset;
+        UpdateLoopInfo(loopTimes, &this->globalInfo, &this->localInfo,  &this->sectionInfo);
+
+        if (isSync) {
+            WaitFlagDev(AIC2AIVFLAGID);
+        }
+        // 如果当前block index 超过block 总数，那么不计算
+        if (curBlockInGlobal < totalBlockNum && isVector) {
+            auto accProcessBlockNum = sectionInfo.sectionNum;
+            for (int32_t section = 0; section < accProcessBlockNum; section ++) {
+
+                auto startBlock = sectionInfo.sectionStartBlock[section];
+                auto endBlock = sectionInfo.sectionEndBlock[section];
+                auto headApplyMask = sectionInfo.headApplyMask[section];
+                auto tailApplyMask = sectionInfo.tailApplyMask[section];
+                auto curProcessBlockNum = sectionInfo.len[section];
+                auto globalLinesInHead = sectionInfo.globalLinesInHead[section];
+                ProcessBackwardFp32(BASE_BLOCK_SIZE_LEN_BACKWARD / 2,
+                            globalLinesInHead * BASE_BLOCK_SIZE_LEN_BACKWARD,
+                            curProcessBlockNum,
+                            startBlock * BASE_BLOCK_DATA_NUM_BACKWARD + totalOffsetEachCore,
+                            headApplyMask,
+                            tailApplyMask,
+                            doGm,
+                            oGm,
+                            sGm,
+                            rowmaxGm,
+                            rowsumGm,
+                            dpGm,
+                            maskGm);
+            }
+        }
+        if (isSync) {
+            FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+        }
+    }
+}
+
+/**
+* 全局信息
+*/
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackward<INPUT_T, IF_BF16>::GetGlobalInfo(GlobalInfo *globalInfo)
+{
+    globalInfo->cubeNum = GetBlockNum();
+
+    int32_t blockNumPerCol = globalInfo->qSeqLen / BASE_BLOCK_SIZE_LEN_BACKWARD;
+    globalInfo->blockNumPerRow = globalInfo->kSeqLen / BASE_BLOCK_SIZE_LEN_BACKWARD;
+    globalInfo->blockNumPerCol = blockNumPerCol;
+
+    if (globalInfo->isTriMatrix) {
+        // 上下拼接后，每行需要处理的block数多一个
+        globalInfo->blockNumPerRow += 1;
+        // 上下拼接后，每列需要处理的block数减半
+        globalInfo->blockNumPerCol /= 2;
+    }
+
+    if (globalInfo->isSparse == 1) {
+        // 滑动窗口的大小再多一个block
+        globalInfo->blockNumPerRow = globalInfo->windowsBlockNum + 1;
+        // 三角阵部分上下拼接后折半
+        globalInfo->blockNumPerCol = blockNumPerCol - globalInfo->windowsBlockNum / 2;
+    }
+
+    // 一次循环处理的block数量
+    globalInfo->blockNumPerLoop = globalInfo->cubeNum * globalInfo->blockNumPerCube;
+    globalInfo->blockNumPerHead = globalInfo->blockNumPerRow * globalInfo->blockNumPerCol;
+
+    // 一个[b,n,s1,s2]有多少个block
+    int32_t totalBlockNum = globalInfo->blockNumPerHead *
+                            globalInfo->headNum * globalInfo->batchNum;
+    // loop_times展开为：seqLenK/128 * qSeqLen/128 * headNum * batchNum / (24 * 16)
+    globalInfo->loopTimes = totalBlockNum / globalInfo->blockNumPerLoop;
+    // 最后的尾块需要处理的block数量
+    globalInfo->tailBlockNum = totalBlockNum % globalInfo->blockNumPerLoop;
+
+    // 如果有tail_block，loop_times需要多加一次
+    if (globalInfo->tailBlockNum > 0) {
+        globalInfo->loopTimes += 1;
+    }
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackward<INPUT_T, IF_BF16>::GetLocalInfo(GlobalInfo *globalInfo,
+                                                                LocalInfo *localInfo,
+                                                                SectionInfo * sectionInfo)
+{
+    localInfo->cubeIndex = get_block_idx();
+    // 0或1
+    localInfo->vectorIndex = get_subblockid();
+
+    // 128 / 2 * 0或1，代表每个vcore从base_block的第几行开始处理，编号0从第0行，编号1从第64行开始
+    localInfo->startLineInBaseBlock = BASE_BLOCK_SIZE_LEN_BACKWARD / 2 * localInfo->vectorIndex;
+
+    // 初始化secion_info
+    sectionInfo->curBlockNum = globalInfo->blockNumPerCube;
+
+    // 当前一组vector处理的偏移量总是相同的
+    sectionInfo->curBlockOffset = globalInfo->blockNumPerCube * localInfo->cubeIndex;
+
+    localInfo->processTail = false;
+
+    // 判断尾块处理信息
+    if (globalInfo->tailBlockNum > 0)
+    {
+        int32_t cubeUsed = globalInfo->tailBlockNum / globalInfo->blockNumPerCube;
+        int32_t tailBlock = globalInfo->tailBlockNum % globalInfo->blockNumPerCube;
+
+
+        if (localInfo->cubeIndex < cubeUsed) {
+            localInfo->processTail = true;
+            localInfo->tailProcessedBlockNum = globalInfo->blockNumPerCube;
+        }
+
+        if (tailBlock > 0) {
+            if (localInfo->cubeIndex == cubeUsed) {
+                localInfo->processTail = true;
+                // 会有两个vector处理少于blockNumPerCube个block
+                localInfo->tailProcessedBlockNum = tailBlock;
+            }
+        }
+    }
+}
+
+/**
+* 获得循环信息
+*/
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackward<INPUT_T, IF_BF16>::UpdateLoopInfo(int32_t curLoop,
+                                                                GlobalInfo *globalInfo,
+                                                                LocalInfo *localInfo,
+                                                                SectionInfo * sectionInfo)
+{
+    // 主要要求解的：1. 每段的长度 2. 每段最后一个是否要加mask 3.每一段 的全局行index
+    int64_t blocksPerBatch = globalInfo->headNum *
+                    globalInfo->blockNumPerRow * globalInfo->blockNumPerCol;
+    int64_t allBlocks = globalInfo->batchNum * blocksPerBatch;
+    // 16*24*当前第几个loop + 当前cube的index*16，每次处理16个blcok，此为第一个block的id
+    int64_t curBlockId = globalInfo->blockNumPerCube * globalInfo->cubeNum * curLoop +
+                localInfo->cubeIndex * globalInfo->blockNumPerCube;
+
+    int32_t blocksPerRow = globalInfo->blockNumPerRow;
+    int32_t blocksPerCol = globalInfo->blockNumPerCol;
+
+    // 倒三角或者sparse场景负载均衡后折叠的行数
+    int32_t balancedRowNum = globalInfo->windowsBlockNum / 2;
+
+    // 第一个block所在的第b个batch
+    int64_t b = curBlockId / blocksPerBatch;
+    // 第一个block所在的第h个head
+    int64_t n = (curBlockId % blocksPerBatch) / (blocksPerRow * blocksPerCol);
+    // 第一个block所在的第ir行
+    int64_t ir = (curBlockId % blocksPerBatch) % (blocksPerRow * blocksPerCol) / (blocksPerRow);
+    // 第一个block所在的第ic行
+    int64_t ic = (curBlockId % blocksPerBatch) % (blocksPerRow * blocksPerCol) % (blocksPerRow);
+
+    // 当前轮次cur_core_index处理的行
+
+    // 默认remain为每个cube处理16个block
+    int64_t remain = globalInfo->blockNumPerCube;
+    // 如果是最后一个cube，则remain的数量更新为剩余的block数量
+    if (curBlockId + globalInfo->blockNumPerCube > allBlocks) {
+        remain = allBlocks - curBlockId;
+    }
+    // 当前处理的block条，包含几个block，如果已经没有remain了，自然就没有block了，结束
+    if (remain <= 0) {
+        sectionInfo->sectionNum = 0;
+        return;
+    }
+    // 当前16个block的block条占的行数
+    int64_t rows = (ic + remain + blocksPerRow - 1) / blocksPerRow;
+
+    Addr addr[32];
+    addr[0].b = b;
+    addr[0].n = n;
+    addr[0].iR = ir;
+    addr[0].iC = ic;
+    addr[0].k = remain;
+    int64_t curAddrLen = 0;
+    // 给addr赋值
+    for (int i = 0; i < rows && remain > 0; ++i) {
+        // 如果剩余需要处理的数量超过一行的长度
+        if (addr[curAddrLen].iC + remain > blocksPerRow) {
+            // 当前行需要处理的剩余block数量为每行的block的数量减去当前block的column index
+            addr[curAddrLen].k = blocksPerRow - addr[curAddrLen].iC;
+            // 下一行的剩余block数量为剩余block数量减去上一行需要处理的block数量
+            addr[curAddrLen + 1].k = remain - addr[curAddrLen].k;
+
+            // 下一行的b，n不变，iR切换到下一行，所以+1，iC从头开始，所以为0
+            addr[curAddrLen + 1].b = addr[curAddrLen].b;
+            addr[curAddrLen + 1].n = addr[curAddrLen].n;
+            addr[curAddrLen + 1].iR = addr[curAddrLen].iR + 1;
+            addr[curAddrLen + 1].iC = 0;
+            // 如果当前的row index大于等于每列的block个数，也就是说row index超出边界
+            if (addr[curAddrLen + 1].iR >= blocksPerCol) {
+                // 则切换到下一个n，且row_index归零从头开始
+                addr[curAddrLen + 1].n = addr[curAddrLen].n + 1;
+                addr[curAddrLen + 1].iR = 0;
+                // 如果n超过n的上线，切换到下一个batch，且n归零从头开始
+                if (addr[curAddrLen + 1].n >= globalInfo->headNum) {
+                    addr[curAddrLen + 1].b = addr[curAddrLen].b + 1;
+                    addr[curAddrLen + 1].n = 0;
+                }
+            }
+        }
+        // 每一个循环处理完之后，把剩余的block数量减去已经处理过的block数量
+        remain -= addr[curAddrLen].k;
+        ++curAddrLen;
+    }
+
+    // 非三角阵（也要知道原始head的行号，head收尾拼接）
+    if (globalInfo->isTriMatrix == false) {
+        int64_t index = 0;
+        // 通过addr给sectionInfo赋值，从addr[0]开始，一直到addr[curAddrLen - 1]
+        for (int i = 0; i < curAddrLen; ++i) {
+            int32_t iR = addr[i].iR;
+            int32_t iC = addr[i].iC;
+            int32_t k = addr[i].k;
+            int32_t rowLeftIndex = iR;
+            int32_t rowRightIndex = blocksPerCol - 1 - iR;
+            int64_t col_left_index = iC;
+            int32_t switchIndex = blocksPerRow;
+            // 用在[b,n,r,c]中的block的index
+            // 减去当前loop已经处理过的block数量，减去当前cube之前的cube已经处理过的block数量
+            // 得到
+            int32_t ColIndexForDb = (addr[i].b*globalInfo->headNum + addr[i].n) *
+                                (blocksPerRow) * (blocksPerCol) +
+                                iR * (blocksPerRow) + iC -
+                                curLoop * globalInfo->blockNumPerCube * globalInfo->cubeNum -
+                                localInfo->cubeIndex * globalInfo->blockNumPerCube;
+
+            // 起始块
+            sectionInfo->sectionStartBlock[index] = ColIndexForDb;
+            // 结束块（不含）
+            sectionInfo->len[index] = k;
+            sectionInfo->apply_mask[index] = false;
+            sectionInfo->headApplyMask[index] = false;
+            sectionInfo->tailApplyMask[index] = false;
+            sectionInfo->globalLinesInHead[index] = (addr[i].b *globalInfo->headNum + addr[i].n) *
+                                    128 * blocksPerCol +
+                                    rowLeftIndex * 128 +
+                                    localInfo->startLineInBaseBlock;
+            index++;
+        }
+        sectionInfo->sectionNum = index;
+    }
+
+    if (globalInfo->isTriMatrix == true || globalInfo->isSparse == 1) {
+        int64_t index = 0;
+        for (int i = 0; i < curAddrLen; ++i) {
+            int32_t iR = addr[i].iR;
+            int32_t iC = addr[i].iC;
+            int32_t k = addr[i].k;
+            int32_t rowLeftIndex = balancedRowNum + iR;
+            int32_t rowRightIndex = balancedRowNum - 1 - iR;
+            int64_t col_left_index = iC;
+            int32_t switchIndex = rowLeftIndex;
+            int64_t col_right_index = iC - (iR + blocksPerCol) - 1;
+            int32_t ColIndexForDb = (addr[i].b * globalInfo->headNum  + addr[i].n) *
+                                (blocksPerRow) * (blocksPerCol) +
+                                iR * (blocksPerRow) + iC -
+                                curLoop * globalInfo->blockNumPerCube * globalInfo->cubeNum -
+                                localInfo->cubeIndex * globalInfo->blockNumPerCube;
+
+            if (iR >= balancedRowNum) {
+                // 起始iC
+                sectionInfo->sectionStartBlock[index] = ColIndexForDb;
+                sectionInfo->len[index] = k;
+                // 最后iC
+                sectionInfo->sectionEndBlock[index] = ColIndexForDb + sectionInfo->len[index];
+                // 起始iR(全局)
+                sectionInfo->globalLinesInHead[index] = (addr[i].b * globalInfo->headNum + addr[i].n) *
+                                        globalInfo->qSeqLen +
+                                        rowLeftIndex * 128 +
+                                        localInfo->startLineInBaseBlock;
+                sectionInfo->headApplyMask[index] = (iC == 0);
+                sectionInfo->tailApplyMask[index] = (iC + sectionInfo->len[index] == blocksPerRow);
+                ++index;
+            } else if (switchIndex < iC) {
+                // 起始iC
+                sectionInfo->sectionStartBlock[index] = ColIndexForDb;
+                sectionInfo->len[index] = k;
+                // 最后iC
+                sectionInfo->sectionEndBlock[index] = ColIndexForDb + sectionInfo->len[index];
+                // 起始iR(全局)
+                sectionInfo->globalLinesInHead[index] = (addr[i].b * globalInfo->headNum + addr[i].n) *
+                                        globalInfo->qSeqLen +
+                                        rowRightIndex * 128 +
+                                        localInfo->startLineInBaseBlock;
+                sectionInfo->headApplyMask[index] = false;
+                sectionInfo->tailApplyMask[index] = (iC + sectionInfo->len[index] == blocksPerRow);
+                ++index;
+            } else if (iC <= switchIndex && iC + k - 1 > switchIndex) {
+                // 起始iC
+                sectionInfo->sectionStartBlock[index] = ColIndexForDb;
+                sectionInfo->len[index] = switchIndex - iC + 1 ;
+                // 最后iC
+                sectionInfo->sectionEndBlock[index] = ColIndexForDb + sectionInfo->len[index];
+                // 起始iR(全局)
+                sectionInfo->globalLinesInHead[index] = (addr[i].b * globalInfo->headNum + addr[i].n) *
+                                        globalInfo->qSeqLen +
+                                        rowLeftIndex * 128 +
+                                        localInfo->startLineInBaseBlock;
+                sectionInfo->headApplyMask[index] = false;
+                sectionInfo->tailApplyMask[index] = true;
+
+                // 起始iC (switchIndex + 1)
+                sectionInfo->sectionStartBlock[index+1] = sectionInfo->sectionStartBlock[index] +
+                                    sectionInfo->len[index];
+                sectionInfo->len[index+1] = k - sectionInfo->len[index];
+                // 最后iC
+                sectionInfo->sectionEndBlock[index+1] = sectionInfo->sectionStartBlock[index+1] +
+                                    sectionInfo->len[index+1];
+                // 起始iR(全局)
+                sectionInfo->globalLinesInHead[index+1] = (addr[i].b *globalInfo->headNum + addr[i].n) *
+                                        globalInfo->qSeqLen +
+                                        rowRightIndex * 128 +
+                                        localInfo->startLineInBaseBlock;
+                sectionInfo->headApplyMask[index+1] = false;
+                // 最后一块是否需要 mask, 是否是跳变点
+                sectionInfo->tailApplyMask[index+1] =
+                                    (switchIndex+1 + sectionInfo->len[index+1] == blocksPerRow);
+                index += 2;
+            } else {
+                // 起始iC
+                sectionInfo->sectionStartBlock[index] = ColIndexForDb;
+                sectionInfo->len[index] = k;
+                // 最后iC
+                sectionInfo->sectionEndBlock[index] = ColIndexForDb + sectionInfo->len[index];
+                sectionInfo->globalLinesInHead[index] = (addr[i].b *globalInfo->headNum  + addr[i].n) *
+                                        globalInfo->qSeqLen +
+                                        rowLeftIndex*128 +
+                                        localInfo->startLineInBaseBlock;
+                sectionInfo->headApplyMask[index] = false;
+                sectionInfo->tailApplyMask[index] = ((iC + k - 1) == switchIndex);
+                index++;
+            }
+        }
+        sectionInfo->sectionNum = index;
+    }
+}
+
+/**
+* 为反向计算分配UB空间
+*/
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void  VectorBackward<INPUT_T, IF_BF16>::AllocateUbufForNorm () {
+    AsdopsBuffer<ArchType::ASCEND_V220> UBuf; // 模板参数为TPosition中的VECCALC类型
+    // O 32kb
+    int32_t temp0BufAddrFp32Size = MAX_LENG_PER_UB_PROC_BACKWARD * 4 * 2;
+    // do 32kb
+    int32_t temp1BufAddrFp32Size = MAX_LENG_PER_UB_PROC_BACKWARD * 4 * 2;
+
+    int32_t temp0BufAddrFp16Size = MAX_LENG_PER_UB_PROC_BACKWARD * 2 * 2;
+    int32_t temp1BufAddrFp16Size = MAX_LENG_PER_UB_PROC_BACKWARD * 2 * 2;
+
+    // 最多处理的FP32数字个数  //d=rowsum(odo)
+    int32_t bufForCalcRowsumFp32Size =  128 * 4 * 2;
+    // 展开做32字节对齐 8倍rowsum
+    int32_t bufForBrcbRowsumFp32Size = bufForCalcRowsumFp32Size * (32 / 4);
+
+    // L
+    int32_t bufForLoadLFp32Size = 128 * 4 * 2;
+    // 8倍L
+    int32_t bufForBrcbLFp32Size = bufForLoadLFp32Size * (32 / 4);
+
+    int32_t bufForRowmaxFp32Size = BASE_BLOCK_SIZE_LEN_BACKWARD * 4 * 2;
+    int32_t bufForRowsumFp32Size = BASE_BLOCK_SIZE_LEN_BACKWARD * 4 * 2;
+
+    // exp(S-L) //32kb
+    int32_t temp2BufAddrFp16Size = MAX_LENG_PER_UB_PROC_BACKWARD * 2 * 2;
+
+    int32_t bufForMaskFp16Size = 64 * 128 * 2;
+    int32_t bufForMaskFp32Size = 64 * 128 * 4;
+
+    int32_t lastAddr = 0;
+
+    // load O
+    // D计算完毕后,s复用fp32_temp_0_ub_buf_addr，s-l也可以用该空间，exp(s-l)
+    temp0BufFp32 = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+    temp0BufFp32Ping = temp0BufFp32[MAX_LENG_PER_UB_PROC_BACKWARD];
+
+    lastAddr += temp0BufAddrFp32Size;
+
+    temp1BufFp32 = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+    temp1BufFp32Ping = temp1BufFp32[MAX_LENG_PER_UB_PROC_BACKWARD];
+
+    // load dO
+
+    lastAddr += temp1BufAddrFp32Size;
+
+    temp0BufFp16 = UBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(lastAddr);
+    temp0BufFp16Ping = temp0BufFp16[MAX_LENG_PER_UB_PROC_BACKWARD];
+
+    lastAddr += temp0BufAddrFp16Size;
+
+    temp1BufFp16 = UBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(lastAddr);
+    temp1BufFp16Ping = temp1BufFp16[MAX_LENG_PER_UB_PROC_BACKWARD];
+
+    lastAddr += temp1BufAddrFp16Size;
+
+    bufForCalcRowsumFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+
+    // cacl rowsum  32字节对齐后，输出到 bufForBrcbRowsumFp32
+    lastAddr += bufForCalcRowsumFp32Size;
+
+    bufForBrcbRowsumFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+
+    lastAddr += bufForBrcbRowsumFp32Size;
+
+    bufForLoadLFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+    bufForLoadLFp32BufPing = bufForLoadLFp32Buf[MAX_BLOCK_PER_ONE_PROC_BACKWARD];
+
+    // load l
+    lastAddr += bufForLoadLFp32Size;
+
+    bufForBrcbLFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+
+    // load 8倍l
+    lastAddr += bufForBrcbLFp32Size;
+
+    bufForRowmaxFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+    bufForRowmaxFp32BufPing = bufForRowmaxFp32Buf[MAX_BLOCK_PER_ONE_PROC_BACKWARD];
+
+    // load rowmax
+    lastAddr += bufForRowmaxFp32Size;
+
+    bufForRowsumFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+    bufForRowsumFp32BufPing = bufForRowsumFp32Buf[MAX_BLOCK_PER_ONE_PROC_BACKWARD];
+
+    // load rowsum
+    lastAddr += bufForRowsumFp32Size;
+
+    fp16_temp_2_ub_buf = UBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(lastAddr);
+
+    // exp(s-L)
+    lastAddr += temp2BufAddrFp16Size;
+
+    bufForMaskFp16Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(lastAddr);
+    bufForMaskFp16BufPing = bufForMaskFp16Buf[MAX_LENG_PER_UB_PROC_BACKWARD];
+
+    lastAddr += bufForMaskFp16Size;
+
+    bufForMaskFp32Buf = UBuf.GetBuffer<BufferType::ASCEND_UB, float>(lastAddr);
+    bufForMaskFp32BufPing = bufForMaskFp32Buf[MAX_LENG_PER_UB_PROC_BACKWARD];
+
+    doGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(this->doGm));
+    oGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(this->oGm));
+    sGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ WORKSPACE_DTYPE *>(this->sGm));
+    rowmaxGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->rowmaxGm));
+    rowsumGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(this->rowsumGm));
+    dpGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ WORKSPACE_DTYPE *>(this->dpGm));
+    maskGmGlobalTensor.SetGlobalBuffer(reinterpret_cast<__gm__ INPUT_T *>(this->maskGm));
+}
+
+template<typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackward<INPUT_T, IF_BF16>::ProcessBackwardFp32(int32_t curProcessLines,
+                                        int32_t oTotalOffset,
+                                        int32_t blockNumber,
+                                        int32_t sTotalOffset,
+                                        bool headApplyMask,
+                                        bool tailApplyMask,
+                                        __gm__ INPUT_T * __restrict__ gradAttnOutputGm,
+                                        __gm__ INPUT_T * __restrict__ attnOutputGm,
+                                        __gm__ WORKSPACE_DTYPE * __restrict__ attnScoreGm,
+                                        __gm__ float *__restrict__ attnRowmaxGm,
+                                        __gm__ float *__restrict__ attnRowsumGm,
+                                        __gm__ WORKSPACE_DTYPE_DP * __restrict__ gradPGm,
+                                        __gm__ INPUT_T * __restrict__ attnMaskGm)
+{
+    int32_t pingFlag = 0;
+
+    // dO * O
+    SET_FLAG(V, MTE2, EVENT_ID0);
+    SET_FLAG(V, MTE2, EVENT_ID1);
+
+    for (int i = 0; i < 2;i++) {
+        int32_t oOffset = curProcessLines / 2 * BASE_BLOCK_SIZE_LEN_BACKWARD * i;
+        int32_t lOffset = oOffset / BASE_BLOCK_SIZE_LEN_BACKWARD;
+        auto eventId = pingFlag ? EVENT_ID0:EVENT_ID1;
+
+        WAIT_FLAG(V, MTE2, eventId);
+        AscendC::LocalTensor<INPUT_T> temp0BufPingpongFP16 = pingFlag ? temp0BufFp16Ping : temp0BufFp16;
+        AscendC::LocalTensor<INPUT_T> temp1BufPingpongFP16 = pingFlag ? temp1BufFp16Ping : temp1BufFp16;
+        AscendC::LocalTensor<float> bufForLoadLFp32BufPingpongFp32 = pingFlag ?
+                                                            bufForLoadLFp32BufPing : bufForLoadLFp32Buf;
+        AscendC::LocalTensor<float> bufForRowmaxFp32BufPingpongFp32 = pingFlag ?
+                                                            bufForRowmaxFp32BufPing : bufForRowmaxFp32Buf;
+        AscendC::LocalTensor<float> bufForRowsumFp32BufPingpongFp32 = pingFlag ?
+                                                            bufForRowsumFp32BufPing : bufForRowsumFp32Buf;
+
+        // copy O to ub
+        AscendC::DataCopy(temp0BufPingpongFP16, oGmGlobalTensor[oTotalOffset + oOffset],
+                          AscendC::DataCopyParams(1,
+                                                  MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32, // 一个block=32B N*2/32
+                                                  0,
+                                                  0));
+        // copy dO to ub
+        AscendC::DataCopy(temp1BufPingpongFP16, doGmGlobalTensor[oTotalOffset + oOffset],
+                          AscendC::DataCopyParams(1,
+                                                  MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32, // 一个block=32B N*2/32
+                                                  0,
+                                                  0));
+        // copy rowmax to ub
+        AscendC::DataCopy(bufForRowmaxFp32BufPingpongFp32, rowmaxGmGlobalTensor[oTotalOffset / 128 + lOffset],
+                          AscendC::DataCopyParams(1,
+                                                  MAX_BLOCK_PER_ONE_PROC_BACKWARD * 4 / 32,
+                                                  0,
+                                                  0));
+        // copy rowsum to ub
+        AscendC::DataCopy(bufForRowsumFp32BufPingpongFp32, rowsumGmGlobalTensor[oTotalOffset / 128 + lOffset],
+                          AscendC::DataCopyParams(1,
+                                                  MAX_BLOCK_PER_ONE_PROC_BACKWARD * 4 / 32,
+                                                  0,
+                                                  0));
+        SET_FLAG(MTE2, V, eventId);
+        WAIT_FLAG(MTE2, V, eventId);
+
+        // O,dO fp16--fp32
+        AscendC::SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);
+        if constexpr(IF_BF16) {
+            conv_v<ArchType::ASCEND_V220>(temp0BufFp32, temp0BufPingpongFP16,
+                                          MAX_LENG_PER_UB_PROC_BACKWARD/64, 1, 1, 8, 4);
+            PIPE_BARRIER(V);
+            conv_v<ArchType::ASCEND_V220>(temp1BufFp32, temp1BufPingpongFP16,
+                                          MAX_LENG_PER_UB_PROC_BACKWARD/64, 1, 1, 8, 4);
+            PIPE_BARRIER(V);
+        }
+        else {
+            conv_v<ArchType::ASCEND_V220>(temp0BufFp32, temp0BufPingpongFP16,
+                                          MAX_LENG_PER_UB_PROC_BACKWARD/64, 1, 1, 8, 4);
+            PIPE_BARRIER(V);
+
+            conv_v<ArchType::ASCEND_V220>(temp1BufFp32, temp1BufPingpongFP16,
+                                          MAX_LENG_PER_UB_PROC_BACKWARD/64, 1, 1, 8, 4);
+            PIPE_BARRIER(V);
+        }
+
+        // O*dO
+        AscendC::SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);
+
+        mul_v<ArchType::ASCEND_V220>(temp0BufFp32,
+                                    temp0BufFp32,
+                                    temp1BufFp32,
+                                    MAX_LENG_PER_UB_PROC_BACKWARD/64,
+                                    1, 1, 1, 8, 8, 8);
+
+        PIPE_BARRIER(V);
+
+        // 折半
+        AscendC::SetVectorMask<int8_t>(0x0, 0xffffffffffffffff); // 只对每个repeat的前64个元素处理
+
+        add_v<ArchType::ASCEND_V220>(
+            temp0BufFp32,
+            temp0BufFp32,
+            temp0BufFp32[64],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        AscendC::SetVectorMask<int8_t>(0x0, 0xffffffff); // 只对每个repeat的前32个元素处理
+        add_v<ArchType::ASCEND_V220>(
+            temp0BufFp32,
+            temp0BufFp32,
+            temp0BufFp32[32],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        AscendC::SetVectorMask<int8_t>(0x0, 0xffff); // 只对每个repeat的前16个元素处理
+        add_v<ArchType::ASCEND_V220>(
+            temp0BufFp32,
+            temp0BufFp32,
+            temp0BufFp32[16],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        AscendC::SetVectorMask<int8_t>(0x0, 0xff); // 只对每个repeat的前8个元素处理
+        add_v<ArchType::ASCEND_V220>(
+            temp0BufFp32,
+            temp0BufFp32,
+            temp0BufFp32[8],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 1, 1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        // vcgadd将每32B的元素reduce add成一个数
+        AscendC::SetVectorMask<int8_t>(0x0, 0xffffffffffffffff);
+        auto rowsumOffset = pingFlag ? MAX_BLOCK_PER_ONE_PROC_BACKWARD : 0;
+        BlockReduceSum<float, false>(bufForCalcRowsumFp32Buf[rowsumOffset], temp0BufFp32, 8, 0, 1, 1, 8);
+        PIPE_BARRIER(V);
+
+        SET_FLAG(V, MTE2, eventId);
+        pingFlag = 1 - pingFlag;
+    }
+
+    WAIT_FLAG(V, MTE2, EVENT_ID0);
+    WAIT_FLAG(V, MTE2, EVENT_ID1);
+
+    AscendC::SetVectorMask<int8_t>(0xffffffffffffffff, 0xffffffffffffffff);
+
+    // rowsum广播（怎么控制一个数广播后和下面的一行去做）
+    brcb_v<ArchType::ASCEND_V220>(bufForBrcbRowsumFp32Buf, bufForCalcRowsumFp32Buf, 1, 8, 8);
+
+    PIPE_BARRIER(V);
+
+    ln_v<ArchType::ASCEND_V220>(bufForRowsumFp32Buf, bufForRowsumFp32Buf, 1, 1, 1, 8, 8);
+
+    PIPE_BARRIER(V);
+
+    add_v<ArchType::ASCEND_V220>(bufForLoadLFp32Buf, bufForRowmaxFp32Buf, bufForRowsumFp32Buf,
+        1, 1, 1, 1, BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+        BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+        BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+    PIPE_BARRIER(V);
+
+    // 8倍l
+    brcb_v<ArchType::ASCEND_V220>(bufForBrcbLFp32Buf, bufForLoadLFp32Buf, 1, 8, 8);
+    PIPE_BARRIER(V);
+
+    SET_FLAG(MTE3, MTE2, EVENT_ID0);
+    SET_FLAG(MTE3, MTE2, EVENT_ID1);
+
+    // 逐行计算
+    for (int k = 0; k < blockNumber; k++) {
+        // 每次copy 128 * block_num 个数据  ub = 128 * block_num * sizeof() * 2
+        for (int n = 0; n < 2; n++) {
+            // vector之间的偏移
+            int32_t sOffset = 64 * 128 * get_subblockid() + k * 128 * 128 + n * MAX_LENG_PER_UB_PROC_BACKWARD;
+            int32_t ssOffset = 64 * 128 * get_subblockid() + k * 128 * 128 + n * MAX_LENG_PER_UB_PROC_BACKWARD/2;
+            auto eventId = pingFlag ? EVENT_ID0:EVENT_ID1;
+            WAIT_FLAG(MTE3, MTE2, eventId);
+            AscendC::LocalTensor<float> temp0BufPingpongFP32 = pingFlag ? temp0BufFp32Ping : temp0BufFp32;
+            AscendC::LocalTensor<INPUT_T> temp0BufPingpongFP16 = pingFlag ? temp0BufFp16Ping : temp0BufFp16;
+            AscendC::LocalTensor<INPUT_T> bufForMaskFp16BufPingpong = pingFlag ?
+                                                            bufForMaskFp16BufPing : bufForMaskFp16Buf;
+            AscendC::LocalTensor<float> bufForMaskFp32BufPingpong = pingFlag ?
+                                                            bufForMaskFp32BufPing : bufForMaskFp32Buf;
+
+            // copy S to ub
+
+            AscendC::DataCopy(temp0BufPingpongFP32, sGmGlobalTensor[sTotalOffset + sOffset],
+                              AscendC::DataCopyParams(1,
+                                                      MAX_LENG_PER_UB_PROC_BACKWARD * REPEAT_TIME_FP32 / 32,
+                                                      0,
+                                                      0));
+            SET_FLAG(MTE2, V, eventId);
+            WAIT_FLAG(MTE2, V, eventId);
+            // S*scale
+
+            muls_v<ArchType::ASCEND_V220>(temp0BufPingpongFP32,
+                                          temp0BufPingpongFP32,
+                                          (float)SCALE,
+                                          MAX_BLOCK_PER_ONE_PROC_BACKWARD*2, 1, 1, 8, 8);
+            PIPE_BARRIER(V);
+
+            if (headApplyMask && k == 0) {
+                SET_FLAG(V, MTE2, eventId);
+                WAIT_FLAG(V, MTE2, eventId);
+
+                AscendC::DataCopy(bufForMaskFp16BufPingpong,
+                                maskGmGlobalTensor[64 * maskSeqLength  * get_subblockid() + 1 + n * 32 * maskSeqLength],
+                                AscendC::DataCopyParams(32,
+                                                        128 * 2 / 32,
+                                                        (maskSeqLength  -128)/16,
+                                                        0));
+                SET_FLAG(MTE2, V, eventId);
+                WAIT_FLAG(MTE2, V, eventId);
+
+                Duplicate<float, false>(temp1BufFp32, float(1.0), uint64_t(0),
+                                        MAX_BLOCK_PER_ONE_PROC_BACKWARD*2, 1, 8);
+
+                PIPE_BARRIER(V);
+
+                if constexpr(IF_BF16) {
+                    conv_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong, bufForMaskFp16BufPingpong,
+                                                  MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 8, 4);
+                    PIPE_BARRIER(V);
+                }
+                else {
+                    conv_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong, bufForMaskFp16BufPingpong,
+                                                  MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 8, 4);
+                    PIPE_BARRIER(V);
+                }
+
+                sub_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong,
+                                             temp1BufFp32,
+                                             bufForMaskFp32BufPingpong,
+                                             MAX_BLOCK_PER_ONE_PROC_BACKWARD*2, 1, 1, 1, 8, 8, 8);
+                PIPE_BARRIER(V);
+
+                muls_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong,
+                                              bufForMaskFp32BufPingpong,
+                                              (float)(PADDING_FOR_MAX), 32 *2, 1, 1, 8, 8);
+                PIPE_BARRIER(V);
+
+                add_v<ArchType::ASCEND_V220>(
+                    temp0BufPingpongFP32,
+                    temp0BufPingpongFP32,
+                    bufForMaskFp32BufPingpong, 32*2, 1, 1, 1, 8, 8, 8);
+                PIPE_BARRIER(V);
+            }
+
+            if (tailApplyMask && k == blockNumber - 1) {
+                SET_FLAG(V, MTE2, eventId);
+                WAIT_FLAG(V, MTE2, eventId);
+
+                AscendC::DataCopy(bufForMaskFp16BufPingpong,
+                                  maskGmGlobalTensor[64 * maskSeqLength * get_subblockid() + n * 32 * maskSeqLength],
+                                  AscendC::DataCopyParams(32,
+                                                          128 * 2 / 32,
+                                                          (maskSeqLength - 128)/16,
+                                                          0));
+                SET_FLAG(MTE2, V, eventId);
+                WAIT_FLAG(MTE2, V, eventId);
+
+                if constexpr(IF_BF16) {
+                    conv_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong, bufForMaskFp16BufPingpong,
+                                                  MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 8, 4);
+                    PIPE_BARRIER(V);
+                }
+                else {
+                    conv_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong, bufForMaskFp16BufPingpong,
+                                                  MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 8, 4);
+                    PIPE_BARRIER(V);
+                }
+
+                muls_v<ArchType::ASCEND_V220>(bufForMaskFp32BufPingpong,
+                                              bufForMaskFp32BufPingpong,
+                                              (float)(PADDING_FOR_MAX), 32 *2, 1, 1, 8, 8);
+                PIPE_BARRIER(V);
+
+                add_v<ArchType::ASCEND_V220>(
+                    temp0BufPingpongFP32,
+                    temp0BufPingpongFP32,
+                    bufForMaskFp32BufPingpong, 32*2, 1, 1, 1, 8, 8, 8);
+                PIPE_BARRIER(V);
+            }
+
+            auto rowsumOffset = pingFlag ? 32*8  : 0 ;
+            // vsub一次最多64个数 (s-L)
+            sub_v<ArchType::ASCEND_V220>(temp0BufPingpongFP32,
+                                         temp0BufPingpongFP32,
+                                         bufForBrcbLFp32Buf[rowsumOffset],
+                                         MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 0,
+                                         BASE_BLOCK_SIZE_LEN_BACKWARD / 8,
+                                         BASE_BLOCK_SIZE_LEN_BACKWARD / 8, 1);
+            PIPE_BARRIER(V);
+            sub_v<ArchType::ASCEND_V220>(temp0BufPingpongFP32[64],
+                                         temp0BufPingpongFP32[64],
+                                         bufForBrcbLFp32Buf[rowsumOffset],
+                                         MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 0,
+                                         BASE_BLOCK_SIZE_LEN_BACKWARD / 8,
+                                         BASE_BLOCK_SIZE_LEN_BACKWARD / 8, 1);
+            PIPE_BARRIER(V);
+
+            // exp(s-L)
+            exp_v<ArchType::ASCEND_V220>(
+                temp0BufPingpongFP32,
+                temp0BufPingpongFP32,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 8, 8);
+            PIPE_BARRIER(V);
+
+            if constexpr(IF_BF16) {
+                // fp32 exp(s-L)--fp16 exp(s-L)
+                convr_v<ArchType::ASCEND_V220>(temp0BufPingpongFP16, temp0BufPingpongFP32,
+                                              MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 4, 8);
+            }
+            else {
+                // fp32 exp(s-L)--fp16 exp(s-L)
+                conv_v<ArchType::ASCEND_V220>(temp0BufPingpongFP16, temp0BufPingpongFP32,
+                                              MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 4, 8);
+            }
+
+            SET_FLAG(V, MTE3, eventId);
+            WAIT_FLAG(V, MTE3, eventId);
+
+            // exp(s-L)结果拷回s的原位gm上
+            AscendC::DataCopy(sGmGlobalTensor[sTotalOffset + ssOffset],
+                              temp0BufPingpongFP16.template ReinterpretCast<WORKSPACE_DTYPE>(),
+                              AscendC::DataCopyParams(1,
+                                                      MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32,
+                                                      0,
+                                                      0));
+            SET_FLAG(MTE3, MTE2, eventId);
+            pingFlag = 1 - pingFlag;
+        }
+
+        WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
+        SET_FLAG(MTE3, MTE2, EVENT_ID0);
+
+        for (int m = 0; m < 2; m++) {
+            // vector之间的偏移
+            int32_t dp_offset = 64 * 128 * get_subblockid() + k * 128 * 128 + m * MAX_LENG_PER_UB_PROC_BACKWARD;
+            int32_t p_offset = 64 * 128 * get_subblockid() + k * 128 * 128 +
+                        m * MAX_LENG_PER_UB_PROC_BACKWARD/2;
+            auto eventId = pingFlag ? EVENT_ID0:EVENT_ID1;
+            WAIT_FLAG(MTE3, MTE2, eventId);
+            AscendC::LocalTensor<INPUT_T> temp0BufPingpongFP16 = pingFlag ? temp0BufFp16Ping : temp0BufFp16;
+            AscendC::LocalTensor<INPUT_T> temp1BufPingpongFP16 = pingFlag ? temp1BufFp16Ping : temp1BufFp16;
+            AscendC::LocalTensor<float> temp0BufPingpongFP32 = pingFlag ? temp0BufFp32Ping : temp0BufFp32;
+            AscendC::LocalTensor<float> temp1BufPingpongFP32 = pingFlag ? temp1BufFp32Ping : temp1BufFp32;
+
+            // copy dp to ub
+            AscendC::DataCopy(temp1BufPingpongFP32,
+                              dpGmGlobalTensor[sTotalOffset + dp_offset],
+                              AscendC::DataCopyParams(1,
+                                                      MAX_LENG_PER_UB_PROC_BACKWARD * 4 / 32,
+                                                      0,
+                                                      0));
+            SET_FLAG(MTE2, V, eventId);
+            WAIT_FLAG(MTE2, V, eventId);
+
+            // (dP - D)manm
+            auto rowsumOffset = pingFlag ? 32*8 : 0;
+            sub_v<ArchType::ASCEND_V220>(temp1BufPingpongFP32,
+                                         temp1BufPingpongFP32,
+                                         bufForBrcbRowsumFp32Buf[256*m],
+                                         MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 0, 16, 16, 1);
+
+            PIPE_BARRIER(V);
+            sub_v<ArchType::ASCEND_V220>(temp1BufPingpongFP32[64],
+                                         temp1BufPingpongFP32[64],
+                                         bufForBrcbRowsumFp32Buf[rowsumOffset],
+                                         MAX_BLOCK_PER_ONE_PROC_BACKWARD, 1, 1, 0, 16, 16, 1);
+
+            PIPE_BARRIER(V);
+
+            // P*(dP - D)
+            mul_v<ArchType::ASCEND_V220>(
+                temp1BufPingpongFP32,
+                temp0BufPingpongFP32,
+                temp1BufPingpongFP32,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 1, 8, 8, 8);
+
+            PIPE_BARRIER(V);
+
+            if constexpr(IF_BF16) {
+                // fp32 dS--fp16 dS  //pingpong拷出和addr拷出的区别!
+                convr_v<ArchType::ASCEND_V220>(temp1BufPingpongFP16, temp1BufPingpongFP32,
+                                              MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 4, 8);
+                PIPE_BARRIER(V);
+            }
+            else {
+                // fp32 dS--fp16 dS  //pingpong拷出和addr拷出的区别！
+                PIPE_BARRIER(V);
+                conv_v<ArchType::ASCEND_V220>(temp1BufPingpongFP16, temp1BufPingpongFP32,
+                                              MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2, 1, 1, 4, 8);
+            }
+
+            SET_FLAG(V, MTE3, eventId);
+            WAIT_FLAG(V, MTE3, eventId);
+
+            // ds结果拷回gm dp
+            AscendC::DataCopy(dpGmGlobalTensor[sTotalOffset + p_offset],
+                              temp1BufPingpongFP16.template ReinterpretCast<WORKSPACE_DTYPE_DP>(),
+                              AscendC::DataCopyParams(1,
+                                                      MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32,
+                                                      0,
+                                                      0));
+            SET_FLAG(MTE3, MTE2, eventId);
+            pingFlag = 1 - pingFlag;
+        }
+    }
+    WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
+    WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
+}
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/laser_attention_grad/op_kernel/vector_backward_band_op.h b/src/kernels/mixkernels/laser_attention_grad/op_kernel/vector_backward_band_op.h
new file mode 100644
index 00000000..4c5d6c92
--- /dev/null
+++ b/src/kernels/mixkernels/laser_attention_grad/op_kernel/vector_backward_band_op.h
@@ -0,0 +1,879 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifndef __VECTORBACKWARD_BAND_OP_H__
+#define __VECTORBACKWARD_BAND_OP_H__
+#define USE_ASCENDC 1
+
+#include "address_mapping_vector.h"
+#include "ppmatmul_const_grad.h"
+
+#include "kernel_operator.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/utils.h"
+using namespace AscendC;
+
+#ifdef __DAV_C220_VEC__
+
+namespace VEC_BACKWARD_BAND_OP {
+
+using WORKSPACE_DTYPE = float;
+using WORKSPACE_DTYPE_DP = float;
+constexpr int32_t repeat_time_fp32 = 4;
+
+template <typename TYPE>
+struct UB_FOR_BACKWARD {
+    LocalTensor<float> fp32_temp_0_ub_tensor_addr{};   // O,OdO,S,exp(s-L)
+    LocalTensor<float> fp32_temp_1_ub_tensor_addr{};   // dO,dp
+    LocalTensor<TYPE> fp16_temp_0_ub_tensor_addr{};    // O,OdO,S
+    LocalTensor<TYPE> fp16_temp_1_ub_tensor_addr{};    // dO,dp
+    LocalTensor<float> tensor_for_cacl_rowsum_fp32{};  // d=rowsum(odo)
+    // __ubuf__ TYPE* buf_for_cacl_rowsum_fp16; //d=rowsum(odo)
+    LocalTensor<float> tensor_for_brcb_rowsum_fp32{};  // 8倍rowsum
+    LocalTensor<float> tensor_for_load_l_fp32{};       // 装载参数L
+    LocalTensor<float> tensor_for_brcb_l_fp32{};       // 8倍L
+    LocalTensor<float> tensor_for_rowmax_fp32{};
+    LocalTensor<float> tensor_for_rowsum_fp32{};
+    LocalTensor<TYPE> fp16_temp_2_ub_tensor_addr{};   // exp(s-l)转fp16结果,ds转fp16结果
+    LocalTensor<TYPE> tensor_for_mask_fp16{};         // mask
+    LocalTensor<float> tensor_for_mask_fp32{};        // mask
+};                                                      // UB空间划分，求归一化
+
+template <typename INPUT_T, bool IF_BF16>
+class VectorBackwardBandOp {
+public:
+    __aicore__ inline VectorBackwardBandOp()
+    {}
+    __aicore__ inline void Init (__gm__ INPUT_T *__restrict__ gm_dO,
+                                __gm__ INPUT_T *__restrict__ gm_O,
+                                __gm__ WORKSPACE_DTYPE *__restrict__ gm_S,
+                                __gm__ float *__restrict__ gm_rowmax,
+                                __gm__ float *__restrict__ gm_rowsum,
+                                __gm__ WORKSPACE_DTYPE_DP *__restrict__ gm_dP,
+                                __gm__ INPUT_T *__restrict__ gm_mask, bool isTri,
+                                int32_t qSize,
+                                int32_t kSize,
+                                int32_t batch,
+                                int32_t head,
+                                int32_t base_length,
+                                int32_t blockNumPerCore,
+                                int32_t isSparse,
+                                int32_t windows_length,
+                                int32_t maskSeqLength,
+                                float scale);
+    __aicore__ inline void Run_op();
+
+private:
+    __aicore__ inline void allocate_ubuf_for_norm(UB_FOR_BACKWARD<INPUT_T> *ub_bkd);
+    __aicore__ inline void process_backward_fp32(
+        int32_t cur_proc_lines,  // 总处理64行（2个vector处理一个基本块）BASE_BLOCK_SIZE_LEN_BACKWARD /2
+        int32_t total_offset_o,  // O,dO的总偏移量，global_lines_in_heads * BASE_BLOCK_SIZE_LEN_BACKWARD    //
+                                 // L是global_lines_in_heads
+        int32_t block_number,    // S,dP一次处理几块  end_block - start_block
+        int32_t total_offset_s,  // S,dP的总偏移量start_block * BASE_BLOCK_DATA_NUM_BACKWARD
+        UB_FOR_BACKWARD<INPUT_T> ub_backward, bool head_apply_mask, bool tail_apply_mask);
+
+private:
+    GlobalTensor<INPUT_T>(gm_dO_tensor);             // dO
+    GlobalTensor<INPUT_T>(gm_O_tensor);              // O
+    GlobalTensor<WORKSPACE_DTYPE>(gm_S_tensor);      // S
+    GlobalTensor<float>(gm_rowmax_tensor);
+    GlobalTensor<float>(gm_rowsum_tensor);
+    GlobalTensor<WORKSPACE_DTYPE_DP>(gm_dP_tensor);
+    GlobalTensor<INPUT_T>(gm_mask_tensor);
+
+    AsdopsBuffer<ArchType::ASCEND_V220> calBuf;
+
+    Address::GLOBAL_INFO global_info;
+    Address::LOCAL_INFO local_info;
+    Address::SECTION_INFO section_info;
+    UB_FOR_BACKWARD<INPUT_T> ub_backward;
+    bool is_vector;
+    bool is_syn;
+    int32_t H;
+    int32_t maskSeqLength;
+    float SCALE;
+
+    Address::AddressMappingVector address_vector;
+};
+
+template <typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackwardBandOp<INPUT_T, IF_BF16>::Run_op()
+{
+    set_atomic_none();
+    SetMaskNorm();
+    SetVectorMask<float, AscendC::MaskMode::NORMAL>((uint64_t)-1, (uint64_t)-1);
+
+    for (int64_t round_id = 0; round_id < address_vector.get_total_rounds(); round_id++) {
+        if (is_syn) {
+            wait_flag_dev(AIC2AIVFLAGID);
+        }
+        if (address_vector.is_running(round_id) && is_vector) {
+            auto section_vector = address_vector.get_section_info(round_id);
+            for (int64_t section = 0; section < section_vector.sectionNum; section++) {
+                auto start_block = section_vector.section_start_block[section];
+                auto head_apply_mask = section_vector.head_apply_mask[section];
+                auto tail_apply_mask = section_vector.tail_apply_mask[section];
+                auto cur_process_blk_num = section_vector.len[section];
+                auto processLines = section_vector.processLines;
+                auto O_dO_offset = section_vector.O_dO_offset[section];
+                auto S_dP_offset = section_vector.S_dP_offset[section];
+
+                process_backward_fp32(processLines,
+                    O_dO_offset,
+                    cur_process_blk_num,
+                    S_dP_offset,
+                    ub_backward,
+                    head_apply_mask,
+                    tail_apply_mask);
+            }
+        }
+        if (is_syn) {
+            FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+        }
+    }
+}
+
+template <typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackwardBandOp<INPUT_T, IF_BF16>::Init(__gm__ INPUT_T *__restrict__ gm_dO,
+                                                            __gm__ INPUT_T *__restrict__ gm_O,
+                                                            __gm__ WORKSPACE_DTYPE *__restrict__ gm_S,
+                                                            __gm__ float *__restrict__ gm_rowmax,
+                                                            __gm__ float *__restrict__ gm_rowsum,
+                                                            __gm__ WORKSPACE_DTYPE_DP *__restrict__ gm_dP,
+                                                            __gm__ INPUT_T *__restrict__ gm_mask,
+                                                            bool isTri,
+                                                            int32_t qSize,
+                                                            int32_t kSize,
+                                                            int32_t batch,
+                                                            int32_t head,
+                                                            int32_t base_length,
+                                                            int32_t blockNumPerCore,
+                                                            int32_t isSparse,
+                                                            int32_t windows_length,
+                                                            int32_t maskSeqLength,
+                                                            float scale)
+{
+    this->maskSeqLength = maskSeqLength;
+    this->SCALE = scale;
+    this->gm_dO_tensor.SetGlobalBuffer(gm_dO);
+    this->gm_O_tensor.SetGlobalBuffer(gm_O);
+    this->gm_S_tensor.SetGlobalBuffer(gm_S);
+    this->gm_rowmax_tensor.SetGlobalBuffer(gm_rowmax);
+    this->gm_rowsum_tensor.SetGlobalBuffer(gm_rowsum);
+    this->gm_dP_tensor.SetGlobalBuffer(gm_dP);
+    this->gm_mask_tensor.SetGlobalBuffer(gm_mask);
+
+    is_vector = true;
+    is_syn = true;
+
+    this->global_info.seqLenQ = qSize;  // tiling_para[10]
+    this->global_info.seqLenK = kSize;
+    this->global_info.headNum = head;                          // tiling_para[12]
+    this->global_info.batchNum = batch;                        // tiling_para[0]
+    this->global_info.blockNumPerCube = blockNumPerCore;  //  tiling_para[11] / 128          // 暂定下标1
+    this->global_info.triMatrix = isTri;                       // 是不是也从tiling里读取？
+
+    this->global_info.isSparse = isSparse;
+    this->global_info.windowsBlockNum = windows_length / BASE_BLOCK_SIZE_LEN_BACKWARD;
+
+    H = qSize / 128;
+    allocate_ubuf_for_norm(&this->ub_backward);
+
+    // 寻址全局信息初始化
+    int64_t headDim = 128;
+    int64_t cubeNum = get_block_num();
+    address_vector.init_global_info(batch, head, qSize, kSize, headDim, isTri,
+        isSparse, windows_length, cubeNum, blockNumPerCore, 2);
+
+    // 寻址局部信息初始化
+    int64_t cube_core_index = get_block_idx();
+    int64_t vector_core_index = get_subblockid();
+    address_vector.init_local_info(cube_core_index, vector_core_index);
+}
+
+/**
+ * 为反向计算分配UB空间
+ */
+template <typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackwardBandOp<INPUT_T, IF_BF16>::allocate_ubuf_for_norm(UB_FOR_BACKWARD<INPUT_T> *ub_bkd)
+{
+    int32_t sizeof_fp32_temp_0_ub_buf_addr = MAX_LENG_PER_UB_PROC_BACKWARD * 4 * 2;  // o       //32kb
+    int32_t sizeof_fp32_temp_1_ub_buf_addr = MAX_LENG_PER_UB_PROC_BACKWARD * 4 * 2;  // do       //32kb   //64kb,mask
+
+    int32_t sizeof_fp16_temp_0_ub_buf_addr = MAX_LENG_PER_UB_PROC_BACKWARD * 2 * 2;  //  ping-pong    //16kb
+    int32_t sizeof_fp16_temp_1_ub_buf_addr = MAX_LENG_PER_UB_PROC_BACKWARD * 2 * 2;  //              //16kb
+
+    int32_t sizeof_buf_for_cacl_rowsum_fp32 = 128 * 4 * 2;  // 最多处理的FP32数字个数  //d=rowsum(odo)
+    int32_t sizeof_buf_for_brcb_rowsum_fp32 =
+        sizeof_buf_for_cacl_rowsum_fp32 * (32 / 4);  // 展开做32字节对齐       //8倍rowsum
+
+    int32_t sizeof_buf_for_load_l_fp32 = 128 * 4 * 2;                            // L
+    int32_t sizeof_buf_for_brcb_l_fp32 = sizeof_buf_for_load_l_fp32 * (32 / 4);  // 8倍L
+
+    int32_t sizeof_buf_for_rowmax_fp32 = BASE_BLOCK_SIZE_LEN_BACKWARD * 4 * 2;
+    int32_t sizeof_buf_for_rowsum_fp32 = BASE_BLOCK_SIZE_LEN_BACKWARD * 4 * 2;
+
+    int32_t sizeof_buf_for_mask_fp16 = 64 * 128 * 2;
+    int32_t sizeof_buf_for_mask_fp32 = 64 * 128 * 4;
+
+    int32_t last_addr = 0;
+
+    ub_bkd->fp32_temp_0_ub_tensor_addr = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);
+    last_addr += sizeof_fp32_temp_0_ub_buf_addr;
+
+    ub_bkd->fp32_temp_1_ub_tensor_addr = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);
+    last_addr += sizeof_fp32_temp_1_ub_buf_addr;
+
+    ub_bkd->fp16_temp_0_ub_tensor_addr = calBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(last_addr);
+    last_addr += sizeof_fp16_temp_0_ub_buf_addr;
+
+    ub_bkd->fp16_temp_1_ub_tensor_addr = calBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(last_addr);
+    last_addr += sizeof_fp16_temp_1_ub_buf_addr;
+
+    // cacl rowsum  32字节对齐后，输出到 buf_for_brcb_rowsum_fp32
+    ub_bkd->tensor_for_cacl_rowsum_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);
+    last_addr += sizeof_buf_for_cacl_rowsum_fp32;
+
+    // 32字节对齐后，输出到 buf_for_brcb_rowsum_fp32 last_addr += sizeof_buf_for_cacl_rowsum_fp16;
+    ub_bkd->tensor_for_brcb_rowsum_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);
+    last_addr += sizeof_buf_for_brcb_rowsum_fp32;
+
+    // D计算完毕后,s复用fp32_temp_0_ub_buf_addr，s-l也可以用该空间，exp(s-l)
+    ub_bkd->tensor_for_load_l_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr); // load l
+    last_addr += sizeof_buf_for_load_l_fp32;
+
+    ub_bkd->tensor_for_brcb_l_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);   // load 8倍l
+    last_addr += sizeof_buf_for_brcb_l_fp32;
+
+    ub_bkd->tensor_for_rowmax_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr); // load rowmax
+    last_addr += sizeof_buf_for_rowmax_fp32;
+
+    ub_bkd-> tensor_for_rowsum_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);   // load rowsum
+    last_addr += sizeof_buf_for_rowsum_fp32;
+
+    ub_bkd-> tensor_for_mask_fp16 = calBuf.GetBuffer<BufferType::ASCEND_UB, INPUT_T>(last_addr);
+    last_addr += sizeof_buf_for_mask_fp16;
+
+    ub_bkd-> tensor_for_mask_fp32 = calBuf.GetBuffer<BufferType::ASCEND_UB, float>(last_addr);
+}
+
+template <typename INPUT_T, bool IF_BF16>
+__aicore__ inline void VectorBackwardBandOp<INPUT_T, IF_BF16>::process_backward_fp32(
+    int32_t cur_proc_lines,  // 总处理64行（2个vector处理一个基本块）BASE_BLOCK_SIZE_LEN_BACKWARD /2
+    int32_t total_offset_o,  // O,dO的总偏移量，global_lines_in_heads * BASE_BLOCK_SIZE_LEN_BACKWARD
+                             // L是global_lines_in_heads
+    int32_t block_number,    // S,dP一次处理几块  end_block - start_block
+    int32_t total_offset_s,  // S,dP的总偏移量start_block * BASE_BLOCK_DATA_NUM_BACKWARD
+    UB_FOR_BACKWARD<INPUT_T> ub_backward, bool head_apply_mask, bool tail_apply_mask)
+{
+    int32_t ping_flag = 0;
+
+    SET_FLAG(V, MTE2, EVENT_ID0);
+    SET_FLAG(V, MTE2, EVENT_ID1);
+
+    for (int i = 0; i < 2; i++) {
+
+        int32_t o_offset = cur_proc_lines / 2 * BASE_BLOCK_SIZE_LEN_BACKWARD * i;
+        int32_t l_offset = o_offset / BASE_BLOCK_SIZE_LEN_BACKWARD;
+        auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+        WAIT_FLAG(V, MTE2, event_id);
+
+        LocalTensor<INPUT_T> fp16_temp_0_ub_tensor_addr_pingpong =
+            ping_flag ? ub_backward.fp16_temp_0_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                      : ub_backward.fp16_temp_0_ub_tensor_addr;
+        LocalTensor<INPUT_T> fp16_temp_1_ub_tensor_addr_pingpong =
+            ping_flag ? ub_backward.fp16_temp_1_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                      : ub_backward.fp16_temp_1_ub_tensor_addr;
+        LocalTensor<float> tensor_for_load_l_fp32_pingpong =
+            ping_flag ? ub_backward.tensor_for_load_l_fp32[MAX_BLOCK_PER_ONE_PROC_BACKWARD]
+                      : ub_backward.tensor_for_load_l_fp32;
+        LocalTensor<float> tensor_for_rowmax_fp32_pingpong = ping_flag
+            ? ub_backward.tensor_for_rowmax_fp32[MAX_BLOCK_PER_ONE_PROC_BACKWARD] : ub_backward.tensor_for_rowmax_fp32;
+        LocalTensor<float> tensor_for_rowsum_fp32_pingpong = ping_flag
+            ? ub_backward.tensor_for_rowsum_fp32[MAX_BLOCK_PER_ONE_PROC_BACKWARD] : ub_backward.tensor_for_rowsum_fp32;
+
+        gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(
+            fp16_temp_0_ub_tensor_addr_pingpong,
+            gm_O_tensor[total_offset_o + o_offset],
+            0,
+            1,
+            MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32,  // 一个block=32B N*2/32
+            0,
+            0);
+
+        gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(
+            fp16_temp_1_ub_tensor_addr_pingpong,
+            gm_dO_tensor[total_offset_o + o_offset],
+            0,
+            1,
+            MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32,
+            0,
+            0);
+
+        gm_to_ub<ArchType::ASCEND_V220, float>(
+            tensor_for_rowmax_fp32_pingpong,
+            gm_rowmax_tensor[total_offset_o / 128 + l_offset],
+            0,
+            1,
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD * 4 / 32,
+            0,
+            0);
+
+        gm_to_ub<ArchType::ASCEND_V220, float>(
+            tensor_for_rowsum_fp32_pingpong,
+            gm_rowsum_tensor[total_offset_o / 128 + l_offset],
+            0,
+            1,
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD * 4 / 32,
+            0,
+            0);
+
+        SET_FLAG(MTE2, V, event_id);
+        WAIT_FLAG(MTE2, V, event_id);
+
+        // O,dO fp16--fp32
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0xffffffffffffffff, 0xffffffffffffffff);
+        if constexpr (IF_BF16) {
+            conv_v<ArchType::ASCEND_V220, INPUT_T, float>(
+                ub_backward.fp32_temp_0_ub_tensor_addr,
+                fp16_temp_0_ub_tensor_addr_pingpong,
+                MAX_LENG_PER_UB_PROC_BACKWARD / 64,
+                1,
+                1,
+                8,
+                4);
+            PIPE_BARRIER(V);
+            conv_v<ArchType::ASCEND_V220, INPUT_T, float>(
+                ub_backward.fp32_temp_1_ub_tensor_addr,
+                fp16_temp_1_ub_tensor_addr_pingpong,
+                MAX_LENG_PER_UB_PROC_BACKWARD / 64,
+                1,
+                1,
+                8,
+                4);
+            PIPE_BARRIER(V);
+        } else {
+            conv_v<ArchType::ASCEND_V220, INPUT_T, float>(
+                ub_backward.fp32_temp_0_ub_tensor_addr,
+                fp16_temp_0_ub_tensor_addr_pingpong,
+                MAX_LENG_PER_UB_PROC_BACKWARD / 64,
+                1,
+                1,
+                8,
+                4);
+            PIPE_BARRIER(V);
+            conv_v<ArchType::ASCEND_V220, INPUT_T, float>(
+                ub_backward.fp32_temp_1_ub_tensor_addr,
+                fp16_temp_1_ub_tensor_addr_pingpong,
+                MAX_LENG_PER_UB_PROC_BACKWARD / 64,
+                1,
+                1,
+                8,
+                4);
+            PIPE_BARRIER(V);
+        }
+
+        // O*dO
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0xffffffffffffffff, 0xffffffffffffffff);
+        mul_v<ArchType::ASCEND_V220, float>(ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_1_ub_tensor_addr,
+            MAX_LENG_PER_UB_PROC_BACKWARD / 64,
+            1,
+            1,
+            1,
+            8,
+            8,
+            8);
+
+        PIPE_BARRIER(V);
+
+        // 折半
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0x0, 0xffffffffffffffff);  // 只对每个repeat的前64个元素处理
+        add_v<ArchType::ASCEND_V220, float>(ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr[64],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+            1,
+            1,
+            1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0x0, 0xffffffff);  // 只对每个repeat的前32个元素处理
+        add_v<ArchType::ASCEND_V220, float>(ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr[32],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+            1,
+            1,
+            1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0x0, 0xffff);  // 只对每个repeat的前16个元素处理
+        add_v<ArchType::ASCEND_V220, float>(ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr[16],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+            1,
+            1,
+            1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0x0, 0xff);  // 只对每个repeat的前8个元素处理
+        add_v<ArchType::ASCEND_V220, float>(ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr,
+            ub_backward.fp32_temp_0_ub_tensor_addr[8],
+            MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+            1,
+            1,
+            1,
+            1,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+            BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+        PIPE_BARRIER(V);
+
+        // vcgadd将每32B的元素reduce add成一个数
+        SetVectorMask<float, AscendC::MaskMode::NORMAL>(0x0, 0xffffffffffffffff);
+        auto rowsum_offset = ping_flag ? MAX_BLOCK_PER_ONE_PROC_BACKWARD : 0;
+
+        BlockReduceSum<float, false>(
+            ub_backward.tensor_for_cacl_rowsum_fp32[rowsum_offset],
+            ub_backward.fp32_temp_0_ub_tensor_addr,
+            8, 0, 1, 1, 8);
+        PIPE_BARRIER(V);
+
+        SET_FLAG(V, MTE2, event_id);
+        ping_flag = 1 - ping_flag;
+    }
+
+    WAIT_FLAG(V, MTE2, EVENT_ID0);
+    WAIT_FLAG(V, MTE2, EVENT_ID1);
+
+    SetVectorMask<float, AscendC::MaskMode::NORMAL>(0xffffffffffffffff, 0xffffffffffffffff);
+    brcb_v<ArchType::ASCEND_V220, uint32_t>(
+        ub_backward.tensor_for_brcb_rowsum_fp32.template ReinterpretCast<uint32_t>(),
+        ub_backward.tensor_for_cacl_rowsum_fp32.template ReinterpretCast<uint32_t>(),
+        1,
+        8,
+        8);
+    PIPE_BARRIER(V);
+
+    ln_v<ArchType::ASCEND_V220, float>(
+        ub_backward.tensor_for_rowsum_fp32,
+        ub_backward.tensor_for_rowsum_fp32,
+        1,
+        1,
+        1,
+        8,
+        8);
+    PIPE_BARRIER(V);
+
+    add_v<ArchType::ASCEND_V220, float>(
+        ub_backward.tensor_for_load_l_fp32,
+        ub_backward.tensor_for_rowmax_fp32,
+        ub_backward.tensor_for_rowsum_fp32,
+        1, 1, 1, 1, BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32, BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32,
+        BASE_BLOCK_SIZE_LEN_BACKWARD * 4 / 32);
+    PIPE_BARRIER(V);
+
+    brcb_v<ArchType::ASCEND_V220, uint32_t>(
+        ub_backward.tensor_for_brcb_l_fp32.template ReinterpretCast<uint32_t>(),
+        ub_backward.tensor_for_load_l_fp32.template ReinterpretCast<uint32_t>(),
+        1,
+        8,
+        8);
+    PIPE_BARRIER(V);
+
+    SET_FLAG(MTE3, MTE2, EVENT_ID0);
+    SET_FLAG(MTE3, MTE2, EVENT_ID1);
+
+    // 逐行计算
+    for (int k = 0; k < block_number; k++) {
+        // 每次copy 128 * block_num 个数据  ub = 128 * block_num * sizeof() * 2
+        for (int n = 0; n < 2; n++) {
+            int32_t s_offset =
+                64 * 128 * get_subblockid() + k * 128 * 128 + n * MAX_LENG_PER_UB_PROC_BACKWARD;  // vector之间的偏移
+            int32_t ss_offset = 64 * 128 * get_subblockid() + k * 128 * 128 + n * MAX_LENG_PER_UB_PROC_BACKWARD / 2;
+            auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+            WAIT_FLAG(MTE3, MTE2, event_id);
+
+            LocalTensor<float> fp32_temp_0_ub_tensor_addr_pingpong =
+                ping_flag ? ub_backward.fp32_temp_0_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                      : ub_backward.fp32_temp_0_ub_tensor_addr;
+            LocalTensor<INPUT_T> fp16_temp_0_ub_tensor_addr_pingpong =
+                ping_flag ? ub_backward.fp16_temp_0_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                        : ub_backward.fp16_temp_0_ub_tensor_addr;
+            LocalTensor<INPUT_T> tensor_for_mask_fp16_pingpong =
+                ping_flag ? ub_backward.tensor_for_mask_fp16[MAX_LENG_PER_UB_PROC_BACKWARD]
+                        : ub_backward.tensor_for_mask_fp16;
+            LocalTensor<float> tensor_for_mask_fp32_pingpong =
+                ping_flag ? ub_backward.tensor_for_mask_fp32[MAX_LENG_PER_UB_PROC_BACKWARD]
+                        : ub_backward.tensor_for_mask_fp32;
+
+            gm_to_ub<ArchType::ASCEND_V220, float>(
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                gm_S_tensor[total_offset_s + s_offset],
+                0,                                // sid
+                1,                                // nBurst
+                MAX_LENG_PER_UB_PROC_BACKWARD * repeat_time_fp32 / 32,  // lenBurst
+                0,                                // srcGap
+                0                                 // dstGap
+                );
+
+            SET_FLAG(MTE2, V, event_id);
+            WAIT_FLAG(MTE2, V, event_id);
+
+            muls_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                (float)SCALE,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                8,
+                8); // S*scale
+            PIPE_BARRIER(V);
+
+            if (head_apply_mask && k == 0) {
+                SET_FLAG(V, MTE2, event_id);
+                WAIT_FLAG(V, MTE2, event_id);
+
+                gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(
+                    tensor_for_mask_fp16_pingpong,
+                    gm_mask_tensor[64 * maskSeqLength * get_subblockid() + 1 + n * 32 * maskSeqLength],
+                    0,
+                    32,            // 1
+                    128 * 2 / 32,  // MAX_LENG_PER_UB_PROC_BACKWARD * 2 *2 / 32
+                    (maskSeqLength - 128) / 16,
+                    0);
+
+                SET_FLAG(MTE2, V, event_id);
+                WAIT_FLAG(MTE2, V, event_id);
+
+                Duplicate<float, false>(
+                    ub_backward.fp32_temp_1_ub_tensor_addr,
+                    float(1.0),
+                    uint64_t(0),
+                    MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                    1,
+                    8);
+                PIPE_BARRIER(V);
+
+                conv_v<ArchType::ASCEND_V220, INPUT_T, float>(tensor_for_mask_fp32_pingpong,
+                    tensor_for_mask_fp16_pingpong,
+                    MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                    1,
+                    1,
+                    8,
+                    4);
+                PIPE_BARRIER(V);
+
+                sub_v<ArchType::ASCEND_V220, float>(
+                    tensor_for_mask_fp32_pingpong,
+                    ub_backward.fp32_temp_1_ub_tensor_addr,
+                    tensor_for_mask_fp32_pingpong,
+                    MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                    1,
+                    1,
+                    1,
+                    8,
+                    8,
+                    8);
+                PIPE_BARRIER(V);
+
+                muls_v<ArchType::ASCEND_V220, float>(
+                    tensor_for_mask_fp32_pingpong,
+                    tensor_for_mask_fp32_pingpong,
+                    (float)(PADDING_FOR_MAX),
+                    32 * 2,
+                    1,
+                    1,
+                    8,
+                    8);
+                PIPE_BARRIER(V);
+
+                add_v<ArchType::ASCEND_V220, float>(
+                    fp32_temp_0_ub_tensor_addr_pingpong,
+                    fp32_temp_0_ub_tensor_addr_pingpong,
+                    tensor_for_mask_fp32_pingpong,
+                    32 * 2,
+                    1,
+                    1,
+                    1,
+                    8,
+                    8,
+                    8);  //
+                PIPE_BARRIER(V);
+            }
+
+            if (tail_apply_mask && k == block_number - 1) {
+
+                SET_FLAG(V, MTE2, event_id);
+                WAIT_FLAG(V, MTE2, event_id);
+
+                gm_to_ub<ArchType::ASCEND_V220, INPUT_T>(
+                    tensor_for_mask_fp16_pingpong,
+                    gm_mask_tensor[64 * maskSeqLength * get_subblockid() + n * 32 * maskSeqLength],
+                    0,
+                    32,            // 1
+                    128 * 2 / 32,  // MAX_LENG_PER_UB_PROC_BACKWARD * 2 *2 / 32
+                    (maskSeqLength - 128) / 16,
+                    0);
+
+                SET_FLAG(MTE2, V, event_id);
+                WAIT_FLAG(MTE2, V, event_id);
+
+                conv_v<ArchType::ASCEND_V220, INPUT_T, float>(
+                    tensor_for_mask_fp32_pingpong,
+                    tensor_for_mask_fp16_pingpong,
+                    MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                    1,
+                    1,
+                    8,
+                    4);
+                PIPE_BARRIER(V);
+
+                muls_v<ArchType::ASCEND_V220, float>(
+                    tensor_for_mask_fp32_pingpong,
+                    tensor_for_mask_fp32_pingpong,
+                    (float)(PADDING_FOR_MAX),
+                    32 * 2,
+                    1,
+                    1,
+                    8,
+                    8);
+                PIPE_BARRIER(V);
+
+                add_v<ArchType::ASCEND_V220, float>(
+                    fp32_temp_0_ub_tensor_addr_pingpong,
+                    fp32_temp_0_ub_tensor_addr_pingpong,
+                    tensor_for_mask_fp32_pingpong,
+                    32 * 2,
+                    1,
+                    1,
+                    1,
+                    8,
+                    8,
+                    8);  //
+                PIPE_BARRIER(V);
+            }
+
+            auto rowsum_offset = ping_flag ? 32 * 8 : 0;
+            sub_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                ub_backward.tensor_for_brcb_l_fp32[rowsum_offset],
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+                1,
+                1,
+                0,
+                BASE_BLOCK_SIZE_LEN_BACKWARD / 8,
+                BASE_BLOCK_SIZE_LEN_BACKWARD / 8,
+                1);
+            PIPE_BARRIER(V);
+            sub_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_0_ub_tensor_addr_pingpong[64],
+                fp32_temp_0_ub_tensor_addr_pingpong[64],
+                ub_backward.tensor_for_brcb_l_fp32[rowsum_offset],
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+                1,
+                1,
+                0,
+                BASE_BLOCK_SIZE_LEN_BACKWARD / 8,
+                BASE_BLOCK_SIZE_LEN_BACKWARD / 8,
+                1);
+            PIPE_BARRIER(V);
+
+            exp_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                8,
+                8);  // exp(s-L)
+            PIPE_BARRIER(V);
+
+            if constexpr (IF_BF16) {
+                convr_v<ArchType::ASCEND_V220, float, INPUT_T>(
+                fp16_temp_0_ub_tensor_addr_pingpong,
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                4,
+                8);
+            } else {
+                conv_v<ArchType::ASCEND_V220, float, INPUT_T>(
+                fp16_temp_0_ub_tensor_addr_pingpong,
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                4,
+                8);
+            }
+
+            PIPE_BARRIER(V);
+
+            SET_FLAG(V, MTE3, event_id);
+            WAIT_FLAG(V, MTE3, event_id);
+
+            const auto &tmpGmTensor = gm_S_tensor[total_offset_s + ss_offset];
+            GlobalTensor<INPUT_T> &tmp_gm_S_tensor = (GlobalTensor<INPUT_T>&)(tmpGmTensor);
+
+            ub_to_gm<ArchType::ASCEND_V220, INPUT_T>(
+                tmp_gm_S_tensor,
+                fp16_temp_0_ub_tensor_addr_pingpong,
+                0,
+                1,
+                MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32,
+                0,
+                0);
+
+            SET_FLAG(MTE3, MTE2, event_id);
+
+            ping_flag = 1 - ping_flag;
+        }
+
+        for (int m = 0; m < 2; m++) {
+            int32_t dp_offset =
+                64 * 128 * get_subblockid() + k * 128 * 128 + m * MAX_LENG_PER_UB_PROC_BACKWARD;  // vector之间的偏移
+            int32_t p_offset = 64 * 128 * get_subblockid() + k * 128 * 128 + m * MAX_LENG_PER_UB_PROC_BACKWARD / 2;
+            auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+            WAIT_FLAG(MTE3, MTE2, event_id);
+
+            LocalTensor<INPUT_T> fp16_temp_0_ub_tensor_addr_pingpong =
+                ping_flag ? ub_backward.fp16_temp_0_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                      : ub_backward.fp16_temp_0_ub_tensor_addr;
+            LocalTensor<INPUT_T> fp16_temp_1_ub_tensor_addr_pingpong =
+                ping_flag ? ub_backward.fp16_temp_1_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                        : ub_backward.fp16_temp_1_ub_tensor_addr;
+            LocalTensor<float> fp32_temp_0_ub_tensor_addr_pingpong =
+                ping_flag ? ub_backward.fp32_temp_0_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                        : ub_backward.fp32_temp_0_ub_tensor_addr;
+            LocalTensor<float> fp32_temp_1_ub_tensor_addr_pingpong =
+                ping_flag ? ub_backward.fp32_temp_1_ub_tensor_addr[MAX_LENG_PER_UB_PROC_BACKWARD]
+                        : ub_backward.fp32_temp_1_ub_tensor_addr;
+            // copy dp to ub
+            gm_to_ub<ArchType::ASCEND_V220, float>(
+                    fp32_temp_1_ub_tensor_addr_pingpong,
+                    gm_dP_tensor[total_offset_s + dp_offset],
+                    0,
+                    1,            // 1
+                    MAX_LENG_PER_UB_PROC_BACKWARD * 4 / 32,
+                    0,
+                    0);
+
+            SET_FLAG(MTE2, V, event_id);
+            WAIT_FLAG(MTE2, V, event_id);
+
+            // (dP - D)manm
+            auto rowsum_offset = ping_flag ? 32 * 8 : 0;
+            sub_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_1_ub_tensor_addr_pingpong,
+                fp32_temp_1_ub_tensor_addr_pingpong,
+                ub_backward.tensor_for_brcb_rowsum_fp32[256 * m],
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+                1,
+                1,
+                0,
+                16,
+                16,
+                1);
+            PIPE_BARRIER(V);
+            sub_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_1_ub_tensor_addr_pingpong[64],
+                fp32_temp_1_ub_tensor_addr_pingpong[64],
+                ub_backward.tensor_for_brcb_rowsum_fp32[256 * m],
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD,
+                1,
+                1,
+                0,
+                16,
+                16,
+                1);
+            PIPE_BARRIER(V);
+
+            // P * (dP - D)
+            mul_v<ArchType::ASCEND_V220, float>(
+                fp32_temp_1_ub_tensor_addr_pingpong,
+                fp32_temp_0_ub_tensor_addr_pingpong,
+                fp32_temp_1_ub_tensor_addr_pingpong,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                1,
+                8,
+                8,
+                8);
+            PIPE_BARRIER(V);
+
+            if constexpr (IF_BF16) {
+                convr_v<ArchType::ASCEND_V220, float, INPUT_T>(
+                fp16_temp_1_ub_tensor_addr_pingpong,
+                fp32_temp_1_ub_tensor_addr_pingpong,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                4,
+                8);
+                PIPE_BARRIER(V);
+            } else {
+                conv_v<ArchType::ASCEND_V220, float, INPUT_T>(
+                fp16_temp_1_ub_tensor_addr_pingpong,
+                fp32_temp_1_ub_tensor_addr_pingpong,
+                MAX_BLOCK_PER_ONE_PROC_BACKWARD * 2,
+                1,
+                1,
+                4,
+                8);
+                PIPE_BARRIER(V);
+            }
+
+
+            SET_FLAG(V, MTE3, event_id);
+            WAIT_FLAG(V, MTE3, event_id);
+
+            const auto &tmpGmTensor = gm_dP_tensor[total_offset_s + p_offset];
+            GlobalTensor<INPUT_T> &tmp_gm_dP_tensor = (GlobalTensor<INPUT_T>&)(tmpGmTensor);
+            ub_to_gm<ArchType::ASCEND_V220, INPUT_T>(
+                tmp_gm_dP_tensor,
+                fp16_temp_1_ub_tensor_addr_pingpong,
+                0,
+                1,
+                MAX_LENG_PER_UB_PROC_BACKWARD * 2 / 32,
+                0,
+                0);
+
+            SET_FLAG(MTE3, MTE2, event_id);
+            ping_flag = 1 - ping_flag;
+        }
+    }
+    WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
+    WAIT_FLAG(MTE3, MTE2, EVENT_ID1);
+    // }
+}
+}  // namespace VEC_BACKWARD_FP32_OP
+#endif
+
+#endif
\ No newline at end of file
diff --git a/src/kernels/mixkernels/moe_gmm/CMakeLists.txt b/src/kernels/mixkernels/moe_gmm/CMakeLists.txt
index f330c78b..3340e038 100644
--- a/src/kernels/mixkernels/moe_gmm/CMakeLists.txt
+++ b/src/kernels/mixkernels/moe_gmm/CMakeLists.txt
@@ -12,3 +12,6 @@ set(moe_gmm_src
 )
 
 add_operation(MoeGmmOperation "${moe_gmm_src}")
+
+add_kernel(moe_gmm ascend910b mix op_kernel/moe_gmm.cce MoeGmmKernel)
+add_kernel(moe_gmm_w8a8 ascend910b mix op_kernel/moe_gmm_w8a8.cce MoeGmmW8a8Kernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/moe_gmm/op_kernel/moe_gmm.cce b/src/kernels/mixkernels/moe_gmm/op_kernel/moe_gmm.cce
new file mode 100644
index 00000000..1c216b00
--- /dev/null
+++ b/src/kernels/mixkernels/moe_gmm/op_kernel/moe_gmm.cce
@@ -0,0 +1,1339 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#ifdef __CCE_KT_TEST__
+#include "stub_def.h"
+#include "stub_fun.h"
+using __bf16 = bfloat16_t;
+#else
+#define __aicore__ [aicore]
+#endif
+
+#include "mixkernels/moe_gmm/tiling/tiling_data.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/mem.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/utils.h"
+
+// 同步信号
+constexpr int AIC2AIVFLAGID = 2;
+constexpr int AIV2AICFLAGID = 3;
+constexpr uint32_t L0_PINGPONG_BUFFER_LEN = 16384;
+constexpr uint32_t L1_PINGPONG_BUFFER_LEN = 131072;
+constexpr uint32_t CONST_16 = 16;
+constexpr uint32_t CONST_64 = 64;
+constexpr uint32_t CONST_256 = 256;
+constexpr uint64_t ND2NZ_STRIDE_LIMIT = 65536;
+constexpr int32_t BLOCK_SIZE = 32;
+constexpr int AIVFLAGID = 1;
+
+#ifdef __DAV_C220_CUBE__
+
+template <uint32_t SwizzleDir, bool TA, bool TB, typename InDtype = half, typename OutDtype = half>
+class PpMatmul {
+public:
+    __aicore__ explicit PpMatmul(){};
+
+    __aicore__ __force_inline__ void InitCube(__gm__ uint8_t *__restrict__ gmPtrA,
+                                              __gm__ uint8_t *__restrict__ gmPtrB,
+                                              __gm__ uint8_t *__restrict__ gmPtrFlag,
+                                              __gm__ uint8_t *__restrict__ gmPtrIndex,
+                                              __gm__ uint8_t *__restrict__ gmPtrC,
+                                              __gm__ uint8_t *__restrict__ gmPtrWorkspace,
+                                              __gm__ uint8_t *__restrict__ gmPtrWorkspaceHp,
+                                              __gm__ uint8_t *__restrict__ tiling_data)
+    {
+        auto gm_tiling_data = reinterpret_cast<__gm__ AtbOps::MoeGmmTilingData *>(tiling_data);
+        batch_size = gm_tiling_data->batch;
+        m = gm_tiling_data->m * gm_tiling_data->batch;
+        k = gm_tiling_data->k;
+        n = gm_tiling_data->n;
+        m0 = gm_tiling_data->m0;
+        k0 = gm_tiling_data->k0;
+        n0 = gm_tiling_data->n0;
+        m_loop = gm_tiling_data->mLoop;
+        k_loop = gm_tiling_data->kLoop;
+        n_loop = gm_tiling_data->nLoop;
+        core_loop = gm_tiling_data->coreLoop;
+        swizzle_cnt = gm_tiling_data->swizzlCount;
+        en_shuffle_k = gm_tiling_data->enShuffleK;
+        allM = gm_tiling_data->allM;
+        num_core = AscendC::GetBlockNum();
+        core_idx = AscendC::GetBlockIdx();
+        ping_flag = 1;
+        moeUp = gm_tiling_data->moeUp;
+
+        gmA_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(gmPtrA));
+        gmB_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(gmPtrB));
+        gmFlag_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(gmPtrFlag));
+        gmC_.SetGlobalBuffer(reinterpret_cast<__gm__ OutDtype *>(gmPtrC));
+        gmWorkspace_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(gmPtrWorkspace));
+        gmWorkspaceHp_.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmPtrWorkspace));
+
+        AsdopsBuffer<ArchType::ASCEND_V220> buf;
+        l1BaseA_ = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
+        l1BaseB_ =
+            buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(RoundUp<CONST_256>((uint64_t)m0 * k0 * sizeof(InDtype)));
+        l0BaseA_ = buf.GetBuffer<BufferType::ASCEND_L0A, InDtype>(0);
+        l0BaseB_ = buf.GetBuffer<BufferType::ASCEND_L0B, InDtype>(0);
+        l0C_ = buf.GetBuffer<BufferType::ASCEND_L0C, float>(0);
+    }
+
+    __aicore__ __force_inline__ void
+    GetBlockIdx(uint32_t index, uint32_t &m_idx, uint32_t &n_idx, const uint32_t &mm_loop)
+    {
+        uint32_t in_batch_idx = index % (mm_loop * n_loop);
+        if constexpr (SwizzleDir == 0) { // Zn
+            uint32_t tile_block_loop = (mm_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * n_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * n_loop);
+
+            uint32_t n_row = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_row = mm_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
+            n_idx = in_tile_block_idx / n_row;
+            if (tile_block_idx % 2 != 0) {
+                n_idx = n_loop - n_idx - 1;
+            }
+        } else if constexpr (SwizzleDir == 1) { // Nz
+            uint32_t tile_block_loop = (n_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * mm_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * mm_loop);
+
+            uint32_t n_col = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_col = n_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = in_tile_block_idx / n_col;
+            n_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
+            if (tile_block_idx % 2 != 0) {
+                m_idx = mm_loop - m_idx - 1;
+            }
+        }
+    }
+    __aicore__ void Process()
+    {
+        if (!moeUp) {
+            ProcessMoeDownCube();
+        } else {
+            ProcessMoeUpCube();
+        }
+    }
+
+    __aicore__ void ProcessMoeDownCube()
+    {
+        using LocalTensor = AscendC::LocalTensor<InDtype>;
+        using CopyGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>;
+        using CopyGmToCbufNd2Nz = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>;
+        using LoadCbufToCa = l1_to_l0_a<ArchType::ASCEND_V220, InDtype, TA, DataFormat::ZN, DataFormat::ZZ>;
+        using LoadCbufToCb = l1_to_l0_b<ArchType::ASCEND_V220, InDtype, TB, DataFormat::ZN, DataFormat::NZ>;
+        using CopyCcToGm = l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, float, float>;
+        using Mmad = mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>;
+
+        SET_FLAG(MTE1, MTE2, EVENT_ID0);
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(MTE1, MTE2, EVENT_ID2);
+        SET_FLAG(MTE1, MTE2, EVENT_ID3);
+        SET_FLAG(M, MTE1, EVENT_ID0);
+        SET_FLAG(M, MTE1, EVENT_ID1);
+        SET_FLAG(FIX, M, EVENT_ID0);
+        uint32_t read_flag{1};
+        int32_t ping_flag_gm = 1;
+        uint32_t loop_count = 0;
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (int)(expert_num + m0 - 1) / m0;
+            for (; loop_idx < core_loop + mm_loop_batch; loop_idx += num_core) {
+                uint32_t m_idx = loop_idx - core_loop;
+                uint32_t n_idx = 0;
+                if (loop_count > 1) {
+                    WaitFlagDev(AIV2AICFLAGID);
+                }
+                loop_count += 1;
+                for (n_idx = 0; n_idx < n_loop; n_idx += 1) {
+                    // Cube等待Vector发送运行指令
+                    uint32_t m_batch = expert_num;
+                    uint64_t offset_a, offset_b, offset_a_next, offset_b_next;
+                    uint64_t offset_c = expert_cumsum * n + m_idx * m0 * n + n_idx * n0;
+                    uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                    uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
+                    uint32_t m_round = (m_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+                    uint32_t n_round = (n_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+                    uint32_t mn_max = m_round > n_round ? m_round : n_round;
+                    uint32_t k_part_len = L0_PINGPONG_BUFFER_LEN / mn_max / CONST_16 * CONST_16;
+                    uint64_t shuffle_k = en_shuffle_k ? core_idx % k_loop : 0;
+
+                    offset_a = expert_cumsum * k + m_idx * m0 * k + shuffle_k * k0;
+
+                    if (TB) {
+                        offset_b = batch_idx * k * n + n_idx * n0 * k + shuffle_k * k0;
+                    } else {
+                        offset_b = batch_idx * k * n + shuffle_k * k0 * n + n_idx * n0;
+                    }
+
+                    uint32_t k_actual = (shuffle_k == k_loop - 1) ? k - shuffle_k * k0 : k0;
+                    uint32_t k_round = (k_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+
+                    LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                    LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                    LocalTensor l0a_buf = ping_flag ? l0BaseA_ : l0BaseA_[L0_PINGPONG_BUFFER_LEN];
+                    LocalTensor l0b_buf = ping_flag ? l0BaseB_ : l0BaseB_[L0_PINGPONG_BUFFER_LEN];
+                    event_t event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                    if (read_flag) {
+                        WAIT_FLAG(MTE1, MTE2, event_id);
+                        // *** load matrix A to L1
+                        if ((m_batch == 1) || (m_actual == 1)) {
+                            CopyGmToCbuf(l1_buf_a,       // dst
+                                         gmA_[offset_a], // src
+                                         1,              // nTileActual
+                                         16,             // nTileCeil
+                                         1,              // nVal
+                                         k_actual,       // dTileActual
+                                         k_round,        // dTileCeil
+                                         k);             // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_a,       // dst
+                                              gmA_[offset_a], // src
+                                              m_actual,       // nTileActual
+                                              m_round,        // nTileCeil
+                                              m_batch,        // nVal
+                                              k_actual,       // dTileActual
+                                              k_round,        // dTileCeil
+                                              k);             // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id);
+                        // *** load matrix B to L1
+                        WAIT_FLAG(MTE1, MTE2, event_id + 2);
+                        if (TB) {
+                            CopyGmToCbufNd2Nz(l1_buf_b,       // dst
+                                              gmB_[offset_b], // src
+                                              n_actual,       // nTileActual
+                                              n_round,        // nTileCeil
+                                              n,              // nVal
+                                              k_actual,       // dTileActual
+                                              k_round,        // dTileCeil
+                                              k);             // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_b,       // dst
+                                              gmB_[offset_b], // src
+                                              k_actual,       // nTileActual
+                                              k_round,        // nTileCeil
+                                              k,              // nVal
+                                              n_actual,       // dTileActual
+                                              n_round,        // dTileCeil
+                                              n);             // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id + 2);
+                        read_flag = 0;
+                    }
+
+                    for (uint64_t k_idx = 0; k_idx < k_loop; k_idx++) {
+                        shuffle_k = en_shuffle_k ? (k_idx + core_idx) % k_loop : k_idx;
+                        uint32_t k_actual = (shuffle_k == (k_loop - 1)) ? (k - shuffle_k * k0) : k0;
+                        uint32_t k_round = (k_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+                        uint32_t k_part_loop = (k_actual + k_part_len - 1) / k_part_len;
+
+                        LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                        LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                        auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                        if (k_idx < k_loop - 1) {
+                            uint64_t shuffle_k_next = en_shuffle_k ? (core_idx + k_idx + 1) % k_loop : k_idx + 1;
+                            offset_a_next = expert_cumsum * k + m_idx * m0 * k + shuffle_k_next * k0;
+
+                            if (TB) {
+                                offset_b_next = batch_idx * k * n + n_idx * n0 * k + shuffle_k_next * k0;
+                            } else {
+                                offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + n_idx * n0;
+                            }
+
+                            uint32_t k_actual_next = (shuffle_k_next == (k_loop - 1)) ? (k - shuffle_k_next * k0) : k0;
+                            uint32_t k_round_next = (k_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
+
+                            LocalTensor l1_buf_a_next = (1 - ping_flag) ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                            LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                            event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+
+                            WAIT_FLAG(MTE1, MTE2, event_id_next);
+                            // *** load matrix A to L1
+                            if ((m_batch == 1) || (m_actual == 1)) {
+                                CopyGmToCbuf(l1_buf_a_next,       // dst
+                                             gmA_[offset_a_next], // src
+                                             1,                   // nTileActual
+                                             16,                  // nTileCeil
+                                             1,                   // nVal
+                                             k_actual_next,       // dTileActual
+                                             k_round_next,        // dTileCeil
+                                             k);                  // dVal
+                            } else {
+                                CopyGmToCbufNd2Nz(l1_buf_a_next,       // dst
+                                                  gmA_[offset_a_next], // src
+                                                  m_actual,            // nTileActual
+                                                  m_round,             // nTileCeil
+                                                  m_batch,             // nVal
+                                                  k_actual_next,       // dTileActual
+                                                  k_round_next,        // dTileCeil
+                                                  k);                  // dVal
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next);
+
+                            // *** load matrix B to L1
+                            WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                            if (TB) {
+                                CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                                  gmB_[offset_b_next], // src
+                                                  n_actual,            // nTileActual
+                                                  n_round,             // nTileCeil
+                                                  n,                   // nVal
+                                                  k_actual_next,       // dTileActual
+                                                  k_round_next,        // dTileCeil
+                                                  k);                  // dVal
+                            } else {
+                                CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                                  gmB_[offset_b_next], // src
+                                                  k_actual_next,       // nTileActual
+                                                  k_round_next,        // nTileCeil
+                                                  k,                   // nVal
+                                                  n_actual,            // dTileActual
+                                                  n_round,             // dTileCeil
+                                                  n);                  // dVal
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                        } else if (n_idx + 1 < n_loop) {
+                            uint32_t m_idx_next = m_idx, n_idx_next = n_idx + 1;
+                            uint32_t b_idx_next = batch_idx;
+                            uint32_t mm_loop_batch_next = mm_loop_batch;
+                            uint32_t m_batch_next = m_batch;
+                            uint64_t shuffle_k_next = en_shuffle_k ? core_idx % k_loop : 0;
+                            uint32_t m_actual_next =
+                                (m_idx_next == (mm_loop_batch_next - 1)) ? (m_batch_next - m_idx_next * m0) : m0;
+
+                            uint32_t n_actual_next = (n_idx_next == (n_loop - 1)) ? (n - n_idx_next * n0) : n0;
+                            uint32_t m_round_next = (m_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
+                            uint32_t n_round_next = (n_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
+                            uint32_t k_actual_next = (shuffle_k_next == k_loop - 1) ? k - shuffle_k_next * k0 : k0;
+                            uint32_t k_round_next = (k_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
+                            offset_a_next = expert_cumsum * k + m_idx_next * m0 * k + shuffle_k_next * k0;
+                            if (TB) {
+                                offset_b_next = batch_idx * k * n + n_idx_next * n0 * k + shuffle_k_next * k0;
+                            } else {
+                                offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + n_idx_next * n0;
+                            }
+
+                            LocalTensor l1_buf_a_next = (1 - ping_flag) ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                            LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                            event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+
+                            WAIT_FLAG(MTE1, MTE2, event_id_next);
+                            // *** load matrix A to L1
+                            if (m_batch_next == 1 || m_actual_next == 1) {
+                                CopyGmToCbuf(l1_buf_a_next,       // dst
+                                             gmA_[offset_a_next], // src
+                                             1,                   // nTileActual
+                                             16,                  // nTileCeil
+                                             1,                   // nVal
+                                             k_actual_next,       // dTileActual
+                                             k_round_next,        // dTileCeil
+                                             k);                  // dVal
+                            } else {
+                                CopyGmToCbufNd2Nz(l1_buf_a_next,       // dst
+                                                  gmA_[offset_a_next], // src
+                                                  m_actual_next,       // nTileActual
+                                                  m_round_next,        // nTileCeil
+                                                  m_batch_next,        // nVal
+                                                  k_actual_next,       // dTileActual
+                                                  k_round_next,        // dTileCeil
+                                                  k);                  // dVal
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next);
+
+                            // *** load matrix B to L1
+                            WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                            if (TB) {
+                                CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                                  gmB_[offset_b_next], // src
+                                                  n_actual_next,       // nTileActual
+                                                  n_round_next,        // nTileCeil
+                                                  n,                   // nVal
+                                                  k_actual_next,       // dTileActual
+                                                  k_round_next,        // dTileCeil
+                                                  k);                  // dVal
+                            } else {
+                                CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                                  gmB_[offset_b_next], // src
+                                                  k_actual_next,       // nTileActual
+                                                  k_round_next,        // nTileCeil
+                                                  k,                   // nVal
+                                                  n_actual_next,       // dTileActual
+                                                  n_round_next,        // dTileCeil
+                                                  n);                  // dVal
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                        } else {
+                            read_flag = 1;
+                        }
+
+                        for (uint32_t k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
+                            uint32_t k0_round =
+                                (k_part_idx < k_part_loop - 1) ? k_part_len : k_round - k_part_idx * k_part_len;
+                            uint32_t k0_actual =
+                                (k_part_idx < k_part_loop - 1) ? k_part_len : k_actual - k_part_idx * k_part_len;
+
+                            auto mte1_mad_ping_flag = 1 - k_part_idx % 2;
+                            auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
+                            LocalTensor l0a_buf = l0BaseA_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN];
+                            LocalTensor l0b_buf = l0BaseB_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN];
+
+                            // *** load matrix A from L1 to L0A
+                            if (k_part_idx == 0) {
+                                WAIT_FLAG(MTE2, MTE1, event_id);
+                            }
+                            WAIT_FLAG(M, MTE1, mte1_mad_event_id);
+                            if ((m_batch == 1) || (m_actual == 1)) {
+                                l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR,
+                                           DataFormat::VECTOR>(l0a_buf,                           // dst
+                                                               l1_buf_a[k_part_idx * k_part_len], // src
+                                                               0,
+                                                               (k0_round + CONST_256 - 1) / CONST_256, // repeat
+                                                               0,
+                                                               1, // srcStride
+                                                               0,
+                                                               0); // dstStride
+                            } else {
+                                LoadCbufToCa(l0a_buf,                                     // l0Tensor
+                                             l1_buf_a[k_part_idx * k_part_len * m_round], // l1Tensor
+                                             m_round,                                     // mTileCeil
+                                             k0_round,                                    // kPartCeil
+                                             1,                                           // mSrcStride
+                                             m_round / CONST_16,                          // kSrcStride
+                                             k0_round / CONST_16,                         // mDstStride
+                                             1);                                          // kDstStride
+                            }
+                            if (k_part_idx == k_part_loop - 1) {
+                                SET_FLAG(MTE1, MTE2, event_id);
+                            }
+
+                            // *** load matrix B from L1 to L0B
+                            if (k_part_idx == 0) {
+                                WAIT_FLAG(MTE2, MTE1, event_id + 2);
+                            }
+                            if (TB) {
+                                LoadCbufToCb(l0b_buf,                                     // l0Tensor
+                                             l1_buf_b[k_part_idx * k_part_len * n_round], // l1Tensor
+                                             n_round,                                     // nTileCeil
+                                             k0_round,                                    // kPartCeil
+                                             1,                                           // nSrcStride
+                                             n_round / CONST_16,                          // kSrcStride
+                                             1,                                           // nDstStride
+                                             k0_round / CONST_16);                        // kDstStride
+                            } else {
+                                LoadCbufToCb(l0b_buf,                                      // l0Tensor
+                                             l1_buf_b[k_part_idx * k_part_len * CONST_16], // l1Tensor
+                                             n_round,                                      // nTileCeil
+                                             k0_round,                                     // kPartCeil
+                                             k_round / CONST_16,                           // nSrcStride
+                                             1,                                            // kSrcStride
+                                             1,                                            // nDstStride
+                                             n_round / CONST_16);                          // kDstStride
+                            }
+                            if (k_part_idx == k_part_loop - 1) {
+                                SET_FLAG(MTE1, MTE2, event_id + 2);
+                            }
+
+                            SET_FLAG(MTE1, M, mte1_mad_event_id);
+                            WAIT_FLAG(MTE1, M, mte1_mad_event_id);
+
+                            bool init_c = (k_idx == 0 && k_part_idx == 0);
+                            if (init_c) {
+                                WAIT_FLAG(FIX, M, EVENT_ID0);
+                            }
+
+                            Mmad(l0C_,      // c
+                                 l0a_buf,   // a
+                                 l0b_buf,   // b
+                                 m_actual,  // m
+                                 n_actual,  // n
+                                 k0_actual, // k
+                                 init_c);   // cmatrixInitVal
+
+                            PIPE_BARRIER(M);
+                            SET_FLAG(M, MTE1, mte1_mad_event_id);
+                        }
+
+                        ping_flag = 1 - ping_flag;
+                    }
+
+                    SET_FLAG(M, FIX, EVENT_ID0);
+                    WAIT_FLAG(M, FIX, EVENT_ID0);
+
+                    // copy from L0C to workspace
+                    offset_c = (ping_flag_gm * m0 + core_idx * m0 * 2) * n + n_idx * n0;
+                    CopyCcToGm(gmWorkspaceHp_[offset_c], // dst
+                               l0C_,                     // srcF
+                               m_actual,                 // mTileActual
+                               n_actual,                 // nTileActual
+                               m_round,                  // mTileCeil
+                               n);                       // nActual
+                    SET_FLAG(FIX, M, EVENT_ID0);
+                }
+
+                ping_flag_gm = 1 - ping_flag_gm;
+                // Cube给Vector发送同步指令
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            }
+            core_loop += mm_loop_batch;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID0);
+        WAIT_FLAG(M, MTE1, EVENT_ID1);
+        WAIT_FLAG(FIX, M, EVENT_ID0);
+        PIPE_BARRIER(ALL);
+    }
+
+    // Cube_run
+    __aicore__ void ProcessMoeUpCube()
+    {
+        using LocalTensor = AscendC::LocalTensor<InDtype>;
+        using CopyGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>;
+        using CopyGmToCbufNd2Nz = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>;
+        using LoadCbufToCa = l1_to_l0_a<ArchType::ASCEND_V220, InDtype, TA, DataFormat::ZN, DataFormat::ZZ>;
+        using LoadCbufToCb = l1_to_l0_b<ArchType::ASCEND_V220, InDtype, TB, DataFormat::ZN, DataFormat::NZ>;
+        using CopyCcToGm = l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, OutDtype, float>;
+        using Mmad = mmad<ArchType::ASCEND_V220, InDtype, InDtype, float, false>;
+
+        SET_FLAG(MTE1, MTE2, EVENT_ID0);
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(MTE1, MTE2, EVENT_ID2);
+        SET_FLAG(MTE1, MTE2, EVENT_ID3);
+        SET_FLAG(M, MTE1, EVENT_ID0);
+        SET_FLAG(M, MTE1, EVENT_ID1);
+        SET_FLAG(FIX, M, EVENT_ID0);
+        uint32_t read_flag{1};
+        int32_t ping_flag_gm = 1;
+        uint32_t loop_count = 0;
+
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (expert_num + m0 - 1) / m0;
+            for (; loop_idx < core_loop + mm_loop_batch * n_loop; loop_idx += num_core) {
+                uint32_t m_idx, n_idx;
+                GetBlockIdx(loop_idx - core_loop, m_idx, n_idx, mm_loop_batch);
+                // Cube等待Vector发送运行指令
+                uint32_t m_batch = expert_num;
+                uint64_t offset_a, offset_b, offset_a_next, offset_b_next;
+                uint64_t offset_c = expert_cumsum * n + m_idx * m0 * n + n_idx * n0;
+                uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
+                uint32_t m_round = (m_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+                uint32_t n_round = (n_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+                uint32_t mn_max = m_round > n_round ? m_round : n_round;
+                uint32_t k_part_len = L0_PINGPONG_BUFFER_LEN / mn_max / CONST_16 * CONST_16;
+                uint64_t shuffle_k = en_shuffle_k ? core_idx % k_loop : 0;
+                offset_a = (ping_flag_gm * m0 + core_idx * m0 * 2) * k + shuffle_k * k0;
+
+                if (TB) {
+                    offset_b = batch_idx * k * n + n_idx * n0 * k + shuffle_k * k0;
+                } else {
+                    offset_b = batch_idx * k * n + shuffle_k * k0 * n + n_idx * n0;
+                }
+
+                uint32_t k_actual = (shuffle_k == k_loop - 1) ? k - shuffle_k * k0 : k0;
+                uint32_t k_round = (k_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+
+                LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                LocalTensor l0a_buf = ping_flag ? l0BaseA_ : l0BaseA_[L0_PINGPONG_BUFFER_LEN];
+                LocalTensor l0b_buf = ping_flag ? l0BaseB_ : l0BaseB_[L0_PINGPONG_BUFFER_LEN];
+                event_t event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                WaitFlagDev(AIV2AICFLAGID);
+                WAIT_FLAG(MTE1, MTE2, event_id);
+                // *** load matrix A to L1
+                if ((m_batch == 1) || (m_actual == 1)) {
+                    CopyGmToCbuf(l1_buf_a,               // dst
+                                 gmWorkspace_[offset_a], // src
+                                 1,                      // nTileActual
+                                 16,                     // nTileCeil
+                                 1,                      // nVal
+                                 k_actual,               // dTileActual
+                                 k_round,                // dTileCeil
+                                 k);                     // dVal
+                } else {
+                    CopyGmToCbufNd2Nz(l1_buf_a,               // dst
+                                      gmWorkspace_[offset_a], // src
+                                      m_actual,               // nTileActual
+                                      m_round,                // nTileCeil
+                                      m0,                     // nVal
+                                      k_actual,               // dTileActual
+                                      k_round,                // dTileCeil
+                                      k);                     // dVal
+                }
+                SET_FLAG(MTE2, MTE1, event_id);
+                if (read_flag) {
+                    // *** load matrix B to L1
+                    WAIT_FLAG(MTE1, MTE2, event_id + 2);
+                    if (TB) {
+                        CopyGmToCbufNd2Nz(l1_buf_b,       // dst
+                                          gmB_[offset_b], // src
+                                          n_actual,       // nTileActual
+                                          n_round,        // nTileCeil
+                                          n,              // nVal
+                                          k_actual,       // dTileActual
+                                          k_round,        // dTileCeil
+                                          k);             // dVal
+                    } else {
+                        CopyGmToCbufNd2Nz(l1_buf_b,       // dst
+                                          gmB_[offset_b], // src
+                                          k_actual,       // nTileActual
+                                          k_round,        // nTileCeil
+                                          k,              // nVal
+                                          n_actual,       // dTileActual
+                                          n_round,        // dTileCeil
+                                          n);             // dVal
+                    }
+                    SET_FLAG(MTE2, MTE1, event_id + 2);
+                    read_flag = 0;
+                }
+
+                for (uint64_t k_idx = 0; k_idx < k_loop; k_idx++) {
+                    shuffle_k = en_shuffle_k ? (k_idx + core_idx) % k_loop : k_idx;
+                    uint32_t k_actual = (shuffle_k == (k_loop - 1)) ? (k - shuffle_k * k0) : k0;
+                    uint32_t k_round = (k_actual + CONST_16 - 1) / CONST_16 * CONST_16;
+                    uint32_t k_part_loop = (k_actual + k_part_len - 1) / k_part_len;
+
+                    LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                    LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                    auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                    if (k_idx < k_loop - 1) {
+                        uint64_t shuffle_k_next = en_shuffle_k ? (core_idx + k_idx + 1) % k_loop : k_idx + 1;
+                        offset_a_next = core_idx * m0 * 2 * k + ping_flag_gm * m0 * k + shuffle_k_next * k0;
+
+                        if (TB) {
+                            offset_b_next = batch_idx * k * n + n_idx * n0 * k + shuffle_k_next * k0;
+                        } else {
+                            offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + n_idx * n0;
+                        }
+
+                        uint32_t k_actual_next = (shuffle_k_next == (k_loop - 1)) ? (k - shuffle_k_next * k0) : k0;
+                        uint32_t k_round_next = (k_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
+
+                        LocalTensor l1_buf_a_next = (1 - ping_flag) ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN];
+                        LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                        event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+
+                        WAIT_FLAG(MTE1, MTE2, event_id_next);
+                        // *** load matrix A to L1
+                        if ((m_batch == 1) || (m_actual == 1)) {
+                            CopyGmToCbuf(l1_buf_a_next,               // dst
+                                         gmWorkspace_[offset_a_next], // src
+                                         1,                           // nTileActual
+                                         16,                          // nTileCeil
+                                         1,                           // nVal
+                                         k_actual_next,               // kTileActual
+                                         k_round_next,                // kTileCeil
+                                         k);                          // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_a_next,               // dst
+                                              gmWorkspace_[offset_a_next], // src
+                                              m_actual,                    // nTileActual
+                                              m_round,                     // nTileCeil
+                                              m0,                          // nVal
+                                              k_actual_next,               // dTileActual
+                                              k_round_next,                // dTileCeil
+                                              k);                          // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id_next);
+
+                        // *** load matrix B to L1
+                        WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                        if (TB) {
+                            CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                              gmB_[offset_b_next], // src
+                                              n_actual,            // nTileActual
+                                              n_round,             // nTileCeil
+                                              n,                   // nVal
+                                              k_actual_next,       // dTileActual
+                                              k_round_next,        // dTileCeil
+                                              k);                  // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                              gmB_[offset_b_next], // src
+                                              k_actual_next,       // nTileActual
+                                              k_round_next,        // nTileCeil
+                                              k,                   // nVal
+                                              n_actual,            // dTileActual
+                                              n_round,             // dTileCeil
+                                              n);                  // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                    } else if (loop_idx + num_core < core_loop + mm_loop_batch * n_loop) {
+                        uint32_t m_idx_next = 0, n_idx_next = 0;
+                        GetBlockIdx(loop_idx + num_core - core_loop, m_idx_next, n_idx_next, mm_loop_batch);
+                        uint32_t b_idx_next = batch_idx;
+                        uint64_t shuffle_k_next = en_shuffle_k ? core_idx % k_loop : 0;
+                        uint32_t n_actual_next = (n_idx_next == (n_loop - 1)) ? (n - n_idx_next * n0) : n0;
+                        uint32_t n_round_next = RoundUp<CONST_16>(n_actual_next);
+                        uint32_t k_actual_next = (shuffle_k_next == k_loop - 1) ? k - shuffle_k_next * k0 : k0;
+                        uint32_t k_round_next = RoundUp<CONST_16>(k_actual_next);
+                        if (TB) {
+                            offset_b_next = b_idx_next * k * n + n_idx_next * n0 * k + shuffle_k_next * k0;
+                        } else {
+                            offset_b_next = b_idx_next * k * n + shuffle_k_next * k0 * n + n_idx_next * n0;
+                        }
+                        LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN];
+                        event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+                        // *** load matrix B to L1
+                        WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                        if (TB) {
+                            CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                              gmB_[offset_b_next], // src
+                                              n_actual_next,       // nTileActual
+                                              n_round_next,        // nTileCeil
+                                              n,                   // nVal
+                                              k_actual_next,       // dTileActual
+                                              k_round_next,        // dTileCeil
+                                              k);                  // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_b_next,       // dst
+                                              gmB_[offset_b_next], // src
+                                              k_actual_next,       // nTileActual
+                                              k_round_next,        // nTileCeil
+                                              k,                   // nVal
+                                              n_actual,            // dTileActual
+                                              n_round,             // dTileCeil
+                                              n);                  // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                    } else {
+                        read_flag = 1;
+                    }
+
+                    for (uint32_t k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
+                        uint32_t k0_round =
+                            (k_part_idx < k_part_loop - 1) ? k_part_len : k_round - k_part_idx * k_part_len;
+                        uint32_t k0_actual =
+                            (k_part_idx < k_part_loop - 1) ? k_part_len : k_actual - k_part_idx * k_part_len;
+
+                        auto mte1_mad_ping_flag = 1 - k_part_idx % 2;
+                        auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
+                        LocalTensor l0a_buf = l0BaseA_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN];
+                        LocalTensor l0b_buf = l0BaseB_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN];
+
+                        // *** load matrix A from L1 to L0A
+                        if (k_part_idx == 0) {
+                            WAIT_FLAG(MTE2, MTE1, event_id);
+                        }
+                        WAIT_FLAG(M, MTE1, mte1_mad_event_id);
+                        if ((m_batch == 1) || (m_actual == 1)) {
+                            l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR, DataFormat::VECTOR>(
+                                l0a_buf,                           // dst
+                                l1_buf_a[k_part_idx * k_part_len], // src
+                                0,
+                                (k0_round + CONST_256 - 1) / CONST_256, // repeat
+                                0,
+                                1, // srcStride
+                                0,
+                                0); // dstStride
+                        } else {
+                            LoadCbufToCa(l0a_buf,                                     // l0Tensor
+                                         l1_buf_a[k_part_idx * k_part_len * m_round], // l1Tensor
+                                         m_round,                                     // mTileCeil
+                                         k0_round,                                    // kPartCeil
+                                         1,                                           // mSrcStride
+                                         m_round / CONST_16,                          // kSrcStride
+                                         k0_round / CONST_16,                         // mDstStride
+                                         1);                                          // kDstStride
+                        }
+                        if (k_part_idx == k_part_loop - 1) {
+                            SET_FLAG(MTE1, MTE2, event_id);
+                        }
+
+                        // *** load matrix B from L1 to L0B
+                        if (k_part_idx == 0) {
+                            WAIT_FLAG(MTE2, MTE1, event_id + 2);
+                        }
+                        if (TB) {
+                            LoadCbufToCb(l0b_buf,                                     // l0Tensor
+                                         l1_buf_b[k_part_idx * k_part_len * n_round], // l1Tensor
+                                         n_round,                                     // nTileCeil
+                                         k0_round,                                    // kPartCeil
+                                         1,                                           // nSrcStride
+                                         n_round / CONST_16,                          // kSrcStride
+                                         1,                                           // nDstStride
+                                         k0_round / CONST_16);                        // kDstStride
+                        } else {
+                            LoadCbufToCb(l0b_buf,                                      // l0Tensor
+                                         l1_buf_b[k_part_idx * k_part_len * CONST_16], // l1Tensor
+                                         n_round,                                      // nTileCeil
+                                         k0_round,                                     // kPartCeil
+                                         k_round / CONST_16,                           // nSrcStride
+                                         1,                                            // kSrcStride
+                                         1,                                            // nDstStride
+                                         n_round / CONST_16);                          // kDstStride
+                        }
+                        if (k_part_idx == k_part_loop - 1) {
+                            SET_FLAG(MTE1, MTE2, event_id + 2);
+                        }
+
+                        SET_FLAG(MTE1, M, mte1_mad_event_id);
+                        WAIT_FLAG(MTE1, M, mte1_mad_event_id);
+
+                        bool init_c = (k_idx == 0 && k_part_idx == 0);
+                        if (init_c) {
+                            WAIT_FLAG(FIX, M, EVENT_ID0);
+                        }
+
+                        Mmad(l0C_,      // c
+                             l0a_buf,   // a
+                             l0b_buf,   // b
+                             m_actual,  // m
+                             n_actual,  // n
+                             k0_actual, // k
+                             init_c);   // cmatrixInitVal
+
+                        PIPE_BARRIER(M);
+                        SET_FLAG(M, MTE1, mte1_mad_event_id);
+                    }
+                    ping_flag = 1 - ping_flag;
+                }
+
+                SET_FLAG(M, FIX, EVENT_ID0);
+                WAIT_FLAG(M, FIX, EVENT_ID0);
+                // copy from L0C to gm
+                CopyCcToGm(gmC_[offset_c], // dst
+                           l0C_,           // srcF
+                           m_actual,       // mTileActual
+                           n_actual,       // nTileActual
+                           m_round,        // mTileCeil
+                           n);             // nActual
+
+                SET_FLAG(FIX, M, EVENT_ID0);
+                ping_flag_gm = 1 - ping_flag_gm;
+
+                // Cube给Vector发送同步指令
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+            }
+            core_loop += mm_loop_batch * n_loop;
+            expert_cumsum = gmFlag_(batch_idx);
+        }
+
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID0);
+        WAIT_FLAG(M, MTE1, EVENT_ID1);
+        WAIT_FLAG(FIX, M, EVENT_ID0);
+        PIPE_BARRIER(ALL);
+    }
+
+private:
+    AscendC::GlobalTensor<InDtype> gmA_;
+    AscendC::GlobalTensor<InDtype> gmB_;
+    AscendC::GlobalTensor<OutDtype> gmC_;
+    AscendC::GlobalTensor<int32_t> gmFlag_;
+    AscendC::GlobalTensor<InDtype> gmWorkspace_;
+    AscendC::GlobalTensor<float> gmWorkspaceHp_;
+    AscendC::LocalTensor<InDtype> l1BaseA_;
+    AscendC::LocalTensor<InDtype> l1BaseB_;
+    AscendC::LocalTensor<InDtype> l0BaseA_;
+    AscendC::LocalTensor<InDtype> l0BaseB_;
+    AscendC::LocalTensor<float> l0C_;
+
+    __gm__ InDtype *gm_a{nullptr};
+    __gm__ InDtype *gm_b{nullptr};
+    __gm__ int32_t *gm_flag{nullptr};
+    __gm__ OutDtype *gm_c{nullptr};
+    __gm__ InDtype *gm_workspace{nullptr};
+    __gm__ float *gm_workspace_hp{nullptr};
+    __cbuf__ InDtype *l1_base_a{nullptr};
+    __cbuf__ InDtype *l1_base_b{nullptr};
+    __ca__ InDtype *l0a_base = reinterpret_cast<__ca__ InDtype *>((uintptr_t)0);
+    __cb__ InDtype *l0b_base = reinterpret_cast<__cb__ InDtype *>((uintptr_t)0);
+    __cc__ float *l0c_buf = reinterpret_cast<__cc__ float *>((uintptr_t)0);
+
+    uint32_t num_core{0};
+    uint32_t batch_size{0};
+    uint32_t m{0};
+    uint32_t k{0};
+    uint32_t n{0};
+    uint32_t m0{0};
+    uint32_t k0{0};
+    uint32_t n0{0};
+    uint32_t m_loop{0};
+    uint32_t n_loop{0};
+    uint32_t k_loop{0};
+    uint32_t core_loop{0};
+    uint32_t core_idx{0};
+    uint32_t swizzle_cnt{1};
+    uint32_t ping_flag{0};
+    uint32_t en_shuffle_k{0};
+    uint32_t allM{0};
+    uint32_t moeUp{0};
+};
+
+#endif
+#ifdef __DAV_C220_VEC__
+
+template <uint32_t SwizzleDir, bool TA, bool TB, typename InDtype = half, typename OutDtype = half>
+class PpMatmulAiv {
+public:
+    __aicore__ explicit PpMatmulAiv(){};
+
+    __aicore__ __force_inline__ void InitVector(__gm__ uint8_t *__restrict__ gmPtrA,
+                                                __gm__ uint8_t *__restrict__ gmPtrB,
+                                                __gm__ uint8_t *__restrict__ gmPtrFlag,
+                                                __gm__ uint8_t *__restrict__ gmPtrIndex,
+                                                __gm__ uint8_t *__restrict__ gmPtrC,
+                                                __gm__ uint8_t *__restrict__ gmPtrWorkspace,
+                                                __gm__ uint8_t *__restrict__ gmPtrWorkspaceHp,
+                                                __gm__ uint8_t *__restrict__ tiling_data)
+    {
+        auto gm_tiling_data = reinterpret_cast<__gm__ AtbOps::MoeGmmTilingData *>(tiling_data);
+        batch_size = gm_tiling_data->batch;
+        m = gm_tiling_data->m * gm_tiling_data->batch;
+        k = gm_tiling_data->k;
+        n = gm_tiling_data->n;
+        m0 = gm_tiling_data->m0;
+        k0 = gm_tiling_data->k0;
+        n0 = gm_tiling_data->n0;
+        m_loop = gm_tiling_data->mLoop;
+        k_loop = gm_tiling_data->kLoop;
+        n_loop = gm_tiling_data->nLoop;
+        core_loop = gm_tiling_data->coreLoop;
+        swizzle_cnt = gm_tiling_data->swizzlCount;
+        en_shuffle_k = gm_tiling_data->enShuffleK;
+        moeUp = gm_tiling_data->moeUp;
+        allM = gm_tiling_data->allM;
+        num_core = AscendC::GetBlockNum();
+        core_idx = AscendC::GetBlockIdx() / AscendC::GetTaskRation();
+        ping_flag = 1;
+
+        AsdopsBuffer<ArchType::ASCEND_V220> buf;
+        gmA_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(gmPtrA));
+        gmB_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(gmPtrB));
+        gmC_.SetGlobalBuffer(reinterpret_cast<__gm__ OutDtype *>(gmPtrC));
+        gmFlag_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(gmPtrFlag));
+        gmIndex_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(gmPtrIndex));
+        gmWorkspace_.SetGlobalBuffer(reinterpret_cast<__gm__ OutDtype *>(gmPtrWorkspace));
+        gmWorkspace1_.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmPtrWorkspace));
+        gmWorkspaceHp_.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(gmPtrWorkspaceHp));
+        ubBuf_ = buf.GetBuffer<BufferType::ASCEND_UB, InDtype>(0);
+        ubBufHp_ = buf.GetBuffer<BufferType::ASCEND_UB, float>((uintptr_t)n * sizeof(InDtype) * 2);
+    }
+
+    __force_inline__ __aicore__ void
+    GetBlockIdx(uint32_t index, uint32_t &m_idx, uint32_t &n_idx, const uint32_t &mm_loop)
+    {
+        uint32_t in_batch_idx = index % (mm_loop * n_loop);
+        if constexpr (SwizzleDir == 0) { // Zn
+            uint32_t tile_block_loop = (mm_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * n_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * n_loop);
+
+            uint32_t n_row = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_row = mm_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
+            n_idx = in_tile_block_idx / n_row;
+            if (tile_block_idx % 2 != 0) {
+                n_idx = n_loop - n_idx - 1;
+            }
+        } else if constexpr (SwizzleDir == 1) { // Nz
+            uint32_t tile_block_loop = (n_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * mm_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * mm_loop);
+
+            uint32_t n_col = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_col = n_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = in_tile_block_idx / n_col;
+            n_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
+            if (tile_block_idx % 2 != 0) {
+                m_idx = mm_loop - m_idx - 1;
+            }
+        }
+    }
+
+    __aicore__ void run()
+    {
+        if (!moeUp) {
+            ProcessMoeDownVector();
+        } else {
+            ProcessMoeUpVector();
+        }
+    }
+
+    __aicore__ void ProcessMoeDownVector()
+    {
+        using CopyGmToUb = gm_to_ub<ArchType::ASCEND_V220, float>;
+        using CopyUbToGm = ub_to_gm<ArchType::ASCEND_V220, float>;
+        uint32_t sub_block_id = AscendC::GetSubBlockIdx();
+        uint32_t sub_block_dim = AscendC::GetTaskRation();
+        uint32_t num_blocks = n * sizeof(InDtype) / BLOCK_SIZE;  // fp16
+        uint32_t num_blocks_hp = n * sizeof(float) / BLOCK_SIZE; // float
+        uint32_t fp32_total_burst_num = (n + 63) / 64;           // float
+
+        int32_t ping_flag_gm = 1;
+        int32_t vcore_num = num_core * sub_block_dim;
+        int32_t vcore_loop = allM;
+        if (core_idx * sub_block_dim + sub_block_id < vcore_loop) {
+            uint64_t mask[] = {(uint64_t)-1, (uint64_t)-1};
+            AscendC::Duplicate<float>(ubBufHp_, 0.0f, mask, CeilDiv<CONST_64>(n), 1, 8);
+        }
+        SET_FLAG(V, MTE3, EVENT_ID0);
+        WAIT_FLAG(V, MTE3, EVENT_ID0);
+        for (int loop_idx = core_idx * sub_block_dim + sub_block_id; loop_idx < vcore_loop; loop_idx += vcore_num) {
+            CopyUbToGm(gmWorkspaceHp_[loop_idx * n], // dst
+                       ubBufHp_,                     // src
+                       0,                            // sid
+                       1,                            // nBurst
+                       num_blocks_hp,                // lenBurst
+                       0,                            // srcGap
+                       0);                           // dstGap
+        }
+        FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+        AscendC::SetAtomicAdd<float>();
+        WaitFlagDev(AIVFLAGID);
+
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        uint32_t ping_flag_ub = 0;
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (int)(expert_num + m0 - 1) / m0;
+            uint32_t m_start = expert_cumsum;
+            for (; loop_idx < core_loop + mm_loop_batch; loop_idx += num_core) {
+                uint32_t m_idx = loop_idx - core_loop;
+                int32_t output_start = ping_flag_gm * m0 + core_idx * m0 * 2;
+                uint32_t m_batch = expert_num;
+                uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                // 等待 cube 计算
+                WaitFlagDev(AIC2AIVFLAGID);
+
+                for (int j = sub_block_id; j < m_actual; j += sub_block_dim) {
+                    // 执行准备数据
+                    uint32_t cache_start = (uint32_t)(j + output_start) * n;
+                    CopyGmToUb(ubBufHp_[ping_flag_ub * n], // dst
+                               gmWorkspace1_[cache_start], // src
+                               0,                          // sid
+                               1,                          // nBurst
+                               num_blocks_hp,              // lenBurst
+                               0,                          // srcGap
+                               0);                         // dstGap
+                    SET_FLAG(MTE2, MTE3, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE2, MTE3, EVENT_ID0 + ping_flag_ub);
+
+                    uint32_t start = gmIndex_.GetValue(m_start + j + m_idx * m0) * n; // + n_idx * n0;
+                    CopyUbToGm(gmWorkspaceHp_[start],                                 // dst
+                               ubBufHp_[ping_flag_ub * n],                            // src
+                               0,                                                     // sid
+                               1,                                                     // nBurst
+                               num_blocks_hp,                                         // lenBurst
+                               0,                                                     // srcGap
+                               0);                                                    // dstGap
+                    SET_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    ping_flag_ub = 1 - ping_flag_ub;
+                }
+                // Vector给Cube发送同步指令
+                FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+                // 切换GM上的double buffer
+                ping_flag_gm = 1 - ping_flag_gm;
+            }
+            core_loop += mm_loop_batch;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+
+        // Vector同步信号
+        FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+        WaitFlagDev(AIVFLAGID);
+        AscendC::SetAtomicNone();
+        for (int loop_idx = core_idx * sub_block_dim + sub_block_id; loop_idx < vcore_loop; loop_idx += vcore_num) {
+            // 执行准备数据
+            uint32_t cache_start = (uint32_t)loop_idx * n;
+            CopyGmToUb(ubBufHp_[ping_flag_ub * n],  // dst
+                       gmWorkspaceHp_[cache_start], // src
+                       0,                           // sid
+                       1,                           // nBurst
+                       num_blocks_hp,               // lenBurst
+                       0,                           // srcGap
+                       0);                          // dstGap
+
+            SET_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+            WAIT_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+            conv_v<ArchType::ASCEND_V220, float, OutDtype>(ubBuf_[ping_flag_ub * n],   // dst
+                                                           ubBufHp_[ping_flag_ub * n], // src
+                                                           fp32_total_burst_num,       // repeat
+                                                           1,                          // dstBlockStride
+                                                           1,                          // srcBlockStride
+                                                           4,                          // dstRepeatStride
+                                                           8);                         // srcRepeatStride
+
+            SET_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+            WAIT_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+            ub_to_gm<ArchType::ASCEND_V220, OutDtype>(gmC_[cache_start],        // dst
+                                                      ubBuf_[ping_flag_ub * n], // src
+                                                      0,                        // sid
+                                                      1,                        // nBurst
+                                                      num_blocks,               // lenBurst
+                                                      0,                        // srcGap
+                                                      0);                       // dstGap
+            SET_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+            WAIT_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+            ping_flag_ub = 1 - ping_flag_ub;
+        }
+        PIPE_BARRIER(ALL);
+    }
+
+    // Vector_run
+    __aicore__ void ProcessMoeUpVector()
+    {
+        using CopyGmToUb = gm_to_ub<ArchType::ASCEND_V220, InDtype>;
+        using CopyUbToGm = ub_to_gm<ArchType::ASCEND_V220, InDtype>;
+        uint32_t sub_block_id = AscendC::GetSubBlockIdx();
+        uint32_t sub_block_dim = AscendC::GetTaskRation();
+        uint32_t loop_count = 0;
+        uint32_t num_blocks = k * sizeof(InDtype) / BLOCK_SIZE; // fp16
+        int32_t ping_flag_gm = 1;
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (expert_num + m0 - 1) / m0;
+            uint32_t m_start = expert_cumsum;
+            for (; loop_idx < core_loop + mm_loop_batch * n_loop; loop_idx += num_core) {
+                uint32_t m_idx, n_idx;
+                GetBlockIdx(loop_idx - core_loop, m_idx, n_idx, mm_loop_batch);
+                int32_t output_start = ping_flag_gm * m0 + core_idx * m0 * 2;
+                uint32_t m_batch = expert_num;
+                uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                if (loop_count > 1) {
+                    // Vector等待Cube发送运行指令
+                    WaitFlagDev(AIC2AIVFLAGID);
+                }
+                loop_count += 1;
+                for (int j = sub_block_id; j < m_actual; j += sub_block_dim) {
+                    uint32_t start = gmIndex_.GetValue(expert_cumsum + j + m_idx * m0) * k;
+                    uint32_t cache_start = (uint32_t)(j + output_start) * k;
+                    // 执行准备数据
+                    CopyGmToUb(ubBuf_,      // dst
+                               gmA_[start], // src
+                               0,           // sid
+                               1,           // nBurst
+                               num_blocks,  // lenBurst
+                               0,           // srcGap
+                               0);          // dstGap
+
+                    SET_FLAG(MTE2, MTE3, EVENT_ID0);
+                    WAIT_FLAG(MTE2, MTE3, EVENT_ID0);
+                    CopyUbToGm(gmWorkspace_[cache_start], // dst
+                               ubBuf_,                    // src
+                               0,                         // sid
+                               1,                         // nBurst
+                               num_blocks,                // lenBurst
+                               0,                         // srcGap
+                               0);                        // dstGap
+                    SET_FLAG(MTE3, MTE2, EVENT_ID0);
+                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0);
+                }
+                // 切换GM上的double buffer
+                ping_flag_gm = 1 - ping_flag_gm;
+                // Vector给Cube发送同步指令
+                FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+            }
+            core_loop += mm_loop_batch * n_loop;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+        PIPE_BARRIER(ALL);
+    }
+
+private:
+    AscendC::GlobalTensor<InDtype> gmA_;
+    AscendC::GlobalTensor<InDtype> gmB_;
+    AscendC::GlobalTensor<OutDtype> gmC_;
+    AscendC::GlobalTensor<int32_t> gmFlag_;
+    AscendC::GlobalTensor<int32_t> gmIndex_;
+    AscendC::GlobalTensor<InDtype> gmWorkspace_;
+    AscendC::GlobalTensor<float> gmWorkspace1_;
+    AscendC::GlobalTensor<float> gmWorkspaceHp_;
+    AscendC::LocalTensor<InDtype> ubBuf_;
+    AscendC::LocalTensor<float> ubBufHp_;
+
+    uint32_t num_core{0};
+    uint32_t batch_size{0};
+    uint32_t m{0};
+    uint32_t k{0};
+    uint32_t n{0};
+    uint32_t m0{0};
+    uint32_t k0{0};
+    uint32_t n0{0};
+    uint32_t m_loop{0};
+    uint32_t n_loop{0};
+    uint32_t k_loop{0};
+    uint32_t core_loop{0};
+    uint32_t core_idx{0};
+    uint32_t swizzle_cnt{1};
+    uint32_t ping_flag{0};
+    uint32_t en_shuffle_k{0};
+    uint32_t allM{0};
+    uint32_t moeUp{0};
+};
+#endif
+
+extern "C" __global__ __aicore__ void moe_gmm(__gm__ uint8_t *__restrict__ gm_ffts_addr,
+                                              __gm__ uint8_t *__restrict__ gm_a,
+                                              __gm__ uint8_t *__restrict__ gm_b,
+                                              __gm__ uint8_t *__restrict__ gm_flag,
+                                              __gm__ uint8_t *__restrict__ gm_index,
+                                              __gm__ uint8_t *__restrict__ gm_c,
+                                              __gm__ uint8_t *__restrict__ workspace,
+                                              __gm__ uint8_t *__restrict__ workspace_hp,
+                                              __gm__ uint8_t *__restrict__ tiling_data)
+{
+    SetFftsBaseAddr((uint64_t)gm_ffts_addr);
+    SetAtomicnone();
+#ifdef __DAV_C220_VEC__
+    SetMasknorm();
+    SetVectorMask<uint64_t>((uint64_t)-1, (uint64_t)-1);
+
+    PpMatmulAiv<0, false, false, half, half> matmul_aiv_0000; // swizzleDir[0] transA[0] transB[0]
+    PpMatmulAiv<0, false, true, half, half> matmul_aiv_0010;  // swizzleDir[0] transA[0] transB[1]
+    PpMatmulAiv<1, false, false, half, half> matmul_aiv_1000; // swizzleDir[1] transA[0] transB[0]
+    PpMatmulAiv<1, false, true, half, half> matmul_aiv_1010;  // swizzleDir[1] transA[0] transB[1]
+
+    PpMatmulAiv<0, false, false, __bf16, __bf16> matmul_aiv_0001; // swizzleDir[0] transA[0] transB[0]
+    PpMatmulAiv<0, false, true, __bf16, __bf16> matmul_aiv_0011;  // swizzleDir[0] transA[0] transB[1]
+    PpMatmulAiv<1, false, false, __bf16, __bf16> matmul_aiv_1001; // swizzleDir[1] transA[0] transB[0]
+    PpMatmulAiv<1, false, true, __bf16, __bf16> matmul_aiv_1011;  // swizzleDir[1] transA[0] transB[1]
+#endif
+#if __DAV_C220_CUBE__
+    SetPadding<uint64_t>((uint64_t)0);
+    SetNdpara(1, 0, 0);
+
+    PpMatmul<0, false, false, half, half> matmul_0000; // swizzleDir[0] transA[0] transB[0]
+    PpMatmul<0, false, true, half, half> matmul_0010;  // swizzleDir[0] transA[0] transB[1]
+    PpMatmul<1, false, false, half, half> matmul_1000; // swizzleDir[1] transA[0] transB[0]
+    PpMatmul<1, false, true, half, half> matmul_1010;  // swizzleDir[1] transA[0] transB[1]
+
+    PpMatmul<0, false, false, __bf16, __bf16> matmul_0001; // swizzleDir[0] transA[0] transB[0]
+    PpMatmul<0, false, true, __bf16, __bf16> matmul_0011;  // swizzleDir[0] transA[0] transB[1]
+    PpMatmul<1, false, false, __bf16, __bf16> matmul_1001; // swizzleDir[1] transA[0] transB[0]
+    PpMatmul<1, false, true, __bf16, __bf16> matmul_1011;  // swizzleDir[1] transA[0] transB[1]
+#endif
+
+    // get tiling args
+    auto gm_tiling_data = reinterpret_cast<__gm__ AtbOps::MoeGmmTilingData *>(tiling_data);
+    uint32_t masked_key = gm_tiling_data->tilingKey & 0b111100;
+    switch (masked_key) {
+        case 0b000000: // swizzleDir[0] transA[0] transB[0]
+#ifdef __DAV_C220_CUBE__
+            matmul_0000.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_0000.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_0000.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_0000.run();
+#endif
+            break;
+        case 0b001000: // swizzleDir[0] transA[0] transB[1]
+#ifdef __DAV_C220_CUBE__
+            matmul_0010.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_0010.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_0010.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_0010.run();
+#endif
+            break;
+        case 0b100000: // swizzleDir[1] transA[0] transB[0]
+#ifdef __DAV_C220_CUBE__
+            matmul_1000.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_1000.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_1000.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_1000.run();
+#endif
+            break;
+        case 0b101000: // swizzleDir[1] transA[0] transB[1]
+#ifdef __DAV_C220_CUBE__
+            matmul_1010.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_1010.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_1010.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_1010.run();
+#endif
+            break;
+        case 0b000100: // swizzleDir[0] transA[0] transB[0]
+#ifdef __DAV_C220_CUBE__
+            matmul_0001.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_0001.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_0001.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_0001.run();
+#endif
+            break;
+        case 0b001100: // swizzleDir[0] transA[0] transB[1]
+#ifdef __DAV_C220_CUBE__
+            matmul_0011.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_0011.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_0011.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_0011.run();
+#endif
+            break;
+        case 0b100100: // swizzleDir[1] transA[0] transB[0]
+#ifdef __DAV_C220_CUBE__
+            matmul_1001.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_1001.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_1001.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_1001.run();
+#endif
+            break;
+        case 0b101100: // swizzleDir[1] transA[0] transB[1]
+#ifdef __DAV_C220_CUBE__
+            matmul_1011.InitCube(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_1011.Process();
+#elif __DAV_C220_VEC__
+            matmul_aiv_1011.InitVector(gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            matmul_aiv_1011.run();
+#endif
+            break;
+        default: break;
+    }
+}
\ No newline at end of file
diff --git a/src/kernels/mixkernels/moe_gmm/op_kernel/moe_gmm_w8a8.cce b/src/kernels/mixkernels/moe_gmm/op_kernel/moe_gmm_w8a8.cce
new file mode 100644
index 00000000..4dfec99e
--- /dev/null
+++ b/src/kernels/mixkernels/moe_gmm/op_kernel/moe_gmm_w8a8.cce
@@ -0,0 +1,1812 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+#ifdef __CCE_KT_TEST__
+#include "stub_def.h"
+#include "stub_fun.h"
+using __bf16 = bfloat16_t;
+#else
+#define __aicore__ [aicore]
+#endif
+
+#include "mixkernels/moe_gmm/tiling/tiling_data.h"
+#include "kernels/utils/kernel/common.h"
+#include "kernels/utils/kernel/common_func.h"
+#include "kernels/utils/kernel/mem.h"
+#include "kernels/utils/kernel/iterator.h"
+#include "kernels/utils/kernel/mma.h"
+#include "kernels/utils/kernel/simd.h"
+#include "kernels/utils/kernel/utils.h"
+
+// 同步信号
+constexpr int AIC2AIVFLAGID_PING = 2;
+constexpr int AIC2AIVFLAGID_PONG = 3;
+constexpr int AIV2AICFLAGID_PING = 4;
+constexpr int AIV2AICFLAGID_PONG = 5;
+constexpr uint32_t L0_PINGPONG_BUFFER_LEN = 16384;          // 16KB
+constexpr uint32_t L0_PINGPONG_BUFFER_LEN_INT8 = 16384 * 2; // 32KB
+constexpr uint32_t L1_PINGPONG_BUFFER_LEN = 131072;
+constexpr uint64_t L1_PINGPONG_BUFFER_LEN_INT8 = 262144; // 256 KB
+constexpr uint32_t CONST_16 = 16;
+constexpr uint32_t CONST_32 = 32;
+constexpr uint64_t CONST_64 = 64;
+constexpr uint32_t CONST_256 = 256;
+constexpr uint32_t CONST_512 = 512;
+constexpr uint64_t CONST_128 = 128;
+constexpr uint64_t ND2NZ_STRIDE_LIMIT = 65536;
+constexpr int32_t BLOCK_SIZE_16 = 16;
+constexpr int32_t BLOCK_SIZE_32 = 32;
+constexpr uint32_t CUBE_MATRIX_SIZE_512 = 16 * 32; // 16 * 23
+constexpr int AIVFLAGID = 1;
+
+#ifdef __DAV_C220_CUBE__
+template <uint32_t SwizzleDir,
+          bool TA,
+          bool TB,
+          typename InDtype = half,
+          typename OutDtype = half,
+          DataFormat WeightFormat = DataFormat::NZ>
+class MoeGmmW8a8Aic {
+public:
+    __aicore__ explicit MoeGmmW8a8Aic(){};
+
+    __aicore__ void InitCube(__gm__ uint8_t *__restrict__ sync,
+                             __gm__ uint8_t *__restrict__ a,
+                             __gm__ uint8_t *__restrict__ b,
+                             __gm__ uint8_t *__restrict__ flag,
+                             __gm__ uint8_t *__restrict__ index,
+                             __gm__ uint8_t *__restrict__ c,
+                             __gm__ uint8_t *__restrict__ workspace,
+                             __gm__ uint8_t *__restrict__ workspace_hp,
+                             __gm__ uint8_t *__restrict__ tiling_data)
+    {
+        auto gm_tiling_data = reinterpret_cast<__gm__ AtbOps::MoeGmmTilingData *>(tiling_data);
+        batch_size = gm_tiling_data->batch;
+        m = gm_tiling_data->m * gm_tiling_data->batch;
+        k = gm_tiling_data->k;
+        n = gm_tiling_data->n;
+        m0 = gm_tiling_data->m0;
+        k0 = gm_tiling_data->k0;
+        n0 = gm_tiling_data->n0;
+        m_loop = gm_tiling_data->mLoop;
+        k_loop = gm_tiling_data->kLoop;
+        n_loop = gm_tiling_data->nLoop;
+        core_loop = gm_tiling_data->coreLoop;
+        swizzle_cnt = gm_tiling_data->swizzlCount;
+        en_shuffle_k = gm_tiling_data->enShuffleK;
+        allM = gm_tiling_data->allM;
+        moeUp = gm_tiling_data->moeUp;
+        num_core = AscendC::GetBlockNum();
+        core_idx = AscendC::GetBlockIdx();
+        ping_flag = 1;
+
+        gmA_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(a));
+        gmB_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(b));
+        gmFlag_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(flag));
+        gmWorkspace_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(workspace));
+        gmWorkspaceDeq_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(workspace_hp));
+        gmWorkspaceGmm_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(workspace));
+
+        AsdopsBuffer<ArchType::ASCEND_V220> buf;
+        l1BaseA_ = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(0);
+        l1BaseB_ = buf.GetBuffer<BufferType::ASCEND_CB, InDtype>(RoundUp<CONST_256>(m0 * k0 * sizeof(InDtype)));
+        l0BaseA_ = buf.GetBuffer<BufferType::ASCEND_L0A, InDtype>(0);
+        l0BaseB_ = buf.GetBuffer<BufferType::ASCEND_L0B, InDtype>(0);
+        l0C_ = buf.GetBuffer<BufferType::ASCEND_L0C, int32_t>(0);
+    }
+
+    __force_inline__ __aicore__ void
+    GetBlockIdx(uint32_t index, uint32_t &m_idx, uint32_t &n_idx, const uint32_t &mm_loop)
+    {
+        uint32_t in_batch_idx = index % (mm_loop * n_loop);
+        if constexpr (SwizzleDir == 0) { // Zn
+            uint32_t tile_block_loop = (mm_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * n_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * n_loop);
+
+            uint32_t n_row = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_row = mm_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
+            n_idx = in_tile_block_idx / n_row;
+            if (tile_block_idx % 2 != 0) {
+                n_idx = n_loop - n_idx - 1;
+            }
+        } else if constexpr (SwizzleDir == 1) { // Nz
+            uint32_t tile_block_loop = (n_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * mm_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * mm_loop);
+
+            uint32_t n_col = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_col = n_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = in_tile_block_idx / n_col;
+            n_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
+            if (tile_block_idx % 2 != 0) {
+                m_idx = mm_loop - m_idx - 1;
+            }
+        }
+    }
+
+    __aicore__ void Process()
+    {
+        if (!moeUp) {
+            ProcessMoeDownCube();
+        } else {
+            ProcessMoeUpCube();
+        }
+    }
+
+    // Cube_run
+    __aicore__ void ProcessMoeDownCube()
+    {
+        using LocalTensor = AscendC::LocalTensor<InDtype>;
+        using LocalTensor = AscendC::LocalTensor<InDtype>;
+        using CopyGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>;
+        using CopyGmToCbufNd2Nz = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>;
+        using CopyWeightGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, WeightFormat, DataFormat::NZ>;
+        using LoadCbufToCa = l1_to_l0_a<ArchType::ASCEND_V220, InDtype, TA, DataFormat::ZN, DataFormat::ZZ>;
+        using LoadCbufToCb = l1_to_l0_b<ArchType::ASCEND_V220, InDtype, TB, DataFormat::ZN, DataFormat::NZ>;
+        using Mad = mmad<ArchType::ASCEND_V220, InDtype, InDtype, int32_t, false>;
+        using CopyCcToGm = l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, int32_t, int32_t>;
+
+        SET_FLAG(MTE1, MTE2, EVENT_ID0);
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(MTE1, MTE2, EVENT_ID2);
+        SET_FLAG(MTE1, MTE2, EVENT_ID3);
+        SET_FLAG(M, MTE1, EVENT_ID0);
+        SET_FLAG(M, MTE1, EVENT_ID1);
+        SET_FLAG(FIX, M, EVENT_ID0);
+        uint32_t read_flag{1};
+        int32_t ping_flag_gm = 1;
+        uint32_t loop_count = 0;
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        uint32_t n_org_up = RoundUp<CONST_32>(n);
+        uint32_t k_org_up = RoundUp<CONST_16>(k);
+        if constexpr (TB) {
+            n_org_up = RoundUp<CONST_16>(n);
+            k_org_up = RoundUp<CONST_32>(k);
+        }
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (expert_num + m0 - 1) / m0; // 当前group要分给多少个核
+
+            for (; loop_idx < core_loop + mm_loop_batch; loop_idx += num_core) {
+                uint32_t m_idx = loop_idx - core_loop;
+                uint32_t n_idx = 0;
+                if (loop_count > 1) {
+                    WaitFlagDev(AIV2AICFLAGID_PONG);
+                }
+                loop_count += 1;
+                for (n_idx = 0; n_idx < n_loop; n_idx += 1) {
+                    // Cube等待Vector发送运行指令
+                    uint32_t m_batch = expert_num;
+                    uint64_t offset_a, offset_b, offset_a_next, offset_b_next;
+                    uint64_t offset_c = expert_cumsum * n + m_idx * m0 * n + n_idx * n0;
+                    uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                    uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
+                    uint32_t m_round = 0;
+                    uint32_t n_round = RoundUp<CONST_32>(n_actual);
+                    m_round = RoundUp<CONST_16>(m_actual);
+                    if (TB) {
+                        n_round = RoundUp<CONST_16>(n_actual);
+                    }
+                    uint32_t mn_max = m_round > n_round ? m_round : n_round;
+                    uint32_t k_part_len = L0_PINGPONG_BUFFER_LEN_INT8 / mn_max / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                    uint64_t shuffle_k = en_shuffle_k ? core_idx % k_loop : 0;
+
+                    offset_a = expert_cumsum * k + m_idx * m0 * k + shuffle_k * k0;
+
+                    if constexpr (TB) {
+                        if constexpr (WeightFormat == DataFormat::NZ) {
+                            offset_b = batch_idx * k_org_up * n_org_up + shuffle_k * k0 * n_org_up +
+                                       n_idx * n0 * BLOCK_SIZE_32;
+                        } else {
+                            offset_b = batch_idx * k * n + n_idx * n0 * k + shuffle_k * k0;
+                        }
+                    } else {
+                        if constexpr (WeightFormat == DataFormat::NZ) {
+                            offset_b = batch_idx * k_org_up * n_org_up + shuffle_k * k0 * BLOCK_SIZE_32 +
+                                       n_idx * n0 * k_org_up;
+                        } else {
+                            offset_b = batch_idx * k * n + shuffle_k * k0 * n + n_idx * n0;
+                        }
+                    }
+                    uint32_t k_actual = (shuffle_k == k_loop - 1) ? k - shuffle_k * k0 : k0;
+                    uint32_t k_round = (k_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
+                    if (TB) {
+                        k_round = (k_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                    }
+
+                    LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                    LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                    event_t event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                    if (read_flag) {
+                        WAIT_FLAG(MTE1, MTE2, event_id);
+                        // *** load matrix A to L1
+                        if ((m_batch == 1) || (m_actual == 1)) {
+                            CopyGmToCbuf(l1_buf_a,       // dst
+                                         gmA_[offset_a], // src
+                                         1,              // nTileActual
+                                         16,             // nTileCeil
+                                         1,              // nVal
+                                         k_actual,       // dTileActual
+                                         k_round,        // dTileCeil
+                                         k);             // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_a,       // dst
+                                              gmA_[offset_a], // src
+                                              m_actual,       // nTileActual
+                                              m_round,        // nTileCeil
+                                              m_batch,        // nVal
+                                              k_actual,       // dTileActual
+                                              k_round,        // dTileCeil
+                                              k);             // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id);
+                        // *** load matrix B to L1
+                        WAIT_FLAG(MTE1, MTE2, event_id + 2);
+                        if constexpr (TB) {
+                            if constexpr (WeightFormat == DataFormat::ND) {
+                                CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                                   gmB_[offset_b], // src
+                                                   n_actual,       // nTileActual
+                                                   n_round,        // nTileCeil
+                                                   n_org_up,       // nVal
+                                                   k_actual,       // dTileActual
+                                                   k_round,        // dTileCeil
+                                                   k_org_up);      // dVal
+                            } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                                   gmB_[offset_b], // src
+                                                   n_round,
+                                                   n_round,        // nTileActual
+                                                   n_org_up,       // nTileCeil
+                                                   k_round,        // dTileActual
+                                                   k_round,              // unused
+                                                   0);             // unused
+                            }
+                        } else {
+                            if constexpr (WeightFormat == DataFormat::ND) {
+                                CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                                   gmB_[offset_b], // src
+                                                   k_actual,       // nTileActual
+                                                   k_round,        // nTileCeil
+                                                   k_org_up,       // nVal
+                                                   n_actual,       // dTileActual
+                                                   n_round,        // dTileCeil
+                                                   n_org_up);      // dVal
+                            } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                                   gmB_[offset_b], // src
+                                                   k_round,
+                                                   k_round,        // nTileActual
+                                                   k_org_up,       // nTileCeil
+                                                   n_round,        // dTileActual
+                                                   n_round,              // unused
+                                                   0);             // unused
+                            }
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id + 2);
+                        read_flag = 0;
+                    }
+
+                    for (uint64_t k_idx = 0; k_idx < k_loop; k_idx++) {
+                        shuffle_k = en_shuffle_k ? (k_idx + core_idx) % k_loop : k_idx;
+                        uint32_t k_actual = (shuffle_k == (k_loop - 1)) ? (k - shuffle_k * k0) : k0;
+                        uint32_t k_round = (k_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
+                        if (TB) {
+                            k_round = (k_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                        }
+
+                        uint32_t k_part_loop = (k_actual + k_part_len - 1) / k_part_len;
+
+                        LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                        LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                        auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                        if (k_idx < k_loop - 1) {
+                            uint64_t shuffle_k_next = en_shuffle_k ? (core_idx + k_idx + 1) % k_loop : k_idx + 1;
+                            offset_a_next = expert_cumsum * k + m_idx * m0 * k + shuffle_k_next * k0;
+
+                            if (TB) {
+                                if constexpr (WeightFormat == DataFormat::NZ) {
+                                    offset_b_next = batch_idx * k_org_up * n_org_up + shuffle_k_next * k0 * n_org_up +
+                                                    n_idx * n0 * BLOCK_SIZE_32;
+                                } else {
+                                    offset_b_next = batch_idx * k * n + n_idx * n0 * k + shuffle_k_next * k0;
+                                }
+                            } else {
+                                if constexpr (WeightFormat == DataFormat::NZ) {
+                                    offset_b_next = batch_idx * k_org_up * n_org_up +
+                                                    shuffle_k_next * k0 * BLOCK_SIZE_32 + n_idx * n0 * k_org_up;
+                                } else {
+                                    offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + n_idx * n0;
+                                }
+                            }
+                            uint32_t k_actual_next = (shuffle_k_next == (k_loop - 1)) ? (k - shuffle_k_next * k0) : k0;
+                            uint32_t k_round_next = (k_actual_next + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
+                            if (TB) {
+                                k_round_next = (k_actual_next + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                            }
+
+                            LocalTensor l1_buf_a_next =
+                                (1 - ping_flag) ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                            LocalTensor l1_buf_b_next =
+                                (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                            event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+
+                            WAIT_FLAG(MTE1, MTE2, event_id_next);
+                            // *** load matrix A to L1
+                            if ((m_batch == 1) || (m_actual == 1)) {
+                                CopyGmToCbuf(l1_buf_a_next,       // dst
+                                             gmA_[offset_a_next], // src
+                                             1,                   // nTileActual
+                                             16,                  // nTileCeil
+                                             1,                   // nVal
+                                             k_actual_next,       // kTileActual
+                                             k_round_next,        // kTileCeil
+                                             k);                  // dVal
+                            } else {
+                                CopyGmToCbufNd2Nz(l1_buf_a_next,       // dst
+                                                  gmA_[offset_a_next], // src
+                                                  m_actual,            // nTileActual
+                                                  m_round,             // nTileCeil
+                                                  m_batch,             // nVal
+                                                  k_actual_next,       // dTileActual
+                                                  k_round_next,        // dTileCeil
+                                                  k);                  // dVal
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next);
+
+                            // *** load matrix B to L1
+                            WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                            if constexpr (TB) {
+                                if constexpr (WeightFormat == DataFormat::ND) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       n_actual,            // nTileActual
+                                                       n_round,             // nTileCeil
+                                                       n,                   // nVal
+                                                       k_actual_next,       // dTileActual
+                                                       k_round_next,        // dTileCeil
+                                                       k);                  // dVal
+                                } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       n_round,
+                                                       n_round,             // nTileActual
+                                                       n_org_up,            // nTileCeil
+                                                       k_round_next,        // dTileActual
+                                                       k_round_next,                   // unused
+                                                       0);                  // unused
+                                }
+                            } else {
+                                if constexpr (WeightFormat == DataFormat::ND) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       k_actual_next,       // nTileActual
+                                                       k_round_next,        // nTileCeil
+                                                       k,                   // nVal
+                                                       n_actual,            // dTileActual
+                                                       n_round,             // dTileCeil
+                                                       n);                  // dVal
+                                } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       k_round_next,
+                                                       k_round_next,        // nTileActual
+                                                       k_org_up,            // nTileCeil
+                                                       n_round,             // dTileActual
+                                                       n_round,             
+                                                       0);                  // unused
+                                }
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                        } else if (n_idx + 1 < n_loop) {
+                            uint32_t m_idx_next = m_idx, n_idx_next = n_idx + 1;
+                            uint32_t b_idx_next = batch_idx;
+                            uint32_t mm_loop_batch_next = mm_loop_batch;
+                            uint32_t m_batch_next = m_batch;
+                            uint64_t shuffle_k_next = en_shuffle_k ? core_idx % k_loop : 0;
+                            uint32_t m_actual_next =
+                                (m_idx_next == (mm_loop_batch_next - 1)) ? (m_batch_next - m_idx_next * m0) : m0;
+
+                            uint32_t n_actual_next = (n_idx_next == (n_loop - 1)) ? (n - n_idx_next * n0) : n0;
+                            uint32_t m_round_next = (m_actual_next + CONST_16 - 1) / CONST_16 * CONST_16;
+                            uint32_t n_round_next = RoundUp<CONST_32>(n_actual_next);
+                            if (TB) {
+                                n_round_next = RoundUp<CONST_16>(n_actual_next);
+                            }
+                            uint32_t k_actual_next = (shuffle_k_next == k_loop - 1) ? k - shuffle_k_next * k0 : k0;
+                            uint32_t k_round_next = RoundUp<CONST_16>(k_actual_next);
+                            if (TB) {
+                                k_round_next = RoundUp<CONST_32>(k_actual_next);
+                            }
+                            offset_a_next = expert_cumsum * k + m_idx_next * m0 * k + shuffle_k_next * k0;
+                            if (TB) {
+                                if constexpr (WeightFormat == DataFormat::NZ) {
+                                    offset_b_next = b_idx_next * n_org_up * k_org_up + shuffle_k_next * k0 * n_org_up +
+                                                    n_idx_next * n0 * BLOCK_SIZE_32;
+                                } else {
+                                    offset_b_next = b_idx_next * k * n + n_idx_next * n0 * k + shuffle_k_next * k0;
+                                }
+                            } else {
+                                if constexpr (WeightFormat == DataFormat::NZ) {
+                                    offset_b_next = b_idx_next * n_org_up * k_org_up +
+                                                    shuffle_k_next * k0 * BLOCK_SIZE_32 + n_idx_next * n0 * k_org_up;
+                                } else {
+                                    offset_b_next = b_idx_next * k * n + shuffle_k_next * k0 * n + n_idx_next * n0;
+                                }
+                            }
+
+                            LocalTensor l1_buf_a_next =
+                                (1 - ping_flag) ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                            LocalTensor l1_buf_b_next =
+                                (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                            event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+
+                            WAIT_FLAG(MTE1, MTE2, event_id_next);
+                            // *** load matrix A to L1
+                            if (m_batch_next == 1 || m_actual_next == 1) {
+                                CopyGmToCbuf(l1_buf_a_next,       // dst
+                                             gmA_[offset_a_next], // src
+                                             1,                   // nTileActual
+                                             16,                  // nTileCeil
+                                             1,                   // nVal
+                                             k_actual_next,       // kTileActual
+                                             k_round_next,        // kTileCeil
+                                             k);                  // dVal
+                            } else {
+                                CopyGmToCbufNd2Nz(l1_buf_a_next,       // dst
+                                                  gmA_[offset_a_next], // src
+                                                  m_actual_next,       // nTileActual
+                                                  m_round_next,        // nTileCeil
+                                                  m_batch_next,        // nVal
+                                                  k_actual_next,       // dTileActual
+                                                  k_round_next,        // dTileCeil
+                                                  k);                  // dVal
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next);
+
+                            // *** load matrix B to L1
+                            WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                            if constexpr (TB) {
+                                if constexpr (WeightFormat == DataFormat::ND) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       n_actual_next,       // nTileActual
+                                                       n_round_next,        // nTileCeil
+                                                       n,                   // nVal
+                                                       k_actual_next,       // dTileActual
+                                                       k_round_next,        // dTileCeil
+                                                       k);                  // dVal
+                                } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       n_round_next,
+                                                       n_round_next,        // nTileActual
+                                                       n_org_up,            // nTileCeil
+                                                       k_round_next,        // dTileActual
+                                                       k_round_next,                   // dTileCeil
+                                                       0);                  // dVal
+                                }
+                            } else {
+                                if constexpr (WeightFormat == DataFormat::ND) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       k_actual_next,       // nTileActual
+                                                       k_round_next,        // nTileCeil
+                                                       k,                   // nVal
+                                                       n_actual_next,       // dTileActual
+                                                       n_round_next,        // dTileCeil
+                                                       n);                  // dVal
+                                } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                    CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                       gmB_[offset_b_next], // src
+                                                       k_round_next,
+                                                       k_round_next,        // nTileActual
+                                                       k_org_up,            // nTileCeil
+                                                       n_round_next,        // dTileActual
+                                                       n_round_next,                   // dTileCeil
+                                                       0);                  // dVal
+                                }
+                            }
+                            SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                        } else {
+                            read_flag = 1;
+                        }
+
+                        for (uint32_t k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
+                            uint32_t k0_round =
+                                (k_part_idx < k_part_loop - 1) ? k_part_len : k_round - k_part_idx * k_part_len;
+                            uint32_t k0_actual =
+                                (k_part_idx < k_part_loop - 1) ? k_part_len : k_actual - k_part_idx * k_part_len;
+
+                            auto mte1_mad_ping_flag = 1 - k_part_idx % 2;
+                            auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
+                            LocalTensor l0a_buf = l0BaseA_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN_INT8];
+                            LocalTensor l0b_buf = l0BaseB_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN_INT8];
+
+                            // *** load matrix A from L1 to L0A
+                            if (k_part_idx == 0) {
+                                WAIT_FLAG(MTE2, MTE1, event_id);
+                            }
+                            WAIT_FLAG(M, MTE1, mte1_mad_event_id);
+                            if ((m_batch == 1) || (m_actual == 1)) {
+                                l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR,
+                                           DataFormat::VECTOR>(l0a_buf,                           // dst
+                                                               l1_buf_a[k_part_idx * k_part_len], // src
+                                                               0,
+                                                               CeilDiv<CONST_512>(k0_round), // repeat
+                                                               0,
+                                                               1, // srcStride
+                                                               0,
+                                                               0); // dstStride
+                            } else {
+                                LoadCbufToCa(l0a_buf,                                     // l0Tensor
+                                             l1_buf_a[k_part_idx * k_part_len * m_round], // l1Tensor
+                                             m_round,                                     // mTileCeil
+                                             k0_round,                                    // kPartCeil
+                                             1,                                           // mSrcStride
+                                             m_round / CONST_16,                          // kSrcStride
+                                             k0_round / CONST_32,                         // mDstStride
+                                             1);                                          // kDstStride
+                            }
+                            if (k_part_idx == k_part_loop - 1) {
+                                SET_FLAG(MTE1, MTE2, event_id);
+                            }
+
+                            // *** load matrix B from L1 to L0B
+                            if (k_part_idx == 0) {
+                                WAIT_FLAG(MTE2, MTE1, event_id + 2);
+                            }
+                            if constexpr (TB) {
+                                LoadCbufToCb(l0b_buf,                                     // l0Tensor
+                                             l1_buf_b[k_part_idx * k_part_len * n_round], // l1Tensor
+                                             n_round,                                     // nTileCeil
+                                             k0_round,                                    // kPartCeil
+                                             1,                                           // nSrcStride
+                                             n_round / CONST_32,                          // kSrcStride
+                                             1,                                           // nDstStride
+                                             k0_round / CONST_32);                        // kDstStride
+                            } else {
+                                for (uint64_t i = 0; i < k0_round / BLOCK_SIZE_32; i++) {
+                                    AscendC::LoadDataWithTranspose(
+                                        l0b_buf[i * RoundUp<CONST_16>(n_actual) * BLOCK_SIZE_32],
+                                        l1_buf_b[(k_part_idx * k_part_len + i * BLOCK_SIZE_32) * BLOCK_SIZE_32],
+                                        AscendC::LoadData2dTransposeParams(0,                       // baseIdx
+                                                                           n_round / BLOCK_SIZE_32, // repeat
+                                                                           k_round / BLOCK_SIZE_32, // srcStride
+                                                                           1,                       // dstStride
+                                                                           0,                       // addrmode
+                                                                           0));                     // dstFracStride
+                                }
+                            }
+                            if (k_part_idx == k_part_loop - 1) {
+                                SET_FLAG(MTE1, MTE2, event_id + 2);
+                            }
+
+                            SET_FLAG(MTE1, M, mte1_mad_event_id);
+                            WAIT_FLAG(MTE1, M, mte1_mad_event_id);
+
+                            bool init_c = (k_idx == 0 && k_part_idx == 0);
+                            if (init_c) {
+                                WAIT_FLAG(FIX, M, EVENT_ID0);
+                            }
+
+                            Mad(l0C_,      // c
+                                l0a_buf,   // a
+                                l0b_buf,   // b
+                                m_actual,  // m
+                                n_actual,  // n
+                                k0_actual, // k
+                                init_c);   // cmatrixInitVal
+
+                            PIPE_BARRIER(M);
+                            SET_FLAG(M, MTE1, mte1_mad_event_id);
+                        }
+
+                        ping_flag = 1 - ping_flag;
+                    }
+
+                    SET_FLAG(M, FIX, EVENT_ID0);
+                    WAIT_FLAG(M, FIX, EVENT_ID0);
+                    offset_c = (ping_flag_gm * m0 + core_idx * m0 * 2) * n + n_idx * n0;
+                    // copy from L0C to gm
+                    CopyCcToGm(gmWorkspaceGmm_[offset_c], // dst
+                               l0C_,                      // src
+                               m_actual,                  // mTileActual
+                               n_actual,                  // nTileActual
+                               m_round,                   // mTileCeil
+                               n);                        // nActual
+
+                    SET_FLAG(FIX, M, EVENT_ID0);
+                }
+                // Cube给Vector发送同步指令
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID_PONG);
+                ping_flag_gm = 1 - ping_flag_gm;
+            }
+            core_loop += mm_loop_batch;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID0);
+        WAIT_FLAG(M, MTE1, EVENT_ID1);
+        WAIT_FLAG(FIX, M, EVENT_ID0);
+        PIPE_BARRIER(ALL);
+    }
+
+    // Cube_run
+    __aicore__ void ProcessMoeUpCube()
+    {
+        using LocalTensor = AscendC::LocalTensor<InDtype>;
+        using CopyGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::ND>;
+        using CopyGmToCbufNd2Nz = gm_to_l1<ArchType::ASCEND_V220, InDtype, DataFormat::ND, DataFormat::NZ>;
+        using CopyWeightGmToCbuf = gm_to_l1<ArchType::ASCEND_V220, InDtype, WeightFormat, DataFormat::NZ>;
+        using LoadCbufToCa = l1_to_l0_a<ArchType::ASCEND_V220, InDtype, TA, DataFormat::ZN, DataFormat::ZZ>;
+        using LoadCbufToCb = l1_to_l0_b<ArchType::ASCEND_V220, InDtype, TB, DataFormat::ZN, DataFormat::NZ>;
+        using Mad = mmad<ArchType::ASCEND_V220, InDtype, InDtype, int32_t, false>;
+        using CopyCcToGm = l0c_to_gm<ArchType::ASCEND_V220, DataFormat::ND, int32_t, int32_t>;
+
+        SET_FLAG(MTE1, MTE2, EVENT_ID0);
+        SET_FLAG(MTE1, MTE2, EVENT_ID1);
+        SET_FLAG(MTE1, MTE2, EVENT_ID2);
+        SET_FLAG(MTE1, MTE2, EVENT_ID3);
+        SET_FLAG(M, MTE1, EVENT_ID0);
+        SET_FLAG(M, MTE1, EVENT_ID1);
+        SET_FLAG(FIX, M, EVENT_ID0);
+        uint32_t read_flag{1};
+        int32_t ping_flag_gm = 1;
+        uint32_t loop_count = 0;
+        uint32_t n_org_up = RoundUp<CONST_32>(n);
+        uint32_t k_org_up = RoundUp<CONST_16>(k);
+        if constexpr (TB) {
+            n_org_up = RoundUp<CONST_16>(n);
+            k_org_up = RoundUp<CONST_32>(k);
+        }
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (expert_num + m0 - 1) / m0; // 当前group要分给多少个核
+            uint32_t core_loop_end = core_loop + mm_loop_batch * n_loop;
+            for (; loop_idx < core_loop_end; loop_idx += num_core) {
+                uint32_t m_idx, n_idx;
+                GetBlockIdx(loop_idx - core_loop, m_idx, n_idx, mm_loop_batch);
+
+                uint32_t m_batch = expert_num;
+                uint64_t offset_a, offset_b, offset_a_next, offset_b_next;
+                uint64_t offset_c = expert_cumsum * n + m_idx * m0 * n + n_idx * n0;
+                uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
+
+                uint32_t m_round = RoundUp<CONST_16>(m_actual);
+                uint32_t n_round = RoundUp<CONST_32>(n_actual);
+                if (TB) {
+                    n_round = RoundUp<CONST_16>(n_actual);
+                }
+                uint32_t mn_max = m_round > n_round ? m_round : n_round;
+                uint32_t k_part_len = L0_PINGPONG_BUFFER_LEN_INT8 / mn_max / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                uint64_t shuffle_k = en_shuffle_k ? core_idx % k_loop : 0;
+                offset_a = (ping_flag_gm * m0 + core_idx * m0 * 2) * k + shuffle_k * k0;
+
+                if constexpr (TB) {
+                    if constexpr (WeightFormat == DataFormat::NZ) {
+                        offset_b =
+                            batch_idx * k_org_up * n_org_up + shuffle_k * k0 * n_org_up + n_idx * n0 * BLOCK_SIZE_32;
+                    } else {
+                        offset_b = batch_idx * k * n + n_idx * n0 * k + shuffle_k * k0;
+                    }
+                } else {
+                    if constexpr (WeightFormat == DataFormat::NZ) {
+                        offset_b =
+                            batch_idx * k_org_up * n_org_up + shuffle_k * k0 * BLOCK_SIZE_32 + n_idx * n0 * k_org_up;
+                    } else {
+                        offset_b = batch_idx * k * n + shuffle_k * k0 * n + n_idx * n0;
+                    }
+                }
+                uint32_t k_actual = (shuffle_k == k_loop - 1) ? k - shuffle_k * k0 : k0;
+                uint32_t k_round = (k_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
+                if constexpr (TB) {
+                    k_round = (k_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                }
+                // Cube等待Vector发送运行指令
+                uint64_t AIV2AICFLAGID = ping_flag_gm == 0 ? AIV2AICFLAGID_PING : AIV2AICFLAGID_PONG;
+                WaitFlagDev(AIV2AICFLAGID);
+
+                LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                event_t event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                WAIT_FLAG(MTE1, MTE2, event_id);
+                // *** load matrix A to L1
+                if ((m_batch == 1) || (m_actual == 1)) {
+                    CopyGmToCbuf(l1_buf_a,               // dst
+                                 gmWorkspace_[offset_a], // src
+                                 1,                      // nTileActual
+                                 16,                     // nTileCeil
+                                 1,                      // nVal
+                                 k_actual,               // dTileActual
+                                 k_round,                // dTileCeil
+                                 k);                     // dVal
+                } else {
+                    CopyGmToCbufNd2Nz(l1_buf_a,               // dst
+                                      gmWorkspace_[offset_a], // src
+                                      m_actual,               // nTileActual
+                                      m_round,                // nTileCeil
+                                      m0,                     // nVal
+                                      k_actual,               // dTileActual
+                                      k_round,                // dTileCeil
+                                      k);                     // dVal
+                }
+                SET_FLAG(MTE2, MTE1, event_id);
+                if (read_flag) {
+                    // *** load matrix B to L1
+                    WAIT_FLAG(MTE1, MTE2, event_id + 2);
+                    if constexpr (TB) {
+                        if constexpr (WeightFormat == DataFormat::ND) {
+                            CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                               gmB_[offset_b], // src
+                                               n_actual,       // nTileActual
+                                               n_round,        // nTileCeil
+                                               n,              // nVal
+                                               k_actual,       // dTileActual
+                                               k_round,        // dTileCeil
+                                               k);             // dVal
+                        } else if constexpr (WeightFormat == DataFormat::NZ) {
+                            CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                               gmB_[offset_b], // src
+                                               n_round,
+                                               n_round,        // nTileActual
+                                               n_org_up,       // nTileCeil
+                                               k_round,        // dTileActual
+                                               k_round,              // unused
+                                               0);             // unused
+                        }
+                    } else {
+                        if constexpr (WeightFormat == DataFormat::ND) {
+                            CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                               gmB_[offset_b], // src
+                                               k_actual,       // nTileActual
+                                               k_round,        // nTileCeil
+                                               k,              // nVal
+                                               n_actual,       // dTileActual
+                                               n_round,        // dTileCeil
+                                               n);             // dVal
+                        } else if constexpr (WeightFormat == DataFormat::NZ) {
+                            CopyWeightGmToCbuf(l1_buf_b,       // dst
+                                               gmB_[offset_b], // src
+                                               k_round,
+                                               k_round,        // nTileActual
+                                               k_org_up,       // nTileCeil
+                                               n_round,        // dTileActual
+                                               n_round,              // unused
+                                               0);             // unused
+                        }
+                    }
+                    SET_FLAG(MTE2, MTE1, event_id + 2);
+                    read_flag = 0;
+                }
+
+                for (uint64_t k_idx = 0; k_idx < k_loop; k_idx++) {
+                    shuffle_k = en_shuffle_k ? (k_idx + core_idx) % k_loop : k_idx;
+                    uint32_t k_actual = (shuffle_k == (k_loop - 1)) ? (k - shuffle_k * k0) : k0;
+                    uint32_t k_round = (k_actual + BLOCK_SIZE_16 - 1) / BLOCK_SIZE_16 * BLOCK_SIZE_16;
+                    if constexpr (TB) {
+                        k_round = (k_actual + BLOCK_SIZE_32 - 1) / BLOCK_SIZE_32 * BLOCK_SIZE_32;
+                    }
+                    uint32_t k_part_loop = (k_actual + k_part_len - 1) / k_part_len;
+
+                    LocalTensor l1_buf_a = ping_flag ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                    LocalTensor l1_buf_b = ping_flag ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                    auto event_id = ping_flag ? EVENT_ID0 : EVENT_ID1;
+
+                    if (k_idx < k_loop - 1) {
+                        uint64_t shuffle_k_next = en_shuffle_k ? (core_idx + k_idx + 1) % k_loop : k_idx + 1;
+                        offset_a_next = core_idx * m0 * 2 * k + ping_flag_gm * m0 * k + shuffle_k_next * k0;
+
+                        if constexpr (TB) {
+                            if constexpr (WeightFormat == DataFormat::NZ) {
+                                offset_b_next = batch_idx * k_org_up * n_org_up + shuffle_k_next * k0 * n_org_up +
+                                                n_idx * n0 * BLOCK_SIZE_32;
+                            } else {
+                                offset_b_next = batch_idx * k * n + n_idx * n0 * k + shuffle_k_next * k0;
+                            }
+                        } else {
+                            if constexpr (WeightFormat == DataFormat::NZ) {
+                                offset_b_next = batch_idx * k_org_up * n_org_up + shuffle_k_next * k0 * BLOCK_SIZE_32 +
+                                                n_idx * n0 * k_org_up;
+                            } else {
+                                offset_b_next = batch_idx * k * n + shuffle_k_next * k0 * n + n_idx * n0;
+                            }
+                        }
+
+                        uint32_t k_actual_next = (shuffle_k_next == (k_loop - 1)) ? (k - shuffle_k_next * k0) : k0;
+                        uint32_t k_round_next = RoundUp<CONST_16>(k_actual_next);
+                        if constexpr (TB) {
+                            k_round_next = RoundUp<CONST_32>(k_actual_next);
+                        }
+
+                        LocalTensor l1_buf_a_next = (1 - ping_flag) ? l1BaseA_ : l1BaseA_[L1_PINGPONG_BUFFER_LEN_INT8];
+                        LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                        event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+
+                        WAIT_FLAG(MTE1, MTE2, event_id_next);
+                        // *** load matrix A to L1
+                        if ((m_batch == 1) || (m_actual == 1)) {
+                            CopyGmToCbuf(l1_buf_a_next,               // dst
+                                         gmWorkspace_[offset_a_next], // src
+                                         1,                           // nTileActual
+                                         16,                          // nTileCeil
+                                         1,                           // nVal
+                                         k_actual_next,               // kTileActual
+                                         k_round_next,                // kTileCeil
+                                         k);                          // dVal
+                        } else {
+                            CopyGmToCbufNd2Nz(l1_buf_a_next,               // dst
+                                              gmWorkspace_[offset_a_next], // src
+                                              m_actual,                    // nTileActual
+                                              m_round,                     // nTileCeil
+                                              m0,                          // nVal
+                                              k_actual_next,               // dTileActual
+                                              k_round_next,                // dTileCeil
+                                              k);                          // dVal
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id_next);
+
+                        // *** load matrix B to L1
+                        WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                        if constexpr (TB) {
+                            if constexpr (WeightFormat == DataFormat::ND) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   n_actual,            // nTileActual
+                                                   n_round,             // nTileCeil
+                                                   n,                   // nVal
+                                                   k_actual_next,       // dTileActual
+                                                   k_round_next,        // dTileCeil
+                                                   k);                  // dVal
+                            } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   n_round,
+                                                   n_round,             // nTileActual
+                                                   n_org_up,            // nTileCeil
+                                                   k_round_next,        // dTileActual
+                                                   k_round_next,                   // unused
+                                                   0);                  // unused
+                            }
+                        } else {
+                            if constexpr (WeightFormat == DataFormat::ND) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   k_actual_next,       // nTileActual
+                                                   k_round_next,        // nTileCeil
+                                                   k,                   // nVal
+                                                   n_actual,            // dTileActual
+                                                   n_round,             // dTileCeil
+                                                   n);                  // dVal
+                            } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   k_round_next,
+                                                   k_round_next,        // nTileActual
+                                                   k_org_up,            // nTileCeil
+                                                   n_round,             // dTileActual
+                                                   n_round,            
+                                                   0);                  // unused
+                            }
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                    } else if (loop_idx + num_core < core_loop_end) {
+                        uint32_t m_idx_next = 0, n_idx_next = 0;
+                        GetBlockIdx(loop_idx + num_core - core_loop, m_idx_next, n_idx_next, mm_loop_batch);
+                        uint32_t b_idx_next = batch_idx;
+                        uint64_t shuffle_k_next = en_shuffle_k ? core_idx % k_loop : 0;
+                        uint32_t n_actual_next = (n_idx_next == (n_loop - 1)) ? (n - n_idx_next * n0) : n0;
+                        uint32_t k_actual_next = (shuffle_k_next == k_loop - 1) ? k - shuffle_k_next * k0 : k0;
+                        uint32_t n_round_next = RoundUp<CONST_16>(n_actual_next);
+                        uint32_t k_round_next = RoundUp<CONST_16>(k_actual_next);
+                        if constexpr (TB) {
+                            k_round_next = RoundUp<CONST_32>(k_actual_next);
+                        }
+
+                        if constexpr (TB) {
+                            if constexpr (WeightFormat == DataFormat::NZ) {
+                                offset_b_next = b_idx_next * n_org_up * k_org_up + shuffle_k_next * k0 * n_org_up +
+                                                n_idx_next * n0 * BLOCK_SIZE_32;
+                            } else {
+                                offset_b_next = b_idx_next * k * n + n_idx_next * n0 * k + shuffle_k_next * k0;
+                            }
+                        } else {
+                            if constexpr (WeightFormat == DataFormat::NZ) {
+                                offset_b_next = b_idx_next * k_org_up * n_org_up + shuffle_k_next * k0 * BLOCK_SIZE_32 +
+                                                n_idx_next * n0 * k_org_up;
+                            } else {
+                                offset_b_next = b_idx_next * k * n + shuffle_k_next * k0 * n + n_idx_next * n0;
+                            }
+                        }
+
+                        LocalTensor l1_buf_b_next = (1 - ping_flag) ? l1BaseB_ : l1BaseB_[L1_PINGPONG_BUFFER_LEN_INT8];
+                        event_t event_id_next = (1 - ping_flag) ? EVENT_ID0 : EVENT_ID1;
+                        // *** load matrix B to L1
+                        WAIT_FLAG(MTE1, MTE2, event_id_next + 2);
+                        if constexpr (TB) {
+                            if constexpr (WeightFormat == DataFormat::ND) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   n_actual_next,       // nTileActual
+                                                   n_round_next,        // nTileCeil
+                                                   n,                   // nVal
+                                                   k_actual_next,       // dTileActual
+                                                   k_round_next,        // dTileCeil
+                                                   k);                  // dVal
+                            } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   n_round_next,
+                                                   n_round_next,        // nTileActual
+                                                   n_org_up,            // nTileCeil
+                                                   k_round_next,        // dTileActual
+                                                   k_round_next,                   // unused
+                                                   0);                  // unused
+                            }
+                        } else {
+                            if constexpr (WeightFormat == DataFormat::ND) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   k_actual_next,       // nTileActual
+                                                   k_round_next,        // nTileCeil
+                                                   k,                   // nVal
+                                                   n_actual_next,       // dTileActual
+                                                   n_round_next,        // dTileCeil
+                                                   n);                  // dVal
+                            } else if constexpr (WeightFormat == DataFormat::NZ) {
+                                CopyWeightGmToCbuf(l1_buf_b_next,       // dst
+                                                   gmB_[offset_b_next], // src
+                                                   k_round_next,
+                                                   k_round_next,        // nTileActual
+                                                   k_org_up,            // nTileCeil
+                                                   n_round_next,        // dTileActual
+                                                   n_round_next,                   // unused
+                                                   0);                  // unused
+                            }
+                        }
+                        SET_FLAG(MTE2, MTE1, event_id_next + 2);
+                    } else {
+                        read_flag = 1;
+                    }
+
+                    for (uint32_t k_part_idx = 0; k_part_idx < k_part_loop; k_part_idx++) {
+                        uint32_t k0_round =
+                            (k_part_idx < k_part_loop - 1) ? k_part_len : k_round - k_part_idx * k_part_len;
+                        uint32_t k0_actual =
+                            (k_part_idx < k_part_loop - 1) ? k_part_len : k_actual - k_part_idx * k_part_len;
+
+                        auto mte1_mad_ping_flag = 1 - k_part_idx % 2;
+                        auto mte1_mad_event_id = mte1_mad_ping_flag ? EVENT_ID0 : EVENT_ID1;
+                        LocalTensor l0a_buf = l0BaseA_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN_INT8];
+                        LocalTensor l0b_buf = l0BaseB_[(k_part_idx % 2) * L0_PINGPONG_BUFFER_LEN_INT8];
+
+                        // *** load matrix A from L1 to L0A
+                        if (k_part_idx == 0) {
+                            WAIT_FLAG(MTE2, MTE1, event_id);
+                        }
+                        WAIT_FLAG(M, MTE1, mte1_mad_event_id);
+                        if ((m_batch == 1) || (m_actual == 1)) {
+                            l1_to_l0_a<ArchType::ASCEND_V220, InDtype, false, DataFormat::VECTOR,
+                                       DataFormat::VECTOR>(l0a_buf,                           // dst
+                                                           l1_buf_a[k_part_idx * k_part_len], // src
+                                                           0,
+                                                           CeilDiv<CONST_512>(k0_round), // repeat
+                                                           0,
+                                                           1, // srcStride
+                                                           0,
+                                                           0); // dstStride
+                        } else {
+                            LoadCbufToCa(l0a_buf,                                     // l0Tensor
+                                         l1_buf_a[k_part_idx * k_part_len * m_round], // l1Tensor
+                                         m_round,                                     // mTileCeil
+                                         k0_round,                                    // kPartCeil
+                                         1,                                           // mSrcStride
+                                         m_round / CONST_16,                          // kSrcStride
+                                         k0_round / CONST_32,                         // mDstStride
+                                         1);                                          // kDstStride
+                        }
+                        if (k_part_idx == k_part_loop - 1) {
+                            SET_FLAG(MTE1, MTE2, event_id);
+                        }
+
+                        // *** load matrix B from L1 to L0B
+                        if (k_part_idx == 0) {
+                            WAIT_FLAG(MTE2, MTE1, event_id + 2);
+                        }
+                        if constexpr (TB) {
+                            LoadCbufToCb(l0b_buf,                                     // l0Tensor
+                                         l1_buf_b[k_part_idx * k_part_len * n_round], // l1Tensor
+                                         n_round,                                     // nTileCeil
+                                         k0_round,                                    // kPartCeil
+                                         1,                                           // nSrcStride
+                                         n_round / CONST_32,                          // kSrcStride
+                                         1,                                           // nDstStride
+                                         k0_round / CONST_32);                        // kDstStride
+                        } else {
+                            for (uint64_t i = 0; i < k0_round / BLOCK_SIZE_32; i++) {
+                                AscendC::LoadDataWithTranspose(
+                                    l0b_buf[i * RoundUp<CONST_16>(n_actual) * BLOCK_SIZE_32],
+                                    l1_buf_b[(k_part_idx * k_part_len + i * BLOCK_SIZE_32) * BLOCK_SIZE_32],
+                                    AscendC::LoadData2dTransposeParams(0,                       // baseIdx
+                                                                       n_round / BLOCK_SIZE_32, // repeat
+                                                                       k_round / BLOCK_SIZE_32, // srcStride
+                                                                       1,                       // dstStride
+                                                                       0,                       // addrmode
+                                                                       0));                     // dstFracStride
+                            }
+                        }
+                        if (k_part_idx == k_part_loop - 1) {
+                            SET_FLAG(MTE1, MTE2, event_id + 2);
+                        }
+
+                        SET_FLAG(MTE1, M, mte1_mad_event_id);
+                        WAIT_FLAG(MTE1, M, mte1_mad_event_id);
+
+                        bool init_c = (k_idx == 0 && k_part_idx == 0);
+                        if (init_c) {
+                            WAIT_FLAG(FIX, M, EVENT_ID0);
+                        }
+
+                        Mad(l0C_,      // c
+                            l0a_buf,   // a
+                            l0b_buf,   // b
+                            m_actual,  // m
+                            n_actual,  // n
+                            k0_actual, // k
+                            init_c);   // cmatrixInitVal
+
+                        PIPE_BARRIER(M);
+                        SET_FLAG(M, MTE1, mte1_mad_event_id);
+                    }
+                    ping_flag = 1 - ping_flag;
+                }
+
+                SET_FLAG(M, FIX, EVENT_ID0);
+                WAIT_FLAG(M, FIX, EVENT_ID0);
+                // copy from L0C to gm
+                CopyCcToGm(gmWorkspaceDeq_[offset_c], // dst
+                           l0C_,                      // src
+                           m_actual,                  // mTileActual
+                           n_actual,                  // nTileActual
+                           m_round,                   // mTileCeil
+                           n);                        // nActual
+
+                SET_FLAG(FIX, M, EVENT_ID0);
+                // Cube给Vector发送同步指令
+                uint64_t AIC2AIVFLAGID = ping_flag_gm == 0 ? AIC2AIVFLAGID_PING : AIC2AIVFLAGID_PONG;
+                FftsCrossCoreSync<PIPE_FIX, 2>(AIC2AIVFLAGID);
+                ping_flag_gm = 1 - ping_flag_gm;
+            }
+            core_loop += mm_loop_batch * n_loop;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID0);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID1);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID2);
+        WAIT_FLAG(MTE1, MTE2, EVENT_ID3);
+        WAIT_FLAG(M, MTE1, EVENT_ID0);
+        WAIT_FLAG(M, MTE1, EVENT_ID1);
+        WAIT_FLAG(FIX, M, EVENT_ID0);
+        PIPE_BARRIER(ALL);
+    }
+
+private:
+    AscendC::GlobalTensor<InDtype> gmA_;
+    AscendC::GlobalTensor<InDtype> gmB_;
+    AscendC::GlobalTensor<int32_t> gmFlag_;
+    AscendC::GlobalTensor<InDtype> gmWorkspace_;
+    AscendC::GlobalTensor<int32_t> gmWorkspaceDeq_;
+    AscendC::GlobalTensor<int32_t> gmWorkspaceGmm_;
+
+    AscendC::LocalTensor<InDtype> l1BaseA_;
+    AscendC::LocalTensor<InDtype> l1BaseB_;
+    AscendC::LocalTensor<InDtype> l0BaseA_;
+    AscendC::LocalTensor<InDtype> l0BaseB_;
+    AscendC::LocalTensor<int32_t> l0C_;
+
+    uint32_t num_core{0};
+    uint32_t batch_size{0};
+    uint32_t m{0};
+    uint32_t k{0};
+    uint32_t n{0};
+    uint32_t m0{0};
+    uint32_t k0{0};
+    uint32_t n0{0};
+    uint32_t m_loop{0};
+    uint32_t n_loop{0};
+    uint32_t k_loop{0};
+    uint32_t core_loop{0};
+    uint32_t core_idx{0};
+    uint32_t swizzle_cnt{1};
+    uint32_t ping_flag{0};
+    uint32_t en_shuffle_k{0};
+    uint32_t allM{0};
+    uint32_t moeUp{0};
+};
+#endif
+
+#if __DAV_C220_VEC__
+template <typename Dtype>
+using CopyGmToUb = gm_to_ub<ArchType::ASCEND_V220, Dtype>;
+
+template <typename Dtype>
+using CopyUbToGm = ub_to_gm<ArchType::ASCEND_V220, Dtype, DataFormat::ND, DataFormat::ND>;
+
+template <uint32_t SwizzleDir, bool TA, bool TB, typename InDtype = half, typename OutDtype = half>
+class MoeGmmW8a8Aiv {
+public:
+    __aicore__ explicit MoeGmmW8a8Aiv(){};
+
+    __aicore__ void SetArgs(__gm__ uint8_t *__restrict__ sync,
+                            __gm__ uint8_t *__restrict__ a,
+                            __gm__ uint8_t *__restrict__ b,
+                            __gm__ uint8_t *__restrict__ flag,
+                            __gm__ uint8_t *__restrict__ index,
+                            __gm__ uint8_t *__restrict__ nscale,
+                            __gm__ uint8_t *__restrict__ mscale,
+                            __gm__ uint8_t *__restrict__ c,
+                            __gm__ uint8_t *__restrict__ workspace,
+                            __gm__ uint8_t *__restrict__ workspace_hp,
+                            __gm__ uint8_t *__restrict__ tiling_data)
+    {
+        auto gm_tiling_data = reinterpret_cast<__gm__ AtbOps::MoeGmmTilingData *>(tiling_data);
+        batch_size = gm_tiling_data->batch;
+        m = gm_tiling_data->m * gm_tiling_data->batch;
+        k = gm_tiling_data->k;
+        n = gm_tiling_data->n;
+        m0 = gm_tiling_data->m0;
+        k0 = gm_tiling_data->k0;
+        n0 = gm_tiling_data->n0;
+        m_loop = gm_tiling_data->mLoop;
+        k_loop = gm_tiling_data->kLoop;
+        n_loop = gm_tiling_data->nLoop;
+        core_loop = gm_tiling_data->coreLoop;
+        swizzle_cnt = gm_tiling_data->swizzlCount;
+        en_shuffle_k = gm_tiling_data->enShuffleK;
+        moeUp = gm_tiling_data->moeUp;
+        allM = gm_tiling_data->allM;
+        num_core = AscendC::GetBlockNum();
+        core_idx = AscendC::GetBlockIdx() / AscendC::GetTaskRation();
+        ping_flag = 1;
+        gmA_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(a));
+        gmB_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(b));
+        gmFlag_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(flag));
+        gmC_.SetGlobalBuffer(reinterpret_cast<__gm__ OutDtype *>(c));
+        gmScaleM_.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(mscale));
+        gmScaleN_.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(nscale));
+        gmIndex_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(index));
+        gmWorkspace_.SetGlobalBuffer(reinterpret_cast<__gm__ InDtype *>(workspace));
+        gmWorkspaceDeq_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(workspace_hp));
+        gmWorkspaceGmm_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(workspace));
+        gmWorkspaceHp_.SetGlobalBuffer(reinterpret_cast<__gm__ float *>(workspace_hp));
+    }
+
+    __force_inline__ __aicore__ void
+    GetBlockIdx(uint32_t index, uint32_t &m_idx, uint32_t &n_idx, const uint32_t &mm_loop)
+    {
+        uint32_t in_batch_idx = index % (mm_loop * n_loop);
+        if constexpr (SwizzleDir == 0) { // Zn
+            uint32_t tile_block_loop = (mm_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * n_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * n_loop);
+
+            uint32_t n_row = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_row = mm_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_row;
+            n_idx = in_tile_block_idx / n_row;
+            if (tile_block_idx % 2 != 0) {
+                n_idx = n_loop - n_idx - 1;
+            }
+        } else if constexpr (SwizzleDir == 1) { // Nz
+            uint32_t tile_block_loop = (n_loop + swizzle_cnt - 1) / swizzle_cnt;
+            uint32_t tile_block_idx = in_batch_idx / (swizzle_cnt * mm_loop);
+            uint32_t in_tile_block_idx = in_batch_idx % (swizzle_cnt * mm_loop);
+
+            uint32_t n_col = swizzle_cnt;
+            if (tile_block_idx == tile_block_loop - 1) {
+                n_col = n_loop - swizzle_cnt * tile_block_idx;
+            }
+            m_idx = in_tile_block_idx / n_col;
+            n_idx = tile_block_idx * swizzle_cnt + in_tile_block_idx % n_col;
+            if (tile_block_idx % 2 != 0) {
+                m_idx = mm_loop - m_idx - 1;
+            }
+        }
+    }
+
+    __aicore__ void run()
+    {
+        if (!moeUp) {
+            ProcessMoeDownVector();
+        } else {
+            ProcessMoeUpVector();
+        }
+    }
+
+    __aicore__ void ProcessMoeDownVector()
+    {
+        uint32_t sub_block_id = AscendC::GetSubBlockIdx();
+        uint32_t sub_block_dim = AscendC::GetTaskRation();
+        AsdopsBuffer<ArchType::ASCEND_V220> buf;
+        auto ubBuf = buf.GetBuffer<BufferType::ASCEND_UB, OutDtype>((uintptr_t)0);
+        auto ubBufHp = buf.GetBuffer<BufferType::ASCEND_UB, float>((uintptr_t)n * sizeof(float) * 2);
+        auto ubBufScale = buf.GetBuffer<BufferType::ASCEND_UB, float>((uintptr_t)n * sizeof(float) * 4);
+        auto ubBufI32 = buf.GetBuffer<BufferType::ASCEND_UB, int32_t>((uintptr_t)n * sizeof(float) * 6);
+        uint32_t num_blocks = n * sizeof(OutDtype) / BLOCK_SIZE_32; // fp16
+        uint32_t num_blocks_16 = n * sizeof(half) / BLOCK_SIZE_32;  // fp16
+        uint32_t num_blocks_hp = n * sizeof(float) / BLOCK_SIZE_32; // float
+        uint32_t fp32_total_burst_num = (n + 63) / 64;              // float
+
+        int32_t ping_flag_gm = 1;
+        int32_t vcore_num = num_core * sub_block_dim;
+        int32_t vcore_loop = allM;
+        if (core_idx * sub_block_dim + sub_block_id < vcore_loop) {
+            uint64_t mask[] = {(uint64_t)-1, (uint64_t)-1};
+            AscendC::Duplicate<float>(ubBufHp, 0.0f, mask, CeilDiv<CONST_64>(n), 1, 8);
+        }
+        SET_FLAG(V, MTE3, EVENT_ID0);
+        WAIT_FLAG(V, MTE3, EVENT_ID0);
+        for (int loop_idx = core_idx * sub_block_dim + sub_block_id; loop_idx < vcore_loop; loop_idx += vcore_num) {
+            CopyUbToGm<float>(gmWorkspaceHp_[loop_idx * n], // dst
+                              ubBufHp,                      // src
+                              0,                            // sid
+                              1,                            // nBurst
+                              num_blocks_hp,                // lenBurst
+                              0,                            // srcGap
+                              0);                           // dstGap
+        }
+        FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+        WaitFlagDev(AIVFLAGID);
+
+        core_loop = m_loop * batch_size;
+        AscendC::SetAtomicAdd<float>();
+
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        uint32_t ping_flag_ub = 0;
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (expert_num + m0 - 1) / m0;
+            uint32_t m_start = expert_cumsum;
+            for (; loop_idx < core_loop + mm_loop_batch; loop_idx += num_core) {
+                uint32_t m_idx = loop_idx - core_loop;
+                int32_t output_start = ping_flag_gm * m0 + core_idx * m0 * 2;
+                uint32_t m_batch = expert_num;
+                uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                // 等待 cube 计算
+                WaitFlagDev(AIC2AIVFLAGID_PONG);
+
+                for (int j = sub_block_id; j < m_actual; j += sub_block_dim) {
+                    // 执行准备数据
+                    uint32_t cache_start = (uint32_t)(j + output_start) * n;
+                    CopyGmToUb<int32_t>(ubBufI32[ping_flag_ub * n],   // dst
+                                        gmWorkspaceGmm_[cache_start], // src
+                                        0,                            // sid
+                                        1,                            // nBurst
+                                        num_blocks_hp,                // lenBurst
+                                        0,                            // srcGap
+                                        0);                           // dstGap
+                    CopyGmToUb<float>(ubBufScale[ping_flag_ub * n],   // dst
+                                      gmScaleN_[batch_idx * n],       // src
+                                      0,                              // sid
+                                      1,                              // nBurst
+                                      num_blocks_hp,                  // lenBurst
+                                      0,                              // srcGap
+                                      0);                             // dstGap
+                    SET_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+                    conv_v<ArchType::ASCEND_V220, int32_t, float>(ubBufHp[ping_flag_ub * n],  // dst
+                                                                  ubBufI32[ping_flag_ub * n], // src
+                                                                  fp32_total_burst_num, 1, 1, 8, 8);
+                    PIPE_BARRIER(V);
+                    mul_v<ArchType::ASCEND_V220, float>(ubBufHp[ping_flag_ub * n],    // dst
+                                                        ubBufHp[ping_flag_ub * n],    // src0
+                                                        ubBufScale[ping_flag_ub * n], // src1
+                                                        fp32_total_burst_num, 1, 1, 1, 8, 8, 8);
+                    PIPE_BARRIER(V);
+                    uint32_t sstart = m_start + j + m_idx * m0;
+                    float scale_tokens = gmScaleM_.GetValue(sstart);
+                    muls_v<ArchType::ASCEND_V220, float>(ubBufHp[ping_flag_ub * n], // dst
+                                                         ubBufHp[ping_flag_ub * n], // src0
+                                                         scale_tokens,              // src1
+                                                         fp32_total_burst_num, 1, 1, 8, 8);
+                    SET_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+                    uint32_t start = gmIndex_.GetValue(m_start + j + m_idx * m0) * n;
+                    CopyUbToGm<float>(gmWorkspaceHp_[start],     // dst
+                                      ubBufHp[ping_flag_ub * n], // src
+                                      0,                         // sid
+                                      1,                         // nBurst
+                                      num_blocks_hp,             // lenBurst
+                                      0,                         // srcGap
+                                      0);                        // dstGap
+                    SET_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    ping_flag_ub = 1 - ping_flag_ub;
+                }
+                // Vector给Cube发送同步指令
+                FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID_PONG);
+                // 切换GM上的double buffer
+                ping_flag_gm = 1 - ping_flag_gm;
+            }
+            core_loop += mm_loop_batch;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+
+        // Vector同步信号
+        FftsCrossCoreSync<PIPE_MTE3, 0>(AIVFLAGID);
+        WaitFlagDev(AIVFLAGID);
+        AscendC::SetAtomicNone();
+        for (int loop_idx = core_idx * sub_block_dim + sub_block_id; loop_idx < vcore_loop; loop_idx += vcore_num) {
+            // 执行准备数据
+            uint32_t cache_start = (uint32_t)loop_idx * n;
+            CopyGmToUb<float>(ubBufHp[ping_flag_ub * n],   // dst
+                              gmWorkspaceHp_[cache_start], // src
+                              0,                           // sid
+                              1,                           // nBurst
+                              num_blocks_hp,               // lenBurst
+                              0,                           // srcGap
+                              0);                          // dstGap
+            SET_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+            WAIT_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+
+            conv_v<ArchType::ASCEND_V220, float, OutDtype>(ubBuf[ping_flag_ub * n],   // dst
+                                                           ubBufHp[ping_flag_ub * n], // src
+                                                           fp32_total_burst_num, 1, 1, 4, 8);
+
+            SET_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+            WAIT_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+            CopyUbToGm<OutDtype>(gmC_[cache_start],       // dst
+                                 ubBuf[ping_flag_ub * n], // src
+                                 0,                       // sid
+                                 1,                       // nBurst
+                                 num_blocks,              // lenBurst
+                                 0,                       // srcGap
+                                 0                        // dstGap
+            );
+            SET_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+            WAIT_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+            ping_flag_ub = 1 - ping_flag_ub;
+        }
+        PIPE_BARRIER(ALL);
+    }
+
+    // Vector_run
+    __aicore__ void ProcessMoeUpVector()
+    {
+        uint32_t sub_block_id = AscendC::GetSubBlockIdx();
+        uint32_t sub_block_dim = AscendC::GetTaskRation();
+        uint32_t loop_count = 0;
+        AsdopsBuffer<ArchType::ASCEND_V220> buf;
+        auto ubBufF32 = buf.GetBuffer<BufferType::ASCEND_UB, float>((uintptr_t)0);
+        auto ubBufScale = buf.GetBuffer<BufferType::ASCEND_UB, float>((uintptr_t)n0 * sizeof(float) * 2);
+        auto ubBufOut = buf.GetBuffer<BufferType::ASCEND_UB, OutDtype>((uintptr_t)n0 * sizeof(float) * 4);
+        auto ubBuf = buf.GetBuffer<BufferType::ASCEND_UB, InDtype>((uintptr_t)n0 * sizeof(float) * 6);
+        auto ubBufI32 =
+            buf.GetBuffer<BufferType::ASCEND_UB, int32_t>((uintptr_t)n0 * sizeof(float) * 12 + k * sizeof(InDtype) * 2);
+        uint32_t num_blocks = k * sizeof(InDtype) / BLOCK_SIZE_32;    // int8
+        uint32_t num_blocks_32 = n * sizeof(int32_t) / BLOCK_SIZE_32; // fp32
+        uint32_t num_blocks_o = n * sizeof(OutDtype) / BLOCK_SIZE_32; // fp16
+        uint32_t fp32_total_burst_num = (n + 63) / 64;                // float
+
+        int32_t ping_flag_gm = 1;
+        core_loop = 0;
+        uint32_t loop_idx = core_idx;
+        uint32_t expert_cumsum = 0;
+        uint32_t expert_num = 0;
+        uint32_t mm_loop_batch = 0;
+        int32_t ping_flag_ub = 0;
+        SetVectorMask<uint8_t>((uint64_t)-1, (uint64_t)-1);
+        for (uint32_t batch_idx = 0; batch_idx < batch_size; batch_idx += 1) {
+            expert_num = gmFlag_.GetValue(batch_idx) - expert_cumsum;
+            mm_loop_batch = (expert_num + m0 - 1) / m0;
+            uint32_t m_start = expert_cumsum;
+
+            uint32_t core_loop_end = core_loop + mm_loop_batch * n_loop;
+            for (; loop_idx < core_loop_end; loop_idx += num_core) {
+                uint32_t m_idx, n_idx;
+                GetBlockIdx(loop_idx - core_loop, m_idx, n_idx, mm_loop_batch);
+                int32_t output_start = ping_flag_gm * m0 + core_idx * m0 * 2;
+                uint32_t m_batch = expert_num;
+                uint32_t m_actual = (m_idx == (mm_loop_batch - 1)) ? (m_batch - m_idx * m0) : m0;
+                // gather
+                for (int j = sub_block_id; j < m_actual; j += sub_block_dim) {
+                    uint32_t start = gmIndex_.GetValue(expert_cumsum + j + m_idx * m0) * k;
+                    uint32_t cache_start = (uint32_t)(j + output_start) * k;
+                    // 执行准备数据
+                    CopyGmToUb<InDtype>(ubBuf[ping_flag_ub * k], // dst
+                                        gmA_[start],             // src
+                                        0,                       // sid
+                                        1,                       // nBurst
+                                        num_blocks,              // lenBurst
+                                        0,                       // srcGap
+                                        0);                      // dstGap
+                    SET_FLAG(MTE2, MTE3, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE2, MTE3, EVENT_ID0 + ping_flag_ub);
+                    CopyUbToGm<InDtype>(gmWorkspace_[cache_start], // dst
+                                        ubBuf[ping_flag_ub * k],   // src
+                                        0,                         // sid
+                                        1,                         // nBurst
+                                        num_blocks,                // lenBurst
+                                        0,                         // srcGap
+                                        0);                        // dstGap
+                    SET_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    ping_flag_ub = 1 - ping_flag_ub;
+                }
+                // Vector给Cube发送同步指令
+                uint64_t AIV2AICFLAGID = ping_flag_gm == 0 ? AIV2AICFLAGID_PING : AIV2AICFLAGID_PONG;
+                FftsCrossCoreSync<PIPE_MTE3, 2>(AIV2AICFLAGID);
+                // 等待cube计算
+                uint64_t AIC2AIVFLAGID = ping_flag_gm == 0 ? AIC2AIVFLAGID_PING : AIC2AIVFLAGID_PONG;
+                WaitFlagDev(AIC2AIVFLAGID);
+                uint64_t offset_c = expert_cumsum * n + m_idx * m0 * n + n_idx * n0;
+                // dequant
+                uint32_t n_actual = (n_idx == (n_loop - 1)) ? (n - n_idx * n0) : n0;
+                uint32_t n_round = RoundUp<CONST_16>(n_actual);
+                num_blocks_32 = n_round * sizeof(int32_t) / BLOCK_SIZE_32; // fp32
+                num_blocks_o = n_round * sizeof(OutDtype) / BLOCK_SIZE_32; // fp16
+                fp32_total_burst_num = (n_round + 63) / 64;                // float
+                for (int j = sub_block_id; j < m_actual; j += sub_block_dim) {
+                    uint32_t start = gmIndex_.GetValue(expert_cumsum + j + m_idx * m0);
+                    // 执行准备数据
+                    CopyGmToUb<int32_t>(ubBufI32[ping_flag_ub * n0],         // dst
+                                        gmWorkspaceDeq_[offset_c + j * n],   // src
+                                        0,                                   // sid
+                                        1,                                   // nBurst
+                                        num_blocks_32,                       // lenBurst
+                                        0,                                   // srcGap
+                                        0);                                  // dstGap
+                    CopyGmToUb<float>(ubBufScale[ping_flag_ub * n0],         // dst
+                                      gmScaleN_[batch_idx * n + n_idx * n0], // src
+                                      0,                                     // sid
+                                      1,                                     // nBurst
+                                      num_blocks_32,                         // lenBurst
+                                      0,                                     // srcGap
+                                      0);                                    // dstGap
+                    SET_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE2, V, EVENT_ID0 + ping_flag_ub);
+                    conv_v<ArchType::ASCEND_V220, int32_t, float>(ubBufF32[ping_flag_ub * n0], // dst
+                                                                  ubBufI32[ping_flag_ub * n0], // src
+                                                                  fp32_total_burst_num, 1, 1, 8, 8);
+                    PIPE_BARRIER(V);
+                    mul_v<ArchType::ASCEND_V220, float>(ubBufF32[ping_flag_ub * n0],   // dst
+                                                        ubBufF32[ping_flag_ub * n0],   // src0
+                                                        ubBufScale[ping_flag_ub * n0], // src1
+                                                        fp32_total_burst_num, 1, 1, 1, 8, 8, 8);
+                    PIPE_BARRIER(V);
+                    float scale_tokens = gmScaleM_.GetValue(start);
+                    muls_v<ArchType::ASCEND_V220, float>(ubBufF32[ping_flag_ub * n0], // dst
+                                                         ubBufF32[ping_flag_ub * n0], // src0
+                                                         scale_tokens,                // src1
+                                                         fp32_total_burst_num, 1, 1, 8, 8);
+                    PIPE_BARRIER(V);
+                    conv_v<ArchType::ASCEND_V220, float, OutDtype>(ubBufOut[ping_flag_ub * n0], // dst
+                                                                   ubBufF32[ping_flag_ub * n0], // src
+                                                                   fp32_total_burst_num, 1, 1, sizeof(OutDtype) * 2, 8);
+                    SET_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(V, MTE3, EVENT_ID0 + ping_flag_ub);
+                    CopyUbToGm<OutDtype>(gmC_[offset_c + j * n],      // dst
+                                         ubBufOut[ping_flag_ub * n0], // src
+                                         0,                           // sid
+                                         1,                           // nBurst
+                                         num_blocks_o,                // lenBurst
+                                         0,                           // srcGap
+                                         0);                          // dstGap
+                    SET_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    WAIT_FLAG(MTE3, MTE2, EVENT_ID0 + ping_flag_ub);
+                    ping_flag_ub = 1 - ping_flag_ub;
+                }
+                // dequantF
+                loop_count += 1;
+                // 切换GM上的double buffer
+                ping_flag_gm = 1 - ping_flag_gm;
+            }
+            core_loop += mm_loop_batch * n_loop;
+            expert_cumsum = gmFlag_.GetValue(batch_idx);
+        }
+        PIPE_BARRIER(ALL);
+    }
+
+private:
+    AscendC::GlobalTensor<InDtype> gmA_;
+    AscendC::GlobalTensor<InDtype> gmB_;
+    AscendC::GlobalTensor<float> gmScaleN_;
+    AscendC::GlobalTensor<float> gmScaleM_;
+    AscendC::GlobalTensor<int32_t> gmFlag_;
+    AscendC::GlobalTensor<int32_t> gmWorkspaceDeq_;
+    AscendC::GlobalTensor<OutDtype> gmC_;
+    AscendC::GlobalTensor<int32_t> gmIndex_;
+    AscendC::GlobalTensor<InDtype> gmWorkspace_;
+    AscendC::GlobalTensor<int32_t> gmWorkspaceGmm_;
+    AscendC::GlobalTensor<float> gmWorkspaceHp_;
+
+    uint32_t num_core{0};
+    uint32_t batch_size{0};
+    uint32_t m{0};
+    uint32_t k{0};
+    uint32_t n{0};
+    uint32_t m0{0};
+    uint32_t k0{0};
+    uint32_t n0{0};
+    uint32_t m_loop{0};
+    uint32_t n_loop{0};
+    uint32_t k_loop{0};
+    uint32_t core_loop{0};
+    uint32_t core_idx{0};
+    uint32_t swizzle_cnt{1};
+    uint32_t ping_flag{0};
+    uint32_t en_shuffle_k{0};
+    uint32_t allM{0};
+    uint32_t moeUp{0};
+};
+#endif
+
+extern "C" __global__ __aicore__ void moe_gmm_w8a8(__gm__ uint8_t *__restrict__ sync,
+                                                   __gm__ uint8_t *__restrict__ gm_a,
+                                                   __gm__ uint8_t *__restrict__ gm_b,
+                                                   __gm__ uint8_t *__restrict__ gm_flag,
+                                                   __gm__ uint8_t *__restrict__ gm_index,
+                                                   __gm__ uint8_t *__restrict__ gm_nscale,
+                                                   __gm__ uint8_t *__restrict__ gm_mscale,
+                                                   __gm__ uint8_t *__restrict__ gm_c,
+                                                   __gm__ uint8_t *__restrict__ workspace,
+                                                   __gm__ uint8_t *__restrict__ workspace_hp,
+                                                   __gm__ uint8_t *__restrict__ tiling_data)
+{
+    set_ffts_base_addr((uint64_t)sync);
+#ifdef __DAV_C220_VEC__
+    MoeGmmW8a8Aiv<0, false, false, int8_t, half> kernel_aiv_0000; // swizzleDir[0] transB[0] half[0] ND[0]
+    MoeGmmW8a8Aiv<0, false, true, int8_t, half> kernel_aiv_0100;  // swizzleDir[0] transB[1] half[0] ND[0]
+    MoeGmmW8a8Aiv<1, false, false, int8_t, half> kernel_aiv_1000; // swizzleDir[1] transB[0] half[0] ND[0]
+    MoeGmmW8a8Aiv<1, false, true, int8_t, half> kernel_aiv_1100;  // swizzleDir[1] transB[1] half[0] ND[0]
+
+    MoeGmmW8a8Aiv<0, false, false, int8_t, half> kernel_aiv_0001; // swizzleDir[0] transB[0] half[0] NZ[1]
+    MoeGmmW8a8Aiv<0, false, true, int8_t, half> kernel_aiv_0101;  // swizzleDir[0] transB[1] half[0] NZ[1]
+    MoeGmmW8a8Aiv<1, false, false, int8_t, half> kernel_aiv_1001; // swizzleDir[1] transB[0] half[0] NZ[1]
+    MoeGmmW8a8Aiv<1, false, true, int8_t, half> kernel_aiv_1101;  // swizzleDir[1] transB[1] half[0] NZ[1]
+
+    MoeGmmW8a8Aiv<0, false, false, int8_t, __bf16> kernel_aiv_0010; // swizzleDir[0] transB[0] bf16[1] ND[0]
+    MoeGmmW8a8Aiv<0, false, true, int8_t, __bf16> kernel_aiv_0110;  // swizzleDir[0] transB[1] bf16[1] ND[0]
+    MoeGmmW8a8Aiv<1, false, false, int8_t, __bf16> kernel_aiv_1010; // swizzleDir[1] transB[0] bf16[1] ND[0]
+    MoeGmmW8a8Aiv<1, false, true, int8_t, __bf16> kernel_aiv_1110;  // swizzleDir[1] transB[1] bf16[1] ND[0]
+
+    MoeGmmW8a8Aiv<0, false, false, int8_t, __bf16> kernel_aiv_0011; // swizzleDir[0] transB[0] bf16[1] NZ[1]
+    MoeGmmW8a8Aiv<0, false, true, int8_t, __bf16> kernel_aiv_0111;  // swizzleDir[0] transB[1] bf16[1] NZ[1]
+    MoeGmmW8a8Aiv<1, false, false, int8_t, __bf16> kernel_aiv_1011; // swizzleDir[1] transB[0] bf16[1] NZ[1]
+    MoeGmmW8a8Aiv<1, false, true, int8_t, __bf16> kernel_aiv_1111;  // swizzleDir[1] transB[1] bf16[1] NZ[1]
+
+    AscendC::SetMaskNorm();
+    SetVectorMask<uint8_t>((uint64_t)-1, (uint64_t)-1);
+#endif
+#if __DAV_C220_CUBE__
+    MoeGmmW8a8Aic<0, false, false, int8_t, half, DataFormat::ND> kernel_aic_0000; // swizzleDir[0] transB[0] half[0] ND[0]
+    MoeGmmW8a8Aic<0, false, true, int8_t, half, DataFormat::ND> kernel_aic_0100;  // swizzleDir[0] transB[1] half[0] ND[0]
+    MoeGmmW8a8Aic<1, false, false, int8_t, half, DataFormat::ND> kernel_aic_1000; // swizzleDir[1] transB[0] half[0] ND[0]
+    MoeGmmW8a8Aic<1, false, true, int8_t, half, DataFormat::ND> kernel_aic_1100;  // swizzleDir[1] transB[1] half[0] ND[0]
+
+    MoeGmmW8a8Aic<0, false, false, int8_t, half, DataFormat::NZ> kernel_aic_0001; // swizzleDir[0] transB[0] half[0] NZ[1]
+    MoeGmmW8a8Aic<0, false, true, int8_t, half, DataFormat::NZ> kernel_aic_0101;  // swizzleDir[0] transB[1] half[0] NZ[1]
+    MoeGmmW8a8Aic<1, false, false, int8_t, half, DataFormat::NZ> kernel_aic_1001; // swizzleDir[1] transB[0] half[0] NZ[1]
+    MoeGmmW8a8Aic<1, false, true, int8_t, half, DataFormat::NZ> kernel_aic_1101;  // swizzleDir[1] transB[1] half[0] NZ[1]
+
+    MoeGmmW8a8Aic<0, false, false, int8_t, __bf16, DataFormat::ND> kernel_aic_0010; // swizzleDir[0] transB[0] bf16[1] ND[0]
+    MoeGmmW8a8Aic<0, false, true, int8_t, __bf16, DataFormat::ND> kernel_aic_0110;  // swizzleDir[0] transB[1] bf16[1] ND[0]
+    MoeGmmW8a8Aic<1, false, false, int8_t, __bf16, DataFormat::ND> kernel_aic_1010; // swizzleDir[1] transB[0] bf16[1] ND[0]
+    MoeGmmW8a8Aic<1, false, true, int8_t, __bf16, DataFormat::ND> kernel_aic_1110;  // swizzleDir[1] transB[1] bf16[1] ND[0]
+
+    MoeGmmW8a8Aic<0, false, false, int8_t, __bf16, DataFormat::NZ> kernel_aic_0011; // swizzleDir[0] transB[0] bf16[1] NZ[1]
+    MoeGmmW8a8Aic<0, false, true, int8_t, __bf16, DataFormat::NZ> kernel_aic_0111;  // swizzleDir[0] transB[1] bf16[1] NZ[1]
+    MoeGmmW8a8Aic<1, false, false, int8_t, __bf16, DataFormat::NZ> kernel_aic_1011; // swizzleDir[1] transB[0] bf16[1] NZ[1]
+    MoeGmmW8a8Aic<1, false, true, int8_t, __bf16, DataFormat::NZ> kernel_aic_1111;  // swizzleDir[1] transB[1] bf16[1] NZ[1]
+
+    SetPadding<uint64_t>((uint64_t)0);
+    SetNdpara(1, 0, 0);
+#endif
+    AscendC::SetAtomicNone();
+
+    // get tiling args
+    auto gm_tiling_data = reinterpret_cast<__gm__ AtbOps::MoeGmmTilingData *>(tiling_data);
+    uint32_t masked_key = gm_tiling_data->tilingKey & 0b101101;
+
+    switch (masked_key) {
+        case 0b000000: // swizzleDir[0] transA[0] transB[0]
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0000.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0000.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0000.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0000.run();
+#endif
+            break;
+        case 0b001000: // swizzleDir[0] transA[0] transB[1]
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0100.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0100.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0100.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0100.run();
+#endif
+            break;
+        case 0b100000: // swizzleDir[1] transA[0] transB[0]
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1000.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1000.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1000.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1000.run();
+#endif
+            break;
+        case 0b101000: // swizzleDir[1] transA[0] transB[1]
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1100.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1100.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1100.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1100.run();
+#endif
+            break;
+
+        case 0b000001: // swizzleDir[0] transA[0] transB[0] nz
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0001.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0001.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0001.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0001.run();
+#endif
+            break;
+        case 0b001001: // swizzleDir[0] transA[0] transB[1]  nz
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0101.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0101.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0101.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0101.run();
+#endif
+            break;
+        case 0b100001: // swizzleDir[1] transA[0] transB[0]  nz
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1001.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1001.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1001.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1001.run();
+#endif
+            break;
+        case 0b101001: // swizzleDir[1] transA[0] transB[1]  nz
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1101.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1101.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1101.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1101.run();
+#endif
+            break;
+        case 0b000100:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0010.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0010.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0010.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0010.run();
+#endif
+            break;
+        case 0b001100:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0110.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0110.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0110.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0110.run();
+#endif
+            break;
+        case 0b100100:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1010.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1010.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1010.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1010.run();
+#endif
+            break;
+        case 0b101100:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1110.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1110.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1110.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1110.run();
+#endif
+            break;
+
+        case 0b000101:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0011.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0011.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0011.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0011.run();
+#endif
+            break;
+        case 0b001101:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_0111.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_0111.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_0111.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_0111.run();
+#endif
+            break;
+        case 0b100101:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1011.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1011.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1011.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1011.run();
+#endif
+            break;
+        case 0b101101:
+#ifdef __DAV_C220_CUBE__
+            kernel_aic_1111.InitCube(sync, gm_a, gm_b, gm_flag, gm_index, gm_c, workspace, workspace_hp, tiling_data);
+            kernel_aic_1111.Process();
+#elif __DAV_C220_VEC__
+            kernel_aiv_1111.SetArgs(sync, gm_a, gm_b, gm_flag, gm_index, gm_nscale, gm_mscale, gm_c, workspace,
+                                    workspace_hp, tiling_data);
+            kernel_aiv_1111.run();
+#endif
+            break;
+        default: break;
+    }
+}
\ No newline at end of file
diff --git a/src/kernels/mixkernels/pad_with_hidden_state/CMakeLists.txt b/src/kernels/mixkernels/pad_with_hidden_state/CMakeLists.txt
index 378093ef..947439e5 100644
--- a/src/kernels/mixkernels/pad_with_hidden_state/CMakeLists.txt
+++ b/src/kernels/mixkernels/pad_with_hidden_state/CMakeLists.txt
@@ -12,3 +12,8 @@ set(pad_with_hidden_state_srcs
 )
 
 add_operation(PadWithHiddenStateOperation "${pad_with_hidden_state_srcs}")
+
+
+add_kernel(pad_with_hidden_state ascend910b vector
+        op_kernel/pad_with_hidden_state.cpp
+        PadWithHiddenStateKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/pad_with_hidden_state/op_kernel/pad_with_hidden_state.cpp b/src/kernels/mixkernels/pad_with_hidden_state/op_kernel/pad_with_hidden_state.cpp
new file mode 100644
index 00000000..7f934e6d
--- /dev/null
+++ b/src/kernels/mixkernels/pad_with_hidden_state/op_kernel/pad_with_hidden_state.cpp
@@ -0,0 +1,152 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#include "kernel_operator.h"
+#include "mixkernels/pad_with_hidden_state/tiling/tiling_data.h"
+
+using namespace AscendC;
+using namespace AtbOps;
+
+inline __aicore__ PadWithHiddenStateTilingData GetTilingData(const GM_ADDR tiling);
+
+class PadWithHiddenState {
+public:
+    __aicore__ inline PadWithHiddenState() {}
+
+    __aicore__ inline void Init(GM_ADDR dataInput, GM_ADDR dataOutput, const GM_ADDR tiling)
+    {
+        tilingData = GetTilingData(tiling);
+        unpadDataGm.SetGlobalBuffer((__gm__ half *)dataInput, tilingData.unpadDataLength);
+        padDataGm.SetGlobalBuffer((__gm__ half *)dataOutput, tilingData.totalDataLength);
+        pipe.InitBuffer(inputQueue, BUFFER_NUM, tilingData.bufferSize);
+        pipe.InitBuffer(outputQueue, BUFFER_NUM, tilingData.bufferSize);
+    }
+
+    __aicore__ inline void Process()
+    {
+        bool isLastCore = (GetBlockIdx() == tilingData.coreNum - 1);
+        for (uint32_t sampleIndex = 0; sampleIndex < tilingData.batchSize; ++sampleIndex) {
+            uint64_t inputOffset = static_cast<uint64_t>(tilingData.inputOffset[sampleIndex]);
+            uint64_t outputOffset = static_cast<uint64_t>(tilingData.outputOffset[sampleIndex]);
+
+            TileInfo &copyTileInfo = tilingData.copyTileInfo[sampleIndex];
+            uint64_t tileNum = GetTileNum(copyTileInfo);
+            uint64_t innerOffset = GetInnerOffset(copyTileInfo);
+            for (uint32_t tileIndex = 0; tileIndex < tileNum; ++tileIndex, innerOffset += tilingData.tileLength) {
+                CopyIn(inputOffset + innerOffset, tilingData.tileLength);
+                Compute();
+                CopyOut(outputOffset + innerOffset, tilingData.tileLength);
+            }
+            if (isLastCore && (copyTileInfo.lastTileLength > 0)) {
+                CopyIn(inputOffset + innerOffset, copyTileInfo.lastTileLength);
+                Compute();
+                CopyOut(outputOffset + innerOffset, copyTileInfo.lastTileLength);
+            }
+        }
+    }
+
+    __aicore__ inline void CopyIn(uint64_t offset, uint32_t length)
+    {
+        LocalTensor<half> dataInputUb = inputQueue.AllocTensor<half>();
+        if (length % ELEMENT_PER_BASIC_BLOCK == 0) {
+            DataCopy(dataInputUb, unpadDataGm[offset], length);
+        } else {
+            DataCopyPad(dataInputUb, unpadDataGm[offset], DataCopyParams(1, length * ELEMENT_SIZE, 0, 0),
+                DataCopyPadParams());
+        }
+        inputQueue.EnQue<half>(dataInputUb);
+    }
+
+    __aicore__ inline void Compute()
+    {
+        LocalTensor<half> dataInputUb = inputQueue.DeQue<half>();
+        LocalTensor<half> dataOutputUb = outputQueue.AllocTensor<half>();
+        DataCopy(dataOutputUb, dataInputUb, tilingData.tileLength);
+        inputQueue.FreeTensor(dataInputUb);
+        outputQueue.EnQue<half>(dataOutputUb);
+    }
+
+    __aicore__ inline void CopyOut(uint64_t offset, uint32_t length)
+    {
+        LocalTensor<half> dataOutputUb = outputQueue.DeQue<half>();
+        if (length % ELEMENT_PER_BASIC_BLOCK == 0) {
+            DataCopy(padDataGm[offset], dataOutputUb, length);
+        } else {
+            DataCopyPad(padDataGm[offset], dataOutputUb, DataCopyParams(1, length * ELEMENT_SIZE, 0, 0));
+        }
+        outputQueue.FreeTensor(dataOutputUb);
+    }
+
+private:
+
+    __aicore__ inline uint64_t GetTileNum(const TileInfo &tileInfo)
+    {
+        if (GetBlockIdx() < tileInfo.formerCoreNum) {
+            return tileInfo.formerCoreTileNum;
+        } else {
+            return tileInfo.formerCoreTileNum - 1;
+        }
+    }
+
+    __aicore__ inline uint64_t GetInnerOffset(const TileInfo &tileInfo)
+    {
+        if (GetBlockIdx() < tileInfo.formerCoreNum) {
+            return GetBlockIdx() * tileInfo.formerCoreTileNum * tilingData.tileLength;
+        } else {
+            return (tileInfo.formerCoreNum * tileInfo.formerCoreTileNum +
+                (GetBlockIdx() - tileInfo.formerCoreNum) * (tileInfo.formerCoreTileNum - 1)) * tilingData.tileLength;
+        }
+    }
+
+    TPipe pipe;
+    TQue<QuePosition::VECIN, BUFFER_NUM> inputQueue;
+    TQue<QuePosition::VECOUT, BUFFER_NUM> outputQueue;
+    GlobalTensor<half> unpadDataGm;
+    GlobalTensor<half> padDataGm;
+    PadWithHiddenStateTilingData tilingData;
+};
+
+inline __aicore__ PadWithHiddenStateTilingData GetTilingData(const GM_ADDR tiling)
+{
+    auto tilingDataPointer = reinterpret_cast<const __gm__ PadWithHiddenStateTilingData *>(tiling);
+    PadWithHiddenStateTilingData tilingData;
+    tilingData.coreNum = tilingDataPointer->coreNum;
+    tilingData.batchSize = tilingDataPointer->batchSize;
+    tilingData.maxSeqLen = tilingDataPointer->maxSeqLen;
+    tilingData.bufferSize = tilingDataPointer->bufferSize;
+    tilingData.tileLength = tilingDataPointer->tileLength;
+    tilingData.totalDataLength = tilingDataPointer->totalDataLength;
+    tilingData.unpadDataLength = tilingDataPointer->unpadDataLength;
+    for (uint32_t sampleIndex = 0; sampleIndex < tilingData.batchSize; ++sampleIndex) {
+        tilingData.inputOffset[sampleIndex] = tilingDataPointer->inputOffset[sampleIndex];
+        tilingData.outputOffset[sampleIndex] = tilingDataPointer->outputOffset[sampleIndex];
+        tilingData.paddingOffset[sampleIndex] = tilingDataPointer->paddingOffset[sampleIndex];
+        tilingData.copyTileInfo[sampleIndex].formerCoreNum =
+            tilingDataPointer->copyTileInfo[sampleIndex].formerCoreNum;
+        tilingData.copyTileInfo[sampleIndex].formerCoreTileNum =
+            tilingDataPointer->copyTileInfo[sampleIndex].formerCoreTileNum;
+        tilingData.copyTileInfo[sampleIndex].lastTileLength =
+            tilingDataPointer->copyTileInfo[sampleIndex].lastTileLength;
+        tilingData.paddingTileInfo[sampleIndex].formerCoreNum =
+            tilingDataPointer->paddingTileInfo[sampleIndex].formerCoreNum;
+        tilingData.paddingTileInfo[sampleIndex].formerCoreTileNum =
+            tilingDataPointer->paddingTileInfo[sampleIndex].formerCoreTileNum;
+        tilingData.paddingTileInfo[sampleIndex].lastTileLength =
+            tilingDataPointer->paddingTileInfo[sampleIndex].lastTileLength;
+    }
+    return tilingData;
+}
+
+extern "C" __global__ __aicore__ void pad_with_hidden_state(GM_ADDR dataInput, GM_ADDR dataOutput, GM_ADDR tiling)
+{
+    PadWithHiddenState op;
+    op.Init(dataInput, dataOutput, tiling);
+    op.Process();
+}
diff --git a/src/kernels/mixkernels/rope_grad/CMakeLists.txt b/src/kernels/mixkernels/rope_grad/CMakeLists.txt
index 4a5c3ba5..5d9276c3 100644
--- a/src/kernels/mixkernels/rope_grad/CMakeLists.txt
+++ b/src/kernels/mixkernels/rope_grad/CMakeLists.txt
@@ -12,3 +12,7 @@ set(rope_grad_srcs
 )
 
 add_operation(RopeGradOperation "${rope_grad_srcs}")
+
+add_kernel(rope_grad ascend910b vector
+        op_kernel/rope_grad.cpp
+        RopeGradKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/rope_grad/op_kernel/rope_grad.cpp b/src/kernels/mixkernels/rope_grad/op_kernel/rope_grad.cpp
new file mode 100644
index 00000000..3d136aa3
--- /dev/null
+++ b/src/kernels/mixkernels/rope_grad/op_kernel/rope_grad.cpp
@@ -0,0 +1,208 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+#include "kernel_operator.h"
+#include "mixkernels/rope_grad/tiling/tiling_data.h"
+using namespace AscendC;
+
+namespace {
+    constexpr uint32_t MAX_PROCESS_NUM = 192 * 1024 / sizeof(half) / 8; // max process num of one loop
+    constexpr int32_t BUFFER_NUM = 1;                                   // tensor num for each queue
+} // namespace
+
+class RopeGrad {
+public:
+    __aicore__ inline RopeGrad(int64_t maxSeqLen, int64_t hiddenSize, int64_t headSize, int64_t sumSeqLen,
+                                    int64_t batch)
+    {
+        this->maxSeqLen = maxSeqLen;
+        this->hiddenSize = hiddenSize;
+        this->headSize = headSize;
+        this->sumSeqLen = sumSeqLen;
+        this->batch = batch;
+    }
+
+    __aicore__ inline void Init(GM_ADDR qembedGrad, GM_ADDR kembedGrad, GM_ADDR cos, GM_ADDR sin,
+                                GM_ADDR qGrad, GM_ADDR kGrad)
+    {
+        // get start index for current core, core parallel
+        qembedgradGm.SetGlobalBuffer((__gm__ half*)qembedGrad + GetBlockIdx() * this->headSize,
+            this->sumSeqLen * this->headSize);
+        kembedgradGm.SetGlobalBuffer((__gm__ half*)kembedGrad + GetBlockIdx() * this->headSize,
+            this->sumSeqLen * this->headSize);
+        cosGm.SetGlobalBuffer((__gm__ half*)cos, this->maxSeqLen * this->headSize);
+        sinGm.SetGlobalBuffer((__gm__ half*)sin, this->maxSeqLen * this->headSize);
+        qgradGm.SetGlobalBuffer((__gm__ half*)qGrad + GetBlockIdx() * this->headSize,
+            this->sumSeqLen * this->headSize);
+        kgradGm.SetGlobalBuffer((__gm__ half*)kGrad + GetBlockIdx() * this->headSize,
+            this->sumSeqLen * this->headSize);
+        this->rowsPerLoop = MAX_PROCESS_NUM / this->headSize;
+        // pipe alloc memory to queue, the unit is Bytes
+        pipe.InitBuffer(qembedgradQueue, BUFFER_NUM, MAX_PROCESS_NUM * sizeof(half));
+        pipe.InitBuffer(kembedgradQueue, BUFFER_NUM, MAX_PROCESS_NUM * sizeof(half));
+        pipe.InitBuffer(cosQueue, BUFFER_NUM, MAX_PROCESS_NUM * sizeof(half));
+        pipe.InitBuffer(sinQueue, BUFFER_NUM, MAX_PROCESS_NUM * sizeof(half));
+        pipe.InitBuffer(qgradQueue, BUFFER_NUM, MAX_PROCESS_NUM * sizeof(half));
+        pipe.InitBuffer(kgradQueue, BUFFER_NUM, MAX_PROCESS_NUM * sizeof(half));
+        pipe.InitBuffer(workbuffer, MAX_PROCESS_NUM * sizeof(half));
+    } // 初始化函数，完成内存初始化相关操作
+
+    __aicore__ inline void Process(int32_t curnum, int32_t seqleni)
+    {
+        uint64_t rowsRepeat = seqleni / this->rowsPerLoop;
+        uint64_t rowsRemain = seqleni % this->rowsPerLoop;
+        for (uint64_t j = 0; j < rowsRepeat; j++) {
+            CopyIn(curnum, j, this->rowsPerLoop);
+            Compute(this->rowsPerLoop);
+            CopyOut(curnum, j, this->rowsPerLoop);
+        }
+        if (rowsRemain > 0) {
+            CopyIn(curnum, rowsRepeat, rowsRemain);
+            Compute(rowsRemain);
+            CopyOut(curnum, rowsRepeat, rowsRemain);
+        }
+        this->cursumseqlen += seqleni;
+        pipe_barrier(PIPE_ALL);
+    } // 核心处理函数，实现算子逻辑，调用私有成员函数CopyIn、Compute、CopyOut完成矢量算子的三级流水操作
+private:
+    __aicore__ inline void CopyIn(int32_t b, uint64_t progress, int64_t currentloopRows)
+    {
+        // alloc tensor from queue memory
+        LocalTensor<half> qembedgradLocal = qembedgradQueue.AllocTensor<half>();
+        LocalTensor<half> kembedgradLocal = kembedgradQueue.AllocTensor<half>();
+        LocalTensor<half> cosLocal = cosQueue.AllocTensor<half>();
+        LocalTensor<half> sinLocal = sinQueue.AllocTensor<half>();
+        // copy progress_th tile from global tensor to local tensor
+        DataCopy(qembedgradLocal, qembedgradGm[(cursumseqlen + progress * this->rowsPerLoop) * this->hiddenSize],
+            {static_cast<uint16_t>(currentloopRows), static_cast<uint16_t>(this->headSize / 16),
+             static_cast<uint16_t>((this->hiddenSize - this->headSize) / 16), 0});
+        DataCopy(kembedgradLocal, kembedgradGm[(cursumseqlen + progress * this->rowsPerLoop) * this->hiddenSize],
+            {static_cast<uint16_t>(currentloopRows), static_cast<uint16_t>(this->headSize / 16),
+             static_cast<uint16_t>((this->hiddenSize - this->headSize) / 16), 0});
+        DataCopy(cosLocal, cosGm[progress * this->rowsPerLoop * this->headSize], currentloopRows * this->headSize);
+        DataCopy(sinLocal, sinGm[progress * this->rowsPerLoop * this->headSize], currentloopRows * this->headSize);
+        // enque input tensors to VECIN queue
+        qembedgradQueue.EnQue(qembedgradLocal);
+        kembedgradQueue.EnQue(kembedgradLocal);
+        cosQueue.EnQue(cosLocal);
+        sinQueue.EnQue(sinLocal);
+    }
+    __aicore__ inline void Compute(uint64_t currentloopRows)
+    {
+        // deque input tensors from VECIN queue
+        LocalTensor<half> qembedgradLocal = qembedgradQueue.DeQue<half>();
+        LocalTensor<half> kembedgradLocal = kembedgradQueue.DeQue<half>();
+        LocalTensor<half> cosLocal = cosQueue.DeQue<half>();
+        LocalTensor<half> sinLocal = sinQueue.DeQue<half>();
+        LocalTensor<half> qgradLocal = qgradQueue.AllocTensor<half>();
+        LocalTensor<half> kgradLocal = kgradQueue.AllocTensor<half>();
+        LocalTensor<half> workLocal = workbuffer.Get<half>();
+        half scalars = -1;
+        uint64_t mask = this->headSize / 2;
+        DataCopy(workLocal, sinLocal[mask],
+            {static_cast<uint16_t>(currentloopRows), static_cast<uint16_t>(this->headSize / 32),
+             static_cast<uint16_t>(this->headSize / 32), static_cast<uint16_t>(this->headSize / 32)});
+        pipe_barrier(PIPE_V);
+        Muls(workLocal[mask], sinLocal, scalars, mask, currentloopRows, {1, 1, 8, 8});
+        pipe_barrier(PIPE_V);
+        Add(workLocal, cosLocal, workLocal, currentloopRows * this->headSize);
+        pipe_barrier(PIPE_V);
+        Mul(qgradLocal, qembedgradLocal, workLocal, currentloopRows * this->headSize);
+        pipe_barrier(PIPE_V);
+        Mul(kgradLocal, kembedgradLocal, workLocal, currentloopRows * this->headSize);
+        pipe_barrier(PIPE_V);
+        // enque the output tensor to VECOUT queue
+        qgradQueue.EnQue<half>(qgradLocal);
+        kgradQueue.EnQue<half>(kgradLocal);
+        // free input tensors for reuse
+        qembedgradQueue.FreeTensor(qembedgradLocal);
+        kembedgradQueue.FreeTensor(kembedgradLocal);
+        cosQueue.FreeTensor(cosLocal);
+        sinQueue.FreeTensor(sinLocal);
+    }
+    __aicore__ inline void CopyOut(int32_t b, int64_t progress, int64_t currentloopRows)
+    {
+        // deque output tensor from VECOUT queue
+        LocalTensor<half> qgradLocal = qgradQueue.DeQue<half>();
+        LocalTensor<half> kgradLocal = kgradQueue.DeQue<half>();
+        // copy progress_th tile from local tensor to global tensor
+        DataCopy(qgradGm[(cursumseqlen + progress * this->rowsPerLoop) * this->hiddenSize], qgradLocal,
+            {static_cast<uint16_t>(currentloopRows), static_cast<uint16_t>(this->headSize / 16), 0,
+             static_cast<uint16_t>((this->hiddenSize - this->headSize) / 16)});
+        DataCopy(kgradGm[(cursumseqlen + progress * this->rowsPerLoop) * this->hiddenSize], kgradLocal,
+            {static_cast<uint16_t>(currentloopRows), static_cast<uint16_t>(this->headSize / 16), 0,
+             static_cast<uint16_t>((this->hiddenSize - this->headSize) / 16)});
+        // free output tensor for reuse
+        qgradQueue.FreeTensor(qgradLocal);
+        kgradQueue.FreeTensor(kgradLocal);
+    }
+private:
+    int64_t maxSeqLen = 0;
+    int64_t hiddenSize = 0;
+    int64_t headSize = 0;
+    int64_t sumSeqLen = 0;
+    int64_t batch = 0;
+    int64_t rowsPerLoop = 0;
+    uint64_t cursumseqlen = 0;
+
+    TPipe pipe;
+    // create queues for input, in this case depth is equal to buffer num
+    TQue<QuePosition::VECIN, 1> qembedgradQueue, kembedgradQueue, cosQueue, sinQueue;
+    // create queue for output, in this case depth is equal to buffer num
+    TQue<QuePosition::VECOUT, 1> qgradQueue, kgradQueue;
+    TBuf<> workbuffer;
+    GlobalTensor<half> qembedgradGm, kembedgradGm, cosGm, sinGm, qgradGm, kgradGm;
+};
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, AtbOps::RopeGradTilingData *tilingData)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingData->maxSeqLen = (*(const __gm__ int64_t *)(p_tilingdata + 0));
+    tilingData->hiddenSize = (*(const __gm__ int64_t *)(p_tilingdata + 8));
+    tilingData->headSize = (*(const __gm__ int64_t *)(p_tilingdata + 16));
+    tilingData->sumSeqLen = (*(const __gm__ int64_t *)(p_tilingdata + 24));
+    tilingData->batch = (*(const __gm__ int64_t *)(p_tilingdata + 32));
+    pipe_barrier(PIPE_ALL);
+#else
+    __ubuf__ uint8_t *tilingdata_in_ub = (__ubuf__ uint8_t *)get_imm(0);
+    copy_gm_to_ubuf(((__ubuf__ uint8_t *)tilingdata_in_ub), p_tilingdata, 0, 1, 42, 0, 0);
+    pipe_barrier(PIPE_ALL);
+    tilingData->maxSeqLen = (*(__ubuf__ int64_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 0));
+    tilingData->hiddenSize = (*(__ubuf__ int64_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 8));
+    tilingData->headSize = (*(__ubuf__ int64_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 16));
+    tilingData->sumSeqLen = (*(__ubuf__ int64_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 24));
+    tilingData->batch = (*(__ubuf__ int64_t *)((__ubuf__ uint8_t *)tilingdata_in_ub + 32));
+    pipe_barrier(PIPE_ALL);
+#endif
+}
+
+inline __aicore__ AtbOps::RopeGradSampleTilingData GetSampleTilingData(const __gm__ uint8_t *p_tilingdata)
+{
+    AtbOps::RopeGradSampleTilingData sampleTilingData;
+    sampleTilingData.qSeqLen = (*(const __gm__ int32_t *)p_tilingdata);
+    return sampleTilingData;
+}
+
+// implementation of kernel function
+extern "C" __global__ __aicore__ void rope_grad(GM_ADDR qembedGrad, GM_ADDR kembedGrad,
+                                                    GM_ADDR cos, GM_ADDR sin, GM_ADDR qGrad,
+                                                    GM_ADDR kGrad, GM_ADDR tiling)
+{
+    AtbOps::RopeGradTilingData tilingData;
+    InitTilingData(tiling, &tilingData);
+    RopeGrad op(tilingData.maxSeqLen, tilingData.hiddenSize, tilingData.headSize,
+                        tilingData.sumSeqLen, tilingData.batch);
+    op.Init(qembedGrad, kembedGrad, cos, sin, qGrad, kGrad);
+    tiling += sizeof(AtbOps::RopeGradTilingData);
+    for (uint32_t i = 0; i < tilingData.batch; i++) {
+        AtbOps::RopeGradSampleTilingData sampleTilingData = GetSampleTilingData(tiling);
+        op.Process(i, sampleTilingData.qSeqLen);
+        tiling += sizeof(AtbOps::RopeGradSampleTilingData);
+    }
+}
diff --git a/src/kernels/mixkernels/stridedbatchmatmul/CMakeLists.txt b/src/kernels/mixkernels/stridedbatchmatmul/CMakeLists.txt
index ce42d025..4c29e437 100644
--- a/src/kernels/mixkernels/stridedbatchmatmul/CMakeLists.txt
+++ b/src/kernels/mixkernels/stridedbatchmatmul/CMakeLists.txt
@@ -12,3 +12,7 @@ set(stridedbatchmatmul_srcs
 )
 
 add_operation(StridedBatchMatmulOperation "${stridedbatchmatmul_srcs}")
+
+add_kernel(stridedbatchmatmul ascend910b cube
+        op_kernel/stridedbatchmatmul.cpp
+        StridedBatchMatmulKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/stridedbatchmatmul/op_kernel/stridedbatchmatmul.cpp b/src/kernels/mixkernels/stridedbatchmatmul/op_kernel/stridedbatchmatmul.cpp
new file mode 100644
index 00000000..351e4d0a
--- /dev/null
+++ b/src/kernels/mixkernels/stridedbatchmatmul/op_kernel/stridedbatchmatmul.cpp
@@ -0,0 +1,123 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#include "kernel_operator.h"
+#include "lib/matrix/matmul/matmul.h"
+#include "mixkernels/stridedbatchmatmul/tiling/tiling_data.h"
+
+using namespace AscendC;
+using namespace matmul;
+
+__aicore__ inline void CopyTiling(TCubeTiling *tiling, GM_ADDR tilingGM, int32_t batch, int32_t batchIdx)
+{
+    int32_t *ptr = reinterpret_cast<int32_t *>(tiling);
+    auto tiling32 = reinterpret_cast<__gm__ int32_t *>(tilingGM + sizeof(AtbOps::StridedBatchMatmulTilingData) +
+        batch * sizeof(AtbOps::StridedBatchMatmulSampleTilingData));
+
+    for (int i = 0; i < sizeof(TCubeTiling) / sizeof(int32_t); i++, ptr++) {
+        *ptr = *(tiling32 + sizeof(TCubeTiling) / sizeof(int32_t) * batchIdx + i);
+    }
+
+    return;
+}
+
+extern "C" __global__ __aicore__ void stridedbatchmatmul(GM_ADDR a, GM_ADDR b, GM_ADDR c, GM_ADDR tilingGm)
+{
+    int32_t transA =  (*(const __gm__ int32_t *)(tilingGm));
+    int32_t transB = (*(const __gm__ int32_t *)(tilingGm + sizeof(int32_t)));
+    int32_t batch =  (*(const __gm__ int32_t *)(tilingGm + 2 * sizeof(int32_t)));
+    int32_t headNum = (*(const __gm__ int32_t *)(tilingGm + 3 * sizeof(int32_t)));
+    int32_t blockNum = (*(const __gm__ int32_t *)(tilingGm + 4 * sizeof(int32_t)));
+
+    using A_T = half;
+    using B_T = half;
+    using C_T = half;
+    using BiasT = half;
+    GlobalTensor<A_T> aGlobal;
+    GlobalTensor<B_T> bGlobal;
+    GlobalTensor<C_T> cGlobal;
+
+    // cube core cases, ignore vector core
+    if (g_coreType == AIV) {
+        return;
+    }
+
+    int numsPerCore = batch * headNum / blockNum;
+    int tailNum = batch * headNum % blockNum;
+    int currentNums;
+    int gmNumsIdx;
+    if (GetBlockIdx() < tailNum) {
+        currentNums = numsPerCore + 1;
+        gmNumsIdx = GetBlockIdx() * currentNums;
+    } else {
+        currentNums = numsPerCore;
+        gmNumsIdx = tailNum + GetBlockIdx() * numsPerCore;
+    }
+
+    int batchIdx = gmNumsIdx / headNum;
+
+    TCubeTiling tiling;
+    CopyTiling(&tiling, tilingGm, batch, batchIdx);
+    int32_t tilingOffset = sizeof(AtbOps::StridedBatchMatmulTilingData) +
+        batchIdx * sizeof(AtbOps::StridedBatchMatmulSampleTilingData);
+    int32_t batchOffsetA = (*(const __gm__ int32_t *)(tilingGm + tilingOffset));
+    int32_t batchOffsetB = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + sizeof(int32_t)));
+    int32_t batchOffsetC = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 2 * sizeof(int32_t)));
+    int32_t strideA = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 3 * sizeof(int32_t)));
+    int32_t strideB = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 4 * sizeof(int32_t)));
+    int32_t strideC = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 5 * sizeof(int32_t)));
+
+    TPipe que;
+    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ A_T *>(a), tiling.M * tiling.Ka);
+    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ B_T *>(b), tiling.Kb * tiling.N);
+    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ C_T *>(c), tiling.M * tiling.N);
+    auto gmA = aGlobal;
+    auto gmB = bGlobal;
+    auto gmC = cGlobal;
+
+    using aType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, A_T, true>;
+    using bType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, B_T, true>;
+    using cType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, C_T, true>;
+    using biasType = MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BiasT>;
+    MatmulImpl<aType, bType, cType, biasType> mm;
+    mm.SetSubBlockIdx(0);
+
+    mm.Init(&tiling, &que);
+
+    for (int j = 0; j < currentNums; j++) {
+        int gmIdx = gmNumsIdx % numsPerCore;
+        if (gmNumsIdx / headNum > batchIdx) {
+            pipe_barrier(PIPE_ALL);
+            batchIdx = gmNumsIdx / headNum;
+            CopyTiling(&tiling, tilingGm, batch, batchIdx);
+            tilingOffset = sizeof(AtbOps::StridedBatchMatmulTilingData) +
+                batchIdx * sizeof(AtbOps::StridedBatchMatmulSampleTilingData);
+            mm.Init(&tiling, &que);
+            batchOffsetA = (*(const __gm__ int32_t *)(tilingGm + tilingOffset));
+            batchOffsetB = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + sizeof(int32_t)));
+            batchOffsetC = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 2 * sizeof(int32_t)));
+            strideA = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 3 * sizeof(int32_t)));
+            strideB = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 4 * sizeof(int32_t)));
+            strideC = (*(const __gm__ int32_t *)(tilingGm + tilingOffset + 5 * sizeof(int32_t)));
+        }
+        int offsetA = batchOffsetA + strideA * (gmNumsIdx - batchIdx * headNum);
+        int offsetB = batchOffsetB + strideB * (gmNumsIdx - batchIdx * headNum);
+        int offsetC = batchOffsetC + strideC * (gmNumsIdx - batchIdx * headNum);
+        gmA = aGlobal[offsetA];
+        gmB = bGlobal[offsetB];
+        gmC = cGlobal[offsetC];
+        mm.SetTensorA(gmA, bool(transA));
+        mm.SetTensorB(gmB, bool(transB));
+        mm.IterateAll(gmC);
+        mm.End();
+        gmNumsIdx += 1;
+    }
+}
+
diff --git a/src/kernels/mixkernels/toppsample_rand/CMakeLists.txt b/src/kernels/mixkernels/toppsample_rand/CMakeLists.txt
index 9593b661..50edd9ea 100644
--- a/src/kernels/mixkernels/toppsample_rand/CMakeLists.txt
+++ b/src/kernels/mixkernels/toppsample_rand/CMakeLists.txt
@@ -12,3 +12,11 @@ ${CMAKE_CURRENT_LIST_DIR}/toppsample_rand_operation.cpp
 )
 
 add_operation(ToppsampleRandOperation "${toppsample_rand_srcs}")
+
+add_kernel(toppsample_rand ascend910b vector
+        op_kernel/toppsample_rand.cpp
+        ToppsampleRandKernel)
+
+add_kernel(toppsample_rand ascend310p vector
+        op_kernel/toppsample_rand.cpp
+        ToppsampleRandKernel)
\ No newline at end of file
diff --git a/src/kernels/mixkernels/toppsample_rand/op_kernel/toppsample_rand.cpp b/src/kernels/mixkernels/toppsample_rand/op_kernel/toppsample_rand.cpp
new file mode 100644
index 00000000..1e5deb1c
--- /dev/null
+++ b/src/kernels/mixkernels/toppsample_rand/op_kernel/toppsample_rand.cpp
@@ -0,0 +1,422 @@
+/*
+ * Copyright (c) 2025 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+
+#include "kernel_operator.h"
+#include "mixkernels/toppsample_rand/tiling/tiling_data.h"
+#include "mixkernels/utils/common/kernel/kernel_utils.h"
+namespace {
+static constexpr int32_t BUFFER_NUM = 1;
+static constexpr uint32_t BYTE_2 = 2;
+static constexpr uint32_t BYTE_4 = 4;
+static constexpr uint32_t MAX_CORE_NUM = 512;
+static constexpr uint32_t BLK_SIZE = 32;
+static constexpr uint32_t DEFAULT_STRIDE = 8;
+static constexpr uint32_t FP32_PER_REPEAT = 64;
+static constexpr uint32_t FP16_PER_REPEAT = 128;
+static constexpr uint32_t FP16_PER_BLOCK = 16;
+static constexpr uint32_t FP32_PER_BLOCK = 8;
+static constexpr uint32_t NUM_4 = 4;
+
+using AscendC::HardEvent;
+
+template <bool MULTI, typename T> class KernelToppsampleRand {
+public:
+    __aicore__ inline KernelToppsampleRand() {}
+    __aicore__ inline void Init(GM_ADDR cumsumed_probs, GM_ADDR topp, GM_ADDR rand, GM_ADDR select_index,
+                                GM_ADDR select_range, AtbOps::ToppsampleRandTilingData &tiling_data)
+    {
+        realLastDim_ = tiling_data.realLastDim;
+        firstDim_ = tiling_data.firstDim;
+        expandLastDim_ = tiling_data.expandLastDim;
+        perCoreRunNum_ = tiling_data.perCoreRunNum;
+        nlElePerCorePerRun_ = tiling_data.nlElePerCorePerRun;
+        lElePerCoreLastRun_ = tiling_data.lElePerCoreLastRun;
+        tempUbEleAligened_ = tiling_data.tempUbEleAligened;
+        realCore_ = AscendC::GetBlockNum();
+        blockIdx_ = AscendC::GetBlockIdx();
+        nlCoreRun_ = (firstDim_ + realCore_ - 1) / realCore_;
+        lCoreRun_ = firstDim_ - (realCore_ - 1) * nlCoreRun_;
+        dynamicRound_ = (blockIdx_ == realCore_ - 1) ? lCoreRun_ : nlCoreRun_;
+        batchAligenedFp16_ = (firstDim_ + FP16_PER_BLOCK -1) / FP16_PER_BLOCK * FP16_PER_BLOCK;
+        batchAligenedFp32_ = (firstDim_ + FP32_PER_BLOCK -1) / FP32_PER_BLOCK * FP32_PER_BLOCK;
+
+        xGm_.SetGlobalBuffer((__gm__ T *)cumsumed_probs);
+        yGm_.SetGlobalBuffer((__gm__ T *)topp); // batch,num_samples
+        randGm_.SetGlobalBuffer((__gm__ float *)rand);
+        zGm_.SetGlobalBuffer((__gm__ int32_t *)select_index);
+        selectRangeGm_.SetGlobalBuffer((__gm__ int32_t *)select_range);
+
+        pipe_.InitBuffer(yBuf_, batchAligenedFp16_ * BYTE_2);                    // topp
+        pipe_.InitBuffer(yF32Buf_, batchAligenedFp16_ * BYTE_4);                 // toppfp32
+        pipe_.InitBuffer(int8Buf_, tempUbEleAligened_ / DEFAULT_STRIDE);   // compare
+        pipe_.InitBuffer(blockBuf_, BLK_SIZE);                             // 存下标
+        pipe_.InitBuffer(int32Buf_, MAX_CORE_NUM * BYTE_4);             // 每个核做几个batch
+        pipe_.InitBuffer(vecIn_, 1, NUM_4 * MAX_CORE_NUM * sizeof(int32_t));   // 核间同步专享
+        pipe_.InitBuffer(selectRangeBlkBuf_, MAX_CORE_NUM * BYTE_4);
+        pipe_.InitBuffer(inputBuf_, tempUbEleAligened_ * BYTE_2);
+        pipe_.InitBuffer(tempBuf_, tempUbEleAligened_ * BYTE_4);
+        pipe_.InitBuffer(fp32Buf_, tempUbEleAligened_ * BYTE_4);
+        pipe_.InitBuffer(randBuf_, batchAligenedFp32_ * BYTE_4);
+    }
+
+    __aicore__ inline void FirstPick(uint32_t cid, uint32_t offset)
+    {
+        // 寻找第一个比截断数大的数字。removeVal选择的值一定比removeVal小， 因此找到第一个比截断大的值存下标，
+        // 从此记录，等待后续往前找。
+        AscendC::LocalTensor<T> buf = inputBuf_.Get<T>();
+        AscendC::LocalTensor<float> fp32Buf = fp32Buf_.Get<float>();
+        AscendC::LocalTensor<float> randBuf = randBuf_.Get<float>();
+        AscendC::SetFlag<HardEvent::S_MTE2>(EVENT_ID0);
+        AscendC::WaitFlag<HardEvent::S_MTE2>(EVENT_ID0);
+        DataCopy(randBuf, randGm_, batchAligenedFp32_);
+        AscendC::PipeBarrier<PIPE_MTE2>();
+        flag_ = 0;
+        AscendC::SetFlag<HardEvent::MTE2_S>(EVENT_ID0);
+        AscendC::WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
+        tempRandVal_ = removeVal_ * randBuf.GetValue(offset); // 得到截断数
+
+        for (int j = 0; j < perCoreRunNum_; j++) {
+            uint32_t RunNum = (j == perCoreRunNum_ - 1) ? lElePerCoreLastRun_ : nlElePerCorePerRun_;
+            uint64_t offsetXgm = (uint64_t)blockIdx_ * nlCoreRun_ * realLastDim_ + (uint64_t)cid * realLastDim_ +
+                                 (uint64_t)j * tempUbEleAligened_;
+            uint32_t RunNumF16Align = (RunNum + FP16_PER_BLOCK - 1) / FP16_PER_BLOCK * FP16_PER_BLOCK;
+            AscendC::PipeBarrier<PIPE_MTE2>();
+            DataCopy(buf, xGm_[offsetXgm], RunNumF16Align); // 拷贝单核每次的一块数。
+            AscendC::PipeBarrier<PIPE_MTE2>();
+            AscendC::SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+
+            AscendC::WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+            Cast(fp32Buf, buf, AscendC::RoundMode::CAST_NONE, RunNumF16Align);
+            AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
+
+            AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
+            // 此处多拷贝，底下仍然根据原长度做判断。
+            if (fp32Buf.GetValue(RunNum - 1) < tempRandVal_) {
+                continue;
+            } else if (flag_ == 0) {
+                flag_ = 1;      // 只需选择一次，增加标志位
+                idxReturn_ = j; // 保留这一块的idx
+            }
+            if (fp32Buf.GetValue(RunNum - 1) < removeVal_) {
+                continue;
+            } else {
+                idxForward_ = j; // 保留这一块的最后一个值的绝对idx
+                AscendC::SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                AscendC::WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+                break;
+            }
+        }
+    }
+
+    __aicore__ inline void PickUpTopp()
+    {
+        AscendC::LocalTensor<T> buf = yBuf_.Get<T>();
+        DataCopy(buf, yGm_, batchAligenedFp16_);
+    }
+
+    __aicore__ inline void Process(__gm__ uint8_t *sync)
+    {
+        AscendC::LocalTensor<T> buf = inputBuf_.Get<T>();
+        AscendC::LocalTensor<float> fp32Buf = fp32Buf_.Get<float>();
+        AscendC::LocalTensor<T> toppBuf_ = yBuf_.Get<T>();
+        AscendC::LocalTensor<float> toppBufF32_ = yF32Buf_.Get<float>();
+        AscendC::LocalTensor<float> randBuf = randBuf_.Get<float>();
+        AscendC::LocalTensor<int32_t> int32BlkBuf = int32Buf_.Get<int32_t>();
+
+        DataCopy(randBuf, randGm_, batchAligenedFp32_);
+        AscendC::PipeBarrier<PIPE_MTE2>();
+        PickUpTopp();
+        AscendC::SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        AscendC::WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+        // 最开始清空数据
+        Duplicate(int32BlkBuf, (int32_t)0, MAX_CORE_NUM);
+        AscendC::LocalTensor<uint32_t> uint32Buf_ = int8Buf_.Get<uint32_t>();
+        Duplicate(uint32Buf_, uint32_t(0), tempUbEleAligened_ / BLK_SIZE);
+        // 截断数可能是batch个，也可能是1个
+        // 每个batch往后取一个随机数。randBuf.GetValue(batchOffset)
+        Cast(toppBufF32_, toppBuf_, AscendC::RoundMode::CAST_NONE, firstDim_);
+        for (int cid = 0; cid < dynamicRound_; cid++) { // 每个核做多少次
+            absIdx_ = 0;
+            uint32_t batchOffset = (blockIdx_ * nlCoreRun_ + cid) % MAX_CORE_NUM;
+            if constexpr (MULTI) {
+                AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
+
+                AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
+                removeVal_ = toppBufF32_.GetValue((blockIdx_ * nlCoreRun_ + cid));
+            } else {
+                AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
+
+                AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
+                removeVal_ = toppBufF32_.GetValue(0);
+            }
+            FirstPick(cid, batchOffset);
+            uint32_t RunNum = (idxForward_ == perCoreRunNum_ - 1) ? lElePerCoreLastRun_ : nlElePerCorePerRun_;
+            Compute(RunNum, removeVal_, cid, 0);
+            Cast(fp32Buf, buf, AscendC::RoundMode::CAST_NONE, RunNum);
+            AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
+
+            AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
+            float finalVal = FigureOutValue(cid); // 寻找到最后一个比截断值肖的值;
+            float cutOff = finalVal * randBuf.GetValue(batchOffset);
+            // 拿到比removeVal 小的第一个数，开始往前做，取下标。
+            finalPick(cutOff, cid);
+        }
+        AscendC::SetFlag<HardEvent::S_MTE3>(EVENT_ID0);
+        AscendC::WaitFlag<HardEvent::S_MTE3>(EVENT_ID0);
+        CoreSyncOut(sync);
+    }
+
+private:
+    __aicore__ inline void CopyIn(uint32_t coreIdx, uint32_t loopIdx, uint32_t copyEleNum)
+    {
+        AscendC::LocalTensor<T> buf = inputBuf_.Get<T>();
+
+        uint32_t copyEleNumAligned_ = (copyEleNum + FP16_PER_BLOCK - 1) / FP16_PER_BLOCK * FP16_PER_BLOCK;
+        uint64_t xGmOffsetSec = (uint64_t)blockIdx_ * nlCoreRun_ * realLastDim_ + (uint64_t)coreIdx * realLastDim_ +
+                                (uint64_t)loopIdx * tempUbEleAligened_;
+        DataCopy(buf, xGm_[xGmOffsetSec], copyEleNumAligned_); // 拷贝单核每次的一块数。
+        AscendC::SetFlag<HardEvent::MTE2_S>(EVENT_ID0);
+        AscendC::WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
+    }
+
+    __aicore__ inline void CopyOut()
+    {
+        AscendC::LocalTensor<int32_t> int32BlkBuf = int32Buf_.Get<int32_t>();
+        AscendC::LocalTensor<int32_t> selectRangeBlkBuf = selectRangeBlkBuf_.Get<int32_t>();
+        uint32_t dynamicRoundAlign_ = DEFAULT_STRIDE * ((dynamicRound_ + DEFAULT_STRIDE - 1) / DEFAULT_STRIDE);
+        DataCopy(zGm_[static_cast<uint64_t>(blockIdx_) * nlCoreRun_], int32BlkBuf, dynamicRoundAlign_);
+        DataCopy(selectRangeGm_[static_cast<uint64_t>(blockIdx_) * nlCoreRun_], selectRangeBlkBuf,
+                 dynamicRoundAlign_);
+    }
+
+    __aicore__ inline void CoreSyncOut(__gm__ uint8_t *sync)
+    {
+        syncGm_.SetGlobalBuffer((__gm__ int32_t *)(sync), BLK_SIZE * MAX_CORE_NUM * firstDim_);
+        if (realCore_ != 1) {
+            if (blockIdx_ == realCore_ - 1) {
+                auto syncBuf = vecIn_.AllocTensor<int32_t>();
+                AscendC::IBWait(syncGm_, syncBuf, realCore_ - 2, 0);
+                CopyOut();
+                vecIn_.FreeTensor(syncBuf);
+            } else if (blockIdx_ == 0) {
+                auto syncBuf = vecIn_.AllocTensor<int32_t>();
+                CopyOut();
+                AscendC::IBSet(syncGm_, syncBuf, 0, 0);
+                vecIn_.FreeTensor(syncBuf);
+            } else {
+                auto syncBuf = vecIn_.AllocTensor<int32_t>();
+                AscendC::IBWait(syncGm_, syncBuf, blockIdx_ - 1, 0);
+                CopyOut();
+                AscendC::IBSet(syncGm_, syncBuf, blockIdx_, 0);
+                vecIn_.FreeTensor(syncBuf);
+            }
+        } else {
+            CopyOut();
+        }
+    }
+
+    // 首个找到的一块的最后一个数一定是> removeVal的,并且前一块一定是小于的
+    __aicore__ inline void Compute(uint32_t copyEleNum, float compareVal, uint32_t cid, uint32_t flag)
+    {
+        AscendC::LocalTensor<T> buf = inputBuf_.Get<T>();
+        AscendC::LocalTensor<half> tempBuf = tempBuf_.Get<half>();
+        AscendC::LocalTensor<uint8_t> uint8Buf = int8Buf_.Get<uint8_t>();
+        AscendC::LocalTensor<float> blkBuf = blockBuf_.Get<float>();
+        AscendC::LocalTensor<float> fp32Buf = fp32Buf_.Get<float>();
+        AscendC::LocalTensor<half> fp16Buf = fp32Buf_.Get<half>();
+        AscendC::LocalTensor<float> fp32TempBuf = tempBuf_.Get<float>();
+        uint32_t copyEleNumAlignF16_ = (copyEleNum + FP16_PER_REPEAT - 1) / FP16_PER_REPEAT * FP16_PER_REPEAT;
+        uint32_t copyEleNumAlignF32_ = (copyEleNum + FP32_PER_REPEAT - 1) / FP32_PER_REPEAT * FP32_PER_REPEAT;
+        for (uint32_t dupVal = copyEleNum; dupVal < copyEleNumAlignF16_; dupVal++) {
+            buf.SetValue(dupVal, T(1));
+        }
+        AscendC::SetFlag<HardEvent::S_V>(EVENT_ID0);
+        AscendC::WaitFlag<HardEvent::S_V>(EVENT_ID0);
+        Cast(fp32TempBuf, buf, AscendC::RoundMode::CAST_NONE, copyEleNumAlignF16_);
+        AscendC::PipeBarrier<PIPE_V>();
+        Duplicate(fp32Buf, compareVal, tempUbEleAligened_); // duplicate一个removeVal的tensor去比较 250
+        AscendC::PipeBarrier<PIPE_V>();
+        if (flag == 0) {
+            Compare(uint8Buf, fp32TempBuf, fp32Buf, AscendC::CMPMODE::LT, copyEleNumAlignF32_); // 找到小于removeVal的；
+        } else {
+            Compare(uint8Buf, fp32TempBuf, fp32Buf, AscendC::CMPMODE::LE,
+                    copyEleNumAlignF32_); // 找到小于等于removeVal的；
+        }
+        AscendC::PipeBarrier<PIPE_V>();
+
+        Duplicate(fp16Buf, (half)1, tempUbEleAligened_); // duplicate一个removeVal的tensor去比较
+        AscendC::PipeBarrier<PIPE_V>();
+
+        Select(tempBuf, uint8Buf, fp16Buf, (half)0, AscendC::SELMODE::VSEL_TENSOR_SCALAR_MODE,
+               copyEleNumAlignF16_); // 根据compare去选择1 还是 0；
+        AscendC::PipeBarrier<PIPE_V>();
+
+        Cast(fp32Buf, tempBuf, AscendC::RoundMode::CAST_NONE, copyEleNum);
+        AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
+
+        AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
+        for (uint32_t dupVal = copyEleNum; dupVal < copyEleNumAlignF32_; dupVal++) {
+            fp32Buf.SetValue(dupVal, static_cast<float>(0.0));
+        }
+        AscendC::SetFlag<HardEvent::S_V>(EVENT_ID0);
+
+        AscendC::WaitFlag<HardEvent::S_V>(EVENT_ID0);
+        ReduceSum(blkBuf, fp32Buf, fp32TempBuf, copyEleNumAlignF32_);
+        AscendC::PipeBarrier<PIPE_V>();
+    }
+
+    __aicore__ inline void finalPick(float cutOff, uint32_t cid)
+    {
+        AscendC::LocalTensor<float> blkBuf = blockBuf_.Get<float>();
+        AscendC::LocalTensor<int32_t> int32BlkBuf = int32Buf_.Get<int32_t>();
+        for (int j = idxReturn_; j >= 0; j--) {
+            uint32_t RunNum = (j == perCoreRunNum_ - 1) ? lElePerCoreLastRun_ : nlElePerCorePerRun_;
+            CopyIn(cid, j, RunNum);
+            AscendC::SetFlag<HardEvent::MTE2_V>(EVENT_ID0);
+
+            AscendC::WaitFlag<HardEvent::MTE2_V>(EVENT_ID0);
+            Compute(RunNum, cutOff, cid, 1);
+            AscendC::SetFlag<HardEvent::V_S>(EVENT_ID0);
+
+            AscendC::WaitFlag<HardEvent::V_S>(EVENT_ID0);
+            lastValTemp = blkBuf.GetValue(0);
+            if (lastValTemp > static_cast<float>(0.0)) {
+                absIdx_ = j;
+                break;
+            }
+        }
+        lastVal_ = (int32_t)lastValTemp + (int32_t)absIdx_ * (int32_t)tempUbEleAligened_; // 绝对位置
+        if (lastVal_ > 0) {
+            int32BlkBuf.SetValue(cid, lastVal_);
+        }
+    }
+
+    __aicore__ inline float FigureOutValue(uint32_t cid)
+    {
+        AscendC::LocalTensor<float> blkBuf = blockBuf_.Get<float>();
+        AscendC::LocalTensor<float> fp32Buf = fp32Buf_.Get<float>();
+        AscendC::LocalTensor<int32_t> selectRangeBlkBuf = selectRangeBlkBuf_.Get<int32_t>();
+        float sec2Last = blkBuf.GetValue(0);
+        auto relativeSelectRange = static_cast<int32_t>(sec2Last);
+        lastVal_ = relativeSelectRange - 1;
+        relativeSelectRange += idxForward_ * tempUbEleAligened_;
+        selectRangeBlkBuf.SetValue(cid, relativeSelectRange);
+        if (lastVal_ < 0) {
+            return fp32Buf.GetValue(0);
+        }
+        return fp32Buf.GetValue((uint32_t)lastVal_); // buf里面本就是相对idx 因此不需要修改。
+    }
+
+private:
+    AscendC::GlobalTensor<T> xGm_;
+    AscendC::GlobalTensor<T> yGm_;
+    AscendC::GlobalTensor<float> randGm_;
+    AscendC::GlobalTensor<int32_t> syncGm_;
+    AscendC::GlobalTensor<int32_t> zGm_;
+    AscendC::GlobalTensor<int32_t> selectRangeGm_;
+    AscendC::TPipe pipe_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> int32Buf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> selectRangeBlkBuf_;
+    AscendC::TQue<AscendC::QuePosition::VECIN, BUFFER_NUM> vecIn_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> inputBuf_, yF32Buf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> tempBuf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> fp32Buf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> int8Buf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> blockBuf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> yBuf_;
+    AscendC::TBuf<AscendC::TPosition::VECCALC> randBuf_;
+    int32_t *resOut_{nullptr};
+    float removeVal_{0};
+    float tempRandVal_{0};
+    uint32_t idxReturn_{0};
+    uint32_t idxForward_{0};
+    uint32_t loopReturn_{0};
+    uint32_t flag_{0};
+    int32_t lastVal_{0};
+    uint32_t absIdx_{0};
+    uint32_t nlCoreRun_{1};
+    uint32_t lCoreRun_{1};
+    uint32_t dynamicRound_{1};
+    uint32_t batchAligenedFp16_{0};
+    uint32_t batchAligenedFp32_{0};
+    uint32_t realCore_{1};
+    uint32_t blockIdx_{0};
+    uint32_t realLastDimAlignF32_{64};
+    uint32_t realLastDimAlignF16_{128};
+    float lastValTemp;
+    uint32_t realLastDim_{0};
+    uint32_t expandLastDim_{0};
+    uint32_t firstDim_{0};
+    float maxNum_{0};
+    float tempValue_{0};
+    uint32_t perCoreRunNum_{0};
+    uint32_t nlElePerCorePerRun_{0};
+    uint32_t lElePerCoreLastRun_{0};
+    uint32_t tempUbEleAligened_{19456};
+    uint32_t realLastDimTemp_{0};
+};
+} // namespace
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, AtbOps::ToppsampleRandTilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->realLastDim = (*(const __gm__ uint32_t *)(p_tilingdata + 0));
+    tilingdata->expandLastDim = (*(const __gm__ uint32_t *)(p_tilingdata + 4));
+    tilingdata->firstDim = (*(const __gm__ uint32_t *)(p_tilingdata + 8));
+    tilingdata->perCoreRunNum = (*(const __gm__ int32_t *)(p_tilingdata + 12));
+    tilingdata->nlElePerCorePerRun = (*(const __gm__ uint32_t *)(p_tilingdata + 16));
+    tilingdata->lElePerCoreLastRun = (*(const __gm__ uint32_t *)(p_tilingdata + 20));
+    tilingdata->tempUbEleAligened = (*(const __gm__ uint32_t *)(p_tilingdata + 24));
+#else
+    AscendC::TPipe pipe_;
+    __ubuf__ uint8_t *tilingdata_in_ub = nullptr;
+    CopyGmTilingToUb(tilingdata_in_ub, p_tilingdata, sizeof(AtbOps::ToppsampleRandTilingData), &pipe_);
+    AscendC::PipeBarrier<PIPE_ALL>();
+    tilingdata->realLastDim = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 0));
+    tilingdata->expandLastDim = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 4));
+    tilingdata->firstDim = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 8));
+    tilingdata->perCoreRunNum = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 12));
+    tilingdata->nlElePerCorePerRun = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 16));
+    tilingdata->lElePerCoreLastRun = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 20));
+    tilingdata->tempUbEleAligened = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 24));
+    AscendC::PipeBarrier<PIPE_ALL>();
+#endif
+}
+
+#define GET_TILING_DATA(tiling_data, tiling_arg)                                                                       \
+    AtbOps::ToppsampleRandTilingData tiling_data;                                                                      \
+    InitTilingData(tiling_arg, &tiling_data)
+
+extern "C" __global__ __aicore__ void toppsample_rand(GM_ADDR cumsumed_probs, GM_ADDR topp, GM_ADDR rand,
+                                                      GM_ADDR select_index, GM_ADDR select_range, GM_ADDR workspace,
+                                                      GM_ADDR tiling)
+{
+    GET_TILING_DATA(tiling_data, tiling);
+    if (TILING_KEY_IS(1)) {
+        KernelToppsampleRand<false, half> op;
+        op.Init(cumsumed_probs, topp, rand, select_index, select_range, tiling_data);
+        op.Process(workspace);
+    } else if (TILING_KEY_IS(0)) {
+        KernelToppsampleRand<true, half> op;
+        op.Init(cumsumed_probs, topp, rand, select_index, select_range, tiling_data);
+        op.Process(workspace);
+    }
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    if (TILING_KEY_IS(3)) {
+        KernelToppsampleRand<false, bfloat16_t> op;
+        op.Init(cumsumed_probs, topp, rand, select_index, select_range, tiling_data);
+        op.Process(workspace);
+    } else if (TILING_KEY_IS(2)) {
+        KernelToppsampleRand<true, bfloat16_t> op;
+        op.Init(cumsumed_probs, topp, rand, select_index, select_range, tiling_data);
+        op.Process(workspace);
+    }
+#endif
+}
\ No newline at end of file
diff --git a/src/kernels/mixkernels/unpad_with_hidden_state/CMakeLists.txt b/src/kernels/mixkernels/unpad_with_hidden_state/CMakeLists.txt
index 3187f1a0..581e0e33 100644
--- a/src/kernels/mixkernels/unpad_with_hidden_state/CMakeLists.txt
+++ b/src/kernels/mixkernels/unpad_with_hidden_state/CMakeLists.txt
@@ -12,3 +12,7 @@ set(unpad_with_hidden_state_srcs
 )
 
 add_operation(UnpadWithHiddenStateOperation "${unpad_with_hidden_state_srcs}")
+
+add_kernel(unpad_with_hidden_state ascend910b vector
+        op_kernel/unpad_with_hidden_state.cpp
+        UnpadWithHiddenStateKernel)
diff --git a/src/kernels/mixkernels/unpad_with_hidden_state/op_kernel/unpad_with_hidden_state.cpp b/src/kernels/mixkernels/unpad_with_hidden_state/op_kernel/unpad_with_hidden_state.cpp
new file mode 100644
index 00000000..bcc7f4b4
--- /dev/null
+++ b/src/kernels/mixkernels/unpad_with_hidden_state/op_kernel/unpad_with_hidden_state.cpp
@@ -0,0 +1,137 @@
+/*
+* Copyright (c) 2024 Huawei Technologies Co., Ltd.
+* This file is a part of the CANN Open Software.
+* Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+* Please refer to the License for details. You may not use this file except in compliance with the License.
+* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+* See LICENSE in the root of the software repository for the full text of the License.
+*/
+
+#include "kernel_operator.h"
+#include "mixkernels/unpad_with_hidden_state/tiling/tiling_data.h"
+
+using namespace AscendC;
+using namespace AtbOps;
+
+inline __aicore__ UnpadWithHiddenStateTilingData GetTilingData(const GM_ADDR tiling);
+
+class UnpadWithHiddenState {
+public:
+    __aicore__ inline UnpadWithHiddenState() {}
+
+    __aicore__ inline void Init(GM_ADDR dataInput, GM_ADDR dataOutput, const GM_ADDR tiling)
+    {
+        tilingData = GetTilingData(tiling);
+        padDataGm.SetGlobalBuffer((__gm__ half *)dataInput, tilingData.totalDataLength);
+        unpadDataGm.SetGlobalBuffer((__gm__ half *)dataOutput, tilingData.unpadDataLength);
+        pipe.InitBuffer(inputQueue, BUFFER_NUM, tilingData.bufferSize);
+        pipe.InitBuffer(outputQueue, BUFFER_NUM, tilingData.bufferSize);
+    }
+
+    __aicore__ inline void Process()
+    {
+        bool isLastCore = (GetBlockIdx() == tilingData.coreNum - 1);
+        for (uint32_t sampleIndex = 0; sampleIndex < tilingData.batchSize; ++sampleIndex) {
+            uint64_t inputOffset = static_cast<uint64_t>(tilingData.inputOffset[sampleIndex]);
+            uint64_t outputOffset = static_cast<uint64_t>(tilingData.outputOffset[sampleIndex]);
+
+            TileInfo &tileInfo = tilingData.tileInfo[sampleIndex];
+            uint32_t tileNum;
+            if (GetBlockIdx() < tileInfo.formerCoreNum) {
+                tileNum = tileInfo.formerCoreTileNum;
+            } else {
+                tileNum = tileInfo.formerCoreTileNum - 1;
+            }
+            uint64_t innerOffset;
+            if (GetBlockIdx() < tileInfo.formerCoreNum) {
+                innerOffset = static_cast<uint64_t>(GetBlockIdx()) * tileInfo.formerCoreTileNum *
+                    tilingData.tileLength;
+            } else {
+                innerOffset = static_cast<uint64_t>((tileInfo.formerCoreNum * tileInfo.formerCoreTileNum +
+                    (GetBlockIdx() - tileInfo.formerCoreNum) * (tileInfo.formerCoreTileNum - 1)) *
+                        tilingData.tileLength);
+            }
+
+            for (uint32_t tileIndex = 0; tileIndex < tileNum; ++tileIndex) {
+                CopyIn(inputOffset + innerOffset, tilingData.tileLength);
+                Compute();
+                CopyOut(outputOffset + innerOffset, tilingData.tileLength);
+                innerOffset += tilingData.tileLength;
+            }
+            if (isLastCore && (tileInfo.lastTileLength > 0)) {
+                CopyIn(inputOffset + innerOffset, tileInfo.lastTileLength);
+                Compute();
+                CopyOut(outputOffset + innerOffset, tileInfo.lastTileLength);
+            }
+        }
+    }
+
+    __aicore__ inline void CopyIn(uint64_t offset, uint32_t length)
+    {
+        LocalTensor<half> dataInputUb = inputQueue.AllocTensor<half>();
+        if (length % ELEMENT_PER_BASIC_BLOCK == 0) {
+            DataCopy(dataInputUb, padDataGm[offset], length);
+        } else {
+            DataCopyPad(dataInputUb, padDataGm[offset], DataCopyParams(1, length * ELEMENT_SIZE, 0, 0),
+                DataCopyPadParams());
+        }
+        inputQueue.EnQue<half>(dataInputUb);
+    }
+
+    __aicore__ inline void Compute()
+    {
+        LocalTensor<half> dataInputUb = inputQueue.DeQue<half>();
+        LocalTensor<half> dataOutputUb = outputQueue.AllocTensor<half>();
+        DataCopy(dataOutputUb, dataInputUb, tilingData.tileLength);
+        inputQueue.FreeTensor(dataInputUb);
+        outputQueue.EnQue<half>(dataOutputUb);
+    }
+
+    __aicore__ inline void CopyOut(uint64_t offset, uint32_t length)
+    {
+        LocalTensor<half> dataOutputUb = outputQueue.DeQue<half>();
+        if (length % ELEMENT_PER_BASIC_BLOCK == 0) {
+            DataCopy(unpadDataGm[offset], dataOutputUb, length);
+        } else {
+            DataCopyPad(unpadDataGm[offset], dataOutputUb, DataCopyParams(1, length * ELEMENT_SIZE, 0, 0));
+        }
+        outputQueue.FreeTensor(dataOutputUb);
+    }
+
+private:
+    TPipe pipe;
+    TQue<QuePosition::VECIN, BUFFER_NUM> inputQueue;
+    TQue<QuePosition::VECOUT, BUFFER_NUM> outputQueue;
+    GlobalTensor<half> padDataGm;
+    GlobalTensor<half> unpadDataGm;
+    UnpadWithHiddenStateTilingData tilingData;
+};
+
+inline __aicore__ UnpadWithHiddenStateTilingData GetTilingData(const GM_ADDR tiling)
+{
+    auto tilingDataPointer = reinterpret_cast<const __gm__ UnpadWithHiddenStateTilingData *>(tiling);
+    UnpadWithHiddenStateTilingData tilingData;
+    tilingData.coreNum = tilingDataPointer->coreNum;
+    tilingData.batchSize = tilingDataPointer->batchSize;
+    tilingData.maxSeqLen = tilingDataPointer->maxSeqLen;
+    tilingData.bufferSize = tilingDataPointer->bufferSize;
+    tilingData.tileLength = tilingDataPointer->tileLength;
+    tilingData.totalDataLength = tilingDataPointer->totalDataLength;
+    tilingData.unpadDataLength = tilingDataPointer->unpadDataLength;
+    for (uint32_t sampleIndex = 0; sampleIndex < tilingData.batchSize; ++sampleIndex) {
+        tilingData.inputOffset[sampleIndex] = tilingDataPointer->inputOffset[sampleIndex];
+        tilingData.outputOffset[sampleIndex] = tilingDataPointer->outputOffset[sampleIndex];
+        tilingData.tileInfo[sampleIndex].formerCoreNum = tilingDataPointer->tileInfo[sampleIndex].formerCoreNum;
+        tilingData.tileInfo[sampleIndex].formerCoreTileNum = tilingDataPointer->tileInfo[sampleIndex].formerCoreTileNum;
+        tilingData.tileInfo[sampleIndex].lastTileLength = tilingDataPointer->tileInfo[sampleIndex].lastTileLength;
+    }
+    return tilingData;
+}
+
+extern "C" __global__ __aicore__ void unpad_with_hidden_state(GM_ADDR dataInput, GM_ADDR dataOutput, GM_ADDR tiling)
+{
+    UnpadWithHiddenState op;
+    op.Init(dataInput, dataOutput, tiling);
+    op.Process();
+}
diff --git a/tmp/CMakeLists.txt b/tmp/CMakeLists.txt
new file mode 100644
index 00000000..d16964a8
--- /dev/null
+++ b/tmp/CMakeLists.txt
@@ -0,0 +1,63 @@
+set(activation_srcs
+    ${CMAKE_CURRENT_LIST_DIR}/activation_operation.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/fast_gelu/fast_gelu_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/gelu/gelu_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/log/log_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/relu/relu_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/swish/swish_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/swiglu_backward/swiglu_backward_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/swiglu_backward/tiling/swiglu_backward_tiling.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/swiglu_forward/swiglu_forward_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/swiglu_forward/tiling/swiglu_forward_tiling.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/faster_gelu_forward/faster_gelu_forward_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/faster_gelu_forward/tiling/faster_gelu_tiling.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/sigmoid/sigmoid_kernel.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/tiling/activation_tiling.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/gelu_forward/tiling/gelu_tiling.cpp
+    ${CMAKE_CURRENT_LIST_DIR}/gelu_forward/gelu_forward_kernel.cpp
+)
+
+add_operation(ActivationOperation "${activation_srcs}")
+
+add_kernel(swiglu_forward ascend910b vector
+    swiglu_forward/kernel/swiglu_forward.cpp
+    SwiGluForwardKernel)
+
+add_kernel(swiglu_forward ascend310p vector
+    swiglu_forward/kernel/swiglu_forward.cpp
+    SwiGluForwardKernel)
+
+add_kernel(swiglu_forward ascend910 vector
+    swiglu_forward/kernel/swiglu_forward.cpp
+    SwiGluForwardKernel)
+
+add_kernel(swiglu_backward ascend910b vector
+    swiglu_backward/kernel/swiglu_backward.cpp
+    SwiGluBackwardKernel)
+
+add_kernel(faster_gelu_forward ascend310p vector
+    faster_gelu_forward/kernel/faster_gelu_forward.cpp
+    FasterGeluForwardKernel)
+
+add_kernel(faster_gelu_forward ascend910 vector
+    faster_gelu_forward/kernel/faster_gelu_forward.cpp
+    FasterGeluForwardKernel)
+
+add_kernel(faster_gelu_forward ascend910b vector
+    faster_gelu_forward/kernel/faster_gelu_forward.cpp
+    FasterGeluForwardKernel)
+
+add_kernel(gelu_forward ascend310p vector
+        gelu_forward/kernel/gelu_forward.cpp
+        GeluForwardKernel
+        INCLUDE_DIRECTORIES ${ATB_OPS_SRC_DIR})
+
+add_kernel(gelu_forward ascend910b vector
+        gelu_forward/kernel/gelu_forward.cpp
+        GeluForwardKernel
+        INCLUDE_DIRECTORIES ${ATB_OPS_SRC_DIR})
+
+add_kernel(gelu_forward ascend910 vector
+        gelu_forward/kernel/gelu_forward.cpp
+        GeluForwardKernel
+        INCLUDE_DIRECTORIES ${ATB_OPS_SRC_DIR})
diff --git a/tmp/kernel/gelu_forward.cpp b/tmp/kernel/gelu_forward.cpp
new file mode 100644
index 00000000..2ac35cd2
--- /dev/null
+++ b/tmp/kernel/gelu_forward.cpp
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+
+#include "gelu_forward.h"
+
+#include "kernel_operator.h"
+#include "kernels/activation/gelu_forward/tiling/tiling_data.h"
+#include "kernels/utils/kernel/kernel_utils.h"
+
+using namespace AscendC;
+using namespace AsdOps;
+
+inline __aicore__ void InitTilingData(const __gm__ uint8_t *p_tilingdata, GeluForwardTilingData *tilingdata)
+{
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220)
+    tilingdata->blockLength = (*(const __gm__ uint32_t *)(p_tilingdata + 0));
+    tilingdata->tileNum = (*(const __gm__ uint32_t *)(p_tilingdata + 4));
+    tilingdata->tileLength = (*(const __gm__ uint32_t *)(p_tilingdata + 8));
+    tilingdata->tailLength = (*(const __gm__ uint32_t *)(p_tilingdata + 12));
+    tilingdata->bufferNum = (*(const __gm__ uint32_t *)(p_tilingdata + 16));
+#else
+    TPipe pipe;
+    __ubuf__ uint8_t *tilingdata_in_ub = nullptr;
+    CopyGmTilingToUb(tilingdata_in_ub, p_tilingdata, sizeof(AsdOps::GeluForwardTilingData), &pipe);
+    SetFlag<HardEvent::MTE2_S>(EVENT_ID0);
+    WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
+    tilingdata->blockLength = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 0));
+    tilingdata->tileNum = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 4));
+    tilingdata->tileLength = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 8));
+    tilingdata->tailLength = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 12));
+    tilingdata->bufferNum = (*(__ubuf__ uint32_t *)(tilingdata_in_ub + 16));
+    PipeBarrier<PIPE_ALL>();
+#endif
+}
+
+extern "C" __global__ __aicore__ void gelu_forward(GM_ADDR inputAddr, GM_ADDR outputAddr, GM_ADDR tiling)
+{
+    GeluForwardTilingData tilingData;
+    InitTilingData(tiling, &(tilingData));
+    if (TILING_KEY_IS(1)) { // 1代表fp16 type
+        GeluForward<half, half> op;
+        op.Init(inputAddr, outputAddr, tilingData);
+        op.Process();
+    } else if (TILING_KEY_IS(0)) { // 0代表float type
+        GeluForward<float, float> op;
+        op.Init(inputAddr, outputAddr, tilingData);
+        op.ProcessFP32();
+    }
+#if defined(__CCE_KT_TEST__) || (__CCE_AICORE__ == 220) // 220代表910B
+    if (TILING_KEY_IS(27)) { // 27代表bf16
+        GeluForward<bfloat16_t, bfloat16_t> op;
+        op.Init(inputAddr, outputAddr, tilingData);
+        op.Process();
+    }
+#endif
+}
diff --git a/tmp/kernel/gelu_forward.h b/tmp/kernel/gelu_forward.h
new file mode 100644
index 00000000..2a750e74
--- /dev/null
+++ b/tmp/kernel/gelu_forward.h
@@ -0,0 +1,135 @@
+/*
+ * Copyright (c) 2024 Huawei Technologies Co., Ltd.
+ * This file is a part of the CANN Open Software.
+ * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
+ * Please refer to the License for details. You may not use this file except in compliance with the License.
+ * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
+ * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
+ * See LICENSE in the root of the software repository for the full text of the License.
+ */
+
+#ifndef OPP_FASTER_GELU_FORWARD_H
+#define OPP_FASTER_GELU_FORWARD_H
+#include "kernel_operator.h"
+#include "kernels/activation/gelu_forward/tiling/tiling_data.h"
+#include "kernels/utils/kernel/kernel_utils.h"
+using namespace AscendC;
+
+template <typename inType, typename outType, bool highPerformance = true> class GeluForward {
+public:
+    __aicore__ inline GeluForward() {}
+    __aicore__ inline ~GeluForward() {}
+
+    __aicore__ inline void Init(GM_ADDR inputAddr, GM_ADDR outAddr, AsdOps::GeluForwardTilingData &tilingData)
+    {
+        this->blockLength = tilingData.blockLength;
+        this->tileNum = tilingData.tileNum;
+        this->tileLength = tilingData.tileLength;
+        this->tailLength = tilingData.tailLength;
+        this->bufferNum = tilingData.bufferNum;
+
+        inputGM.SetGlobalBuffer((__gm__ inType *)inputAddr + this->blockLength * GetBlockIdx(), this->blockLength);
+        outputGM.SetGlobalBuffer((__gm__ outType *)outAddr + this->blockLength * GetBlockIdx(), this->blockLength);
+
+        // 不能整除的情况下，可能存在浪费
+        pipe.InitBuffer(inQueueX, this->bufferNum, this->tileLength * sizeof(inType));
+        pipe.InitBuffer(outQueueZ, this->bufferNum, this->tileLength * sizeof(outType));
+    }
+
+    __aicore__ inline void Process()
+    {
+        uint64_t offset = 0;
+        pipe.InitBuffer(inFp32Buffer, this->tileLength * sizeof(float));
+        pipe.InitBuffer(outFp32Buffer, this->tileLength * sizeof(float));
+        LocalTensor<float> inLocal = inFp32Buffer.Get<float>();
+        LocalTensor<float> outLocal = outFp32Buffer.Get<float>();
+        for (int32_t i = 0; i < this->tileNum; i++) {
+            CopyInAndCastF32(inLocal, inputGM, inQueueX, offset, this->tileLength);
+            Compute(outLocal, inLocal, this->tileLength);
+            Cast16AndCopyOut(outLocal, outputGM, outQueueZ, offset, this->tileLength);
+            offset = offset + this->tileLength;
+        }
+        // 处理每个核上的尾块
+        if (this->tailLength != 0) {
+            CopyInAndCastF32(inLocal, inputGM, inQueueX, offset, this->tailLength);
+            Compute(outLocal, inLocal, this->tailLength);
+            Cast16AndCopyOut(outLocal, outputGM, outQueueZ, offset, this->tailLength);
+        }
+    }
+
+    __aicore__ inline void ProcessFP32()
+    {
+        uint64_t offset = 0;
+        for (uint32_t i = 0; i < this->tileNum; i++) {
+            CopyIn(inputGM, inQueueX, offset, this->tileLength);
+            LocalTensor<float> inLocal = inQueueX.DeQue<float>();
+            LocalTensor<float> outLocal = outQueueZ.AllocTensor<float>();
+            Compute(outLocal, inLocal, this->tileLength);
+            inQueueX.FreeTensor(inLocal);
+            outQueueZ.EnQue<float>(outLocal);
+            CopyOut(outputGM, outQueueZ, offset, this->tileLength);
+            offset = offset + this->tileLength;
+        }
+        // 处理每个核上的尾块
+        if (this->tailLength != 0) {
+            CopyIn(inputGM, inQueueX, offset, this->tailLength);
+            LocalTensor<float> inLocal = inQueueX.DeQue<float>();
+            LocalTensor<float> outLocal = outQueueZ.AllocTensor<float>();
+            Compute(outLocal, inLocal, this->tailLength);
+            inQueueX.FreeTensor(inLocal);
+            outQueueZ.EnQue<float>(outLocal);
+            CopyOut(outputGM, outQueueZ, offset, this->tailLength);
+        }
+    }
+
+    __aicore__ inline void Compute(LocalTensor<float> &zLocal, LocalTensor<float> &xLocal, uint64_t tileLength)
+    {
+        //  z = x ^ 3
+        Mul(zLocal, xLocal, xLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+        Mul(zLocal, zLocal, xLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = 0.044715 * z
+        Muls(zLocal, zLocal, attr1, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = x + z
+        Add(zLocal, xLocal, zLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = -1.59576912 * z
+        Muls(zLocal, zLocal, attr2, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = e ^ z
+        Exp(zLocal, zLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = 1 + z
+        Adds(zLocal, zLocal, attr3, tileLength);
+        PipeBarrier<PIPE_V>();
+
+        // z = x / z
+        Div(zLocal, xLocal, zLocal, tileLength);
+        PipeBarrier<PIPE_V>();
+    }
+
+private:
+    TBuf<AscendC::TPosition::VECCALC> inFp32Buffer;
+    TBuf<AscendC::TPosition::VECCALC> outFp32Buffer;
+    TQue<QuePosition::VECIN, AsdOps::GELU_FORWARD_BUFF_NUM> inQueueX;
+    TQue<QuePosition::VECOUT, AsdOps::GELU_FORWARD_BUFF_NUM> outQueueZ;
+    GlobalTensor<inType> inputGM;
+    GlobalTensor<outType> outputGM;
+    TPipe pipe;
+    uint32_t blockLength{0};
+    uint32_t tileNum{0};
+    uint32_t tileLength{0};
+    uint32_t tailLength{0};
+    uint32_t bufferNum{2};
+    const float attr1 = 0.044715;
+    const float attr2 = -1.59576912;
+    const float attr3 = 1.0;
+};
+#endif // OPP_FASTER_GELU_FORWARD_H
\ No newline at end of file
